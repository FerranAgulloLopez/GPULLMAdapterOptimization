INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 122.31881627300754,
    "estimated_duration": 3600.0204783764666,
    "input_throughput": 7867.86579968949,
    "output_throughput": 6937.694979797331,
    "total_throughput": 14805.560779486821,
    "itl": 105.03490251317828,
    "ttft": 1708380.782011719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8446946173254389,
    "arrivals": 381994,
    "finished_requests": 114021,
    "scheduler_time": 272.76917197741443
}
#Debug simulation 
Total elapsed time: 122.31902115605772. Arrivals time: 0.5334686147980392 Scheduler time: 121.55751379486173 Scheduler overhead time: 0.09051016019657254 Adapter cache time: 0.018344375304877758 Engine time: 0.08546683564782143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 122.48060461413115,
    "estimated_duration": 3600.075131904767,
    "input_throughput": 7867.746355897238,
    "output_throughput": 6937.589657131824,
    "total_throughput": 14805.336013029062,
    "itl": 105.03590328045658,
    "ttft": 1708404.8613267369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8983924433938272,
    "arrivals": 381994,
    "finished_requests": 114021,
    "scheduler_time": 272.76972752530105
}
#Debug simulation 
Total elapsed time: 122.48080660402775. Arrivals time: 0.5251415725797415 Scheduler time: 121.72543735895306 Scheduler overhead time: 0.0915514100342989 Adapter cache time: 0.01831925055012107 Engine time: 0.08649516105651855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 122.11177112814039,
    "estimated_duration": 3600.0780416021275,
    "input_throughput": 7867.739996934865,
    "output_throughput": 6937.584049951624,
    "total_throughput": 14805.32404688649,
    "itl": 105.0359483111171,
    "ttft": 1708406.342121199,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9003785037808165,
    "arrivals": 381994,
    "finished_requests": 114021,
    "scheduler_time": 272.7698508536277
}
#Debug simulation 
Total elapsed time: 122.11193803232163. Arrivals time: 0.5222792322747409 Scheduler time: 121.36102546937764 Scheduler overhead time: 0.09154986962676048 Adapter cache time: 0.01805905206128955 Engine time: 0.08563792100176215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 122.65226770192385,
    "estimated_duration": 3600.036036231734,
    "input_throughput": 7867.8317980528,
    "output_throughput": 6937.664997971233,
    "total_throughput": 14805.496796024034,
    "itl": 105.03518998562267,
    "ttft": 1708387.7113108507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8603930787788727,
    "arrivals": 381994,
    "finished_requests": 114021,
    "scheduler_time": 272.76913140977314
}
#Debug simulation 
Total elapsed time: 122.65244326274842. Arrivals time: 0.5311935949139297 Scheduler time: 121.89329942781478 Scheduler overhead time: 0.09098829235881567 Adapter cache time: 0.018020570743829012 Engine time: 0.08548883348703384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 123.09391810093075,
    "estimated_duration": 3600.089927044356,
    "input_throughput": 7867.714022147819,
    "output_throughput": 6937.561146008639,
    "total_throughput": 14805.275168156459,
    "itl": 105.03617233028957,
    "ttft": 1708411.5281036373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9120736059173987,
    "arrivals": 381994,
    "finished_requests": 114021,
    "scheduler_time": 272.7700411937226
}
#Debug simulation 
Total elapsed time: 123.09408880071715. Arrivals time: 0.5374577110633254 Scheduler time: 122.32495545269921 Scheduler overhead time: 0.0930787012912333 Adapter cache time: 0.018094961531460285 Engine time: 0.08659901050850749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 118.90114491432905,
    "estimated_duration": 3600.006111890399,
    "input_throughput": 7907.598797117454,
    "output_throughput": 7000.243670910532,
    "total_throughput": 14907.842468027986,
    "itl": 105.337786993644,
    "ttft": 1691717.734913056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8910349878808496,
    "arrivals": 381994,
    "finished_requests": 114580,
    "scheduler_time": 271.3629478133075
}
#Debug simulation 
Total elapsed time: 118.90130273625255. Arrivals time: 0.5282842707820237 Scheduler time: 118.14564333623275 Scheduler overhead time: 0.09000600455328822 Adapter cache time: 0.017989065032452345 Engine time: 0.08570902375504375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_192_slots_16_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 122.82963886018842,
    "estimated_duration": 3600.1011879114108,
    "input_throughput": 7867.689412483534,
    "output_throughput": 6937.539445798097,
    "total_throughput": 14805.22885828163,
    "itl": 105.03637891116107,
    "ttft": 1708416.5981338623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9228884315490768,
    "arrivals": 381994,
    "finished_requests": 114021,
    "scheduler_time": 272.7701871194082
}
#Debug simulation 
Total elapsed time: 122.82981514604762. Arrivals time: 0.5341352103278041 Scheduler time: 122.06832082429901 Scheduler overhead time: 0.09127006819471717 Adapter cache time: 0.017348263878375292 Engine time: 0.0853977520018816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 121.6937581920065,
    "estimated_duration": 3600.073092588972,
    "input_throughput": 7876.510079301734,
    "output_throughput": 7004.826388639987,
    "total_throughput": 14881.336467941721,
    "itl": 106.21017243881758,
    "ttft": 1707959.1275041397,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7926663256785822,
    "arrivals": 381208,
    "finished_requests": 114410,
    "scheduler_time": 269.4578090905097
}
#Debug simulation 
Total elapsed time: 121.69402534188703. Arrivals time: 0.5316651584580541 Scheduler time: 120.93563433131203 Scheduler overhead time: 0.09017973905429244 Adapter cache time: 0.017494899220764637 Engine time: 0.08500624587759376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 126.6104596098885,
    "estimated_duration": 3600.1259827035783,
    "input_throughput": 7811.685239659446,
    "output_throughput": 6953.233336907113,
    "total_throughput": 14764.918576566559,
    "itl": 105.74862524886639,
    "ttft": 1714358.7477354216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.840485584901184,
    "arrivals": 381208,
    "finished_requests": 113488,
    "scheduler_time": 272.0318689505535
}
#Debug simulation 
Total elapsed time: 126.61062841117382. Arrivals time: 0.5277731167152524 Scheduler time: 125.85448203794658 Scheduler overhead time: 0.0902463928796351 Adapter cache time: 0.01766357198357582 Engine time: 0.08713079569861293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 126.18363773915917,
    "estimated_duration": 3600.000990237168,
    "input_throughput": 7811.7425734783765,
    "output_throughput": 6953.1666985321945,
    "total_throughput": 14764.909272010571,
    "itl": 105.74845583180122,
    "ttft": 1714355.4106631225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8422226895578246,
    "arrivals": 381208,
    "finished_requests": 113484,
    "scheduler_time": 272.0229761443055
}
#Debug simulation 
Total elapsed time: 126.18380783405155. Arrivals time: 0.541187766008079 Scheduler time: 125.41035379096866 Scheduler overhead time: 0.09248995501548052 Adapter cache time: 0.018189715687185526 Engine time: 0.08722703345119953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 125.95871217502281,
    "estimated_duration": 3600.09310295342,
    "input_throughput": 7811.756583997398,
    "output_throughput": 6953.296840980028,
    "total_throughput": 14765.053424977425,
    "itl": 105.74809462610389,
    "ttft": 1714344.9895002493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8065721734706336,
    "arrivals": 381208,
    "finished_requests": 113488,
    "scheduler_time": 272.0312019559485
}
#Debug simulation 
Total elapsed time: 125.95888326503336. Arrivals time: 0.5366686517372727 Scheduler time: 125.1912639462389 Scheduler overhead time: 0.09137695329263806 Adapter cache time: 0.018391491379588842 Engine time: 0.08739721542224288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 126.5120409890078,
    "estimated_duration": 3600.012894854881,
    "input_throughput": 7811.716741401736,
    "output_throughput": 6953.143705616931,
    "total_throughput": 14764.860447018667,
    "itl": 105.74864044278445,
    "ttft": 1714360.936898646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8526602538302581,
    "arrivals": 381208,
    "finished_requests": 113484,
    "scheduler_time": 272.0232427347857
}
#Debug simulation 
Total elapsed time: 126.51221538381651. Arrivals time: 0.5438341428525746 Scheduler time: 125.73890726780519 Scheduler overhead time: 0.09137825714424253 Adapter cache time: 0.01822825614362955 Engine time: 0.08650119276717305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 121.91432175319642,
    "estimated_duration": 3600.0554010566425,
    "input_throughput": 7876.548786354039,
    "output_throughput": 7004.860812030383,
    "total_throughput": 14881.409598384422,
    "itl": 106.20982653666934,
    "ttft": 1707952.1829442596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7744230263796645,
    "arrivals": 381208,
    "finished_requests": 114410,
    "scheduler_time": 269.4575605487911
}
#Debug simulation 
Total elapsed time: 121.91448925901204. Arrivals time: 0.5351318111643195 Scheduler time: 121.1522563979961 Scheduler overhead time: 0.09059333940967917 Adapter cache time: 0.01790457684546709 Engine time: 0.08552157646045089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 126.83035358088091,
    "estimated_duration": 3600.024371469534,
    "input_throughput": 7811.691838219544,
    "output_throughput": 6953.121539502843,
    "total_throughput": 14764.813377722387,
    "itl": 105.74885479902507,
    "ttft": 1714365.9951772322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8634750794619361,
    "arrivals": 381208,
    "finished_requests": 113484,
    "scheduler_time": 272.0234043309092
}
#Debug simulation 
Total elapsed time: 126.83052418893203. Arrivals time: 0.5452523026615381 Scheduler time: 126.05703302752227 Scheduler overhead time: 0.09000588767230511 Adapter cache time: 0.018432606011629105 Engine time: 0.08602611161768436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 121.75347960414365,
    "estimated_duration": 3600.0370206405564,
    "input_throughput": 7843.13990053803,
    "output_throughput": 6926.51960439095,
    "total_throughput": 14769.659504928979,
    "itl": 106.01490539152054,
    "ttft": 1702428.5523062507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8355131540936407,
    "arrivals": 377644,
    "finished_requests": 113818,
    "scheduler_time": 273.81701662478054
}
#Debug simulation 
Total elapsed time: 121.7536468282342. Arrivals time: 0.5469254963099957 Scheduler time: 120.9760110215284 Scheduler overhead time: 0.0922239487990737 Adapter cache time: 0.018012632615864277 Engine time: 0.08603449538350105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.3189367861487,
    "estimated_duration": 3600.091823913803,
    "input_throughput": 7843.020506433628,
    "output_throughput": 6926.414163761906,
    "total_throughput": 14769.434670195535,
    "itl": 106.01578571777505,
    "ttft": 1702452.8451527823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8894222925091207,
    "arrivals": 377644,
    "finished_requests": 113818,
    "scheduler_time": 273.8178107210056
}
#Debug simulation 
Total elapsed time: 121.31919425120577. Arrivals time: 0.5389640382491052 Scheduler time: 120.54717133007944 Scheduler overhead time: 0.09342624619603157 Adapter cache time: 0.01859043864533305 Engine time: 0.08735782653093338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.52490696124732,
    "estimated_duration": 3600.0935606467865,
    "input_throughput": 7843.016722856292,
    "output_throughput": 6926.410822367653,
    "total_throughput": 14769.427545223945,
    "itl": 106.01582794588646,
    "ttft": 1702453.5512350458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8912479472346648,
    "arrivals": 377644,
    "finished_requests": 113818,
    "scheduler_time": 273.81782183784134
}
#Debug simulation 
Total elapsed time: 121.52506773499772. Arrivals time: 0.5463863466866314 Scheduler time: 120.74786294111982 Scheduler overhead time: 0.09278898919001222 Adapter cache time: 0.017973484005779028 Engine time: 0.08569983812049031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 122.14541498292238,
    "estimated_duration": 3600.053721994145,
    "input_throughput": 7843.103514677475,
    "output_throughput": 6926.48747091129,
    "total_throughput": 14769.590985588764,
    "itl": 106.01518319741834,
    "ttft": 1702435.4408823198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.852240118531047,
    "arrivals": 377644,
    "finished_requests": 113818,
    "scheduler_time": 273.81699101389586
}
#Debug simulation 
Total elapsed time: 122.14557569520548. Arrivals time: 0.5443784715607762 Scheduler time: 121.36801722832024 Scheduler overhead time: 0.09190071420744061 Adapter cache time: 0.01840653782710433 Engine time: 0.08851438434794545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 121.19906304823235,
    "estimated_duration": 3600.1068771254186,
    "input_throughput": 7842.987712227396,
    "output_throughput": 6926.385202183347,
    "total_throughput": 14769.372914410742,
    "itl": 106.0159829580521,
    "ttft": 1702459.875089986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9026915417984174,
    "arrivals": 377644,
    "finished_requests": 113818,
    "scheduler_time": 273.8181941432065
}
#Debug simulation 
Total elapsed time: 121.19922094419599. Arrivals time: 0.5404827669262886 Scheduler time: 120.42672381224111 Scheduler overhead time: 0.09285529283806682 Adapter cache time: 0.01797494664788246 Engine time: 0.08736285381019115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 122.3453767108731,
    "estimated_duration": 3600.0177145378248,
    "input_throughput": 7843.181961571243,
    "output_throughput": 6926.556749791239,
    "total_throughput": 14769.738711362483,
    "itl": 106.01461801037654,
    "ttft": 1702420.233717098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8162837305082951,
    "arrivals": 377644,
    "finished_requests": 113818,
    "scheduler_time": 273.8167398684256
}
#Debug simulation 
Total elapsed time: 122.34553907206282. Arrivals time: 0.5434055286459625 Scheduler time: 121.56830052146688 Scheduler overhead time: 0.09248743206262589 Adapter cache time: 0.018478526268154383 Engine time: 0.08895585779100657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.27592560695484,
    "estimated_duration": 3600.117786209705,
    "input_throughput": 7842.963946389972,
    "output_throughput": 6926.364213836726,
    "total_throughput": 14769.328160226698,
    "itl": 106.0161537240876,
    "ttft": 1702464.4312287641,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9137578750029253,
    "arrivals": 377644,
    "finished_requests": 113818,
    "scheduler_time": 273.8182369714526
}
#Debug simulation 
Total elapsed time: 121.27619093470275. Arrivals time: 0.5382967810146511 Scheduler time: 120.50809402624145 Scheduler overhead time: 0.09091002214699984 Adapter cache time: 0.01829524850472808 Engine time: 0.08699912065640092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 116.21931213000789,
    "estimated_duration": 3600.0002987167873,
    "input_throughput": 7857.606848000258,
    "output_throughput": 6970.686921594111,
    "total_throughput": 14828.293769594367,
    "itl": 106.39397097780694,
    "ttft": 1673532.6341754282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.028323881961405,
    "arrivals": 376253,
    "finished_requests": 114346,
    "scheduler_time": 273.5839611495994
}
#Debug simulation 
Total elapsed time: 116.21946899732575. Arrivals time: 0.5268488964065909 Scheduler time: 115.46286367857829 Scheduler overhead time: 0.09123624674975872 Adapter cache time: 0.018875135574489832 Engine time: 0.08587693236768246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.12767538102344,
    "estimated_duration": 3600.051466043302,
    "input_throughput": 7871.560522757023,
    "output_throughput": 6957.436646740075,
    "total_throughput": 14828.997169497097,
    "itl": 106.23286262075332,
    "ttft": 1693362.6000809395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.899209634030708,
    "arrivals": 376253,
    "finished_requests": 114657,
    "scheduler_time": 271.75667593756305
}
#Debug simulation 
Total elapsed time: 121.1278411000967. Arrivals time: 0.5427949777804315 Scheduler time: 120.35518100205809 Scheduler overhead time: 0.0919850948266685 Adapter cache time: 0.018412848468869925 Engine time: 0.08603759668767452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 120.81772434478626,
    "estimated_duration": 3600.053323750141,
    "input_throughput": 7871.55646085835,
    "output_throughput": 6957.433056549464,
    "total_throughput": 14828.989517407814,
    "itl": 106.23289795316607,
    "ttft": 1693363.3294405492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9010529987700329,
    "arrivals": 376253,
    "finished_requests": 114657,
    "scheduler_time": 271.75669027965847
}
#Debug simulation 
Total elapsed time: 120.81788059184328. Arrivals time: 0.5428867610171437 Scheduler time: 120.04410831956193 Scheduler overhead time: 0.09205218125134706 Adapter cache time: 0.01809309981763363 Engine time: 0.08642780594527721 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 115.85742206033319,
    "estimated_duration": 3600.0209315717675,
    "input_throughput": 7857.5618135780505,
    "output_throughput": 6970.646970389632,
    "total_throughput": 14828.208783967682,
    "itl": 106.39436520882647,
    "ttft": 1673540.8788419778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.048785193478691,
    "arrivals": 376253,
    "finished_requests": 114346,
    "scheduler_time": 273.58413269301667
}
#Debug simulation 
Total elapsed time: 115.85758283408359. Arrivals time: 0.5450924499891698 Scheduler time: 115.08284537121654 Scheduler overhead time: 0.09127624426037073 Adapter cache time: 0.018485712353140116 Engine time: 0.0861026281490922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 121.50621063401923,
    "estimated_duration": 3600.028222303831,
    "input_throughput": 7881.7526552171785,
    "output_throughput": 6959.407385969519,
    "total_throughput": 14841.160041186697,
    "itl": 106.18987048474766,
    "ttft": 1693356.534842123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9158773596771107,
    "arrivals": 376253,
    "finished_requests": 114765,
    "scheduler_time": 271.5800185117447
}
#Debug simulation 
Total elapsed time: 121.50637244107202. Arrivals time: 0.5421821009367704 Scheduler time: 120.7333344258368 Scheduler overhead time: 0.09242092305794358 Adapter cache time: 0.0180487590841949 Engine time: 0.08666823338717222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 116.06748036807403,
    "estimated_duration": 3600.016456337469,
    "input_throughput": 7857.571581430658,
    "output_throughput": 6970.655635705132,
    "total_throughput": 14828.22721713579,
    "itl": 106.39366807357992,
    "ttft": 1673523.5514781084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0046568990871323,
    "arrivals": 376253,
    "finished_requests": 114346,
    "scheduler_time": 273.5891810871205
}
#Debug simulation 
Total elapsed time: 116.06763663003221. Arrivals time: 0.5289834844879806 Scheduler time: 115.31045934231952 Scheduler overhead time: 0.09035120997577906 Adapter cache time: 0.018394922837615013 Engine time: 0.08627759478986263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_192_slots_16_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.64182921918109,
    "estimated_duration": 3600.039346529358,
    "input_throughput": 7881.7283003738985,
    "output_throughput": 6959.3858811997525,
    "total_throughput": 14841.114181573652,
    "itl": 106.19007615534125,
    "ttft": 1693361.1146369195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9273209542408631,
    "arrivals": 376253,
    "finished_requests": 114765,
    "scheduler_time": 271.5800992970336
}
#Debug simulation 
Total elapsed time: 121.6419878299348. Arrivals time: 0.5447592828422785 Scheduler time: 120.86557301878929 Scheduler overhead time: 0.09229697985574603 Adapter cache time: 0.01871546357870102 Engine time: 0.08671145839616656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 117.28102423204109,
    "estimated_duration": 3600.015607056436,
    "input_throughput": 7952.039414464466,
    "output_throughput": 7052.32387055093,
    "total_throughput": 15004.363285015397,
    "itl": 107.24539443879651,
    "ttft": 1693013.7929970035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8661180315329682,
    "arrivals": 375490,
    "finished_requests": 115680,
    "scheduler_time": 266.1118063679292
}
#Debug simulation 
Total elapsed time: 117.28128642309457. Arrivals time: 0.5704567483626306 Scheduler time: 116.48641756922007 Scheduler overhead time: 0.08823344530537724 Adapter cache time: 0.01769256917759776 Engine time: 0.08514234330505133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 120.47883417503908,
    "estimated_duration": 3600.11768329546,
    "input_throughput": 7950.004560351226,
    "output_throughput": 7048.498752621874,
    "total_throughput": 14998.5033129731,
    "itl": 106.56206239958348,
    "ttft": 1694006.1112187435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8932295334409035,
    "arrivals": 375490,
    "finished_requests": 115609,
    "scheduler_time": 266.3698355229033
}
#Debug simulation 
Total elapsed time: 120.47900018235669. Arrivals time: 0.5671406919136643 Scheduler time: 119.68373869359493 Scheduler overhead time: 0.09023222140967846 Adapter cache time: 0.017736279405653477 Engine time: 0.08658030210062861 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.93444411363453,
    "estimated_duration": 3600.1193355137566,
    "input_throughput": 7950.000911821339,
    "output_throughput": 7048.495517823936,
    "total_throughput": 14998.496429645276,
    "itl": 106.56208789367356,
    "ttft": 1694006.710278318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8949659610725984,
    "arrivals": 375490,
    "finished_requests": 115609,
    "scheduler_time": 266.36985135214184
}
#Debug simulation 
Total elapsed time: 119.93462395388633. Arrivals time: 0.5741892154328525 Scheduler time: 119.13241593167186 Scheduler overhead time: 0.09102593595162034 Adapter cache time: 0.01843294780701399 Engine time: 0.08554415125399828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 126.42649235995486,
    "estimated_duration": 3600.10908051603,
    "input_throughput": 7801.290286452737,
    "output_throughput": 6922.073315741866,
    "total_throughput": 14723.363602194604,
    "itl": 105.51306755487997,
    "ttft": 1704026.2814189147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8406885126698788,
    "arrivals": 375490,
    "finished_requests": 113525,
    "scheduler_time": 273.5683460067592
}
#Debug simulation 
Total elapsed time: 126.42665046779439. Arrivals time: 0.5347680915147066 Scheduler time: 125.65817554853857 Scheduler overhead time: 0.0932248430326581 Adapter cache time: 0.01807018555700779 Engine time: 0.08815476857125759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 120.41182136069983,
    "estimated_duration": 3600.0035423404383,
    "input_throughput": 7949.849677479447,
    "output_throughput": 7048.375286737555,
    "total_throughput": 14998.224964217,
    "itl": 106.5614993103114,
    "ttft": 1694032.19577589,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9066610632091806,
    "arrivals": 375490,
    "finished_requests": 115604,
    "scheduler_time": 266.3612002273189
}
#Debug simulation 
Total elapsed time: 120.41199160506949. Arrivals time: 0.796494691632688 Scheduler time: 119.38832875993103 Scheduler overhead time: 0.09060651995241642 Adapter cache time: 0.017776862252503633 Engine time: 0.08518531452864408 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 117.63645893614739,
    "estimated_duration": 3600.1244159606035,
    "input_throughput": 7952.028233546827,
    "output_throughput": 7052.521820478619,
    "total_throughput": 15004.550054025445,
    "itl": 107.24529164180714,
    "ttft": 1693038.3326530678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8461842334573169,
    "arrivals": 375490,
    "finished_requests": 115685,
    "scheduler_time": 266.12051145653265
}
#Debug simulation 
Total elapsed time: 117.63661583932117. Arrivals time: 0.5702184257097542 Scheduler time: 116.84213827364147 Scheduler overhead time: 0.08927288604900241 Adapter cache time: 0.017494078259915113 Engine time: 0.08492245664820075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.8696923092939,
    "estimated_duration": 3600.014757304443,
    "input_throughput": 7949.82491167042,
    "output_throughput": 7048.353329251139,
    "total_throughput": 14998.17824092156,
    "itl": 106.56168014493129,
    "ttft": 1694037.088946306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9177273964136885,
    "arrivals": 375490,
    "finished_requests": 115604,
    "scheduler_time": 266.36135865796314
}
#Debug simulation 
Total elapsed time: 119.86985169304535. Arrivals time: 0.5706534124910831 Scheduler time: 119.07138006063178 Scheduler overhead time: 0.09035816788673401 Adapter cache time: 0.018096188083291054 Engine time: 0.0862839762121439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 118.54498524405062,
    "estimated_duration": 3600.055454553429,
    "input_throughput": 7947.205636461228,
    "output_throughput": 7032.329729248694,
    "total_throughput": 14979.535365709922,
    "itl": 107.0900728080506,
    "ttft": 1688383.6890928715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9150858354358922,
    "arrivals": 373265,
    "finished_requests": 115569,
    "scheduler_time": 267.20046043216695
}
#Debug simulation 
Total elapsed time: 118.54514859290794. Arrivals time: 0.5267821936868131 Scheduler time: 117.79117932217196 Scheduler overhead time: 0.09079828765243292 Adapter cache time: 0.01780944736674428 Engine time: 0.08499700203537941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.70535496901721,
    "estimated_duration": 3600.0942696131656,
    "input_throughput": 7882.38386964494,
    "output_throughput": 6973.021293327801,
    "total_throughput": 14855.405162972742,
    "itl": 106.49276426581378,
    "ttft": 1697060.6957679007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9336046854825741,
    "arrivals": 373265,
    "finished_requests": 114597,
    "scheduler_time": 270.4989002714359
}
#Debug simulation 
Total elapsed time: 119.70559837762266. Arrivals time: 0.5354321952909231 Scheduler time: 118.94013982452452 Scheduler overhead time: 0.0912257949821651 Adapter cache time: 0.018040372524410486 Engine time: 0.08727814862504601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.99673101492226,
    "estimated_duration": 3600.095869393774,
    "input_throughput": 7882.380366936868,
    "output_throughput": 6973.018194714694,
    "total_throughput": 14855.398561651562,
    "itl": 106.49280134585938,
    "ttft": 1697061.386675965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9351979096978951,
    "arrivals": 373265,
    "finished_requests": 114597,
    "scheduler_time": 270.4989068278224
}
#Debug simulation 
Total elapsed time: 119.99688387522474. Arrivals time: 0.5277634100057185 Scheduler time: 119.24157764250413 Scheduler overhead time: 0.09026901889592409 Adapter cache time: 0.01784041291102767 Engine time: 0.08575467020273209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 118.52763662301004,
    "estimated_duration": 3600.07481688519,
    "input_throughput": 7947.162893896161,
    "output_throughput": 7032.291907173266,
    "total_throughput": 14979.454801069427,
    "itl": 107.0904325789637,
    "ttft": 1688392.582790268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9344759747013482,
    "arrivals": 373265,
    "finished_requests": 115569,
    "scheduler_time": 267.2010353060663
}
#Debug simulation 
Total elapsed time: 118.52779466239735. Arrivals time: 0.5320889139547944 Scheduler time: 117.76965942466632 Scheduler overhead time: 0.08959927037358284 Adapter cache time: 0.018149024806916714 Engine time: 0.0856144824065268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 120.16441803565249,
    "estimated_duration": 3600.1079579471375,
    "input_throughput": 7882.353899237341,
    "output_throughput": 6972.994780499472,
    "total_throughput": 14855.348679736811,
    "itl": 106.49304731731688,
    "ttft": 1697066.3956440962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9473960269801366,
    "arrivals": 373265,
    "finished_requests": 114597,
    "scheduler_time": 270.4990973796512
}
#Debug simulation 
Total elapsed time: 120.16457750182599. Arrivals time: 0.5292023466899991 Scheduler time: 119.40649532945827 Scheduler overhead time: 0.09097141399979591 Adapter cache time: 0.01778810005635023 Engine time: 0.08644086262211204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 118.81440934585407,
    "estimated_duration": 3600.0349694888387,
    "input_throughput": 7947.250857972173,
    "output_throughput": 7032.369744895749,
    "total_throughput": 14979.620602867923,
    "itl": 107.08966293512569,
    "ttft": 1688374.9653585737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 299,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8940250381757517,
    "arrivals": 373265,
    "finished_requests": 115569,
    "scheduler_time": 267.20013581756217
}
#Debug simulation 
Total elapsed time: 118.81456400174648. Arrivals time: 0.5286031505092978 Scheduler time: 118.06286970991641 Scheduler overhead time: 0.08785324124619365 Adapter cache time: 0.018132655881345272 Engine time: 0.08457852667197585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_192_slots_16_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 120.04442830430344,
    "estimated_duration": 3600.120815413448,
    "input_throughput": 7882.325748209944,
    "output_throughput": 6972.969877155925,
    "total_throughput": 14855.29562536587,
    "itl": 106.49323693192608,
    "ttft": 1697072.3770181143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9593426366895489,
    "arrivals": 373265,
    "finished_requests": 114597,
    "scheduler_time": 270.49941453799994
}
#Debug simulation 
Total elapsed time: 120.0445935181342. Arrivals time: 0.5254426635801792 Scheduler time: 119.2886332757771 Scheduler overhead time: 0.09106670133769512 Adapter cache time: 0.018387324176728725 Engine time: 0.08743579871952534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 118.78227325482294,
    "estimated_duration": 3600.0500525833017,
    "input_throughput": 8032.002771531166,
    "output_throughput": 7109.149769080272,
    "total_throughput": 15141.152540611438,
    "itl": 107.45758057584708,
    "ttft": 1684840.8914766891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9701746148266817,
    "arrivals": 372565,
    "finished_requests": 116694,
    "scheduler_time": 263.2053407477562
}
#Debug simulation 
Total elapsed time: 118.78244980797172. Arrivals time: 0.5317043396644294 Scheduler time: 118.02731055533513 Scheduler overhead time: 0.08738367026671767 Adapter cache time: 0.018022595439106226 Engine time: 0.08505361108109355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.55646931612864,
    "estimated_duration": 3600.042981636321,
    "input_throughput": 7953.026434975033,
    "output_throughput": 7059.460714674149,
    "total_throughput": 15012.487149649183,
    "itl": 106.68228491056938,
    "ttft": 1683677.0671719203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.873654850397729,
    "arrivals": 372565,
    "finished_requests": 115633,
    "scheduler_time": 267.3623033514395
}
#Debug simulation 
Total elapsed time: 121.55663033621386. Arrivals time: 0.5310423551127315 Scheduler time: 120.79821180040017 Scheduler overhead time: 0.09094907809048891 Adapter cache time: 0.017997650895267725 Engine time: 0.08529869513586164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.20694152684882,
    "estimated_duration": 3600.0448322122497,
    "input_throughput": 7953.0223467817,
    "output_throughput": 7059.457085811545,
    "total_throughput": 15012.479432593245,
    "itl": 106.68231842895386,
    "ttft": 1683677.880802746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8753558580018623,
    "arrivals": 372565,
    "finished_requests": 115633,
    "scheduler_time": 267.3623528811787
}
#Debug simulation 
Total elapsed time: 121.20722596999258. Arrivals time: 0.5334848775528371 Scheduler time: 120.44939159369096 Scheduler overhead time: 0.08918420923873782 Adapter cache time: 0.017846221569925547 Engine time: 0.08419074909761548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 117.27698444202542,
    "estimated_duration": 3600.030630669982,
    "input_throughput": 7988.635361874061,
    "output_throughput": 7071.131779582239,
    "total_throughput": 15059.767141456301,
    "itl": 107.32490575883514,
    "ttft": 1686542.8465460062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9895226659649086,
    "arrivals": 372565,
    "finished_requests": 116071,
    "scheduler_time": 265.2705879719189
}
#Debug simulation 
Total elapsed time: 117.27715076925233. Arrivals time: 0.5280201779678464 Scheduler time: 116.52686826232821 Scheduler overhead time: 0.08767629321664572 Adapter cache time: 0.01785818813368678 Engine time: 0.08374315686523914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 121.66128484997898,
    "estimated_duration": 3600.056646148883,
    "input_throughput": 7952.996248164017,
    "output_throughput": 7059.433919515324,
    "total_throughput": 15012.43016767934,
    "itl": 106.68253882769619,
    "ttft": 1683682.7086823864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8870509601384446,
    "arrivals": 372565,
    "finished_requests": 115633,
    "scheduler_time": 267.36247171568357
}
#Debug simulation 
Total elapsed time: 121.66144914273173. Arrivals time: 0.5351631762459874 Scheduler time: 120.89564849343151 Scheduler overhead time: 0.09372223122045398 Adapter cache time: 0.017872089985758066 Engine time: 0.08487121015787125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 119.11788887390867,
    "estimated_duration": 3600.0263809489356,
    "input_throughput": 8032.055585208822,
    "output_throughput": 7109.196514624938,
    "total_throughput": 15141.252099833759,
    "itl": 107.45703014813954,
    "ttft": 1684830.374049881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.947845943483991,
    "arrivals": 372565,
    "finished_requests": 116694,
    "scheduler_time": 263.2047980933178
}
#Debug simulation 
Total elapsed time: 119.11805217107758. Arrivals time: 0.5429894463159144 Scheduler time: 118.35174110298976 Scheduler overhead time: 0.08818463189527392 Adapter cache time: 0.017924162559211254 Engine time: 0.08459425158798695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.28477067500353,
    "estimated_duration": 3600.0680600098985,
    "input_throughput": 7952.971033531316,
    "output_throughput": 7059.41153788357,
    "total_throughput": 15012.382571414886,
    "itl": 106.68273204746582,
    "ttft": 1683688.1819745004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8976142781972929,
    "arrivals": 372565,
    "finished_requests": 115633,
    "scheduler_time": 267.36282206573884
}
#Debug simulation 
Total elapsed time: 121.284930346068. Arrivals time: 0.7854816308245063 Scheduler time: 120.27410629484802 Scheduler overhead time: 0.08919626520946622 Adapter cache time: 0.017685431521385908 Engine time: 0.08500222116708755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 125.09077981067821,
    "estimated_duration": 3600.060089395255,
    "input_throughput": 7926.659081069285,
    "output_throughput": 7009.0002315040965,
    "total_throughput": 14935.659312573382,
    "itl": 106.0214862022214,
    "ttft": 1674321.3045451913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8140897398861114,
    "arrivals": 371093,
    "finished_requests": 115074,
    "scheduler_time": 269.25354409434357
}
#Debug simulation 
Total elapsed time: 125.09094602102414. Arrivals time: 0.5546934199519455 Scheduler time: 124.30806946195662 Scheduler overhead time: 0.09100499330088496 Adapter cache time: 0.018073914106935263 Engine time: 0.08610271476209164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 126.81052281195298,
    "estimated_duration": 3600.024009622317,
    "input_throughput": 7957.714149524657,
    "output_throughput": 7012.319065796573,
    "total_throughput": 14970.03321532123,
    "itl": 106.0711329924033,
    "ttft": 1685576.179710235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9416277626645803,
    "arrivals": 371093,
    "finished_requests": 115497,
    "scheduler_time": 268.1510125847163
}
#Debug simulation 
Total elapsed time: 126.8106784010306. Arrivals time: 0.5604712809436023 Scheduler time: 126.01992827514187 Scheduler overhead time: 0.09160896204411983 Adapter cache time: 0.018496557138860226 Engine time: 0.0870277239009738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 127.09441632591188,
    "estimated_duration": 3600.0248973719063,
    "input_throughput": 7957.712187188931,
    "output_throughput": 7012.31733659093,
    "total_throughput": 14970.02952377986,
    "itl": 106.0711871456454,
    "ttft": 1685576.4880147383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9429711848683704,
    "arrivals": 371093,
    "finished_requests": 115497,
    "scheduler_time": 268.15105710499245
}
#Debug simulation 
Total elapsed time: 127.09458487015218. Arrivals time: 0.539105849340558 Scheduler time: 126.32397513277829 Scheduler overhead time: 0.09245859086513519 Adapter cache time: 0.018157323356717825 Engine time: 0.08717886172235012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 123.1778813060373,
    "estimated_duration": 3600.032688874515,
    "input_throughput": 7917.5156070349585,
    "output_throughput": 6978.579132806232,
    "total_throughput": 14896.094739841192,
    "itl": 106.32553954048245,
    "ttft": 1690031.7530395368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8546916904416895,
    "arrivals": 371093,
    "finished_requests": 114902,
    "scheduler_time": 269.90925283917375
}
#Debug simulation 
Total elapsed time: 123.17814592691138. Arrivals time: 0.547268916387111 Scheduler time: 122.4013218935579 Scheduler overhead time: 0.09120855294167995 Adapter cache time: 0.018289467319846153 Engine time: 0.08631131332367659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 126.92336292518303,
    "estimated_duration": 3600.0373534004325,
    "input_throughput": 7957.684653727392,
    "output_throughput": 7012.293074169125,
    "total_throughput": 14969.977727896518,
    "itl": 106.07143231059733,
    "ttft": 1685581.2152634694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9554208097234417,
    "arrivals": 371093,
    "finished_requests": 115497,
    "scheduler_time": 268.15116354725257
}
#Debug simulation 
Total elapsed time: 126.92351950332522. Arrivals time: 0.5407848698087037 Scheduler time: 126.15122102107853 Scheduler overhead time: 0.09182204399257898 Adapter cache time: 0.018423537258058786 Engine time: 0.08751947479322553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 124.60751500260085,
    "estimated_duration": 3600.0412323039445,
    "input_throughput": 7926.700601075428,
    "output_throughput": 7009.036944793982,
    "total_throughput": 14935.73754586941,
    "itl": 106.02120840494204,
    "ttft": 1674313.3178749634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7953533784439798,
    "arrivals": 371093,
    "finished_requests": 115074,
    "scheduler_time": 269.2532232872671
}
#Debug simulation 
Total elapsed time: 124.60767571162432. Arrivals time: 0.5370881133712828 Scheduler time: 123.84312552539632 Scheduler overhead time: 0.09044926939532161 Adapter cache time: 0.017755604349076748 Engine time: 0.08559588948264718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_192_slots_16_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 127.86430624173954,
    "estimated_duration": 3600.0499447028137,
    "input_throughput": 7957.6568214430445,
    "output_throughput": 7012.268548425361,
    "total_throughput": 14969.925369868404,
    "itl": 106.07169057717317,
    "ttft": 1685586.3525746195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9677446807920984,
    "arrivals": 371093,
    "finished_requests": 115497,
    "scheduler_time": 268.1513309399947
}
#Debug simulation 
Total elapsed time: 127.86446350812912. Arrivals time: 0.5477798045612872 Scheduler time: 127.08674425026402 Scheduler overhead time: 0.09078508848324418 Adapter cache time: 0.018252271227538586 Engine time: 0.08726774854585528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 110.3752247011289,
    "estimated_duration": 3600.092618716829,
    "input_throughput": 7303.8990339565635,
    "output_throughput": 6450.725706128467,
    "total_throughput": 13754.624740085032,
    "itl": 95.31993277771599,
    "ttft": 1697165.4973642386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2823443647078323,
    "arrivals": 299534,
    "finished_requests": 106282,
    "scheduler_time": 290.16067821589667
}
#Debug simulation 
Total elapsed time: 110.37538632797077. Arrivals time: 0.5144231780432165 Scheduler time: 109.6196309295483 Scheduler overhead time: 0.09631536342203617 Adapter cache time: 0.019150784704834223 Engine time: 0.08926995797082782 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 113.5048030023463,
    "estimated_duration": 3600.013598781495,
    "input_throughput": 7414.009216252407,
    "output_throughput": 6582.029303450475,
    "total_throughput": 13996.03851970288,
    "itl": 98.17479031509558,
    "ttft": 1663949.4004580092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3656033814535535,
    "arrivals": 299534,
    "finished_requests": 107898,
    "scheduler_time": 284.50022594363895
}
#Debug simulation 
Total elapsed time: 113.504959104117. Arrivals time: 0.5232777544297278 Scheduler time: 112.74325026059523 Scheduler overhead time: 0.09489126922562718 Adapter cache time: 0.019608532544225454 Engine time: 0.08843529690057039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 113.74819823540747,
    "estimated_duration": 3600.0163410433793,
    "input_throughput": 7414.003568735019,
    "output_throughput": 6582.0242896821,
    "total_throughput": 13996.02785841712,
    "itl": 98.17482962823482,
    "ttft": 1663950.6527598414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3683147061243732,
    "arrivals": 299534,
    "finished_requests": 107898,
    "scheduler_time": 284.50035691941486
}
#Debug simulation 
Total elapsed time: 113.74835908832029. Arrivals time: 0.5253561018034816 Scheduler time: 112.9851721059531 Scheduler overhead time: 0.09485064074397087 Adapter cache time: 0.019333208445459604 Engine time: 0.08820014260709286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 113.61846467806026,
    "estimated_duration": 3600.040335342976,
    "input_throughput": 7414.209151480827,
    "output_throughput": 6582.430415392106,
    "total_throughput": 13996.639566872933,
    "itl": 98.17324195143193,
    "ttft": 1663891.475723012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3104430134640987,
    "arrivals": 299534,
    "finished_requests": 107901,
    "scheduler_time": 284.51069220803254
}
#Debug simulation 
Total elapsed time: 113.6186230420135. Arrivals time: 0.5215254472568631 Scheduler time: 112.85812946921214 Scheduler overhead time: 0.09546305937692523 Adapter cache time: 0.01975503796711564 Engine time: 0.08804377820342779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 114.2378576551564,
    "estimated_duration": 3600.034380354937,
    "input_throughput": 7413.966418111959,
    "output_throughput": 6581.991308000733,
    "total_throughput": 13995.957726112692,
    "itl": 98.17545259094639,
    "ttft": 1663958.3682431995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.385291467290376,
    "arrivals": 299534,
    "finished_requests": 107898,
    "scheduler_time": 284.50061916118
}
#Debug simulation 
Total elapsed time: 114.23811206780374. Arrivals time: 0.5231102327816188 Scheduler time: 113.47669119900092 Scheduler overhead time: 0.09538614703342319 Adapter cache time: 0.01936298795044422 Engine time: 0.08813571278005838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 110.54317644191906,
    "estimated_duration": 3600.0621475887206,
    "input_throughput": 7303.960854567994,
    "output_throughput": 6450.780305432959,
    "total_throughput": 13754.741160000953,
    "itl": 95.31926361720487,
    "ttft": 1697152.560621739,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 419,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.252831073564004,
    "arrivals": 299534,
    "finished_requests": 106282,
    "scheduler_time": 290.15998932576224
}
#Debug simulation 
Total elapsed time: 110.54333618702367. Arrivals time: 0.525242724455893 Scheduler time: 109.7774620600976 Scheduler overhead time: 0.09655408933758736 Adapter cache time: 0.01963031804189086 Engine time: 0.08865524176508188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 110.39304331596941,
    "estimated_duration": 3600.0344683748717,
    "input_throughput": 7328.032337398423,
    "output_throughput": 6473.277743510024,
    "total_throughput": 13801.310080908448,
    "itl": 95.30963252718263,
    "ttft": 1688048.9115670323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4074552738666541,
    "arrivals": 299534,
    "finished_requests": 106607,
    "scheduler_time": 288.90336349566087
}
#Debug simulation 
Total elapsed time: 110.39321131398901. Arrivals time: 0.524082827847451 Scheduler time: 109.63040025020018 Scheduler overhead time: 0.09557416941970587 Adapter cache time: 0.01934277731925249 Engine time: 0.08788356417790055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 112.05058674002066,
    "estimated_duration": 3600.1127760196723,
    "input_throughput": 7332.946949841814,
    "output_throughput": 6495.989002282357,
    "total_throughput": 13828.935952124171,
    "itl": 97.21650958137437,
    "ttft": 1668230.0013643224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2701024137321009,
    "arrivals": 287913,
    "finished_requests": 106591,
    "scheduler_time": 286.9489229423796
}
#Debug simulation 
Total elapsed time: 112.0507428329438. Arrivals time: 0.5140548478811979 Scheduler time: 111.2982959416695 Scheduler overhead time: 0.09563350630924106 Adapter cache time: 0.01910800626501441 Engine time: 0.08781673246994615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 112.18871389422566,
    "estimated_duration": 3600.0719749291097,
    "input_throughput": 7332.481456990815,
    "output_throughput": 6495.698186828746,
    "total_throughput": 13828.179643819562,
    "itl": 97.21719463668943,
    "ttft": 1668265.162371771,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3536431802739446,
    "arrivals": 287913,
    "finished_requests": 106586,
    "scheduler_time": 286.94161754142624
}
#Debug simulation 
Total elapsed time: 112.18887786008418. Arrivals time: 0.5252170795574784 Scheduler time: 111.42369197262451 Scheduler overhead time: 0.0958646428771317 Adapter cache time: 0.019676601514220238 Engine time: 0.08880914933979511 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 112.21941413590685,
    "estimated_duration": 3600.07451479121,
    "input_throughput": 7332.476283905738,
    "output_throughput": 6495.69360409648,
    "total_throughput": 13828.169888002218,
    "itl": 97.21725536866248,
    "ttft": 1668266.1964663414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3561406307295043,
    "arrivals": 287913,
    "finished_requests": 106586,
    "scheduler_time": 286.94165995305127
}
#Debug simulation 
Total elapsed time: 112.2196733080782. Arrivals time: 0.5139395105652511 Scheduler time: 111.46737187867984 Scheduler overhead time: 0.09499413287267089 Adapter cache time: 0.019390486646443605 Engine time: 0.08822634490206838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 112.37012449698523,
    "estimated_duration": 3600.0154583141066,
    "input_throughput": 7332.5965695608365,
    "output_throughput": 6495.800162744642,
    "total_throughput": 13828.396732305479,
    "itl": 97.21589495539412,
    "ttft": 1668242.6650248014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2976656216476101,
    "arrivals": 287913,
    "finished_requests": 106586,
    "scheduler_time": 286.94087840787137
}
#Debug simulation 
Total elapsed time: 112.37028761440888. Arrivals time: 0.5287545407190919 Scheduler time: 111.60086476895958 Scheduler overhead time: 0.09685785509645939 Adapter cache time: 0.019373612012714148 Engine time: 0.08845098875463009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 112.30263758543879,
    "estimated_duration": 3600.064738553609,
    "input_throughput": 7332.496195778317,
    "output_throughput": 6495.711243624841,
    "total_throughput": 13828.207439403157,
    "itl": 97.21784894853094,
    "ttft": 1668252.8297206748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3733688994683375,
    "arrivals": 287913,
    "finished_requests": 106586,
    "scheduler_time": 286.93685941200926
}
#Debug simulation 
Total elapsed time: 112.30280111031607. Arrivals time: 0.5269439690746367 Scheduler time: 111.53644511522725 Scheduler overhead time: 0.09606743603944778 Adapter cache time: 0.019548031501471996 Engine time: 0.08854561531916261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 112.219459251035,
    "estimated_duration": 3600.0831601048912,
    "input_throughput": 7333.00727398498,
    "output_throughput": 6496.0424412303355,
    "total_throughput": 13829.049715215315,
    "itl": 97.21588434368674,
    "ttft": 1668218.317693493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2408708723843958,
    "arrivals": 287913,
    "finished_requests": 106591,
    "scheduler_time": 286.94833849172113
}
#Debug simulation 
Total elapsed time: 112.21961709391326. Arrivals time: 0.5212891865521669 Scheduler time: 111.45869706152007 Scheduler overhead time: 0.09535360848531127 Adapter cache time: 0.01944095827639103 Engine time: 0.08893282013013959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 112.54963312996551,
    "estimated_duration": 3600.083373158905,
    "input_throughput": 7332.458241609405,
    "output_throughput": 6495.677620788202,
    "total_throughput": 13828.135862397607,
    "itl": 97.21822092112927,
    "ttft": 1668260.440427962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3908486757799994,
    "arrivals": 287913,
    "finished_requests": 106586,
    "scheduler_time": 286.9374435450666
}
#Debug simulation 
Total elapsed time: 112.54978941194713. Arrivals time: 0.524796309415251 Scheduler time: 111.78538625640795 Scheduler overhead time: 0.09533335361629725 Adapter cache time: 0.019933424424380064 Engine time: 0.08859350625425577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 111.45196652738377,
    "estimated_duration": 3600.063984502419,
    "input_throughput": 7382.055184131171,
    "output_throughput": 6510.732337230839,
    "total_throughput": 13892.78752136201,
    "itl": 99.14320873620477,
    "ttft": 1656380.430524816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2792838769638994,
    "arrivals": 282093,
    "finished_requests": 107209,
    "scheduler_time": 284.90248536574995
}
#Debug simulation 
Total elapsed time: 111.45212506409734. Arrivals time: 0.5166440224274993 Scheduler time: 110.69735148083419 Scheduler overhead time: 0.0944120166823268 Adapter cache time: 0.019244927912950516 Engine time: 0.08886726945638657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 111.86809946456924,
    "estimated_duration": 3600.0227317504728,
    "input_throughput": 7381.920054454254,
    "output_throughput": 6510.62000061409,
    "total_throughput": 13892.540055068343,
    "itl": 99.14433579909041,
    "ttft": 1656390.541720504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3642477124324128,
    "arrivals": 282093,
    "finished_requests": 107204,
    "scheduler_time": 284.89510592892947
}
#Debug simulation 
Total elapsed time: 111.86825901968405. Arrivals time: 0.5194589495658875 Scheduler time: 111.10853768046945 Scheduler overhead time: 0.09521968988701701 Adapter cache time: 0.019579336047172546 Engine time: 0.08984404150396585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 111.30779992230237,
    "estimated_duration": 3600.024915717697,
    "input_throughput": 7381.915576187623,
    "output_throughput": 6510.616050924567,
    "total_throughput": 13892.53162711219,
    "itl": 99.14442160836944,
    "ttft": 1656391.328319672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3666201772540887,
    "arrivals": 282093,
    "finished_requests": 107204,
    "scheduler_time": 284.89511750847277
}
#Debug simulation 
Total elapsed time: 111.30796187510714. Arrivals time: 0.527338816318661 Scheduler time: 110.53841895377263 Scheduler overhead time: 0.0973199144937098 Adapter cache time: 0.019929565954953432 Engine time: 0.08936306461691856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.36056113895029,
    "estimated_duration": 3600.0915672567917,
    "input_throughput": 7381.998625176737,
    "output_throughput": 6510.6824540744,
    "total_throughput": 13892.681079251137,
    "itl": 99.1437434581749,
    "ttft": 1656390.8577691629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3062271772138763,
    "arrivals": 282093,
    "finished_requests": 107209,
    "scheduler_time": 284.90292474266386
}
#Debug simulation 
Total elapsed time: 111.36082720011473. Arrivals time: 0.5155936134979129 Scheduler time: 110.60232035955414 Scheduler overhead time: 0.09681951720267534 Adapter cache time: 0.01943516917526722 Engine time: 0.0906111509539187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 111.27584105916321,
    "estimated_duration": 3600.0433174525747,
    "input_throughput": 7381.877843293503,
    "output_throughput": 6510.582771705432,
    "total_throughput": 13892.460614998934,
    "itl": 99.14483671884167,
    "ttft": 1656398.7127520097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3844772149249955,
    "arrivals": 282093,
    "finished_requests": 107204,
    "scheduler_time": 284.8954621285368
}
#Debug simulation 
Total elapsed time: 111.27599922334775. Arrivals time: 0.5131473485380411 Scheduler time: 110.52169125666842 Scheduler overhead time: 0.09571448620408773 Adapter cache time: 0.019559596199542284 Engine time: 0.0900296256877482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 112.87684867391363,
    "estimated_duration": 3600.0014437243835,
    "input_throughput": 7407.090085056509,
    "output_throughput": 6536.640156359228,
    "total_throughput": 13943.730241415737,
    "itl": 100.10102630763959,
    "ttft": 1649397.9724889942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 415,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2408708723843958,
    "arrivals": 282093,
    "finished_requests": 107523,
    "scheduler_time": 283.46870495332325
}
#Debug simulation 
Total elapsed time: 112.87701199483126. Arrivals time: 0.5211133360862732 Scheduler time: 112.11360548157245 Scheduler overhead time: 0.09685515565797687 Adapter cache time: 0.01962688099592924 Engine time: 0.08981478214263916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 111.62255946965888,
    "estimated_duration": 3600.060731763309,
    "input_throughput": 7381.842135475179,
    "output_throughput": 6510.551278539095,
    "total_throughput": 13892.393414014274,
    "itl": 99.14514809452793,
    "ttft": 1656405.3088644587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 418,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4018312374502435,
    "arrivals": 282093,
    "finished_requests": 107204,
    "scheduler_time": 284.8957224939254
}
#Debug simulation 
Total elapsed time: 111.62271765666083. Arrivals time: 0.5154781774617732 Scheduler time: 110.8634274052456 Scheduler overhead time: 0.09678463032469153 Adapter cache time: 0.019983654376119375 Engine time: 0.09056601719930768 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 116.04196337005123,
    "estimated_duration": 3600.033748394226,
    "input_throughput": 7462.128656983422,
    "output_throughput": 6585.540485717637,
    "total_throughput": 14047.66914270106,
    "itl": 99.66095051253018,
    "ttft": 1633672.892732889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1844087569019808,
    "arrivals": 279137,
    "finished_requests": 108297,
    "scheduler_time": 282.6355481061328
}
#Debug simulation 
Total elapsed time: 116.04212755803019. Arrivals time: 0.5210242974571884 Scheduler time: 115.27848117938265 Scheduler overhead time: 0.09724935283884406 Adapter cache time: 0.019517147913575172 Engine time: 0.08944544289261103 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 116.25478615611792,
    "estimated_duration": 3600.032337567871,
    "input_throughput": 7462.131581337091,
    "output_throughput": 6585.543066542811,
    "total_throughput": 14047.674647879901,
    "itl": 99.66255718284559,
    "ttft": 1633706.7784348135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.26461003287696,
    "arrivals": 279137,
    "finished_requests": 108297,
    "scheduler_time": 282.62637685447595
}
#Debug simulation 
Total elapsed time: 116.25494694104418. Arrivals time: 0.5096147148869932 Scheduler time: 115.5051858574152 Scheduler overhead time: 0.09595920937135816 Adapter cache time: 0.019326696638017893 Engine time: 0.08921270444989204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 116.36573560815305,
    "estimated_duration": 3600.0343237611833,
    "input_throughput": 7462.127464366387,
    "output_throughput": 6585.539433199231,
    "total_throughput": 14047.666897565618,
    "itl": 99.662571749606,
    "ttft": 1633707.6243197143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2665378855355156,
    "arrivals": 279137,
    "finished_requests": 108297,
    "scheduler_time": 282.62633515653226
}
#Debug simulation 
Total elapsed time: 116.36589494720101. Arrivals time: 0.5194574734196067 Scheduler time: 115.60532326018438 Scheduler overhead time: 0.09657678660005331 Adapter cache time: 0.019312649965286255 Engine time: 0.08959720749408007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 116.1775923371315,
    "estimated_duration": 3600.02068331447,
    "input_throughput": 7462.155738301734,
    "output_throughput": 6585.5643857502355,
    "total_throughput": 14047.72012405197,
    "itl": 99.66159264642998,
    "ttft": 1633684.7689854342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.21230983211659,
    "arrivals": 279137,
    "finished_requests": 108297,
    "scheduler_time": 282.63069781228694
}
#Debug simulation 
Total elapsed time: 116.17775566922501. Arrivals time: 0.5172925032675266 Scheduler time: 115.41962487064302 Scheduler overhead time: 0.09612689493224025 Adapter cache time: 0.01972899306565523 Engine time: 0.08910898538306355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 116.08818120369688,
    "estimated_duration": 3600.0240456604365,
    "input_throughput": 7462.148768806827,
    "output_throughput": 6585.558234973027,
    "total_throughput": 14047.707003779855,
    "itl": 99.66330148531796,
    "ttft": 1633694.053595908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2826343701966152,
    "arrivals": 279137,
    "finished_requests": 108297,
    "scheduler_time": 282.6216610769034
}
#Debug simulation 
Total elapsed time: 116.08847501175478. Arrivals time: 0.5202994826249778 Scheduler time: 115.3254337287508 Scheduler overhead time: 0.09717981796711683 Adapter cache time: 0.01923439558595419 Engine time: 0.08960428275167942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 115.8961924011819,
    "estimated_duration": 3600.0060936702075,
    "input_throughput": 7462.185979972114,
    "output_throughput": 6585.591074883297,
    "total_throughput": 14047.77705485541,
    "itl": 99.66029634556533,
    "ttft": 1633661.5261236154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1571494641271378,
    "arrivals": 279137,
    "finished_requests": 108297,
    "scheduler_time": 282.6350526362463
}
#Debug simulation 
Total elapsed time: 115.89635391021147. Arrivals time: 0.5152752725407481 Scheduler time: 115.14063444687054 Scheduler overhead time: 0.0963272056542337 Adapter cache time: 0.019582957960665226 Engine time: 0.08909210562705994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 116.23897894565016,
    "estimated_duration": 3600.0416145286404,
    "input_throughput": 7462.112352142168,
    "output_throughput": 6585.526096232127,
    "total_throughput": 14047.638448374295,
    "itl": 99.66365267331925,
    "ttft": 1633701.3602324713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2996111313626186,
    "arrivals": 279137,
    "finished_requests": 108297,
    "scheduler_time": 282.6220826423353
}
#Debug simulation 
Total elapsed time: 116.23914853064343. Arrivals time: 0.5035372176207602 Scheduler time: 115.49334748089314 Scheduler overhead time: 0.0971677671186626 Adapter cache time: 0.019268298521637917 Engine time: 0.09034910332411528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 113.63777919020504,
    "estimated_duration": 3600.033764256948,
    "input_throughput": 7417.333210904333,
    "output_throughput": 6552.714375685587,
    "total_throughput": 13970.047586589919,
    "itl": 97.95636649430655,
    "ttft": 1646771.11123995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1752272936701822,
    "arrivals": 277607,
    "finished_requests": 108016,
    "scheduler_time": 282.1593264452904
}
#Debug simulation 
Total elapsed time: 113.63794066291302. Arrivals time: 0.5115619241259992 Scheduler time: 112.88699262170121 Scheduler overhead time: 0.09576494293287396 Adapter cache time: 0.01935661118477583 Engine time: 0.08918267209082842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 113.12784858373925,
    "estimated_duration": 3600.1108521150645,
    "input_throughput": 7417.174386258745,
    "output_throughput": 6552.574064807167,
    "total_throughput": 13969.74845106591,
    "itl": 97.95805384523426,
    "ttft": 1646803.6326251854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.250328142852528,
    "arrivals": 277607,
    "finished_requests": 108016,
    "scheduler_time": 282.1610568251111
}
#Debug simulation 
Total elapsed time: 113.12801401270553. Arrivals time: 0.512964392080903 Scheduler time: 112.37389095174149 Scheduler overhead time: 0.09672876494005322 Adapter cache time: 0.019131748005747795 Engine time: 0.08942689979448915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 113.14177282527089,
    "estimated_duration": 3600.1136204333598,
    "input_throughput": 7417.168682799988,
    "output_throughput": 6552.569026185452,
    "total_throughput": 13969.73770898544,
    "itl": 97.95812528314598,
    "ttft": 1646804.7979161395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2530231115594577,
    "arrivals": 277607,
    "finished_requests": 108016,
    "scheduler_time": 282.1611301746848
}
#Debug simulation 
Total elapsed time: 113.14193677296862. Arrivals time: 0.5016047293320298 Scheduler time: 112.400701737497 Scheduler overhead time: 0.09595519863069057 Adapter cache time: 0.01942611439153552 Engine time: 0.08883485617116094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 113.36426524678245,
    "estimated_duration": 3600.057460160475,
    "input_throughput": 7417.284389346862,
    "output_throughput": 6552.671245127421,
    "total_throughput": 13969.955634474281,
    "itl": 97.95672258638801,
    "ttft": 1646781.278957099,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.198436537410598,
    "arrivals": 277607,
    "finished_requests": 108016,
    "scheduler_time": 282.15981310503247
}
#Debug simulation 
Total elapsed time: 113.36442599166185. Arrivals time: 0.5023197489790618 Scheduler time: 112.62110421340913 Scheduler overhead time: 0.0966762462630868 Adapter cache time: 0.01937997154891491 Engine time: 0.08888601837679744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 117.19475452788174,
    "estimated_duration": 3600.074212814631,
    "input_throughput": 7462.025895015411,
    "output_throughput": 6587.411702676781,
    "total_throughput": 14049.437597692191,
    "itl": 99.36667776036472,
    "ttft": 1644197.9523364252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2503954818472311,
    "arrivals": 277607,
    "finished_requests": 108500,
    "scheduler_time": 280.8477022928623
}
#Debug simulation 
Total elapsed time: 117.19491148507223. Arrivals time: 0.506017763633281 Scheduler time: 116.44670933997259 Scheduler overhead time: 0.09698460204526782 Adapter cache time: 0.019692689646035433 Engine time: 0.0895619303919375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 113.5117492643185,
    "estimated_duration": 3600.005972740625,
    "input_throughput": 7417.390471625166,
    "output_throughput": 6552.764961676252,
    "total_throughput": 13970.155433301417,
    "itl": 97.95572022929937,
    "ttft": 1646759.3355974024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 384,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1481793132424316,
    "arrivals": 277607,
    "finished_requests": 108016,
    "scheduler_time": 282.158682947914
}
#Debug simulation 
Total elapsed time: 113.51200515311211. Arrivals time: 0.5050202803686261 Scheduler time: 112.76824712846428 Scheduler overhead time: 0.09544722642749548 Adapter cache time: 0.018975652754306793 Engine time: 0.08807979803532362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 117.00275644380599,
    "estimated_duration": 3600.0153305598924,
    "input_throughput": 7461.949056706409,
    "output_throughput": 6587.360281132966,
    "total_throughput": 14049.309337839375,
    "itl": 99.36685490120946,
    "ttft": 1644229.1226297882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.265737443789842,
    "arrivals": 277607,
    "finished_requests": 108498,
    "scheduler_time": 280.84119551226365
}
#Debug simulation 
Total elapsed time: 117.00291387969628. Arrivals time: 0.5176195623353124 Scheduler time: 116.24480167822912 Scheduler overhead time: 0.09651353815570474 Adapter cache time: 0.019338304176926613 Engine time: 0.08870567847043276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 115.35335042607039,
    "estimated_duration": 3600.062482746315,
    "input_throughput": 7503.924203946555,
    "output_throughput": 6632.070725002041,
    "total_throughput": 14135.994928948596,
    "itl": 101.99098251437437,
    "ttft": 1644272.5058357627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.135440952999055,
    "arrivals": 276925,
    "finished_requests": 109183,
    "scheduler_time": 277.5487967158486
}
#Debug simulation 
Total elapsed time: 115.35351365199313. Arrivals time: 0.5124285989440978 Scheduler time: 114.59984675468877 Scheduler overhead time: 0.09546997724100947 Adapter cache time: 0.019470303785055876 Engine time: 0.09055896429345012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 114.92831349000335,
    "estimated_duration": 3600.121201464839,
    "input_throughput": 7453.988768233999,
    "output_throughput": 6584.102499203457,
    "total_throughput": 14038.091267437456,
    "itl": 100.88451257745746,
    "ttft": 1645822.7162817977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2124045627214997,
    "arrivals": 276925,
    "finished_requests": 108490,
    "scheduler_time": 280.158341621322
}
#Debug simulation 
Total elapsed time: 114.92847005696967. Arrivals time: 0.5171271585859358 Scheduler time: 114.16740654362366 Scheduler overhead time: 0.09593467600643635 Adapter cache time: 0.019191164523363113 Engine time: 0.09262359933927655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 114.73661206290126,
    "estimated_duration": 3600.123685924043,
    "input_throughput": 7453.983624207677,
    "output_throughput": 6584.097955488996,
    "total_throughput": 14038.081579696673,
    "itl": 100.88455374462471,
    "ttft": 1645823.660690436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.21481464790181,
    "arrivals": 276925,
    "finished_requests": 108490,
    "scheduler_time": 280.158415995331
}
#Debug simulation 
Total elapsed time: 114.7367696869187. Arrivals time: 0.5127703594043851 Scheduler time: 113.98228983720765 Scheduler overhead time: 0.09668031707406044 Adapter cache time: 0.019324322696775198 Engine time: 0.0898464908823371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 114.73737438442186,
    "estimated_duration": 3600.0859963985663,
    "input_throughput": 7503.875192710593,
    "output_throughput": 6632.02740820214,
    "total_throughput": 14135.902600912732,
    "itl": 101.99167419418583,
    "ttft": 1644281.9681748496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1587486929399897,
    "arrivals": 276925,
    "finished_requests": 109183,
    "scheduler_time": 277.54890258953407
}
#Debug simulation 
Total elapsed time: 114.73753847414628. Arrivals time: 0.5140338046476245 Scheduler time: 113.98308896971866 Scheduler overhead time: 0.09611953143030405 Adapter cache time: 0.019243473652750254 Engine time: 0.08968688314780593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 115.45146278478205,
    "estimated_duration": 3600.005139730912,
    "input_throughput": 7503.677342532669,
    "output_throughput": 6631.823865058299,
    "total_throughput": 14135.501207590969,
    "itl": 101.9927704786454,
    "ttft": 1644249.682735306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2287536024116028,
    "arrivals": 276925,
    "finished_requests": 109177,
    "scheduler_time": 277.5368771962917
}
#Debug simulation 
Total elapsed time: 115.45161962788552. Arrivals time: 0.5190626070834696 Scheduler time: 114.69337344216183 Scheduler overhead time: 0.0956715913489461 Adapter cache time: 0.019148786552250385 Engine time: 0.08920820290222764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 115.4419152312912,
    "estimated_duration": 3600.035759855514,
    "input_throughput": 7503.979905211892,
    "output_throughput": 6632.119954541299,
    "total_throughput": 14136.09985975319,
    "itl": 101.99034235027733,
    "ttft": 1644261.2631834806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1093086594087047,
    "arrivals": 276925,
    "finished_requests": 109183,
    "scheduler_time": 277.5482061185779
}
#Debug simulation 
Total elapsed time: 115.44207369303331. Arrivals time: 0.5113682323135436 Scheduler time: 114.69125392707065 Scheduler overhead time: 0.09577605221420527 Adapter cache time: 0.01914348965510726 Engine time: 0.08943488914519548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 115.36967003531754,
    "estimated_duration": 3600.0205305028417,
    "input_throughput": 7503.6452628860025,
    "output_throughput": 6631.7955127509385,
    "total_throughput": 14135.44077563694,
    "itl": 101.99318870444914,
    "ttft": 1644256.0243130452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2439698105677992,
    "arrivals": 276925,
    "finished_requests": 109177,
    "scheduler_time": 277.53705176008157
}
#Debug simulation 
Total elapsed time: 115.36994228838012. Arrivals time: 0.5190291162580252 Scheduler time: 114.60942864743993 Scheduler overhead time: 0.09623035881668329 Adapter cache time: 0.01934967329725623 Engine time: 0.08950528735294938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 121.14267101883888,
    "estimated_duration": 3600.018498763845,
    "input_throughput": 7047.193232121286,
    "output_throughput": 6279.18328968644,
    "total_throughput": 13326.376521807726,
    "itl": 93.90269875078924,
    "ttft": 1540029.031987807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1109570510475921,
    "arrivals": 218234,
    "finished_requests": 102702,
    "scheduler_time": 291.51700922368326
}
#Debug simulation 
Total elapsed time: 121.14282474201173. Arrivals time: 0.4812278589233756 Scheduler time: 120.40968240099028 Scheduler overhead time: 0.10091399867087603 Adapter cache time: 0.01969522750005126 Engine time: 0.09330818941816688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.6589079881087,
    "estimated_duration": 3600.0590821018272,
    "input_throughput": 7061.227446622493,
    "output_throughput": 6292.869501123152,
    "total_throughput": 13354.096947745646,
    "itl": 95.0839823921248,
    "ttft": 1546405.499805959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.222600499561528,
    "arrivals": 218234,
    "finished_requests": 102986,
    "scheduler_time": 290.29915642110956
}
#Debug simulation 
Total elapsed time: 119.65907203033566. Arrivals time: 0.48316705878823996 Scheduler time: 118.92288594739512 Scheduler overhead time: 0.101853396743536 Adapter cache time: 0.01994031574577093 Engine time: 0.09346600342541933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.62105477089062,
    "estimated_duration": 3600.0077211491684,
    "input_throughput": 7045.996832446504,
    "output_throughput": 6274.22404327229,
    "total_throughput": 13320.220875718795,
    "itl": 93.96551761064774,
    "ttft": 1547917.7184729858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1816814794577724,
    "arrivals": 218234,
    "finished_requests": 102712,
    "scheduler_time": 291.4835791586146
}
#Debug simulation 
Total elapsed time: 121.62121279165149. Arrivals time: 0.4830572954379022 Scheduler time: 120.88429242465645 Scheduler overhead time: 0.10212366748601198 Adapter cache time: 0.019763216376304626 Engine time: 0.09414148237556219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 121.31079561309889,
    "estimated_duration": 3600.04094275499,
    "input_throughput": 7047.149297303596,
    "output_throughput": 6279.144142927725,
    "total_throughput": 13326.29344023132,
    "itl": 93.90312908081327,
    "ttft": 1540038.158018638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1327853139885713,
    "arrivals": 218234,
    "finished_requests": 102702,
    "scheduler_time": 291.5174388257979
}
#Debug simulation 
Total elapsed time: 121.31095550209284. Arrivals time: 0.4855420384556055 Scheduler time: 120.57243299437687 Scheduler overhead time: 0.10130349406972528 Adapter cache time: 0.01995708793401718 Engine time: 0.09382086340337992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 118.77306406619027,
    "estimated_duration": 3600.0666760088934,
    "input_throughput": 7049.664987909603,
    "output_throughput": 6270.867189889855,
    "total_throughput": 13320.532177799458,
    "itl": 94.62744337941976,
    "ttft": 1563719.9820012106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1512989223375967,
    "arrivals": 218234,
    "finished_requests": 102786,
    "scheduler_time": 291.1981646024618
}
#Debug simulation 
Total elapsed time: 118.77321755513549. Arrivals time: 0.4864809983409941 Scheduler time: 118.03270882321522 Scheduler overhead time: 0.10141972359269857 Adapter cache time: 0.020386415999382734 Engine time: 0.09468484809622169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 121.12161412090063,
    "estimated_duration": 3600.0422751072188,
    "input_throughput": 7047.506684972019,
    "output_throughput": 6279.379594042887,
    "total_throughput": 13326.886279014907,
    "itl": 93.90097611544952,
    "ttft": 1539978.2660227923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0853882570494882,
    "arrivals": 218234,
    "finished_requests": 102705,
    "scheduler_time": 291.522346068464
}
#Debug simulation 
Total elapsed time: 121.12176947016269. Arrivals time: 0.48226628825068474 Scheduler time: 120.38572294684127 Scheduler overhead time: 0.10194395808503032 Adapter cache time: 0.020047593396157026 Engine time: 0.09383705724030733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.4610890247859,
    "estimated_duration": 3600.0127147930975,
    "input_throughput": 7049.4464910406705,
    "output_throughput": 6270.684241540914,
    "total_throughput": 13320.130732581585,
    "itl": 94.62973174072951,
    "ttft": 1563683.0999245087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 348,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1656348539888897,
    "arrivals": 218234,
    "finished_requests": 102781,
    "scheduler_time": 291.191882692017
}
#Debug simulation 
Total elapsed time: 118.46125339576975. Arrivals time: 0.4811194301582873 Scheduler time: 117.72438794188201 Scheduler overhead time: 0.10317823709920049 Adapter cache time: 0.01994751999154687 Engine time: 0.09455708041787148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 117.20653017889708,
    "estimated_duration": 3600.0643108865706,
    "input_throughput": 7205.602667028947,
    "output_throughput": 6381.388779786117,
    "total_throughput": 13586.991446815064,
    "itl": 97.09716628325155,
    "ttft": 1523899.0153818245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1660458304383836,
    "arrivals": 212464,
    "finished_requests": 104507,
    "scheduler_time": 283.6664021946874
}
#Debug simulation 
Total elapsed time: 117.20678707119077. Arrivals time: 0.48437557462602854 Scheduler time: 116.47354414407164 Scheduler overhead time: 0.09955338155850768 Adapter cache time: 0.019834006670862436 Engine time: 0.09242109581828117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 117.4116415628232,
    "estimated_duration": 3600.0360592210463,
    "input_throughput": 7205.450882514969,
    "output_throughput": 6381.331915039418,
    "total_throughput": 13586.782797554388,
    "itl": 97.09875508160745,
    "ttft": 1523971.2332792764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2429923732415828,
    "arrivals": 212464,
    "finished_requests": 104504,
    "scheduler_time": 283.6598464781916
}
#Debug simulation 
Total elapsed time: 117.41179386107251. Arrivals time: 0.4877322022803128 Scheduler time: 116.67214311938733 Scheduler overhead time: 0.10095239477232099 Adapter cache time: 0.020153232850134373 Engine time: 0.09330447437241673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 117.52100823167711,
    "estimated_duration": 3600.0383395356475,
    "input_throughput": 7205.446318481671,
    "output_throughput": 6381.327873014593,
    "total_throughput": 13586.774191496264,
    "itl": 97.09881153851828,
    "ttft": 1523972.0906306386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2452415449917387,
    "arrivals": 212464,
    "finished_requests": 104504,
    "scheduler_time": 283.65987762102736
}
#Debug simulation 
Total elapsed time: 117.52117218775675. Arrivals time: 0.4902441552840173 Scheduler time: 116.78027136670426 Scheduler overhead time: 0.10040157940238714 Adapter cache time: 0.020009550731629133 Engine time: 0.09275243245065212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 117.45186734711751,
    "estimated_duration": 3600.089577012071,
    "input_throughput": 7205.552096714682,
    "output_throughput": 6381.343993964451,
    "total_throughput": 13586.896090679133,
    "itl": 97.09769791717589,
    "ttft": 1523908.9635902883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1906921724812132,
    "arrivals": 212464,
    "finished_requests": 104507,
    "scheduler_time": 283.6670219780962
}
#Debug simulation 
Total elapsed time: 117.4520198488608. Arrivals time: 0.48696055449545383 Scheduler time: 116.7145510208793 Scheduler overhead time: 0.10022679064422846 Adapter cache time: 0.020070250146090984 Engine time: 0.09290519403293729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 117.53815954830498,
    "estimated_duration": 3600.0547354132495,
    "input_throughput": 7205.413502420642,
    "output_throughput": 6381.298810270153,
    "total_throughput": 13586.712312690795,
    "itl": 97.09916691657749,
    "ttft": 1523978.354371953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2613380296528385,
    "arrivals": 212464,
    "finished_requests": 104504,
    "scheduler_time": 283.66017701398084
}
#Debug simulation 
Total elapsed time: 117.53831723798066. Arrivals time: 0.49390599876642227 Scheduler time: 116.79310459457338 Scheduler overhead time: 0.10064795101061463 Adapter cache time: 0.020087788347154856 Engine time: 0.09286909038200974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 117.10714881913736,
    "estimated_duration": 3600.0369344401256,
    "input_throughput": 7205.657461965529,
    "output_throughput": 6381.437306996075,
    "total_throughput": 13587.094768961604,
    "itl": 97.09647934209632,
    "ttft": 1523888.5003004847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1392091623577254,
    "arrivals": 212464,
    "finished_requests": 104507,
    "scheduler_time": 283.66586241625885
}
#Debug simulation 
Total elapsed time: 117.10731476312503. Arrivals time: 0.4825237658806145 Scheduler time: 116.37356809992343 Scheduler overhead time: 0.10105302045121789 Adapter cache time: 0.019934797659516335 Engine time: 0.0931744109839201 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.02923823473975,
    "estimated_duration": 3600.0709643487053,
    "input_throughput": 7205.381020785746,
    "output_throughput": 6381.270043702066,
    "total_throughput": 13586.651064487813,
    "itl": 97.09929734001338,
    "ttft": 1523984.6508050046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.277183006741109,
    "arrivals": 212464,
    "finished_requests": 104504,
    "scheduler_time": 283.66056097236384
}
#Debug simulation 
Total elapsed time: 118.02939624898136. Arrivals time: 0.4967254991643131 Scheduler time: 117.28134285379201 Scheduler overhead time: 0.10134619753807783 Adapter cache time: 0.019661312457174063 Engine time: 0.09304917324334383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 118.53295753197744,
    "estimated_duration": 3600.0767578951245,
    "input_throughput": 7219.321072252852,
    "output_throughput": 6388.498786744479,
    "total_throughput": 13607.81985899733,
    "itl": 98.72668200377962,
    "ttft": 1515667.729283725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0956546123279278,
    "arrivals": 209539,
    "finished_requests": 105142,
    "scheduler_time": 281.1539774020256
}
#Debug simulation 
Total elapsed time: 118.53311428520828. Arrivals time: 0.48190139373764396 Scheduler time: 117.79909921158105 Scheduler overhead time: 0.10117280203849077 Adapter cache time: 0.01976214163005352 Engine time: 0.09351612208411098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.50633128127083,
    "estimated_duration": 3600.037926826708,
    "input_throughput": 7219.313665094909,
    "output_throughput": 6388.469362674876,
    "total_throughput": 13607.783027769785,
    "itl": 98.72812980839706,
    "ttft": 1515727.1934411973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1668665007269066,
    "arrivals": 209539,
    "finished_requests": 105140,
    "scheduler_time": 281.1467654229551
}
#Debug simulation 
Total elapsed time: 118.50659057637677. Arrivals time: 0.49409466329962015 Scheduler time: 117.75984771223739 Scheduler overhead time: 0.10153811518102884 Adapter cache time: 0.019582901615649462 Engine time: 0.09386805817484856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.31959194643423,
    "estimated_duration": 3600.040289899814,
    "input_throughput": 7219.3089263240645,
    "output_throughput": 6388.465169271768,
    "total_throughput": 13607.774095595832,
    "itl": 98.72817375044391,
    "ttft": 1515728.0950221622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1691701565682953,
    "arrivals": 209539,
    "finished_requests": 105140,
    "scheduler_time": 281.1468248402076
}
#Debug simulation 
Total elapsed time: 118.3197459471412. Arrivals time: 0.48648039204999804 Scheduler time: 117.5781638356857 Scheduler overhead time: 0.10226719314232469 Adapter cache time: 0.020107742864638567 Engine time: 0.0941889495588839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 118.32098472025245,
    "estimated_duration": 3600.100050969155,
    "input_throughput": 7219.274362389847,
    "output_throughput": 6388.457452400135,
    "total_throughput": 13607.73181478998,
    "itl": 98.72717641934581,
    "ttft": 1515677.398062708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.118243657832501,
    "arrivals": 209539,
    "finished_requests": 105142,
    "scheduler_time": 281.1544813533473
}
#Debug simulation 
Total elapsed time: 118.32114742929116. Arrivals time: 0.4998320215381682 Scheduler time: 117.56819845037535 Scheduler overhead time: 0.10170544870197773 Adapter cache time: 0.020029453560709953 Engine time: 0.09390110522508621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 118.41006065672264,
    "estimated_duration": 3600.0556526239143,
    "input_throughput": 7219.278118952754,
    "output_throughput": 6388.437907407705,
    "total_throughput": 13607.716026360458,
    "itl": 98.72855553266686,
    "ttft": 1515734.126255251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1841348571516621,
    "arrivals": 209539,
    "finished_requests": 105140,
    "scheduler_time": 281.147034287734
}
#Debug simulation 
Total elapsed time: 118.41021468769759. Arrivals time: 0.48387686582282186 Scheduler time: 117.67112995591015 Scheduler overhead time: 0.10238690860569477 Adapter cache time: 0.019760932307690382 Engine time: 0.09474222688004375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 118.71649612905458,
    "estimated_duration": 3600.05111290116,
    "input_throughput": 7219.372499146392,
    "output_throughput": 6388.544295268577,
    "total_throughput": 13607.91679441497,
    "itl": 98.72617224175332,
    "ttft": 1515657.695510658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0704380055749778,
    "arrivals": 209539,
    "finished_requests": 105142,
    "scheduler_time": 281.1535449314918
}
#Debug simulation 
Total elapsed time: 118.7166621168144. Arrivals time: 0.5004746061749756 Scheduler time: 117.96400428703055 Scheduler overhead time: 0.10178934223949909 Adapter cache time: 0.01966775069013238 Engine time: 0.09366630436852574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.27867494430393,
    "estimated_duration": 3600.070859039973,
    "input_throughput": 7219.247625289984,
    "output_throughput": 6388.41092314862,
    "total_throughput": 13607.658548438605,
    "itl": 98.72889623342704,
    "ttft": 1515740.7872507193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1988480501621994,
    "arrivals": 209539,
    "finished_requests": 105140,
    "scheduler_time": 281.14734628466624
}
#Debug simulation 
Total elapsed time: 118.27882940741256. Arrivals time: 0.4832933875732124 Scheduler time: 117.54197119548917 Scheduler overhead time: 0.10149342939257622 Adapter cache time: 0.019910170696675777 Engine time: 0.09400549996644258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 120.76834305003285,
    "estimated_duration": 3600.0064671171936,
    "input_throughput": 7414.673624565755,
    "output_throughput": 6521.811617411099,
    "total_throughput": 13936.485241976854,
    "itl": 99.3065917567973,
    "ttft": 1495949.3673242973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.178287781414115,
    "arrivals": 208072,
    "finished_requests": 107849,
    "scheduler_time": 272.9637763506584
}
#Debug simulation 
Total elapsed time: 120.76850070198998. Arrivals time: 0.5011381078511477 Scheduler time: 120.01803542906418 Scheduler overhead time: 0.0995491505600512 Adapter cache time: 0.019856104627251625 Engine time: 0.09240252710878849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 120.01924115186557,
    "estimated_duration": 3600.09475232778,
    "input_throughput": 7315.318849030723,
    "output_throughput": 6457.7822528053475,
    "total_throughput": 13773.10110183607,
    "itl": 97.6629233382443,
    "ttft": 1499180.1517679954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.245573828218045,
    "arrivals": 208072,
    "finished_requests": 106480,
    "scheduler_time": 277.6841383153264
}
#Debug simulation 
Total elapsed time: 120.01940027670935. Arrivals time: 0.48227106872946024 Scheduler time: 119.28850013110787 Scheduler overhead time: 0.1007496495731175 Adapter cache time: 0.019728252198547125 Engine time: 0.09135965025052428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.7704273541458,
    "estimated_duration": 3600.09721912977,
    "input_throughput": 7315.313836543004,
    "output_throughput": 6457.777827905367,
    "total_throughput": 13773.091664448371,
    "itl": 97.66297450500782,
    "ttft": 1499181.160327619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2479478163458477,
    "arrivals": 208072,
    "finished_requests": 106480,
    "scheduler_time": 277.68423112917236
}
#Debug simulation 
Total elapsed time: 119.77067105192691. Arrivals time: 0.4903129432350397 Scheduler time: 119.02839960344136 Scheduler overhead time: 0.09994741762056947 Adapter cache time: 0.01984645240008831 Engine time: 0.0947725591249764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 120.49094121903181,
    "estimated_duration": 3600.0293977607107,
    "input_throughput": 7414.62639627429,
    "output_throughput": 6521.770076267747,
    "total_throughput": 13936.396472542036,
    "itl": 99.30716248787478,
    "ttft": 1495959.0647548912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2010179923870596,
    "arrivals": 208072,
    "finished_requests": 107849,
    "scheduler_time": 272.9639767831535
}
#Debug simulation 
Total elapsed time: 120.49110203003511. Arrivals time: 0.48763526091352105 Scheduler time: 119.75447571650147 Scheduler overhead time: 0.10031029675155878 Adapter cache time: 0.019798978231847286 Engine time: 0.09211905859410763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 120.72832530271262,
    "estimated_duration": 3600.113895624667,
    "input_throughput": 7315.27995045012,
    "output_throughput": 6457.74791410205,
    "total_throughput": 13773.02786455217,
    "itl": 97.6633582927753,
    "ttft": 1499187.6260717912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.264421562366192,
    "arrivals": 208072,
    "finished_requests": 106480,
    "scheduler_time": 277.6846544395681
}
#Debug simulation 
Total elapsed time: 120.72849864885211. Arrivals time: 0.4931762991473079 Scheduler time: 119.9827947425656 Scheduler overhead time: 0.10210957517847419 Adapter cache time: 0.020099991466850042 Engine time: 0.09341659024357796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 120.1011503720656,
    "estimated_duration": 3600.0150897314147,
    "input_throughput": 7315.480725377968,
    "output_throughput": 6457.925153234428,
    "total_throughput": 13773.405878612397,
    "itl": 97.66033740734265,
    "ttft": 1499157.4393366403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1421992126526275,
    "arrivals": 208072,
    "finished_requests": 106480,
    "scheduler_time": 277.68675904221203
}
#Debug simulation 
Total elapsed time: 120.10131120309234. Arrivals time: 0.48663936741650105 Scheduler time: 119.36624801438302 Scheduler overhead time: 0.10031794849783182 Adapter cache time: 0.019518021028488874 Engine time: 0.09165011718869209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.94718796433881,
    "estimated_duration": 3600.012929055855,
    "input_throughput": 7315.178450458118,
    "output_throughput": 6457.857362778182,
    "total_throughput": 13773.0358132363,
    "itl": 97.66407909166477,
    "ttft": 1499199.6095925933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 382,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2797635243088035,
    "arrivals": 208072,
    "finished_requests": 106477,
    "scheduler_time": 277.6763788927775
}
#Debug simulation 
Total elapsed time: 119.94735266827047. Arrivals time: 0.48276428040117025 Scheduler time: 119.21469750953838 Scheduler overhead time: 0.10005774488672614 Adapter cache time: 0.020188514608889818 Engine time: 0.09247897146269679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 115.30363821797073,
    "estimated_duration": 3600.0857172932947,
    "input_throughput": 7348.564472484393,
    "output_throughput": 6510.088048020392,
    "total_throughput": 13858.652520504786,
    "itl": 100.24170759259067,
    "ttft": 1494326.7079303053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1446224162308536,
    "arrivals": 207395,
    "finished_requests": 106947,
    "scheduler_time": 273.43329141194624
}
#Debug simulation 
Total elapsed time: 115.30380012886599. Arrivals time: 0.4809112693183124 Scheduler time: 114.5768339317292 Scheduler overhead time: 0.09955250984057784 Adapter cache time: 0.019163094460964203 Engine time: 0.09115053713321686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 120.01836490025744,
    "estimated_duration": 3600.01647581806,
    "input_throughput": 7213.083932928186,
    "output_throughput": 6404.17957941734,
    "total_throughput": 13617.263512345527,
    "itl": 97.45908482463831,
    "ttft": 1496410.7972875938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.131524375572341,
    "arrivals": 207395,
    "finished_requests": 104921,
    "scheduler_time": 280.9435758359371
}
#Debug simulation 
Total elapsed time: 120.01852340297773. Arrivals time: 0.47903962433338165 Scheduler time: 119.28786459797993 Scheduler overhead time: 0.10159133654087782 Adapter cache time: 0.01985869323834777 Engine time: 0.09319311147555709 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.87127604708076,
    "estimated_duration": 3600.0192125586213,
    "input_throughput": 7213.078449529847,
    "output_throughput": 6404.174710949429,
    "total_throughput": 13617.253160479275,
    "itl": 97.45913647002874,
    "ttft": 1496412.2667509376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1336679642647567,
    "arrivals": 207395,
    "finished_requests": 104921,
    "scheduler_time": 280.9436687948898
}
#Debug simulation 
Total elapsed time: 119.87143870117143. Arrivals time: 0.47776186000555754 Scheduler time: 119.14173839380965 Scheduler overhead time: 0.10131583083420992 Adapter cache time: 0.0196142322383821 Engine time: 0.09325144765898585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 120.07797247404233,
    "estimated_duration": 3600.0759195048513,
    "input_throughput": 7213.2237154519835,
    "output_throughput": 6404.345773677768,
    "total_throughput": 13617.569489129752,
    "itl": 97.45741585184132,
    "ttft": 1496392.1896218162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0857616999070183,
    "arrivals": 207395,
    "finished_requests": 104925,
    "scheduler_time": 280.95085311104185
}
#Debug simulation 
Total elapsed time: 120.07822817796841. Arrivals time: 0.49235522001981735 Scheduler time: 119.3356465427205 Scheduler overhead time: 0.10065319342538714 Adapter cache time: 0.02009136090055108 Engine time: 0.09228380490094423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 120.14218457089737,
    "estimated_duration": 3600.0336725943876,
    "input_throughput": 7213.0494771974045,
    "output_throughput": 6404.1489876913165,
    "total_throughput": 13617.198464888721,
    "itl": 97.45946978780819,
    "ttft": 1496417.596101346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.14775238834322,
    "arrivals": 207395,
    "finished_requests": 104921,
    "scheduler_time": 280.9439443680089
}
#Debug simulation 
Total elapsed time: 120.14233818696812. Arrivals time: 0.47638455871492624 Scheduler time: 119.41532856225967 Scheduler overhead time: 0.100682457908988 Adapter cache time: 0.01925707934424281 Engine time: 0.09341042814776301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 115.49766059499234,
    "estimated_duration": 3600.0586618552766,
    "input_throughput": 7348.619698981871,
    "output_throughput": 6510.1369731352925,
    "total_throughput": 13858.756672117162,
    "itl": 100.24097457181902,
    "ttft": 1494316.1659052996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.118278810293411,
    "arrivals": 207395,
    "finished_requests": 106947,
    "scheduler_time": 273.4327796569607
}
#Debug simulation 
Total elapsed time: 115.49782087188214. Arrivals time: 0.4885329604148865 Scheduler time: 114.76132019376382 Scheduler overhead time: 0.09980099275708199 Adapter cache time: 0.0196985243819654 Engine time: 0.091607925016433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 122.79482070822269,
    "estimated_duration": 3600.049029817751,
    "input_throughput": 7213.018707502038,
    "output_throughput": 6404.121668633814,
    "total_throughput": 13617.140376135852,
    "itl": 97.45981626394644,
    "ttft": 1496423.5423006224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 347,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.162591335140172,
    "arrivals": 207395,
    "finished_requests": 104921,
    "scheduler_time": 280.9443019028056
}
#Debug simulation 
Total elapsed time: 122.79498932696879. Arrivals time: 0.48199570504948497 Scheduler time: 122.05772456107661 Scheduler overhead time: 0.10316387889906764 Adapter cache time: 0.019481536466628313 Engine time: 0.09495958499610424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 120.13347563706338,
    "estimated_duration": 3600.013368811686,
    "input_throughput": 7209.001284505384,
    "output_throughput": 6383.19435118799,
    "total_throughput": 13592.195635693373,
    "itl": 97.77659961462432,
    "ttft": 1463176.3834114107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1170780265354578,
    "arrivals": 200999,
    "finished_requests": 104515,
    "scheduler_time": 281.7656844751876
}
#Debug simulation 
Total elapsed time: 120.13362914230675. Arrivals time: 0.4744703248143196 Scheduler time: 119.40560352802277 Scheduler overhead time: 0.10206281580030918 Adapter cache time: 0.019760059658437967 Engine time: 0.0942310350947082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.22940595308319,
    "estimated_duration": 3600.0139890446876,
    "input_throughput": 7208.951987124584,
    "output_throughput": 6383.161862684293,
    "total_throughput": 13592.113849808877,
    "itl": 97.77847618514066,
    "ttft": 1463201.9056215247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.189839829383424,
    "arrivals": 200999,
    "finished_requests": 104514,
    "scheduler_time": 281.7565686963158
}
#Debug simulation 
Total elapsed time: 119.22955884691328. Arrivals time: 0.4803230199031532 Scheduler time: 118.49472054420039 Scheduler overhead time: 0.10195874748751521 Adapter cache time: 0.02034270390868187 Engine time: 0.09483816474676132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.55406042607501,
    "estimated_duration": 3600.0160559741776,
    "input_throughput": 7208.947848144307,
    "output_throughput": 6383.1581978268905,
    "total_throughput": 13592.106045971197,
    "itl": 97.77852736432554,
    "ttft": 1463202.744410295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1921610259823567,
    "arrivals": 200999,
    "finished_requests": 104514,
    "scheduler_time": 281.7566145449345
}
#Debug simulation 
Total elapsed time: 119.55422305082902. Arrivals time: 0.484627366065979 Scheduler time: 118.81402090005577 Scheduler overhead time: 0.10273273987695575 Adapter cache time: 0.020102146081626415 Engine time: 0.09440462198108435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 119.8868810981512,
    "estimated_duration": 3600.03677478587,
    "input_throughput": 7208.954414512516,
    "output_throughput": 6383.152850255766,
    "total_throughput": 13592.107264768283,
    "itl": 97.77692690758654,
    "ttft": 1463185.3545025336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1403997958521372,
    "arrivals": 200999,
    "finished_requests": 104515,
    "scheduler_time": 281.76626887291263
}
#Debug simulation 
Total elapsed time: 119.88703613495454. Arrivals time: 0.4818376796320081 Scheduler time: 119.15046069817618 Scheduler overhead time: 0.10216678492724895 Adapter cache time: 0.020004823803901672 Engine time: 0.09438666934147477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_192_slots_16_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 119.71334567293525,
    "estimated_duration": 3600.031589962183,
    "input_throughput": 7208.916741831318,
    "output_throughput": 6383.130654762224,
    "total_throughput": 13592.047396593543,
    "itl": 97.77882382648639,
    "ttft": 1463208.7747900249,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2073772341385531,
    "arrivals": 200999,
    "finished_requests": 104514,
    "scheduler_time": 281.75693232479136
}
#Debug simulation 
Total elapsed time: 119.7136054430157. Arrivals time: 0.4745886982418597 Scheduler time: 118.98531238781288 Scheduler overhead time: 0.10209279134869576 Adapter cache time: 0.01986395614221692 Engine time: 0.09414507215842605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_192_slots_16_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_192_slots_16_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 120.02691521076486,
    "estimated_duration": 3600.034305563954,
    "input_throughput": 7208.959636826149,
    "output_throughput": 6383.270838415005,
    "total_throughput": 13592.230475241153,
    "itl": 97.77566557901093,
    "ttft": 1463152.5897420642,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0913683576392923,
    "arrivals": 200999,
    "finished_requests": 104516,
    "scheduler_time": 281.77080553389237
}
#Debug simulation 
Total elapsed time: 120.02706853393465. Arrivals time: 0.47267640801146626 Scheduler time: 119.29915404552594 Scheduler overhead time: 0.10273341741412878 Adapter cache time: 0.019767496269196272 Engine time: 0.09482592809945345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_192_slots_16_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_192_slots_16_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.45950838597491,
    "estimated_duration": 3600.04767305567,
    "input_throughput": 7208.884536235051,
    "output_throughput": 6383.102138337892,
    "total_throughput": 13591.986674572943,
    "itl": 97.77910733553954,
    "ttft": 1463215.372182346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2224676885083348,
    "arrivals": 200999,
    "finished_requests": 104514,
    "scheduler_time": 281.75737148571005
}
#Debug simulation 
Total elapsed time: 119.45966362720355. Arrivals time: 0.48937190510332584 Scheduler time: 118.71367130894214 Scheduler overhead time: 0.10367333656176925 Adapter cache time: 0.020260063000023365 Engine time: 0.09493878949433565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 122.93719436321408,
    "estimated_duration": 3600.0527257932226,
    "input_throughput": 7225.147791208766,
    "output_throughput": 6409.215852502845,
    "total_throughput": 13634.363643711611,
    "itl": 99.25961745176419,
    "ttft": 1470338.7831611852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9946585167781437,
    "arrivals": 198139,
    "finished_requests": 105186,
    "scheduler_time": 278.0520121740806
}
#Debug simulation 
Total elapsed time: 122.93735227780417. Arrivals time: 0.4779767198488116 Scheduler time: 122.2037569694221 Scheduler overhead time: 0.10338652320206165 Adapter cache time: 0.019456472247838974 Engine time: 0.09460533829405904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.33675715420395,
    "estimated_duration": 3600.0112121908383,
    "input_throughput": 7198.000637400889,
    "output_throughput": 6384.037616931089,
    "total_throughput": 13582.038254331977,
    "itl": 98.62036824685455,
    "ttft": 1476995.7697593463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.180869678498717,
    "arrivals": 198139,
    "finished_requests": 104937,
    "scheduler_time": 278.60846846629346
}
#Debug simulation 
Total elapsed time: 119.3369158403948. Arrivals time: 0.47381623554974794 Scheduler time: 118.60863475967199 Scheduler overhead time: 0.10264739487320185 Adapter cache time: 0.020178052131086588 Engine time: 0.09414306143298745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.13129679812118,
    "estimated_duration": 3600.0135116910924,
    "input_throughput": 7197.99603969473,
    "output_throughput": 6384.033539141916,
    "total_throughput": 13582.029578836646,
    "itl": 98.62041558587238,
    "ttft": 1476996.7524135709,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.183030469436205,
    "arrivals": 198139,
    "finished_requests": 104937,
    "scheduler_time": 278.6085071370176
}
#Debug simulation 
Total elapsed time: 119.13145654601976. Arrivals time: 0.48451565066352487 Scheduler time: 118.39396920800209 Scheduler overhead time: 0.10165364434942603 Adapter cache time: 0.020008595660328865 Engine time: 0.09434149879962206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 123.12195339798927,
    "estimated_duration": 3600.003608175754,
    "input_throughput": 7220.321374392078,
    "output_throughput": 6423.919117047443,
    "total_throughput": 13644.240491439521,
    "itl": 99.16259304088382,
    "ttft": 1466647.6531861285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0593897256371592,
    "arrivals": 198139,
    "finished_requests": 105145,
    "scheduler_time": 278.4862039923436
}
#Debug simulation 
Total elapsed time: 123.12210580566898. Arrivals time: 0.47912954073399305 Scheduler time: 122.38721265923232 Scheduler overhead time: 0.10355454683303833 Adapter cache time: 0.019851908087730408 Engine time: 0.09453134657815099 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 119.57815917069092,
    "estimated_duration": 3600.0292364917163,
    "input_throughput": 7197.964599102118,
    "output_throughput": 6384.005653908773,
    "total_throughput": 13581.97025301089,
    "itl": 98.6206790730528,
    "ttft": 1477002.5873219657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.198623938951646,
    "arrivals": 198139,
    "finished_requests": 104937,
    "scheduler_time": 278.6088385452886
}
#Debug simulation 
Total elapsed time: 119.5783178480342. Arrivals time: 0.47831618692725897 Scheduler time: 118.84839886473492 Scheduler overhead time: 0.10110532445833087 Adapter cache time: 0.019461195915937424 Engine time: 0.09352968260645866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 118.4030546261929,
    "estimated_duration": 3600.0534502459122,
    "input_throughput": 7181.902812646818,
    "output_throughput": 6381.58580629704,
    "total_throughput": 13563.488618943858,
    "itl": 98.48803118260818,
    "ttft": 1471200.6443393426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0913683576392923,
    "arrivals": 198139,
    "finished_requests": 104679,
    "scheduler_time": 279.887504071322
}
#Debug simulation 
Total elapsed time: 118.40330906026065. Arrivals time: 0.46977432630956173 Scheduler time: 117.67927213106304 Scheduler overhead time: 0.10201121820136905 Adapter cache time: 0.019769280217587948 Engine time: 0.09427160490304232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.23752802005038,
    "estimated_duration": 3600.0436512126626,
    "input_throughput": 7197.935778159615,
    "output_throughput": 6383.980092090936,
    "total_throughput": 13581.91587025055,
    "itl": 98.6210644987843,
    "ttft": 1477008.1781676202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.213337131962183,
    "arrivals": 198139,
    "finished_requests": 104937,
    "scheduler_time": 278.60914030471395
}
#Debug simulation 
Total elapsed time: 119.23768288083375. Arrivals time: 0.47837878996506333 Scheduler time: 118.50814320193604 Scheduler overhead time: 0.10113174328580499 Adapter cache time: 0.01883224956691265 Engine time: 0.09380204416811466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 121.5136216529645,
    "estimated_duration": 3600.0150364127253,
    "input_throughput": 7237.361715567278,
    "output_throughput": 6479.5632140592315,
    "total_throughput": 13716.92492962651,
    "itl": 102.13947779526434,
    "ttft": 1473830.7405027107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8936624212283629,
    "arrivals": 196768,
    "finished_requests": 105332,
    "scheduler_time": 274.2448837134597
}
#Debug simulation 
Total elapsed time: 121.51377496216446. Arrivals time: 0.48172995541244745 Scheduler time: 120.78425410902128 Scheduler overhead time: 0.10041674505919218 Adapter cache time: 0.018407294992357492 Engine time: 0.092936419416219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.63810233259574,
    "estimated_duration": 3600.012817398391,
    "input_throughput": 7237.129233008726,
    "output_throughput": 6479.483597188158,
    "total_throughput": 13716.612830196884,
    "itl": 102.14085071282226,
    "ttft": 1473875.7108019532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9519535825704276,
    "arrivals": 196768,
    "finished_requests": 105331,
    "scheduler_time": 274.2395984690949
}
#Debug simulation 
Total elapsed time: 119.63825799757615. Arrivals time: 0.4610502286814153 Scheduler time: 118.93305044900626 Scheduler overhead time: 0.09918233100324869 Adapter cache time: 0.017467514146119356 Engine time: 0.09178934106603265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.75903046876192,
    "estimated_duration": 3600.0143656603695,
    "input_throughput": 7237.1261205289165,
    "output_throughput": 6479.480810549807,
    "total_throughput": 13716.606931078724,
    "itl": 102.14086094147538,
    "ttft": 1473876.3886219913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9537962702848066,
    "arrivals": 196768,
    "finished_requests": 105331,
    "scheduler_time": 274.23970419767386
}
#Debug simulation 
Total elapsed time: 118.75918088899925. Arrivals time: 0.41120163444429636 Scheduler time: 118.11434579920024 Scheduler overhead time: 0.09480303945019841 Adapter cache time: 0.01687562745064497 Engine time: 0.08702276227995753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 127.37624631728977,
    "estimated_duration": 3600.07801438372,
    "input_throughput": 7032.970090881288,
    "output_throughput": 6283.190227996269,
    "total_throughput": 13316.160318877557,
    "itl": 99.9445675093012,
    "ttft": 1516120.5248786493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8342998167616338,
    "arrivals": 196768,
    "finished_requests": 102477,
    "scheduler_time": 284.2166223069476
}
#Debug simulation 
Total elapsed time: 127.37639276031405. Arrivals time: 0.42672880878672004 Scheduler time: 126.70650354586542 Scheduler overhead time: 0.09916300931945443 Adapter cache time: 0.017407503444701433 Engine time: 0.0903722015209496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 119.44074482703581,
    "estimated_duration": 3600.026270347627,
    "input_throughput": 7237.102188558249,
    "output_throughput": 6479.459383985985,
    "total_throughput": 13716.561572544235,
    "itl": 102.14110143137313,
    "ttft": 1473882.0154010796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 292,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9649883572757295,
    "arrivals": 196768,
    "finished_requests": 105331,
    "scheduler_time": 274.2400166436198
}
#Debug simulation 
Total elapsed time: 119.44089519511908. Arrivals time: 0.4318204103037715 Scheduler time: 118.77208855794743 Scheduler overhead time: 0.09619484515860677 Adapter cache time: 0.017416500952094793 Engine time: 0.08814127976074815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 118.74934055097401,
    "estimated_duration": 3600.004785181343,
    "input_throughput": 7215.326520376153,
    "output_throughput": 6472.267508062854,
    "total_throughput": 13687.594028439007,
    "itl": 102.00558820966508,
    "ttft": 1468704.6259248853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9059852393553605,
    "arrivals": 196768,
    "finished_requests": 105015,
    "scheduler_time": 275.56389996105867
}
#Debug simulation 
Total elapsed time: 118.74949282500893. Arrivals time: 0.43972427025437355 Scheduler time: 118.07287065545097 Scheduler overhead time: 0.09622995043173432 Adapter cache time: 0.01722302846610546 Engine time: 0.08818793203681707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_192_slots_16_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.20511738583446,
    "estimated_duration": 3600.006879760493,
    "input_throughput": 7215.296766801766,
    "output_throughput": 6472.184298033934,
    "total_throughput": 13687.4810648357,
    "itl": 102.00856108901854,
    "ttft": 1468752.7514785742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.014786467365926,
    "arrivals": 196768,
    "finished_requests": 105014,
    "scheduler_time": 275.5500103493622
}
#Debug simulation 
Total elapsed time: 119.20532422699034. Arrivals time: 0.442418219987303 Scheduler time: 118.52535602357239 Scheduler overhead time: 0.09664967749267817 Adapter cache time: 0.017617187462747097 Engine time: 0.08784837648272514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 120.98986571608111,
    "estimated_duration": 3600.0465927852897,
    "input_throughput": 7301.868829331506,
    "output_throughput": 6411.915347501366,
    "total_throughput": 13713.784176832873,
    "itl": 99.85585532516006,
    "ttft": 1454641.094326654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9273277864116232,
    "arrivals": 196098,
    "finished_requests": 105775,
    "scheduler_time": 276.43020523387514
}
#Debug simulation 
Total elapsed time: 120.99000633694232. Arrivals time: 0.44478113763034344 Scheduler time: 120.3003380894661 Scheduler overhead time: 0.10241341590881348 Adapter cache time: 0.01741165155544877 Engine time: 0.08876996207982302 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 120.83544272370636,
    "estimated_duration": 3600.1088651634936,
    "input_throughput": 7301.742526279469,
    "output_throughput": 6411.804438294871,
    "total_throughput": 13713.54696457434,
    "itl": 99.85700581711664,
    "ttft": 1454667.2981538994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9881128983618748,
    "arrivals": 196098,
    "finished_requests": 105775,
    "scheduler_time": 276.43133658107627
}
#Debug simulation 
Total elapsed time: 120.83558868570253. Arrivals time: 0.4486854816786945 Scheduler time: 120.14267513388768 Scheduler overhead time: 0.09886212088167667 Adapter cache time: 0.01872048294171691 Engine time: 0.09044120600447059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.18070536712185,
    "estimated_duration": 3600.1111089784736,
    "input_throughput": 7301.737975375688,
    "output_throughput": 6411.8004420563075,
    "total_throughput": 13713.538417431995,
    "itl": 99.85706411766407,
    "ttft": 1454668.307994986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9899729575775615,
    "arrivals": 196098,
    "finished_requests": 105775,
    "scheduler_time": 276.43142022109043
}
#Debug simulation 
Total elapsed time: 121.18082648701966. Arrivals time: 0.44232477247714996 Scheduler time: 120.49444655654952 Scheduler overhead time: 0.09983203606680036 Adapter cache time: 0.01792269153520465 Engine time: 0.09006523294374347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 120.59865280706435,
    "estimated_duration": 3600.0667294085033,
    "input_throughput": 7301.827987038176,
    "output_throughput": 6411.879483076305,
    "total_throughput": 13713.707470114481,
    "itl": 99.8562709026128,
    "ttft": 1454649.9246029165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9468447711993971,
    "arrivals": 196098,
    "finished_requests": 105775,
    "scheduler_time": 276.4306436461337
}
#Debug simulation 
Total elapsed time: 120.59877276001498. Arrivals time: 0.4418633272871375 Scheduler time: 119.91630540508777 Scheduler overhead time: 0.09772547893226147 Adapter cache time: 0.017588005866855383 Engine time: 0.08918319130316377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 118.75821163412184,
    "estimated_duration": 3600.0416934660775,
    "input_throughput": 7312.924194123107,
    "output_throughput": 6437.574332003602,
    "total_throughput": 13750.498526126708,
    "itl": 100.8081350116282,
    "ttft": 1454587.8393609563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 334,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1035109044425249,
    "arrivals": 196098,
    "finished_requests": 106086,
    "scheduler_time": 274.42253640181127
}
#Debug simulation 
Total elapsed time: 118.7583256708458. Arrivals time: 0.4365159273147583 Scheduler time: 118.08039915421978 Scheduler overhead time: 0.09824700513854623 Adapter cache time: 0.01843872480094433 Engine time: 0.08896938385441899 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 120.97049034619704,
    "estimated_duration": 3600.0250010971095,
    "input_throughput": 7301.912623381505,
    "output_throughput": 6411.953803922302,
    "total_throughput": 13713.866427303807,
    "itl": 99.85536943798917,
    "ttft": 1454633.5163930294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9059852393553605,
    "arrivals": 196098,
    "finished_requests": 105775,
    "scheduler_time": 276.42975601553456
}
#Debug simulation 
Total elapsed time: 120.97060906514525. Arrivals time: 0.43632557382807136 Scheduler time: 120.29011173499748 Scheduler overhead time: 0.10057641239836812 Adapter cache time: 0.018025501631200314 Engine time: 0.08926578098908067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 120.49729787698016,
    "estimated_duration": 3600.11018372082,
    "input_throughput": 7301.739851981847,
    "output_throughput": 6411.8020899412695,
    "total_throughput": 13713.541941923117,
    "itl": 99.85789826755195,
    "ttft": 1454661.4166641792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0152494686469489,
    "arrivals": 196098,
    "finished_requests": 105775,
    "scheduler_time": 276.4268181029124
}
#Debug simulation 
Total elapsed time: 120.497419860214. Arrivals time: 0.43288165191188455 Scheduler time: 119.82270688237622 Scheduler overhead time: 0.09856751328334212 Adapter cache time: 0.01781047135591507 Engine time: 0.0894670239649713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 124.57555100601166,
    "estimated_duration": 3600.108821481151,
    "input_throughput": 7221.120329719486,
    "output_throughput": 6348.053943157633,
    "total_throughput": 13569.174272877119,
    "itl": 100.82304073407502,
    "ttft": 1483105.7325171805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8171502276300442,
    "arrivals": 192525,
    "finished_requests": 104446,
    "scheduler_time": 279.18776137356264
}
#Debug simulation 
Total elapsed time: 124.57572638895363. Arrivals time: 0.4359703566879034 Scheduler time: 123.89595203148201 Scheduler overhead time: 0.09926829999312758 Adapter cache time: 0.018007873557507992 Engine time: 0.09009833633899689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 123.6097775506787,
    "estimated_duration": 3600.0084344130655,
    "input_throughput": 7178.964847123638,
    "output_throughput": 6317.962419915339,
    "total_throughput": 13496.927267038976,
    "itl": 100.72214468039574,
    "ttft": 1493681.2756394045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8770534960110713,
    "arrivals": 192525,
    "finished_requests": 103954,
    "scheduler_time": 280.8208116101023
}
#Debug simulation 
Total elapsed time: 123.60991050675511. Arrivals time: 0.4422541968524456 Scheduler time: 122.92171503556892 Scheduler overhead time: 0.1002171984873712 Adapter cache time: 0.018273772671818733 Engine time: 0.09081041580066085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 123.517499927897,
    "estimated_duration": 3600.0101745000143,
    "input_throughput": 7178.961377127046,
    "output_throughput": 6317.959366089539,
    "total_throughput": 13496.920743216584,
    "itl": 100.72217903536924,
    "ttft": 1493682.129743578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8787366243451877,
    "arrivals": 192525,
    "finished_requests": 103954,
    "scheduler_time": 280.8208685687098
}
#Debug simulation 
Total elapsed time: 123.5176270189695. Arrivals time: 0.4370567761361599 Scheduler time: 122.83458504034206 Scheduler overhead time: 0.10077165719121695 Adapter cache time: 0.01784725859761238 Engine time: 0.0905866315588355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 124.52598929218948,
    "estimated_duration": 3600.1255765530627,
    "input_throughput": 7221.086722450008,
    "output_throughput": 6348.024399160332,
    "total_throughput": 13569.11112161034,
    "itl": 100.823397783572,
    "ttft": 1483112.8759438328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8342998167616338,
    "arrivals": 192525,
    "finished_requests": 104446,
    "scheduler_time": 279.1878670492078
}
#Debug simulation 
Total elapsed time: 124.52611109428108. Arrivals time: 0.43751464784145355 Scheduler time: 123.84235560055822 Scheduler overhead time: 0.10042468272149563 Adapter cache time: 0.018038921058177948 Engine time: 0.0907483366318047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 123.5653902082704,
    "estimated_duration": 3600.0213693442906,
    "input_throughput": 7178.939052994371,
    "output_throughput": 6317.939719380814,
    "total_throughput": 13496.878772375185,
    "itl": 100.72238150811886,
    "ttft": 1493687.0973281076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8896772037632806,
    "arrivals": 192525,
    "finished_requests": 103954,
    "scheduler_time": 280.8211228335727
}
#Debug simulation 
Total elapsed time: 123.56551206298172. Arrivals time: 0.44308054680004716 Scheduler time: 122.87560836458579 Scheduler overhead time: 0.10101814847439528 Adapter cache time: 0.017973595764487982 Engine time: 0.09130359999835491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 124.03412303095683,
    "estimated_duration": 3600.089179453703,
    "input_throughput": 7221.159728033432,
    "output_throughput": 6348.088578035709,
    "total_throughput": 13569.248306069141,
    "itl": 100.8225550266899,
    "ttft": 1483096.9071829095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 267,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.798343428738882,
    "arrivals": 192525,
    "finished_requests": 104446,
    "scheduler_time": 279.1873262992772
}
#Debug simulation 
Total elapsed time: 124.03425238188356. Arrivals time: 0.43628844525665045 Scheduler time: 123.3534914306365 Scheduler overhead time: 0.09982359549030662 Adapter cache time: 0.017822350841015577 Engine time: 0.09001474687829614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.98773491336033,
    "estimated_duration": 3600.0277336942604,
    "input_throughput": 7183.60493669361,
    "output_throughput": 6313.756915610019,
    "total_throughput": 13497.36185230363,
    "itl": 100.53442045794584,
    "ttft": 1486745.8155542263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9654296743869832,
    "arrivals": 192525,
    "finished_requests": 103836,
    "scheduler_time": 281.4539505579133
}
#Debug simulation 
Total elapsed time: 121.98785497341305. Arrivals time: 0.4363109446130693 Scheduler time: 121.30583970947191 Scheduler overhead time: 0.10095538385212421 Adapter cache time: 0.01804230036213994 Engine time: 0.0901821693405509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 119.64954922813922,
    "estimated_duration": 3600.014504550936,
    "input_throughput": 7270.987093777039,
    "output_throughput": 6458.943976643915,
    "total_throughput": 13729.931070420955,
    "itl": 102.05448206407455,
    "ttft": 1430720.0176393227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 353,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0803521736082635,
    "arrivals": 191098,
    "finished_requests": 106043,
    "scheduler_time": 274.9664662606936
}
#Debug simulation 
Total elapsed time: 119.64967413339764. Arrivals time: 0.4429765399545431 Scheduler time: 118.9637098312378 Scheduler overhead time: 0.099495321046561 Adapter cache time: 0.01845110533758998 Engine time: 0.08841552818194032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.46195293823257,
    "estimated_duration": 3600.0161000112703,
    "input_throughput": 7261.014193774902,
    "output_throughput": 6434.668444934866,
    "total_throughput": 13695.682638709768,
    "itl": 101.71481286710706,
    "ttft": 1433348.3720855697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1421289077308086,
    "arrivals": 191098,
    "finished_requests": 105947,
    "scheduler_time": 274.8379781524884
}
#Debug simulation 
Total elapsed time: 118.46212903596461. Arrivals time: 0.438605644274503 Scheduler time: 117.78350172843784 Scheduler overhead time: 0.09733147406950593 Adapter cache time: 0.018040571827441454 Engine time: 0.08834164962172508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.27773870481178,
    "estimated_duration": 3600.018156313565,
    "input_throughput": 7261.010046340221,
    "output_throughput": 6434.664769502433,
    "total_throughput": 13695.674815842654,
    "itl": 101.71484637589388,
    "ttft": 1433349.2761933778,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.144147510789341,
    "arrivals": 191098,
    "finished_requests": 105947,
    "scheduler_time": 274.8380158517106
}
#Debug simulation 
Total elapsed time: 118.277861665003. Arrivals time: 0.43491287622600794 Scheduler time: 117.6027061957866 Scheduler overhead time: 0.09804379753768444 Adapter cache time: 0.01811355445533991 Engine time: 0.08781655319035053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 118.55813164217398,
    "estimated_duration": 3600.0320258756765,
    "input_throughput": 7260.982627964741,
    "output_throughput": 6434.833310785664,
    "total_throughput": 13695.815938750406,
    "itl": 101.71488916540392,
    "ttft": 1433296.0531243347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0939146601548435,
    "arrivals": 191098,
    "finished_requests": 105949,
    "scheduler_time": 274.84390449352327
}
#Debug simulation 
Total elapsed time: 118.55825014039874. Arrivals time: 0.4362601609900594 Scheduler time: 117.88069042237476 Scheduler overhead time: 0.09836477832868695 Adapter cache time: 0.017951170448213816 Engine time: 0.0884484052658081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 119.22556168586016,
    "estimated_duration": 3600.03246471789,
    "input_throughput": 7260.981187303931,
    "output_throughput": 6434.639194792727,
    "total_throughput": 13695.620382096657,
    "itl": 101.71517590034425,
    "ttft": 1433354.6548422973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1589864575862936,
    "arrivals": 191098,
    "finished_requests": 105947,
    "scheduler_time": 274.8381896625796
}
#Debug simulation 
Total elapsed time: 119.22567842295393. Arrivals time: 0.42959696846082807 Scheduler time: 118.5535415764898 Scheduler overhead time: 0.0984281525015831 Adapter cache time: 0.018344284500926733 Engine time: 0.08873306354507804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 119.46213646326214,
    "estimated_duration": 3600.0408356423495,
    "input_throughput": 7270.933912984217,
    "output_throughput": 6458.896735223041,
    "total_throughput": 13729.830648207258,
    "itl": 102.05355655031856,
    "ttft": 1430709.5547945986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 353,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0554877541004675,
    "arrivals": 191098,
    "finished_requests": 106043,
    "scheduler_time": 274.9718527845811
}
#Debug simulation 
Total elapsed time: 119.46225588489324. Arrivals time: 0.4337933799251914 Scheduler time: 118.7876698966138 Scheduler overhead time: 0.09794943314045668 Adapter cache time: 0.018076451029628515 Engine time: 0.08885580161586404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_192_slots_16_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.37794208992273,
    "estimated_duration": 3600.0482727944286,
    "input_throughput": 7260.949303801917,
    "output_throughput": 6434.610939819132,
    "total_throughput": 13695.560243621048,
    "itl": 101.71546820826676,
    "ttft": 1433361.6306768416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1735738968104166,
    "arrivals": 191098,
    "finished_requests": 105947,
    "scheduler_time": 274.8387182643922
}
#Debug simulation 
Total elapsed time: 118.37806570110843. Arrivals time: 0.4293146915733814 Scheduler time: 117.70788252539933 Scheduler overhead time: 0.09820952452719212 Adapter cache time: 0.018084547948092222 Engine time: 0.08883731905370951 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 117.61948466813192,
    "estimated_duration": 3600.0075026415902,
    "input_throughput": 7277.129833973069,
    "output_throughput": 6498.332845927145,
    "total_throughput": 13775.462679900214,
    "itl": 102.21045727465672,
    "ttft": 1455237.6904590095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8752994947647664,
    "arrivals": 190328,
    "finished_requests": 106293,
    "scheduler_time": 269.7845818324872
}
#Debug simulation 
Total elapsed time: 117.61961010610685. Arrivals time: 0.42706310003995895 Scheduler time: 116.9535932908766 Scheduler overhead time: 0.09769596252590418 Adapter cache time: 0.01738099195063114 Engine time: 0.08755233325064182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 115.51145012164488,
    "estimated_duration": 3600.0811016849725,
    "input_throughput": 7104.475781956455,
    "output_throughput": 6353.6757517196575,
    "total_throughput": 13458.151533676111,
    "itl": 100.44392516834313,
    "ttft": 1459272.6029421051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1460660317284106,
    "arrivals": 190328,
    "finished_requests": 103735,
    "scheduler_time": 278.60195927703245
}
#Debug simulation 
Total elapsed time: 115.51156991207972. Arrivals time: 0.42213804135099053 Scheduler time: 114.84619142627344 Scheduler overhead time: 0.09900511242449284 Adapter cache time: 0.018185300286859274 Engine time: 0.08893892681226134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 115.29331499664113,
    "estimated_duration": 3600.0836140826727,
    "input_throughput": 7104.470823941439,
    "output_throughput": 6353.671317666992,
    "total_throughput": 13458.14214160843,
    "itl": 100.44396160170393,
    "ttft": 1459273.3548808738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1485483110137347,
    "arrivals": 190328,
    "finished_requests": 103735,
    "scheduler_time": 278.6019893954328
}
#Debug simulation 
Total elapsed time: 115.29347709706053. Arrivals time: 0.4217203510925174 Scheduler time: 114.62968034762889 Scheduler overhead time: 0.09828399121761322 Adapter cache time: 0.017997801769524813 Engine time: 0.08933244179934263 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 117.82024854328483,
    "estimated_duration": 3600.026650216656,
    "input_throughput": 7277.091128873552,
    "output_throughput": 6498.298283039684,
    "total_throughput": 13775.389411913236,
    "itl": 102.21101850441376,
    "ttft": 1455245.7860087687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 286,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8947881302307393,
    "arrivals": 190328,
    "finished_requests": 106293,
    "scheduler_time": 269.78484100353205
}
#Debug simulation 
Total elapsed time: 117.82036099908873. Arrivals time: 0.43807785492390394 Scheduler time: 117.14387772371992 Scheduler overhead time: 0.09765371261164546 Adapter cache time: 0.017303018365055323 Engine time: 0.08786397241055965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 116.58469593292102,
    "estimated_duration": 3600.0984323676316,
    "input_throughput": 7104.441581387346,
    "output_throughput": 6353.6451654620205,
    "total_throughput": 13458.086746849365,
    "itl": 100.4440929092049,
    "ttft": 1459278.6214276669,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1627584888786127,
    "arrivals": 190328,
    "finished_requests": 103735,
    "scheduler_time": 278.6022973867966
}
#Debug simulation 
Total elapsed time: 116.58480901410803. Arrivals time: 0.44273850228637457 Scheduler time: 115.89633643394336 Scheduler overhead time: 0.0988250751979649 Adapter cache time: 0.018929119687527418 Engine time: 0.09073678031563759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 124.12172850873321,
    "estimated_duration": 3600.0812713772693,
    "input_throughput": 7197.07502327793,
    "output_throughput": 6447.29833866232,
    "total_throughput": 13644.37336194025,
    "itl": 100.9142617472689,
    "ttft": 1448146.2193870358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.831233981982806,
    "arrivals": 190328,
    "finished_requests": 105110,
    "scheduler_time": 274.106367018767
}
#Debug simulation 
Total elapsed time: 124.12184471683577. Arrivals time: 0.43002158123999834 Scheduler time: 123.45024417107925 Scheduler overhead time: 0.09873893577605486 Adapter cache time: 0.017360248137265444 Engine time: 0.08893194096162915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_192_slots_16_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 115.99764504190534,
    "estimated_duration": 3600.113112520914,
    "input_throughput": 7104.412611661079,
    "output_throughput": 6353.619257252468,
    "total_throughput": 13458.031868913546,
    "itl": 100.44439678499504,
    "ttft": 1459283.816996033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 352,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.177345928102736,
    "arrivals": 190328,
    "finished_requests": 103735,
    "scheduler_time": 278.6024901394498
}
#Debug simulation 
Total elapsed time: 115.99775965604931. Arrivals time: 0.43937838450074196 Scheduler time: 115.3148199422285 Scheduler overhead time: 0.09967548446729779 Adapter cache time: 0.018525443971157074 Engine time: 0.08841247018426657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_192_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 119.02645483193919,
    "estimated_duration": 3600.039020127895,
    "input_throughput": 7341.279317317257,
    "output_throughput": 6492.5415722770795,
    "total_throughput": 13833.820889594335,
    "itl": 101.74255507893287,
    "ttft": 1437486.127865302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8844809579965647,
    "arrivals": 188114,
    "finished_requests": 106732,
    "scheduler_time": 269.6879406207651
}
#Debug simulation 
Total elapsed time: 119.02657702658325. Arrivals time: 0.43131986912339926 Scheduler time: 118.35661703208461 Scheduler overhead time: 0.09622958488762379 Adapter cache time: 0.017611669842153788 Engine time: 0.08906477736309171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_192_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.63977419910952,
    "estimated_duration": 3600.097649204034,
    "input_throughput": 7341.159761553499,
    "output_throughput": 6492.435838557812,
    "total_throughput": 13833.595600111312,
    "itl": 101.74390020164749,
    "ttft": 1437511.3864931806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9409404550935192,
    "arrivals": 188114,
    "finished_requests": 106732,
    "scheduler_time": 269.68891797136354
}
#Debug simulation 
Total elapsed time: 118.63989027403295. Arrivals time: 0.43256236240267754 Scheduler time: 117.96975574921817 Scheduler overhead time: 0.09644235670566559 Adapter cache time: 0.017534947022795677 Engine time: 0.0877757235430181 
