INFO 06-01 00:47:06 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:07 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.948767499066889,
    "estimated_duration": 3600.1832387676195,
    "input_throughput": 5659.382772687568,
    "output_throughput": 4950.786340003419,
    "total_throughput": 10610.169112690988,
    "itl": 172.49684714682115,
    "ttft": 1617021.0366850037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6877115678275013,
    "arrivals": 185762,
    "finished_requests": 81776,
    "scheduler_time": 80.03438602987964
}
#Debug simulation 
Total elapsed time: 5.948874388821423. Arrivals time: 0.2513802885077894 Scheduler time: 5.606957328971475 Scheduler overhead time: 0.032365188002586365 Adapter cache time: 0.009329188615083694 Engine time: 0.03358579846099019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.933933958876878,
    "estimated_duration": 3600.1438895133233,
    "input_throughput": 5648.684225993279,
    "output_throughput": 4940.740022034103,
    "total_throughput": 10589.424248027382,
    "itl": 170.9878037029285,
    "ttft": 1619163.4749669079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7687310149148135,
    "arrivals": 185762,
    "finished_requests": 81606,
    "scheduler_time": 79.96816053620621
}
#Debug simulation 
Total elapsed time: 5.934035731013864. Arrivals time: 0.24824389349669218 Scheduler time: 5.594384089577943 Scheduler overhead time: 0.03247367776930332 Adapter cache time: 0.009259813465178013 Engine time: 0.03425172949209809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.171022886876017,
    "estimated_duration": 3600.0030713720384,
    "input_throughput": 5396.283451668304,
    "output_throughput": 4762.348436960048,
    "total_throughput": 10158.631888628352,
    "itl": 179.68592256514694,
    "ttft": 1509705.477181661,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5073189545470216,
    "arrivals": 146610,
    "finished_requests": 78611,
    "scheduler_time": 76.19791218107876
}
#Debug simulation 
Total elapsed time: 6.171156145632267. Arrivals time: 0.25168897910043597 Scheduler time: 5.817310892511159 Scheduler overhead time: 0.03247684193775058 Adapter cache time: 0.021046342328190804 Engine time: 0.03359686769545078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.148370957002044,
    "estimated_duration": 3600.115484848251,
    "input_throughput": 5396.276336623929,
    "output_throughput": 4762.129179509542,
    "total_throughput": 10158.40551613347,
    "itl": 179.69574772352917,
    "ttft": 1509775.392010436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7542531911050934,
    "arrivals": 146610,
    "finished_requests": 78611,
    "scheduler_time": 76.19578554166401
}
#Debug simulation 
Total elapsed time: 6.14845762308687. Arrivals time: 0.25465116277337074 Scheduler time: 5.791278302203864 Scheduler overhead time: 0.032622663769870996 Adapter cache time: 0.02122654067352414 Engine time: 0.03360125096514821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.150749416090548,
    "estimated_duration": 3600.076517899225,
    "input_throughput": 5385.15609421352,
    "output_throughput": 4751.750112795479,
    "total_throughput": 10136.906207008999,
    "itl": 177.8590878390739,
    "ttft": 1512802.6046884907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8804479669406446,
    "arrivals": 146610,
    "finished_requests": 78426,
    "scheduler_time": 76.10044625674085
}
#Debug simulation 
Total elapsed time: 6.150841547176242. Arrivals time: 0.2486341423355043 Scheduler time: 5.798404410947114 Scheduler overhead time: 0.032861462328583 Adapter cache time: 0.02201395807787776 Engine time: 0.0337656638585031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.178061781916767,
    "estimated_duration": 3600.1061553457507,
    "input_throughput": 5396.4378164660475,
    "output_throughput": 4762.349014220503,
    "total_throughput": 10158.78683068655,
    "itl": 179.688888834701,
    "ttft": 1509687.7229031564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1147,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5599295948352165,
    "arrivals": 146610,
    "finished_requests": 78614,
    "scheduler_time": 76.19924331288394
}
#Debug simulation 
Total elapsed time: 6.178179378155619. Arrivals time: 0.24907940533012152 Scheduler time: 5.826542711816728 Scheduler overhead time: 0.032668632455170155 Adapter cache time: 0.02135927602648735 Engine time: 0.033461914863437414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.146003638859838,
    "estimated_duration": 3600.1782609181982,
    "input_throughput": 5385.00362897461,
    "output_throughput": 4751.52666347059,
    "total_throughput": 10136.5302924452,
    "itl": 177.81500683194847,
    "ttft": 1512876.9532767332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9566072600334974,
    "arrivals": 146610,
    "finished_requests": 78425,
    "scheduler_time": 76.09871098037871
}
#Debug simulation 
Total elapsed time: 6.146096250973642. Arrivals time: 0.25661340821534395 Scheduler time: 5.785176129080355 Scheduler overhead time: 0.033106956630945206 Adapter cache time: 0.022169989068061113 Engine time: 0.03385351598262787 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.158431611023843,
    "estimated_duration": 3600.101046222779,
    "input_throughput": 5396.607970319105,
    "output_throughput": 4762.579099825354,
    "total_throughput": 10159.18707014446,
    "itl": 179.68118782006414,
    "ttft": 1509611.9329811295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.441547889432319,
    "arrivals": 146610,
    "finished_requests": 78617,
    "scheduler_time": 76.20145723525685
}
#Debug simulation 
Total elapsed time: 6.158523005899042. Arrivals time: 0.25568205351009965 Scheduler time: 5.8002713322639465 Scheduler overhead time: 0.032578840386122465 Adapter cache time: 0.021530053578317165 Engine time: 0.03337232209742069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 540, 540, 8640, 540, 1080, 1080, 1080, 540, 8640, 1080, 540, 8640, 1080, 540, 540, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 1080, 540, 8640, 8640, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 540, 1080, 8640, 8640, 1080, 540, 540, 540, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 540, 540, 1080, 540, 8640, 8640, 540, 540, 1080, 1080, 1080, 540, 8640, 540, 8640, 1080, 540, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 1080, 8640, 8640, 1080, 8640, 540, 1080, 8640, 540, 8640, 1080, 540, 1080, 1080, 8640, 8640, 540, 540, 8640, 540, 540, 540, 1080, 540]
Prompts retrieved: 440640 . Total input tokens: 98156326 . Total output tokens: 88118033
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.159961272962391,
    "estimated_duration": 3600.068863116236,
    "input_throughput": 5384.708664494817,
    "output_throughput": 4751.199671551321,
    "total_throughput": 10135.908336046137,
    "itl": 177.76031998968108,
    "ttft": 1512962.4011598441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1200,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.013826618939697,
    "arrivals": 146610,
    "finished_requests": 78418,
    "scheduler_time": 76.09227006695873
}
#Debug simulation 
Total elapsed time: 6.160079813562334. Arrivals time: 0.25226457696408033 Scheduler time: 5.803285031113774 Scheduler overhead time: 0.032995584420859814 Adapter cache time: 0.022401235066354275 Engine time: 0.03391743591055274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.900211344007403,
    "estimated_duration": 3600.1215617034413,
    "input_throughput": 5420.14919928623,
    "output_throughput": 4758.6570915382945,
    "total_throughput": 10178.806290824525,
    "itl": 179.13574811468422,
    "ttft": 1486597.092536092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1485,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.544824299740261,
    "arrivals": 142827,
    "finished_requests": 78471,
    "scheduler_time": 75.9741845477177
}
#Debug simulation 
Total elapsed time: 5.900300793815404. Arrivals time: 0.23289579572156072 Scheduler time: 5.560985310934484 Scheduler overhead time: 0.03245152113959193 Adapter cache time: 0.025520081166177988 Engine time: 0.03341921279206872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.955797987990081,
    "estimated_duration": 3600.1452374361124,
    "input_throughput": 5419.6819053598665,
    "output_throughput": 4757.963323779371,
    "total_throughput": 10177.645229139238,
    "itl": 179.14991646583925,
    "ttft": 1486709.222049979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1480,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.825697848526692,
    "arrivals": 142827,
    "finished_requests": 78464,
    "scheduler_time": 75.96899389572295
}
#Debug simulation 
Total elapsed time: 5.955884653143585. Arrivals time: 0.23695172648876905 Scheduler time: 5.612216593697667 Scheduler overhead time: 0.03251783875748515 Adapter cache time: 0.02569465944543481 Engine time: 0.03347111260518432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.9177227900363505,
    "estimated_duration": 3600.0237158633377,
    "input_throughput": 5408.982422586686,
    "output_throughput": 4749.408989907072,
    "total_throughput": 10158.391412493758,
    "itl": 177.38098988630676,
    "ttft": 1488190.2818639483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.978268200010008,
    "arrivals": 142827,
    "finished_requests": 78302,
    "scheduler_time": 75.87937216086678
}
#Debug simulation 
Total elapsed time: 5.91783003276214. Arrivals time: 0.23922298150137067 Scheduler time: 5.570213197730482 Scheduler overhead time: 0.03299677325412631 Adapter cache time: 0.026080226991325617 Engine time: 0.034049621783196926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.932168614119291,
    "estimated_duration": 3600.0216327411667,
    "input_throughput": 5419.9974307217935,
    "output_throughput": 4758.231407336542,
    "total_throughput": 10178.228838058336,
    "itl": 179.13641258982656,
    "ttft": 1486676.1467638107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1479,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.595937396497374,
    "arrivals": 142827,
    "finished_requests": 78465,
    "scheduler_time": 75.97122632961599
}
#Debug simulation 
Total elapsed time: 5.932255670893937. Arrivals time: 0.23798608034849167 Scheduler time: 5.587388949934393 Scheduler overhead time: 0.03251256002113223 Adapter cache time: 0.02578092785552144 Engine time: 0.03354155272245407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.888247791212052,
    "estimated_duration": 3600.112184783669,
    "input_throughput": 5408.820336850168,
    "output_throughput": 4748.945622378527,
    "total_throughput": 10157.765959228696,
    "itl": 177.3739059561383,
    "ttft": 1488299.3578476752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.052865623328783,
    "arrivals": 142827,
    "finished_requests": 78300,
    "scheduler_time": 75.87935550882456
}
#Debug simulation 
Total elapsed time: 5.888336625415832. Arrivals time: 0.24694281304255128 Scheduler time: 5.5339251002296805 Scheduler overhead time: 0.03259041393175721 Adapter cache time: 0.026165958493947983 Engine time: 0.03360944101586938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.940753920003772,
    "estimated_duration": 3600.0032336132426,
    "input_throughput": 5420.1976314381,
    "output_throughput": 4758.5121146700585,
    "total_throughput": 10178.70974610816,
    "itl": 179.13110198530637,
    "ttft": 1486534.5679156708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1486,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.443214738224512,
    "arrivals": 142827,
    "finished_requests": 78469,
    "scheduler_time": 75.9736179756299
}
#Debug simulation 
Total elapsed time: 5.940858571790159. Arrivals time: 0.2535205474123359 Scheduler time: 5.580387711524963 Scheduler overhead time: 0.03260240191593766 Adapter cache time: 0.02562254387885332 Engine time: 0.033666861709207296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 270, 270, 8640, 270, 1080, 1080, 1080, 270, 8640, 1080, 270, 8640, 1080, 270, 270, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 1080, 270, 8640, 8640, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 270, 1080, 8640, 8640, 1080, 270, 270, 270, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 270, 270, 1080, 270, 8640, 8640, 270, 270, 1080, 1080, 1080, 270, 8640, 270, 8640, 1080, 270, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 1080, 8640, 8640, 1080, 8640, 270, 1080, 8640, 270, 8640, 1080, 270, 1080, 1080, 8640, 8640, 270, 270, 8640, 270, 270, 270, 1080, 270]
Prompts retrieved: 429300 . Total input tokens: 95646791 . Total output tokens: 85852427
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.91515357978642,
    "estimated_duration": 3600.0489067136264,
    "input_throughput": 5408.499302242632,
    "output_throughput": 4748.805208762108,
    "total_throughput": 10157.30451100474,
    "itl": 177.36895451803844,
    "ttft": 1488404.001997155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.124678133651563,
    "arrivals": 142827,
    "finished_requests": 78294,
    "scheduler_time": 75.8761305507208
}
#Debug simulation 
Total elapsed time: 5.915243270806968. Arrivals time: 0.23889815201982856 Scheduler time: 5.568402993027121 Scheduler overhead time: 0.032690477557480335 Adapter cache time: 0.026248164009302855 Engine time: 0.03380148112773895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.8763808202929795,
    "estimated_duration": 3600.0850646190347,
    "input_throughput": 5439.725908829975,
    "output_throughput": 4835.479075504053,
    "total_throughput": 10275.204984334028,
    "itl": 177.95131329337008,
    "ttft": 1458431.6097184722,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.030662358759541,
    "arrivals": 140936,
    "finished_requests": 79510,
    "scheduler_time": 76.98896252626436
}
#Debug simulation 
Total elapsed time: 5.876466857269406. Arrivals time: 0.2419032952748239 Scheduler time: 5.527474232483655 Scheduler overhead time: 0.0318186329677701 Adapter cache time: 0.026991328690201044 Engine time: 0.033304892014712095 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.884954911656678,
    "estimated_duration": 3600.1626532536916,
    "input_throughput": 5439.582565054741,
    "output_throughput": 4835.119597795953,
    "total_throughput": 10274.702162850694,
    "itl": 177.96038238224577,
    "ttft": 1458508.2265780526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.295955620405708,
    "arrivals": 140936,
    "finished_requests": 79507,
    "scheduler_time": 76.98622322051474
}
#Debug simulation 
Total elapsed time: 5.8850439437665045. Arrivals time: 0.23950187722221017 Scheduler time: 5.538331178482622 Scheduler overhead time: 0.03187662875279784 Adapter cache time: 0.026797694619745016 Engine time: 0.033516256138682365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.838063816074282,
    "estimated_duration": 3600.024137002383,
    "input_throughput": 5431.189418713349,
    "output_throughput": 4829.098177790493,
    "total_throughput": 10260.287596503842,
    "itl": 175.8708001970158,
    "ttft": 1459848.540295213,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.312881976980652,
    "arrivals": 140936,
    "finished_requests": 79384,
    "scheduler_time": 76.9628745648196
}
#Debug simulation 
Total elapsed time: 5.8381552323699. Arrivals time: 0.23651944287121296 Scheduler time: 5.4936736868694425 Scheduler overhead time: 0.031943651381880045 Adapter cache time: 0.027492632158100605 Engine time: 0.033473189920186996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.85098664695397,
    "estimated_duration": 3600.13729383045,
    "input_throughput": 5439.646991674505,
    "output_throughput": 4835.408924496379,
    "total_throughput": 10275.055916170884,
    "itl": 177.95383739901422,
    "ttft": 1458449.1345544453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0907108874827935,
    "arrivals": 140936,
    "finished_requests": 79510,
    "scheduler_time": 76.9890299308184
}
#Debug simulation 
Total elapsed time: 5.851073926780373. Arrivals time: 0.23703779885545373 Scheduler time: 5.506912074051797 Scheduler overhead time: 0.03170623816549778 Adapter cache time: 0.027173038572072983 Engine time: 0.03336433367803693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.8675053906627,
    "estimated_duration": 3600.0729180362514,
    "input_throughput": 5431.115826027587,
    "output_throughput": 4829.03274344871,
    "total_throughput": 10260.148569476296,
    "itl": 175.88216372540091,
    "ttft": 1459865.7398023314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.371749334912741,
    "arrivals": 140936,
    "finished_requests": 79384,
    "scheduler_time": 76.96256303754835
}
#Debug simulation 
Total elapsed time: 5.867594790644944. Arrivals time: 0.23883289424702525 Scheduler time: 5.520274117588997 Scheduler overhead time: 0.03229766013100743 Adapter cache time: 0.027387846261262894 Engine time: 0.03364633722230792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.875117321964353,
    "estimated_duration": 3600.2009924461186,
    "input_throughput": 5439.787956586733,
    "output_throughput": 4835.581134644262,
    "total_throughput": 10275.369091230996,
    "itl": 177.94784057835838,
    "ttft": 1458377.823289753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1318,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9408862886809644,
    "arrivals": 140936,
    "finished_requests": 79514,
    "scheduler_time": 76.993048279248
}
#Debug simulation 
Total elapsed time: 5.875222858041525. Arrivals time: 0.24089690670371056 Scheduler time: 5.526868951506913 Scheduler overhead time: 0.03183860378339887 Adapter cache time: 0.027144986670464277 Engine time: 0.0334446681663394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 135, 135, 8640, 135, 1080, 1080, 1080, 135, 8640, 1080, 135, 8640, 1080, 135, 135, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 1080, 135, 8640, 8640, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 135, 1080, 8640, 8640, 1080, 135, 135, 135, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 135, 135, 1080, 135, 8640, 8640, 135, 135, 1080, 1080, 1080, 135, 8640, 135, 8640, 1080, 135, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 1080, 8640, 8640, 1080, 8640, 135, 1080, 8640, 135, 8640, 1080, 135, 1080, 1080, 8640, 8640, 135, 135, 8640, 135, 135, 135, 1080, 135]
Prompts retrieved: 423630 . Total input tokens: 94414544 . Total output tokens: 84713358
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.862845723051578,
    "estimated_duration": 3600.042304532138,
    "input_throughput": 5430.827292053413,
    "output_throughput": 4828.832699581075,
    "total_throughput": 10259.659991634488,
    "itl": 175.88985530435838,
    "ttft": 1459882.529093875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.424954028651137,
    "arrivals": 140936,
    "finished_requests": 79379,
    "scheduler_time": 76.96078869910527
}
#Debug simulation 
Total elapsed time: 5.862960122991353. Arrivals time: 0.2390969404950738 Scheduler time: 5.5146244508214295 Scheduler overhead time: 0.03230618964880705 Adapter cache time: 0.02787546021863818 Engine time: 0.033824821934103966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.991817342117429,
    "estimated_duration": 3600.1165482766196,
    "input_throughput": 5580.238509116288,
    "output_throughput": 4945.609888247414,
    "total_throughput": 10525.848397363703,
    "itl": 173.58587983468988,
    "ttft": 1423508.023631914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 618,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8913814257504715,
    "arrivals": 139992,
    "finished_requests": 81430,
    "scheduler_time": 78.62087948468806
}
#Debug simulation 
Total elapsed time: 5.991908717900515. Arrivals time: 0.2564204381778836 Scheduler time: 5.632271594833583 Scheduler overhead time: 0.03200299898162484 Adapter cache time: 0.022074243985116482 Engine time: 0.034052515868097544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.986672154162079,
    "estimated_duration": 3600.1657442616693,
    "input_throughput": 5580.16225559082,
    "output_throughput": 4945.5423068727205,
    "total_throughput": 10525.704562463541,
    "itl": 173.59221627985875,
    "ttft": 1423539.8176428329,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.025032621265861,
    "arrivals": 139992,
    "finished_requests": 81430,
    "scheduler_time": 78.61960787153457
}
#Debug simulation 
Total elapsed time: 5.986764394212514. Arrivals time: 0.2472616541199386 Scheduler time: 5.636011654045433 Scheduler overhead time: 0.03214902710169554 Adapter cache time: 0.022215494886040688 Engine time: 0.033977392595261335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.983752395957708,
    "estimated_duration": 3600.0802290555807,
    "input_throughput": 5571.337504681577,
    "output_throughput": 4937.910787800265,
    "total_throughput": 10509.248292481841,
    "itl": 171.36562167316367,
    "ttft": 1425229.5213561526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0289628814347203,
    "arrivals": 139992,
    "finished_requests": 81299,
    "scheduler_time": 78.59916884434344
}
#Debug simulation 
Total elapsed time: 5.983868411742151. Arrivals time: 0.24136307276785374 Scheduler time: 5.637320706620812 Scheduler overhead time: 0.03266227524727583 Adapter cache time: 0.02273052092641592 Engine time: 0.03435113513842225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.999583918135613,
    "estimated_duration": 3600.067261286643,
    "input_throughput": 5580.314905788768,
    "output_throughput": 4945.677596489317,
    "total_throughput": 10525.992502278084,
    "itl": 173.59001351101045,
    "ttft": 1423503.8014210344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9243882898636944,
    "arrivals": 139992,
    "finished_requests": 81430,
    "scheduler_time": 78.61902455448067
}
#Debug simulation 
Total elapsed time: 5.999703706242144. Arrivals time: 0.24347905768081546 Scheduler time: 5.651058936025947 Scheduler overhead time: 0.0322672501206398 Adapter cache time: 0.022425971925258636 Engine time: 0.03521867096424103 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.99312689434737,
    "estimated_duration": 3600.1128658414127,
    "input_throughput": 5571.286997779234,
    "output_throughput": 4937.866023221251,
    "total_throughput": 10509.153021000486,
    "itl": 171.3663793693646,
    "ttft": 1425244.4332864038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0593952977471073,
    "arrivals": 139992,
    "finished_requests": 81299,
    "scheduler_time": 78.59926715174652
}
#Debug simulation 
Total elapsed time: 5.993218830320984. Arrivals time: 0.24287376971915364 Scheduler time: 5.644834454637021 Scheduler overhead time: 0.032643557991832495 Adapter cache time: 0.02293827338144183 Engine time: 0.03454824537038803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.001655572094023,
    "estimated_duration": 3600.0385541335345,
    "input_throughput": 5580.359403910658,
    "output_throughput": 4945.717033934736,
    "total_throughput": 10526.076437845395,
    "itl": 173.5835456808558,
    "ttft": 1423483.1327351816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8508411325444178,
    "arrivals": 139992,
    "finished_requests": 81430,
    "scheduler_time": 78.61985320627848
}
#Debug simulation 
Total elapsed time: 6.001746576279402. Arrivals time: 0.25779396295547485 Scheduler time: 5.640316564124078 Scheduler overhead time: 0.032248061150312424 Adapter cache time: 0.022355032619088888 Engine time: 0.033824244514107704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 66, 66, 8640, 66, 1080, 1080, 1080, 66, 8640, 1080, 66, 8640, 1080, 66, 66, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 1080, 66, 8640, 8640, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 66, 1080, 8640, 8640, 1080, 66, 66, 66, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 66, 66, 1080, 66, 8640, 8640, 66, 66, 1080, 1080, 1080, 66, 8640, 66, 8640, 1080, 66, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 1080, 8640, 8640, 1080, 8640, 66, 1080, 8640, 66, 8640, 1080, 66, 1080, 1080, 8640, 8640, 66, 66, 8640, 66, 66, 66, 1080, 66]
Prompts retrieved: 420732 . Total input tokens: 93781176 . Total output tokens: 84140015
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.975936033762991,
    "estimated_duration": 3600.0196191132536,
    "input_throughput": 5571.4879700962565,
    "output_throughput": 4937.9930891539825,
    "total_throughput": 10509.48105925024,
    "itl": 171.36883406485336,
    "ttft": 1425223.2266107274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.081905225515363,
    "arrivals": 139992,
    "finished_requests": 81298,
    "scheduler_time": 78.59732950532711
}
#Debug simulation 
Total elapsed time: 5.976060574874282. Arrivals time: 0.2563815927132964 Scheduler time: 5.614216516260058 Scheduler overhead time: 0.03263195184990764 Adapter cache time: 0.022957609966397285 Engine time: 0.03442298714071512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.074222377035767,
    "estimated_duration": 3600.1684019986947,
    "input_throughput": 5688.349464050272,
    "output_throughput": 5035.13113162604,
    "total_throughput": 10723.480595676312,
    "itl": 170.40622730014724,
    "ttft": 1395417.0749685247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9089648599480267,
    "arrivals": 139506,
    "finished_requests": 82844,
    "scheduler_time": 79.91227103037815
}
#Debug simulation 
Total elapsed time: 6.074312842916697. Arrivals time: 0.2459025513380766 Scheduler time: 5.725795206148177 Scheduler overhead time: 0.032846017740666866 Adapter cache time: 0.01930153463035822 Engine time: 0.03490771120414138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.082461511716247,
    "estimated_duration": 3600.117215937662,
    "input_throughput": 5688.217013974745,
    "output_throughput": 5035.118556626985,
    "total_throughput": 10723.33557060173,
    "itl": 170.40877768313143,
    "ttft": 1395451.645728725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9746671450953059,
    "arrivals": 139506,
    "finished_requests": 82842,
    "scheduler_time": 79.90942560917779
}
#Debug simulation 
Total elapsed time: 6.082553932908922. Arrivals time: 0.24444443127140403 Scheduler time: 5.735893928445876 Scheduler overhead time: 0.032730035949498415 Adapter cache time: 0.019258896820247173 Engine time: 0.034744284115731716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.048476878087968,
    "estimated_duration": 3600.1142069656985,
    "input_throughput": 5678.888453161451,
    "output_throughput": 5026.412763513869,
    "total_throughput": 10705.301216675321,
    "itl": 168.2673804785508,
    "ttft": 1397658.178183515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9788023332692735,
    "arrivals": 139506,
    "finished_requests": 82696,
    "scheduler_time": 79.87096645942036
}
#Debug simulation 
Total elapsed time: 6.048569459933788. Arrivals time: 0.24462483683601022 Scheduler time: 5.700592414475977 Scheduler overhead time: 0.03301140898838639 Adapter cache time: 0.019792416598647833 Engine time: 0.03490351978689432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.074992607347667,
    "estimated_duration": 3600.027257769488,
    "input_throughput": 5688.1883201872115,
    "output_throughput": 5035.189097771179,
    "total_throughput": 10723.37741795839,
    "itl": 170.40624853320276,
    "ttft": 1395437.7155290127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9248185162455802,
    "arrivals": 139506,
    "finished_requests": 82841,
    "scheduler_time": 79.90890428891386
}
#Debug simulation 
Total elapsed time: 6.075109067372978. Arrivals time: 0.24572821846231818 Scheduler time: 5.726881217211485 Scheduler overhead time: 0.03283023135736585 Adapter cache time: 0.019472745712846518 Engine time: 0.03472571540623903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.067640732042491,
    "estimated_duration": 3600.133521057184,
    "input_throughput": 5678.857986910552,
    "output_throughput": 5026.385797681799,
    "total_throughput": 10705.243784592352,
    "itl": 168.26755288858396,
    "ttft": 1397667.5568783996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9942700489983021,
    "arrivals": 139506,
    "finished_requests": 82696,
    "scheduler_time": 79.87104345337286
}
#Debug simulation 
Total elapsed time: 6.067738234996796. Arrivals time: 0.24964444478973746 Scheduler time: 5.71486354386434 Scheduler overhead time: 0.03302141558378935 Adapter cache time: 0.019728511106222868 Engine time: 0.034841261338442564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.092165085952729,
    "estimated_duration": 3600.0835863454076,
    "input_throughput": 5688.483477904214,
    "output_throughput": 5035.249756076299,
    "total_throughput": 10723.733233980513,
    "itl": 170.40417544511752,
    "ttft": 1395384.88643932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8880449375859474,
    "arrivals": 139506,
    "finished_requests": 82844,
    "scheduler_time": 79.91071481142399
}
#Debug simulation 
Total elapsed time: 6.092255919240415. Arrivals time: 0.2458692817017436 Scheduler time: 5.743986827321351 Scheduler overhead time: 0.032798719592392445 Adapter cache time: 0.0192340356297791 Engine time: 0.03474900685250759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 1080, 8640, 8640, 33, 33, 8640, 33, 1080, 1080, 1080, 33, 8640, 1080, 33, 8640, 1080, 33, 33, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 1080, 33, 8640, 8640, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 8640, 8640, 8640, 8640, 1080, 1080, 33, 1080, 8640, 8640, 1080, 33, 33, 33, 8640, 1080, 1080, 1080, 1080, 8640, 1080, 8640, 33, 33, 1080, 33, 8640, 8640, 33, 33, 1080, 1080, 1080, 33, 8640, 33, 8640, 1080, 33, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 1080, 8640, 8640, 1080, 8640, 33, 1080, 8640, 33, 8640, 1080, 33, 1080, 1080, 8640, 8640, 33, 33, 8640, 33, 33, 33, 1080, 33]
Prompts retrieved: 419346 . Total input tokens: 93489762 . Total output tokens: 83864176
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.049131829757243,
    "estimated_duration": 3600.1439931462605,
    "input_throughput": 5678.841468263853,
    "output_throughput": 5026.371176944433,
    "total_throughput": 10705.212645208285,
    "itl": 168.2672958911522,
    "ttft": 1397672.7484550187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0055878897756396,
    "arrivals": 139506,
    "finished_requests": 82696,
    "scheduler_time": 79.87105620247033
}
#Debug simulation 
Total elapsed time: 6.04924398381263. Arrivals time: 0.24841965874657035 Scheduler time: 5.697607919108123 Scheduler overhead time: 0.03305391361936927 Adapter cache time: 0.01952921785414219 Engine time: 0.03499768581241369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.885057967621833,
    "estimated_duration": 3600.0032195810863,
    "input_throughput": 5458.201229688488,
    "output_throughput": 4812.502362710671,
    "total_throughput": 10270.703592399159,
    "itl": 177.73656757001464,
    "ttft": 1434799.582284116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.837129619945974,
    "arrivals": 135164,
    "finished_requests": 79052,
    "scheduler_time": 76.40989934746162
}
#Debug simulation 
Total elapsed time: 5.885146940592676. Arrivals time: 0.23417850397527218 Scheduler time: 5.53292620787397 Scheduler overhead time: 0.03218148648738861 Adapter cache time: 0.037287216167896986 Engine time: 0.033588049467653036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.862664884887636,
    "estimated_duration": 3600.1253982470434,
    "input_throughput": 5457.821832974852,
    "output_throughput": 4812.084881386459,
    "total_throughput": 10269.90671436131,
    "itl": 177.75572191968266,
    "ttft": 1434897.194502278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.28027759385266,
    "arrivals": 135164,
    "finished_requests": 79048,
    "scheduler_time": 76.40439681349443
}
#Debug simulation 
Total elapsed time: 5.862756261136383. Arrivals time: 0.25268363067880273 Scheduler time: 5.492253209929913 Scheduler overhead time: 0.03202902525663376 Adapter cache time: 0.03721348289400339 Engine time: 0.03358866833150387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.89023326896131,
    "estimated_duration": 3600.1329638711527,
    "input_throughput": 5451.895581905086,
    "output_throughput": 4806.974401687506,
    "total_throughput": 10258.86998359259,
    "itl": 175.48414970210294,
    "ttft": 1436666.2516853486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.3639392419531555,
    "arrivals": 135164,
    "finished_requests": 78960,
    "scheduler_time": 76.4056410296004
}
#Debug simulation 
Total elapsed time: 5.890322596300393. Arrivals time: 0.2420805525034666 Scheduler time: 5.528995511587709 Scheduler overhead time: 0.03231514757499099 Adapter cache time: 0.03754694480448961 Engine time: 0.03424260392785072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.857479379046708,
    "estimated_duration": 3600.1573298102817,
    "input_throughput": 5457.967583054343,
    "output_throughput": 4812.296356202017,
    "total_throughput": 10270.26393925636,
    "itl": 177.74248000628643,
    "ttft": 1434854.1599381466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.9453403696764795,
    "arrivals": 135164,
    "finished_requests": 79052,
    "scheduler_time": 76.4109158676111
}
#Debug simulation 
Total elapsed time: 5.8575724652037024. Arrivals time: 0.24529694020748138 Scheduler time: 5.494001857470721 Scheduler overhead time: 0.032055629417300224 Adapter cache time: 0.03750139661133289 Engine time: 0.033698419108986855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.8828400848433375,
    "estimated_duration": 3600.1312225421702,
    "input_throughput": 5451.277130432464,
    "output_throughput": 4806.869508434229,
    "total_throughput": 10258.146638866692,
    "itl": 175.4739236640371,
    "ttft": 1436737.3093546664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.474290754478231,
    "arrivals": 135164,
    "finished_requests": 78954,
    "scheduler_time": 76.40360105991915
}
#Debug simulation 
Total elapsed time: 5.882960642222315. Arrivals time: 0.2552225487306714 Scheduler time: 5.507971329148859 Scheduler overhead time: 0.032409466337412596 Adapter cache time: 0.03824187954887748 Engine time: 0.03392750583589077 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.901268064044416,
    "estimated_duration": 3600.024934214444,
    "input_throughput": 5458.170529112655,
    "output_throughput": 4812.565278462589,
    "total_throughput": 10270.735807575244,
    "itl": 177.7296250253099,
    "ttft": 1434785.6800382975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.676782308516358,
    "arrivals": 135164,
    "finished_requests": 79053,
    "scheduler_time": 76.41359921461479
}
#Debug simulation 
Total elapsed time: 5.901367950253189. Arrivals time: 0.24439772684127092 Scheduler time: 5.538385015446693 Scheduler overhead time: 0.032163387164473534 Adapter cache time: 0.037602431140840054 Engine time: 0.033715656492859125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 270, 270, 8640, 270, 540, 540, 540, 270, 8640, 540, 270, 8640, 540, 270, 270, 270, 270, 540, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 540, 270, 540, 270, 8640, 8640, 270, 540, 540, 270, 540, 270, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 270, 540, 8640, 8640, 540, 270, 270, 270, 8640, 540, 540, 540, 540, 8640, 540, 8640, 270, 270, 540, 270, 8640, 8640, 270, 270, 540, 540, 540, 270, 8640, 270, 8640, 540, 270, 8640, 8640, 540, 540, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 540, 8640, 8640, 540, 8640, 270, 540, 8640, 270, 8640, 540, 270, 540, 540, 8640, 8640, 270, 270, 8640, 270, 270, 270, 540, 270]
Prompts retrieved: 406080 . Total input tokens: 90518800 . Total output tokens: 81197785
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.844717317260802,
    "estimated_duration": 3600.0033092323,
    "input_throughput": 5451.130266928792,
    "output_throughput": 4806.8614147163735,
    "total_throughput": 10257.991681645164,
    "itl": 175.477398984416,
    "ttft": 1436740.9425140016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.5543959164244,
    "arrivals": 135164,
    "finished_requests": 78950,
    "scheduler_time": 76.39928889833494
}
#Debug simulation 
Total elapsed time: 5.844813670031726. Arrivals time: 0.23553373338654637 Scheduler time: 5.490124055184424 Scheduler overhead time: 0.032197894994169474 Adapter cache time: 0.03780822968110442 Engine time: 0.03401675866916776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.000158465001732,
    "estimated_duration": 3600.1441640639323,
    "input_throughput": 5701.180026310611,
    "output_throughput": 5011.795966423856,
    "total_throughput": 10712.975992734466,
    "itl": 170.31651520600437,
    "ttft": 1369588.430926803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.765179417303427,
    "arrivals": 133259,
    "finished_requests": 82508,
    "scheduler_time": 79.32830400393752
}
#Debug simulation 
Total elapsed time: 6.000272351782769. Arrivals time: 0.2462488990277052 Scheduler time: 5.637416410725564 Scheduler overhead time: 0.0326423360966146 Adapter cache time: 0.03368374519050121 Engine time: 0.03476323140785098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.081535619683564,
    "estimated_duration": 3600.0255232908203,
    "input_throughput": 5700.425973991685,
    "output_throughput": 5011.296137564248,
    "total_throughput": 10711.722111555933,
    "itl": 170.33347834190351,
    "ttft": 1369778.2216983289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.096344765517765,
    "arrivals": 133259,
    "finished_requests": 82494,
    "scheduler_time": 79.31862785912239
}
#Debug simulation 
Total elapsed time: 6.081655960995704. Arrivals time: 0.24526903592050076 Scheduler time: 5.7188685182482 Scheduler overhead time: 0.032949915155768394 Adapter cache time: 0.034048326313495636 Engine time: 0.03486227011308074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.0683923861943185,
    "estimated_duration": 3600.132930361966,
    "input_throughput": 5694.644724670968,
    "output_throughput": 5009.061151024468,
    "total_throughput": 10703.705875695436,
    "itl": 167.90019572525347,
    "ttft": 1370192.9113850193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.106745612453617,
    "arrivals": 133259,
    "finished_requests": 82416,
    "scheduler_time": 79.34685274190119
}
#Debug simulation 
Total elapsed time: 6.068482256028801. Arrivals time: 0.2402363047003746 Scheduler time: 5.709421687759459 Scheduler overhead time: 0.03327446011826396 Adapter cache time: 0.03465798869729042 Engine time: 0.03515668213367462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.085425054188818,
    "estimated_duration": 3600.1077618803793,
    "input_throughput": 5700.895739098697,
    "output_throughput": 5011.782755796162,
    "total_throughput": 10712.678494894859,
    "itl": 170.3220873712964,
    "ttft": 1369580.6764916324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.847677991895367,
    "arrivals": 133259,
    "finished_requests": 82505,
    "scheduler_time": 79.32573262749953
}
#Debug simulation 
Total elapsed time: 6.085531459189951. Arrivals time: 0.2431804402731359 Scheduler time: 5.7234693192876875 Scheduler overhead time: 0.034486953634768724 Adapter cache time: 0.03408623160794377 Engine time: 0.034769386518746614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.058712873607874,
    "estimated_duration": 3600.167302850314,
    "input_throughput": 5694.589799693094,
    "output_throughput": 5008.793892918077,
    "total_throughput": 10703.383692611173,
    "itl": 167.8927515841259,
    "ttft": 1370260.4890571542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.167442709039836,
    "arrivals": 133259,
    "finished_requests": 82414,
    "scheduler_time": 79.3466996927595
}
#Debug simulation 
Total elapsed time: 6.058803492691368. Arrivals time: 0.24145472375676036 Scheduler time: 5.698161897715181 Scheduler overhead time: 0.033461872953921556 Adapter cache time: 0.034692404326051474 Engine time: 0.03524957550689578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.078567242715508,
    "estimated_duration": 3600.0410628568966,
    "input_throughput": 5701.001361276926,
    "output_throughput": 5011.875610574728,
    "total_throughput": 10712.876971851654,
    "itl": 170.31418143782835,
    "ttft": 1369581.0409416421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1560,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.664478460047265,
    "arrivals": 133259,
    "finished_requests": 82505,
    "scheduler_time": 79.32713483534872
}
#Debug simulation 
Total elapsed time: 6.0786571516655385. Arrivals time: 0.2443298939615488 Scheduler time: 5.717208397574723 Scheduler overhead time: 0.03292396944016218 Adapter cache time: 0.03385019861161709 Engine time: 0.03479999164119363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 135, 135, 8640, 135, 540, 540, 540, 135, 8640, 540, 135, 8640, 540, 135, 135, 135, 135, 540, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 540, 135, 540, 135, 8640, 8640, 135, 540, 540, 135, 540, 135, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 135, 540, 8640, 8640, 540, 135, 135, 135, 8640, 540, 540, 540, 540, 8640, 540, 8640, 135, 135, 540, 135, 8640, 8640, 135, 135, 540, 540, 540, 135, 8640, 135, 8640, 540, 135, 8640, 8640, 540, 540, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 540, 8640, 8640, 540, 8640, 135, 540, 8640, 135, 8640, 540, 135, 540, 540, 8640, 8640, 135, 135, 8640, 135, 135, 135, 540, 135]
Prompts retrieved: 400410 . Total input tokens: 89267308 . Total output tokens: 80044816
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.026645706035197,
    "estimated_duration": 3600.100287459602,
    "input_throughput": 5694.586362333399,
    "output_throughput": 5008.79907785134,
    "total_throughput": 10703.38544018474,
    "itl": 167.90622792919063,
    "ttft": 1370212.0943647698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.240658809095559,
    "arrivals": 133259,
    "finished_requests": 82411,
    "scheduler_time": 79.34338300456349
}
#Debug simulation 
Total elapsed time: 6.026738683693111. Arrivals time: 0.24887249991297722 Scheduler time: 5.659565528854728 Scheduler overhead time: 0.03319433657452464 Adapter cache time: 0.034235512372106314 Engine time: 0.035123100969940424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.217190064955503,
    "estimated_duration": 3600.0125818687484,
    "input_throughput": 5840.847642005997,
    "output_throughput": 5144.760074806666,
    "total_throughput": 10985.607716812663,
    "itl": 166.0995836163604,
    "ttft": 1326791.865096482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3749384892918632,
    "arrivals": 132319,
    "finished_requests": 84835,
    "scheduler_time": 81.16672915253821
}
#Debug simulation 
Total elapsed time: 6.2172832461073995. Arrivals time: 0.266232134308666 Scheduler time: 5.837463119998574 Scheduler overhead time: 0.03356431098654866 Adapter cache time: 0.028628223575651646 Engine time: 0.035519239492714405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.224221119191498,
    "estimated_duration": 3600.091498287654,
    "input_throughput": 5840.719328939649,
    "output_throughput": 5144.6467426756835,
    "total_throughput": 10985.366071615332,
    "itl": 166.1069665013464,
    "ttft": 1326825.877981591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 778,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5334169558831534,
    "arrivals": 132319,
    "finished_requests": 84834,
    "scheduler_time": 81.16521212633084
}
#Debug simulation 
Total elapsed time: 6.2243426302447915. Arrivals time: 0.2500810753554106 Scheduler time: 5.86116993566975 Scheduler overhead time: 0.033492987509816885 Adapter cache time: 0.02825884474441409 Engine time: 0.03549866424873471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.238526389934123,
    "estimated_duration": 3600.16537841633,
    "input_throughput": 5834.253372338004,
    "output_throughput": 5140.434689736601,
    "total_throughput": 10974.688062074605,
    "itl": 164.1315440167449,
    "ttft": 1327898.5005772118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5192320409975877,
    "arrivals": 132319,
    "finished_requests": 84748,
    "scheduler_time": 81.17281949041416
}
#Debug simulation 
Total elapsed time: 6.238620015792549. Arrivals time: 0.2504971334710717 Scheduler time: 5.872515163850039 Scheduler overhead time: 0.03427929896861315 Adapter cache time: 0.029149166774004698 Engine time: 0.03612990723922849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.241348600015044,
    "estimated_duration": 3600.1016503383075,
    "input_throughput": 5840.812855388127,
    "output_throughput": 5144.757787119564,
    "total_throughput": 10985.570642507691,
    "itl": 166.1001387468971,
    "ttft": 1326797.7219062448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 777,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.409891286648318,
    "arrivals": 132319,
    "finished_requests": 84837,
    "scheduler_time": 81.16813422812982
}
#Debug simulation 
Total elapsed time: 6.241445072926581. Arrivals time: 0.24883086187765002 Scheduler time: 5.8787105972878635 Scheduler overhead time: 0.03374464251101017 Adapter cache time: 0.02858397737145424 Engine time: 0.03568971250206232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.199932052753866,
    "estimated_duration": 3600.022245744398,
    "input_throughput": 5834.017838313988,
    "output_throughput": 5140.2857362548275,
    "total_throughput": 10974.303574568816,
    "itl": 164.13301196410075,
    "ttft": 1328044.239009007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5562036542035678,
    "arrivals": 132319,
    "finished_requests": 84742,
    "scheduler_time": 81.1683163240873
}
#Debug simulation 
Total elapsed time: 6.200053948909044. Arrivals time: 0.24528607167303562 Scheduler time: 5.839130467735231 Scheduler overhead time: 0.034063609316945076 Adapter cache time: 0.028982273768633604 Engine time: 0.036465834360569715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.2234110487625,
    "estimated_duration": 3600.130066881811,
    "input_throughput": 5840.797584908567,
    "output_throughput": 5144.7818984057985,
    "total_throughput": 10985.579483314366,
    "itl": 166.0978829752499,
    "ttft": 1326715.353910858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.320279028844043,
    "arrivals": 132319,
    "finished_requests": 84840,
    "scheduler_time": 81.1705739237858
}
#Debug simulation 
Total elapsed time: 6.223501960746944. Arrivals time: 0.2575301150791347 Scheduler time: 5.851709401234984 Scheduler overhead time: 0.033717283979058266 Adapter cache time: 0.02872259309515357 Engine time: 0.03595943050459027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 66, 66, 8640, 66, 540, 540, 540, 66, 8640, 540, 66, 8640, 540, 66, 66, 66, 66, 540, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 540, 66, 540, 66, 8640, 8640, 66, 540, 540, 66, 540, 66, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 66, 540, 8640, 8640, 540, 66, 66, 66, 8640, 540, 540, 540, 540, 8640, 540, 8640, 66, 66, 540, 66, 8640, 8640, 66, 66, 540, 540, 540, 66, 8640, 66, 8640, 540, 66, 8640, 8640, 540, 540, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 540, 8640, 8640, 540, 8640, 66, 540, 8640, 66, 8640, 540, 66, 540, 540, 8640, 8640, 66, 66, 8640, 66, 66, 66, 540, 66]
Prompts retrieved: 397512 . Total input tokens: 88623183 . Total output tokens: 79458202
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.21677421592176,
    "estimated_duration": 3600.0189282900537,
    "input_throughput": 5834.103769553235,
    "output_throughput": 5140.347972778498,
    "total_throughput": 10974.451742331734,
    "itl": 164.13936408569828,
    "ttft": 1328006.8759208831,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5824861955642837,
    "arrivals": 132319,
    "finished_requests": 84742,
    "scheduler_time": 81.16799993492714
}
#Debug simulation 
Total elapsed time: 6.216864078771323. Arrivals time: 0.24865265702828765 Scheduler time: 5.852068626321852 Scheduler overhead time: 0.03415418928489089 Adapter cache time: 0.02943911962211132 Engine time: 0.03629733482375741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.319494131021202,
    "estimated_duration": 3600.0350582081896,
    "input_throughput": 5941.377696095499,
    "output_throughput": 5222.604973563207,
    "total_throughput": 11163.982669658706,
    "itl": 163.50523451754728,
    "ttft": 1301089.6476964825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2639814382442351,
    "arrivals": 131875,
    "finished_requests": 86274,
    "scheduler_time": 82.22051730756644
}
#Debug simulation 
Total elapsed time: 6.319602849893272. Arrivals time: 0.25597734143957496 Scheduler time: 5.950479784980416 Scheduler overhead time: 0.03413105243816972 Adapter cache time: 0.026640634518116713 Engine time: 0.03623113967478275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.295568776782602,
    "estimated_duration": 3600.010477308608,
    "input_throughput": 5941.322430814264,
    "output_throughput": 5222.605355875541,
    "total_throughput": 11163.927786689805,
    "itl": 163.50789500468645,
    "ttft": 1301057.4032964148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.345490220026119,
    "arrivals": 131875,
    "finished_requests": 86273,
    "scheduler_time": 82.2184383013109
}
#Debug simulation 
Total elapsed time: 6.295657549053431. Arrivals time: 0.2514313296414912 Scheduler time: 5.930859447456896 Scheduler overhead time: 0.03407863061875105 Adapter cache time: 0.02671624766662717 Engine time: 0.03638942679390311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.2250730358064175,
    "estimated_duration": 3600.053497648819,
    "input_throughput": 5936.6476120308025,
    "output_throughput": 5218.221065956099,
    "total_throughput": 11154.868677986902,
    "itl": 161.7059834806861,
    "ttft": 1302592.4126753383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3510653355158944,
    "arrivals": 131875,
    "finished_requests": 86195,
    "scheduler_time": 82.21538485297462
}
#Debug simulation 
Total elapsed time: 6.2251670877449214. Arrivals time: 0.24826261959969997 Scheduler time: 5.863011747132987 Scheduler overhead time: 0.034402241464704275 Adapter cache time: 0.02693240065127611 Engine time: 0.036359517835080624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.298511099070311,
    "estimated_duration": 3600.0756909128877,
    "input_throughput": 5941.358970309929,
    "output_throughput": 5222.577693979643,
    "total_throughput": 11163.936664289573,
    "itl": 163.50535766667954,
    "ttft": 1301079.2450656418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2839222100074377,
    "arrivals": 131875,
    "finished_requests": 86275,
    "scheduler_time": 82.22128208534785
}
#Debug simulation 
Total elapsed time: 6.2986030098982155. Arrivals time: 0.24934034515172243 Scheduler time: 5.93456923821941 Scheduler overhead time: 0.035121158231049776 Adapter cache time: 0.026774892583489418 Engine time: 0.03632172802463174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.2551180818118155,
    "estimated_duration": 3600.0574911345984,
    "input_throughput": 5936.6410266032435,
    "output_throughput": 5218.215277467533,
    "total_throughput": 11154.856304070778,
    "itl": 161.71429661718017,
    "ttft": 1302592.5350666107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3710601875558537,
    "arrivals": 131875,
    "finished_requests": 86195,
    "scheduler_time": 82.21505896618281
}
#Debug simulation 
Total elapsed time: 6.2552404939197. Arrivals time: 0.24857542105019093 Scheduler time: 5.892485404387116 Scheduler overhead time: 0.034396597649902105 Adapter cache time: 0.026982110925018787 Engine time: 0.03656007722020149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.319530058186501,
    "estimated_duration": 3600.143308620615,
    "input_throughput": 5941.256824077729,
    "output_throughput": 5222.480159325605,
    "total_throughput": 11163.736983403334,
    "itl": 163.50228088845864,
    "ttft": 1301085.5221293236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2319007214996895,
    "arrivals": 131875,
    "finished_requests": 86276,
    "scheduler_time": 82.22326101346033
}
#Debug simulation 
Total elapsed time: 6.319647073280066. Arrivals time: 0.25551993679255247 Scheduler time: 5.950233340729028 Scheduler overhead time: 0.034283075481653214 Adapter cache time: 0.02645799145102501 Engine time: 0.03695495845749974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 540, 8640, 8640, 33, 33, 8640, 33, 540, 540, 540, 33, 8640, 540, 33, 8640, 540, 33, 33, 33, 33, 540, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 540, 33, 540, 33, 8640, 8640, 33, 540, 540, 33, 540, 33, 540, 540, 8640, 8640, 8640, 8640, 540, 540, 33, 540, 8640, 8640, 540, 33, 33, 33, 8640, 540, 540, 540, 540, 8640, 540, 8640, 33, 33, 540, 33, 8640, 8640, 33, 33, 540, 540, 540, 33, 8640, 33, 8640, 540, 33, 8640, 8640, 540, 540, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 540, 8640, 8640, 540, 8640, 33, 540, 8640, 33, 8640, 540, 33, 540, 540, 8640, 8640, 33, 33, 8640, 33, 33, 33, 540, 33]
Prompts retrieved: 396126 . Total input tokens: 88333076 . Total output tokens: 79175117
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.3175411419942975,
    "estimated_duration": 3600.085724737784,
    "input_throughput": 5936.594468609958,
    "output_throughput": 5218.174353714394,
    "total_throughput": 11154.768822324351,
    "itl": 161.71406663310924,
    "ttft": 1302608.061048576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3861506419256344,
    "arrivals": 131875,
    "finished_requests": 86195,
    "scheduler_time": 82.215376382911
}
#Debug simulation 
Total elapsed time: 6.317631337791681. Arrivals time: 0.2521634213626385 Scheduler time: 5.951053073629737 Scheduler overhead time: 0.03445427352562547 Adapter cache time: 0.0270459046587348 Engine time: 0.03662146348506212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.37750010099262,
    "estimated_duration": 3600.046024347063,
    "input_throughput": 5974.915557892415,
    "output_throughput": 5285.295207705286,
    "total_throughput": 11260.210765597702,
    "itl": 161.96567554745778,
    "ttft": 1266725.7393396043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.278561866018102,
    "arrivals": 129404,
    "finished_requests": 86919,
    "scheduler_time": 82.79146510522939
}
#Debug simulation 
Total elapsed time: 6.3776082787662745. Arrivals time: 0.26227393466979265 Scheduler time: 5.995020084548742 Scheduler overhead time: 0.03443049546331167 Adapter cache time: 0.0331322755664587 Engine time: 0.036435900721699 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.325518381316215,
    "estimated_duration": 3600.0693415817223,
    "input_throughput": 5974.309092210514,
    "output_throughput": 5284.696819656556,
    "total_throughput": 11259.005911867069,
    "itl": 161.97885659004461,
    "ttft": 1266854.9340065941,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1401,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.567419728033663,
    "arrivals": 129404,
    "finished_requests": 86913,
    "scheduler_time": 82.78727894590534
}
#Debug simulation 
Total elapsed time: 6.3256068751215935. Arrivals time: 0.2507506567053497 Scheduler time: 5.954634862486273 Scheduler overhead time: 0.03457689145579934 Adapter cache time: 0.03276165574789047 Engine time: 0.03660411946475506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.355507479049265,
    "estimated_duration": 3600.1693366641703,
    "input_throughput": 5969.699475279099,
    "output_throughput": 5280.637165143822,
    "total_throughput": 11250.336640422922,
    "itl": 159.7673895709358,
    "ttft": 1267416.1931556838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1392,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.546508684959206,
    "arrivals": 129404,
    "finished_requests": 86847,
    "scheduler_time": 82.81460600144831
}
#Debug simulation 
Total elapsed time: 6.355596760287881. Arrivals time: 0.25237734196707606 Scheduler time: 5.981403989717364 Scheduler overhead time: 0.03502966556698084 Adapter cache time: 0.033290954772382975 Engine time: 0.03693120973184705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.314816969912499,
    "estimated_duration": 3600.1634558609244,
    "input_throughput": 5974.720665802719,
    "output_throughput": 5285.122809916948,
    "total_throughput": 11259.843475719666,
    "itl": 161.9696551280021,
    "ttft": 1266774.323652449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.348970061854849,
    "arrivals": 129404,
    "finished_requests": 86919,
    "scheduler_time": 82.79309654259139
}
#Debug simulation 
Total elapsed time: 6.314906477928162. Arrivals time: 0.2610199963673949 Scheduler time: 5.933674735948443 Scheduler overhead time: 0.034403963945806026 Adapter cache time: 0.03320073522627354 Engine time: 0.036390476394444704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.308930350001901,
    "estimated_duration": 3600.0535793736376,
    "input_throughput": 5969.383934485497,
    "output_throughput": 5280.529464594115,
    "total_throughput": 11249.91339907961,
    "itl": 159.76872900972782,
    "ttft": 1267487.1529877258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.605224861130092,
    "arrivals": 129404,
    "finished_requests": 86842,
    "scheduler_time": 82.81060424262348
}
#Debug simulation 
Total elapsed time: 6.309020605869591. Arrivals time: 0.24728723475709558 Scheduler time: 5.940417993348092 Scheduler overhead time: 0.03477848134934902 Adapter cache time: 0.0333198313601315 Engine time: 0.036736337933689356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.353195540141314,
    "estimated_duration": 3600.0713584258983,
    "input_throughput": 5974.996842675156,
    "output_throughput": 5285.486065565071,
    "total_throughput": 11260.482908240227,
    "itl": 161.9601365693863,
    "ttft": 1266678.1216870998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.183080362568032,
    "arrivals": 129404,
    "finished_requests": 86923,
    "scheduler_time": 82.79490126219196
}
#Debug simulation 
Total elapsed time: 6.3533175638876855. Arrivals time: 0.25087324855849147 Scheduler time: 5.98123229900375 Scheduler overhead time: 0.034516402054578066 Adapter cache time: 0.033639288041740656 Engine time: 0.03669564984738827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 135, 135, 8640, 135, 270, 270, 270, 135, 8640, 270, 135, 8640, 270, 135, 135, 135, 135, 270, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 270, 135, 270, 135, 8640, 8640, 135, 270, 270, 135, 270, 135, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 135, 270, 8640, 8640, 270, 135, 135, 135, 8640, 270, 270, 270, 270, 8640, 270, 8640, 135, 135, 270, 135, 8640, 8640, 135, 135, 270, 270, 270, 135, 8640, 135, 8640, 270, 135, 8640, 8640, 270, 270, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 270, 8640, 8640, 270, 8640, 135, 270, 8640, 135, 8640, 270, 135, 270, 270, 8640, 8640, 135, 135, 8640, 135, 135, 135, 270, 135]
Prompts retrieved: 388800 . Total input tokens: 86705346 . Total output tokens: 77724893
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.350300981197506,
    "estimated_duration": 3600.0955650682536,
    "input_throughput": 5969.314317241624,
    "output_throughput": 5280.46788103515,
    "total_throughput": 11249.782198276773,
    "itl": 159.7700041035338,
    "ttft": 1267502.2536344056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.65552637569606,
    "arrivals": 129404,
    "finished_requests": 86842,
    "scheduler_time": 82.81067501816642
}
#Debug simulation 
Total elapsed time: 6.350416566245258. Arrivals time: 0.2504349942319095 Scheduler time: 5.978123342152685 Scheduler overhead time: 0.03489002538844943 Adapter cache time: 0.03337220661342144 Engine time: 0.03702136594802141 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.543309843167663,
    "estimated_duration": 3600.017528223972,
    "input_throughput": 6205.125065327245,
    "output_throughput": 5448.570415621508,
    "total_throughput": 11653.695480948752,
    "itl": 156.41850556901895,
    "ttft": 1213737.2915368865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 840,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5708097049035663,
    "arrivals": 128419,
    "finished_requests": 89657,
    "scheduler_time": 84.89169469254395
}
#Debug simulation 
Total elapsed time: 6.543399112299085. Arrivals time: 0.2682020687498152 Scheduler time: 6.156311466824263 Scheduler overhead time: 0.035359036177396774 Adapter cache time: 0.029296854976564646 Engine time: 0.03747192118316889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.566708400845528,
    "estimated_duration": 3600.067099533596,
    "input_throughput": 6204.764073117951,
    "output_throughput": 5448.387615481153,
    "total_throughput": 11653.151688599104,
    "itl": 156.42622248611755,
    "ttft": 1213832.9655279159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 841,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7356823652889606,
    "arrivals": 128419,
    "finished_requests": 89653,
    "scheduler_time": 84.89075550016294
}
#Debug simulation 
Total elapsed time: 6.566798876971006. Arrivals time: 0.2580847362987697 Scheduler time: 6.188897056505084 Scheduler overhead time: 0.035586369689553976 Adapter cache time: 0.029559079091995955 Engine time: 0.03782329708337784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.536586815025657,
    "estimated_duration": 3600.10600040549,
    "input_throughput": 6198.031390599822,
    "output_throughput": 5443.429998392477,
    "total_throughput": 11641.461388992298,
    "itl": 154.17854692495013,
    "ttft": 1215339.5345688134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 837,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.728864428475485,
    "arrivals": 128419,
    "finished_requests": 89564,
    "scheduler_time": 84.90133061549741
}
#Debug simulation 
Total elapsed time: 6.536704488098621. Arrivals time: 0.26591962296515703 Scheduler time: 6.149643790908158 Scheduler overhead time: 0.03596264170482755 Adapter cache time: 0.030066843144595623 Engine time: 0.03807763149961829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.519789086189121,
    "estimated_duration": 3600.085689802291,
    "input_throughput": 6205.007581701974,
    "output_throughput": 5448.467256088344,
    "total_throughput": 11653.474837790318,
    "itl": 156.41991302555041,
    "ttft": 1213775.4153404187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 840,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.605210575640638,
    "arrivals": 128419,
    "finished_requests": 89657,
    "scheduler_time": 84.89220325572047
}
#Debug simulation 
Total elapsed time: 6.519908415153623. Arrivals time: 0.25223557325080037 Scheduler time: 6.148624053224921 Scheduler overhead time: 0.03531087841838598 Adapter cache time: 0.029002821538597345 Engine time: 0.03794283885508776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.517475415952504,
    "estimated_duration": 3600.002690518685,
    "input_throughput": 6197.915367886915,
    "output_throughput": 5443.348987379955,
    "total_throughput": 11641.26435526687,
    "itl": 154.18065104238275,
    "ttft": 1215360.8861741987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 837,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.767722348477695,
    "arrivals": 128419,
    "finished_requests": 89559,
    "scheduler_time": 84.8973127684373
}
#Debug simulation 
Total elapsed time: 6.517562375869602. Arrivals time: 0.2685070810839534 Scheduler time: 6.128259216435254 Scheduler overhead time: 0.03585569653660059 Adapter cache time: 0.02977132983505726 Engine time: 0.0379049563780427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.544376980047673,
    "estimated_duration": 3600.1094029751325,
    "input_throughput": 6205.355032138258,
    "output_throughput": 5448.854410865663,
    "total_throughput": 11654.209443003921,
    "itl": 156.41619495223003,
    "ttft": 1213641.2958986864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 841,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5146322980126774,
    "arrivals": 128419,
    "finished_requests": 89662,
    "scheduler_time": 84.8951325828857
}
#Debug simulation 
Total elapsed time: 6.544486819766462. Arrivals time: 0.27140829199925065 Scheduler time: 6.153514205478132 Scheduler overhead time: 0.03558254102244973 Adapter cache time: 0.029240712523460388 Engine time: 0.037748589646071196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 66, 66, 8640, 66, 270, 270, 270, 66, 8640, 270, 66, 8640, 270, 66, 66, 66, 66, 270, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 270, 66, 270, 66, 8640, 8640, 66, 270, 270, 66, 270, 66, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 66, 270, 8640, 8640, 270, 66, 66, 66, 8640, 270, 270, 270, 270, 8640, 270, 8640, 66, 66, 270, 66, 8640, 8640, 66, 66, 270, 270, 270, 66, 8640, 66, 8640, 270, 66, 8640, 8640, 270, 270, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 270, 8640, 8640, 270, 8640, 66, 270, 8640, 66, 8640, 270, 66, 270, 270, 8640, 8640, 66, 66, 8640, 66, 66, 66, 270, 66]
Prompts retrieved: 385902 . Total input tokens: 86064046 . Total output tokens: 77147793
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.5062500750645995,
    "estimated_duration": 3600.0255478592157,
    "input_throughput": 6197.876015982252,
    "output_throughput": 5443.31442638038,
    "total_throughput": 11641.190442362631,
    "itl": 154.17377318400278,
    "ttft": 1215368.8937833149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 837,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7965199655667163,
    "arrivals": 128419,
    "finished_requests": 89559,
    "scheduler_time": 84.89773455798965
}
#Debug simulation 
Total elapsed time: 6.506361548323184. Arrivals time: 0.25322602363303304 Scheduler time: 6.132538843434304 Scheduler overhead time: 0.035943842492997646 Adapter cache time: 0.029778506606817245 Engine time: 0.037894554901868105 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.613523667212576,
    "estimated_duration": 3600.0557741697507,
    "input_throughput": 6288.137856758823,
    "output_throughput": 5524.308579520981,
    "total_throughput": 11812.446436279803,
    "itl": 154.29396428724053,
    "ttft": 1193863.422392636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 513,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5700302126375212,
    "arrivals": 127927,
    "finished_requests": 91049,
    "scheduler_time": 85.99111486298041
}
#Debug simulation 
Total elapsed time: 6.613615170121193. Arrivals time: 0.2594894119538367 Scheduler time: 6.236425024922937 Scheduler overhead time: 0.0361038688570261 Adapter cache time: 0.02650198759511113 Engine time: 0.03809740347787738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.612794972024858,
    "estimated_duration": 3600.1268853941187,
    "input_throughput": 6288.013650808248,
    "output_throughput": 5524.199461048387,
    "total_throughput": 11812.213111856636,
    "itl": 154.30078770052114,
    "ttft": 1193900.422768569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 513,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6781299486942653,
    "arrivals": 127927,
    "finished_requests": 91049,
    "scheduler_time": 85.99084492880569
}
#Debug simulation 
Total elapsed time: 6.6128981448709965. Arrivals time: 0.2654069745913148 Scheduler time: 6.230288152582943 Scheduler overhead time: 0.03592646401375532 Adapter cache time: 0.02634897967800498 Engine time: 0.03794328821823001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.59582479018718,
    "estimated_duration": 3600.1178226431152,
    "input_throughput": 6280.879158393464,
    "output_throughput": 5519.4610229204745,
    "total_throughput": 11800.340181313939,
    "itl": 152.20025885919296,
    "ttft": 1195733.0817370655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 513,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6803735349886222,
    "arrivals": 127927,
    "finished_requests": 90945,
    "scheduler_time": 85.98807595995994
}
#Debug simulation 
Total elapsed time: 6.595918803010136. Arrivals time: 0.2648180495016277 Scheduler time: 6.21231405204162 Scheduler overhead time: 0.03627442568540573 Adapter cache time: 0.026561591774225235 Engine time: 0.038736384361982346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.6382675231434405,
    "estimated_duration": 3600.118256861711,
    "input_throughput": 6288.207048991277,
    "output_throughput": 5524.453526517579,
    "total_throughput": 11812.660575508857,
    "itl": 154.29531593801204,
    "ttft": 1193851.8234374265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.592195048755957,
    "arrivals": 127927,
    "finished_requests": 91052,
    "scheduler_time": 85.99283638162518
}
#Debug simulation 
Total elapsed time: 6.638360469136387. Arrivals time: 0.25881611788645387 Scheduler time: 6.2624768684618175 Scheduler overhead time: 0.03585679503157735 Adapter cache time: 0.026405947748571634 Engine time: 0.0378744387999177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.61047134315595,
    "estimated_duration": 3600.1353263745027,
    "input_throughput": 6280.848620979812,
    "output_throughput": 5519.434187495028,
    "total_throughput": 11800.28280847484,
    "itl": 152.2003051509341,
    "ttft": 1195738.5115375014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 512,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7023950335010911,
    "arrivals": 127927,
    "finished_requests": 90945,
    "scheduler_time": 85.98839585213025
}
#Debug simulation 
Total elapsed time: 6.610562616959214. Arrivals time: 0.26489478442817926 Scheduler time: 6.2265164204873145 Scheduler overhead time: 0.03641754807904363 Adapter cache time: 0.02669109869748354 Engine time: 0.038784338627010584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.611776903271675,
    "estimated_duration": 3600.1669196541643,
    "input_throughput": 6288.273712090747,
    "output_throughput": 5524.382464441545,
    "total_throughput": 11812.656176532291,
    "itl": 154.29336871083743,
    "ttft": 1193832.7183969768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 513,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5338958012847985,
    "arrivals": 127927,
    "finished_requests": 91053,
    "scheduler_time": 85.99468997924782
}
#Debug simulation 
Total elapsed time: 6.611896716058254. Arrivals time: 0.26111268950626254 Scheduler time: 6.233114037197083 Scheduler overhead time: 0.03587833372876048 Adapter cache time: 0.026587857864797115 Engine time: 0.0381570840254426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 270, 8640, 8640, 33, 33, 8640, 33, 270, 270, 270, 33, 8640, 270, 33, 8640, 270, 33, 33, 33, 33, 270, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 270, 33, 270, 33, 8640, 8640, 33, 270, 270, 33, 270, 33, 270, 270, 8640, 8640, 8640, 8640, 270, 270, 33, 270, 8640, 8640, 270, 33, 33, 33, 8640, 270, 270, 270, 270, 8640, 270, 8640, 33, 33, 270, 33, 8640, 8640, 33, 33, 270, 270, 270, 33, 8640, 33, 8640, 270, 33, 8640, 8640, 270, 270, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 270, 8640, 8640, 270, 8640, 33, 270, 8640, 33, 8640, 270, 33, 270, 270, 8640, 8640, 33, 33, 8640, 33, 33, 33, 270, 33]
Prompts retrieved: 384516 . Total input tokens: 85759189 . Total output tokens: 76857243
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.628886664286256,
    "estimated_duration": 3600.1570790406827,
    "input_throughput": 6280.810671190295,
    "output_throughput": 5519.400838280883,
    "total_throughput": 11800.211509471177,
    "itl": 152.20088594455746,
    "ttft": 1195749.5174915968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 513,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7247646215930559,
    "arrivals": 127927,
    "finished_requests": 90945,
    "scheduler_time": 85.9882874186753
}
#Debug simulation 
Total elapsed time: 6.628978664055467. Arrivals time: 0.25652283476665616 Scheduler time: 6.252687156200409 Scheduler overhead time: 0.03651229850947857 Adapter cache time: 0.027265709824860096 Engine time: 0.038782942574471235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.831630022265017,
    "estimated_duration": 3600.094429948869,
    "input_throughput": 6485.649600122188,
    "output_throughput": 5742.963525624422,
    "total_throughput": 12228.61312574661,
    "itl": 149.38136239531693,
    "ttft": 1128052.1394126886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4177853177069233,
    "arrivals": 126456,
    "finished_requests": 94081,
    "scheduler_time": 88.51335813462116
}
#Debug simulation 
Total elapsed time: 6.831746664363891. Arrivals time: 0.2653152192942798 Scheduler time: 6.44762247055769 Scheduler overhead time: 0.03703988157212734 Adapter cache time: 0.025101319886744022 Engine time: 0.0391241037286818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.876454284414649,
    "estimated_duration": 3600.1423936213514,
    "input_throughput": 6485.160431811408,
    "output_throughput": 5742.75729666444,
    "total_throughput": 12227.917728475848,
    "itl": 149.38810846244164,
    "ttft": 1128163.2104063348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.571340536014181,
    "arrivals": 126456,
    "finished_requests": 94078,
    "scheduler_time": 88.51182208057887
}
#Debug simulation 
Total elapsed time: 6.876545667182654. Arrivals time: 0.2694418132305145 Scheduler time: 6.487685889471322 Scheduler overhead time: 0.03719350462779403 Adapter cache time: 0.025281800888478756 Engine time: 0.03937042923644185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.897378248628229,
    "estimated_duration": 3600.0490980477784,
    "input_throughput": 6473.006718890791,
    "output_throughput": 5733.569025820564,
    "total_throughput": 12206.575744711354,
    "itl": 147.15404724537674,
    "ttft": 1131422.1321317328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.573332593888034,
    "arrivals": 126456,
    "finished_requests": 93908,
    "scheduler_time": 88.48223778927341
}
#Debug simulation 
Total elapsed time: 6.897486731875688. Arrivals time: 0.27271421160548925 Scheduler time: 6.502960368525237 Scheduler overhead time: 0.03778634499758482 Adapter cache time: 0.025910488795489073 Engine time: 0.040231502149254084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.858445044141263,
    "estimated_duration": 3600.163572230954,
    "input_throughput": 6485.525041166697,
    "output_throughput": 5742.853230190305,
    "total_throughput": 12228.378271357002,
    "itl": 149.38361643518104,
    "ttft": 1128091.7484598719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.454073679621771,
    "arrivals": 126456,
    "finished_requests": 94081,
    "scheduler_time": 88.51452523924878
}
#Debug simulation 
Total elapsed time: 6.858562072273344. Arrivals time: 0.27161811385303736 Scheduler time: 6.466856939718127 Scheduler overhead time: 0.03721601888537407 Adapter cache time: 0.025693367701023817 Engine time: 0.03953843703493476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.775515286251903,
    "estimated_duration": 3600.0478171885443,
    "input_throughput": 6473.009021918653,
    "output_throughput": 5733.571065764255,
    "total_throughput": 12206.580087682907,
    "itl": 147.17313568797184,
    "ttft": 1131412.9349668403,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.609761178083724,
    "arrivals": 126456,
    "finished_requests": 93908,
    "scheduler_time": 88.48212361657163
}
#Debug simulation 
Total elapsed time: 6.77560949511826. Arrivals time: 0.26262473966926336 Scheduler time: 6.391707245726138 Scheduler overhead time: 0.037665761075913906 Adapter cache time: 0.025600692722946405 Engine time: 0.040230849757790565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.885152033064514,
    "estimated_duration": 3600.1595631233445,
    "input_throughput": 6485.705033513267,
    "output_throughput": 5743.2434972581805,
    "total_throughput": 12228.948530771448,
    "itl": 149.37916546232793,
    "ttft": 1128037.9585368433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 791,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.365129783267574,
    "arrivals": 126456,
    "finished_requests": 94083,
    "scheduler_time": 88.51610928516192
}
#Debug simulation 
Total elapsed time: 6.8852453590370715. Arrivals time: 0.26598960254341364 Scheduler time: 6.497578237671405 Scheduler overhead time: 0.03844149177893996 Adapter cache time: 0.025781483855098486 Engine time: 0.039676147513091564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 66, 66, 8640, 66, 135, 135, 135, 66, 8640, 135, 66, 8640, 135, 66, 66, 66, 66, 135, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 135, 66, 135, 66, 8640, 8640, 66, 135, 135, 66, 135, 66, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 66, 135, 8640, 8640, 135, 66, 66, 66, 8640, 135, 135, 135, 135, 8640, 135, 8640, 66, 66, 135, 66, 8640, 8640, 66, 66, 135, 135, 135, 66, 8640, 66, 8640, 135, 66, 8640, 8640, 135, 135, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 135, 8640, 8640, 135, 8640, 66, 135, 8640, 66, 8640, 135, 66, 135, 135, 8640, 8640, 66, 66, 8640, 66, 66, 66, 135, 66]
Prompts retrieved: 380097 . Total input tokens: 84755884 . Total output tokens: 75973043
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.842606304679066,
    "estimated_duration": 3600.00966055152,
    "input_throughput": 6473.077629583352,
    "output_throughput": 5733.631836098403,
    "total_throughput": 12206.709465681755,
    "itl": 147.17860011696953,
    "ttft": 1131420.8873220333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 788,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.634423506110924,
    "arrivals": 126456,
    "finished_requests": 93908,
    "scheduler_time": 88.48099970465397
}
#Debug simulation 
Total elapsed time: 6.842726306058466. Arrivals time: 0.2665395229123533 Scheduler time: 6.454574805684388 Scheduler overhead time: 0.03778620017692447 Adapter cache time: 0.025696023367345333 Engine time: 0.040268737357109785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.000980487093329,
    "estimated_duration": 3600.0076087171688,
    "input_throughput": 6558.80641552695,
    "output_throughput": 5840.058490179638,
    "total_throughput": 12398.864905706589,
    "itl": 147.24139263274301,
    "ttft": 1092767.9948077882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4108848499530124,
    "arrivals": 125984,
    "finished_requests": 95646,
    "scheduler_time": 89.42071635193744
}
#Debug simulation 
Total elapsed time: 7.001096174120903. Arrivals time: 0.26716378424316645 Scheduler time: 6.616310972720385 Scheduler overhead time: 0.03772459179162979 Adapter cache time: 0.022073710337281227 Engine time: 0.039944724179804325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.976231363136321,
    "estimated_duration": 3600.0275195672352,
    "input_throughput": 6558.766529328755,
    "output_throughput": 5840.025634728303,
    "total_throughput": 12398.792164057057,
    "itl": 147.24593330512835,
    "ttft": 1092784.779179915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.503443353392654,
    "arrivals": 125984,
    "finished_requests": 95645,
    "scheduler_time": 89.4193903455171
}
#Debug simulation 
Total elapsed time: 6.976322858128697. Arrivals time: 0.2649063910357654 Scheduler time: 6.593523558229208 Scheduler overhead time: 0.03775534126907587 Adapter cache time: 0.02208905154839158 Engine time: 0.0401627910323441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.9782247128896415,
    "estimated_duration": 3600.1483990145325,
    "input_throughput": 6547.737589498417,
    "output_throughput": 5831.375730441176,
    "total_throughput": 12379.113319939594,
    "itl": 144.85730130397417,
    "ttft": 1095130.5310961504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5028791562654167,
    "arrivals": 125984,
    "finished_requests": 95492,
    "scheduler_time": 89.38605417267976
}
#Debug simulation 
Total elapsed time: 6.97831552522257. Arrivals time: 0.2706863940693438 Scheduler time: 6.5885000033304095 Scheduler overhead time: 0.038110730703920126 Adapter cache time: 0.02248754259198904 Engine time: 0.04035187419503927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.996672898996621,
    "estimated_duration": 3600.0365350145307,
    "input_throughput": 6558.753715510472,
    "output_throughput": 5840.011565303501,
    "total_throughput": 12398.765280813974,
    "itl": 147.2411820966847,
    "ttft": 1092800.0402858884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.431530577347142,
    "arrivals": 125984,
    "finished_requests": 95646,
    "scheduler_time": 89.4212448794049
}
#Debug simulation 
Total elapsed time: 6.996760945767164. Arrivals time: 0.2712446548976004 Scheduler time: 6.607190449722111 Scheduler overhead time: 0.03777649300172925 Adapter cache time: 0.022527780383825302 Engine time: 0.04004755849018693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.003963387105614,
    "estimated_duration": 3600.0055294645067,
    "input_throughput": 6547.920220418763,
    "output_throughput": 5831.531042987799,
    "total_throughput": 12379.451263406563,
    "itl": 144.85764946347285,
    "ttft": 1095039.2561703695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5248860688880126,
    "arrivals": 125984,
    "finished_requests": 95491,
    "scheduler_time": 89.38193875052002
}
#Debug simulation 
Total elapsed time: 7.004070135764778. Arrivals time: 0.2695349147543311 Scheduler time: 6.615105890203267 Scheduler overhead time: 0.03833418060094118 Adapter cache time: 0.022387465462088585 Engine time: 0.0405567092821002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.000897060148418,
    "estimated_duration": 3600.1491690768116,
    "input_throughput": 6558.696568135512,
    "output_throughput": 5839.918018005722,
    "total_throughput": 12398.614586141233,
    "itl": 147.24053230565144,
    "ttft": 1092871.5399401311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 461,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.378413185949891,
    "arrivals": 125984,
    "finished_requests": 95649,
    "scheduler_time": 89.4245905870599
}
#Debug simulation 
Total elapsed time: 7.001023660879582. Arrivals time: 0.27697471948340535 Scheduler time: 6.606461067683995 Scheduler overhead time: 0.03768134908750653 Adapter cache time: 0.021823913790285587 Engine time: 0.04014503210783005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 135, 8640, 8640, 33, 33, 8640, 33, 135, 135, 135, 33, 8640, 135, 33, 8640, 135, 33, 33, 33, 33, 135, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 135, 33, 135, 33, 8640, 8640, 33, 135, 135, 33, 135, 33, 135, 135, 8640, 8640, 8640, 8640, 135, 135, 33, 135, 8640, 8640, 135, 33, 33, 33, 8640, 135, 135, 135, 135, 8640, 135, 8640, 33, 33, 135, 33, 8640, 8640, 33, 33, 135, 135, 135, 33, 8640, 33, 8640, 135, 33, 8640, 8640, 135, 135, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 135, 8640, 8640, 135, 8640, 33, 135, 8640, 33, 8640, 135, 33, 135, 135, 8640, 8640, 33, 33, 8640, 33, 33, 33, 135, 33]
Prompts retrieved: 378711 . Total input tokens: 84434790 . Total output tokens: 75692348
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.016630069818348,
    "estimated_duration": 3600.0264176841256,
    "input_throughput": 6547.882227809893,
    "output_throughput": 5831.497207041335,
    "total_throughput": 12379.379434851227,
    "itl": 144.857307571491,
    "ttft": 1095049.325278565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 460,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5412340611219408,
    "arrivals": 125984,
    "finished_requests": 95491,
    "scheduler_time": 89.38232991261364
}
#Debug simulation 
Total elapsed time: 7.016721022780985. Arrivals time: 0.2822185019031167 Scheduler time: 6.614569280762225 Scheduler overhead time: 0.03836260177195072 Adapter cache time: 0.022569915745407343 Engine time: 0.040891474578529596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.219831420108676,
    "estimated_duration": 3600.054230822536,
    "input_throughput": 6869.954843527236,
    "output_throughput": 6035.9135187347565,
    "total_throughput": 12905.868362261994,
    "itl": 141.24445610137954,
    "ttft": 1003760.7954214036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3404936318425567,
    "arrivals": 125093,
    "finished_requests": 99551,
    "scheduler_time": 91.20895389093128
}
#Debug simulation 
Total elapsed time: 7.219924791250378. Arrivals time: 0.2871185992844403 Scheduler time: 6.814575396012515 Scheduler overhead time: 0.03921513631939888 Adapter cache time: 0.01882693637162447 Engine time: 0.041541390120983124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.202965836971998,
    "estimated_duration": 3600.017245039926,
    "input_throughput": 6869.889591244365,
    "output_throughput": 6035.959419344117,
    "total_throughput": 12905.849010588481,
    "itl": 141.24728190225662,
    "ttft": 1003811.3423990158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.426091694922657,
    "arrivals": 125093,
    "finished_requests": 99549,
    "scheduler_time": 91.2069188462049
}
#Debug simulation 
Total elapsed time: 7.203083347063512. Arrivals time: 0.27618174161762 Scheduler time: 6.808237581979483 Scheduler overhead time: 0.0393658266402781 Adapter cache time: 0.018918009009212255 Engine time: 0.041740046348422766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.137042065151036,
    "estimated_duration": 3600.140773933294,
    "input_throughput": 6860.390343296512,
    "output_throughput": 6028.112055265457,
    "total_throughput": 12888.502398561968,
    "itl": 139.80041459334174,
    "ttft": 1009390.552706582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4257960253581488,
    "arrivals": 125093,
    "finished_requests": 99411,
    "scheduler_time": 91.15456848301251
}
#Debug simulation 
Total elapsed time: 7.13713988289237. Arrivals time: 0.2706023179925978 Scheduler time: 6.747515538241714 Scheduler overhead time: 0.03941000998020172 Adapter cache time: 0.01901465840637684 Engine time: 0.041786953806877136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.170767148025334,
    "estimated_duration": 3600.089089629605,
    "input_throughput": 6869.888323387178,
    "output_throughput": 6035.8550744186305,
    "total_throughput": 12905.743397805809,
    "itl": 141.24472327430604,
    "ttft": 1003791.3414651591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3611250392906327,
    "arrivals": 125093,
    "finished_requests": 99551,
    "scheduler_time": 91.2094821214044
}
#Debug simulation 
Total elapsed time: 7.170854901894927. Arrivals time: 0.26908940030261874 Scheduler time: 6.783895616885275 Scheduler overhead time: 0.03911462565883994 Adapter cache time: 0.018757822457700968 Engine time: 0.04144622851163149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.2162923836149275,
    "estimated_duration": 3600.0026519004928,
    "input_throughput": 6860.356613060255,
    "output_throughput": 6028.091948361109,
    "total_throughput": 12888.448561421365,
    "itl": 139.80031629128283,
    "ttft": 1009383.0490662338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4456651236116924,
    "arrivals": 125093,
    "finished_requests": 99407,
    "scheduler_time": 91.15038072693578
}
#Debug simulation 
Total elapsed time: 7.216401207726449. Arrivals time: 0.277333150152117 Scheduler time: 6.819509918335825 Scheduler overhead time: 0.0396739449352026 Adapter cache time: 0.018975846003741026 Engine time: 0.04202893190085888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.2032763618044555,
    "estimated_duration": 3600.1191311731527,
    "input_throughput": 6869.900161315095,
    "output_throughput": 6035.876927472395,
    "total_throughput": 12905.77708878749,
    "itl": 141.2417665542969,
    "ttft": 1003765.6693296283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 438,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3096420291671433,
    "arrivals": 125093,
    "finished_requests": 99552,
    "scheduler_time": 91.2114118781664
}
#Debug simulation 
Total elapsed time: 7.203392557799816. Arrivals time: 0.27091703470796347 Scheduler time: 6.813620808534324 Scheduler overhead time: 0.039417081512510777 Adapter cache time: 0.019008782226592302 Engine time: 0.041496904101222754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 66, 8640, 8640, 33, 33, 8640, 33, 66, 66, 66, 33, 8640, 66, 33, 8640, 66, 33, 33, 33, 33, 66, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 66, 33, 66, 33, 8640, 8640, 33, 66, 66, 33, 66, 33, 66, 66, 8640, 8640, 8640, 8640, 66, 66, 33, 66, 8640, 8640, 66, 33, 33, 33, 8640, 66, 66, 66, 66, 8640, 66, 8640, 33, 33, 66, 33, 8640, 8640, 33, 33, 66, 66, 66, 33, 8640, 33, 8640, 66, 33, 8640, 8640, 66, 66, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 66, 8640, 8640, 66, 8640, 33, 66, 8640, 33, 8640, 66, 33, 66, 66, 8640, 8640, 33, 33, 8640, 33, 33, 33, 66, 33]
Prompts retrieved: 375744 . Total input tokens: 83773370 . Total output tokens: 75085747
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.2126109837554395,
    "estimated_duration": 3600.018324978046,
    "input_throughput": 6860.3267457397205,
    "output_throughput": 6028.065704396751,
    "total_throughput": 12888.392450136473,
    "itl": 139.80010749202893,
    "ttft": 1009390.0065001586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4615101006999618,
    "arrivals": 125093,
    "finished_requests": 99407,
    "scheduler_time": 91.15052603178279
}
#Debug simulation 
Total elapsed time: 7.212732329033315. Arrivals time: 0.27273994125425816 Scheduler time: 6.819782258477062 Scheduler overhead time: 0.03984553972259164 Adapter cache time: 0.018941978458315134 Engine time: 0.042511056177318096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.361605865880847,
    "estimated_duration": 3600.177211009705,
    "input_throughput": 5348.997249665688,
    "output_throughput": 4759.252668896973,
    "total_throughput": 10108.249918562662,
    "itl": 178.28992651783776,
    "ttft": 412054.7610967853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 528,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.615937528796514,
    "arrivals": 85226,
    "finished_requests": 78072,
    "scheduler_time": 67.76838081979132
}
#Debug simulation 
Total elapsed time: 7.361694659572095. Arrivals time: 0.1930369301699102 Scheduler time: 7.0720851556397974 Scheduler overhead time: 0.03359833639115095 Adapter cache time: 0.013504486065357924 Engine time: 0.034127292688935995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.3815926113165915,
    "estimated_duration": 3600.022625119686,
    "input_throughput": 5348.617496358053,
    "output_throughput": 4759.400921662365,
    "total_throughput": 10108.018418020418,
    "itl": 178.30281549140486,
    "ttft": 412300.2045372241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7267879440495808,
    "arrivals": 85226,
    "finished_requests": 78064,
    "scheduler_time": 67.76570414734954
}
#Debug simulation 
Total elapsed time: 7.381698607001454. Arrivals time: 0.19409341923892498 Scheduler time: 7.090857903473079 Scheduler overhead time: 0.03367679379880428 Adapter cache time: 0.013448236510157585 Engine time: 0.0342795611359179 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.257918699178845,
    "estimated_duration": 3600.1689072461127,
    "input_throughput": 5337.465128739967,
    "output_throughput": 4749.167175347523,
    "total_throughput": 10086.63230408749,
    "itl": 176.3710923247935,
    "ttft": 422819.6360089258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8081847438402585,
    "arrivals": 85226,
    "finished_requests": 77892,
    "scheduler_time": 67.62218554298363
}
#Debug simulation 
Total elapsed time: 7.258032492827624. Arrivals time: 0.19302404951304197 Scheduler time: 6.96696220850572 Scheduler overhead time: 0.033961826004087925 Adapter cache time: 0.013657828327268362 Engine time: 0.03492944873869419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 7.384798882063478,
    "estimated_duration": 3600.0178975164854,
    "input_throughput": 5349.513960273807,
    "output_throughput": 4759.845502940734,
    "total_throughput": 10109.359463214541,
    "itl": 178.3004702608743,
    "ttft": 412097.77806673077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 530,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6480589306563973,
    "arrivals": 85226,
    "finished_requests": 78069,
    "scheduler_time": 67.76634292753641
}
#Debug simulation 
Total elapsed time: 7.384885719977319. Arrivals time: 0.1952216038480401 Scheduler time: 7.092692928854376 Scheduler overhead time: 0.03365232888609171 Adapter cache time: 0.013497750274837017 Engine time: 0.034388592932373285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.216853816993535,
    "estimated_duration": 3600.095706408973,
    "input_throughput": 5337.519768097271,
    "output_throughput": 4747.944330916134,
    "total_throughput": 10085.464099013405,
    "itl": 176.34126538254165,
    "ttft": 423054.6317361428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 552,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8313380264304604,
    "arrivals": 85226,
    "finished_requests": 77887,
    "scheduler_time": 67.6190519890371
}
#Debug simulation 
Total elapsed time: 7.216955992858857. Arrivals time: 0.19624563213437796 Scheduler time: 6.922851404175162 Scheduler overhead time: 0.03395761689171195 Adapter cache time: 0.01395286526530981 Engine time: 0.03443364938721061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 7.391742844134569,
    "estimated_duration": 3600.187346715191,
    "input_throughput": 5349.859644824669,
    "output_throughput": 4760.134223469268,
    "total_throughput": 10109.993868293937,
    "itl": 178.27871787447626,
    "ttft": 411965.449559524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 526,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5727664551185254,
    "arrivals": 85226,
    "finished_requests": 78080,
    "scheduler_time": 67.76935975287847
}
#Debug simulation 
Total elapsed time: 7.391861862037331. Arrivals time: 0.19658310199156404 Scheduler time: 7.099042089655995 Scheduler overhead time: 0.03338258434087038 Adapter cache time: 0.013254076708108187 Engine time: 0.034245289862155914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 540, 540, 4320, 540, 1080, 1080, 1080, 540, 4320, 1080, 540, 4320, 1080, 540, 540, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 1080, 540, 4320, 4320, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 540, 1080, 4320, 4320, 1080, 540, 540, 540, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 540, 540, 1080, 540, 4320, 4320, 540, 540, 1080, 1080, 1080, 540, 4320, 540, 4320, 1080, 540, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 540, 4320, 540, 540, 4320, 1080, 4320, 4320, 1080, 4320, 540, 1080, 4320, 540, 4320, 1080, 540, 1080, 1080, 4320, 4320, 540, 540, 4320, 540, 540, 540, 1080, 540]
Prompts retrieved: 254880 . Total input tokens: 56864113 . Total output tokens: 50945279
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.265033595263958,
    "estimated_duration": 3600.153602231387,
    "input_throughput": 5336.775627598663,
    "output_throughput": 4747.980472112473,
    "total_throughput": 10084.756099711136,
    "itl": 176.24302831337832,
    "ttft": 423821.54860031983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 553,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8548393986001577,
    "arrivals": 85226,
    "finished_requests": 77876,
    "scheduler_time": 67.61309190281371
}
#Debug simulation 
Total elapsed time: 7.265125204343349. Arrivals time: 0.19357993640005589 Scheduler time: 6.973539598751813 Scheduler overhead time: 0.034049143083393574 Adapter cache time: 0.01375993387773633 Engine time: 0.03462952794507146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.4812208828516304,
    "estimated_duration": 3600.0682554797972,
    "input_throughput": 5412.579600495119,
    "output_throughput": 4748.400804340231,
    "total_throughput": 10160.980404835349,
    "itl": 176.2341697815825,
    "ttft": 183874.99830503197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 711,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1760067859362273,
    "arrivals": 81441,
    "finished_requests": 78296,
    "scheduler_time": 66.70900696372061
}
#Debug simulation 
Total elapsed time: 6.481311697978526. Arrivals time: 0.1809549806639552 Scheduler time: 6.201178005430847 Scheduler overhead time: 0.033646649681031704 Adapter cache time: 0.015765167772769928 Engine time: 0.03433809522539377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.494057187344879,
    "estimated_duration": 3600.093380927769,
    "input_throughput": 5412.191834584784,
    "output_throughput": 4748.079061103376,
    "total_throughput": 10160.27089568816,
    "itl": 176.25805042912992,
    "ttft": 184418.90796688138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3228497570427167,
    "arrivals": 81441,
    "finished_requests": 78284,
    "scheduler_time": 66.70927246623003
}
#Debug simulation 
Total elapsed time: 6.494164598174393. Arrivals time: 0.18398865731433034 Scheduler time: 6.21047214185819 Scheduler overhead time: 0.033684794791042805 Adapter cache time: 0.016044416930526495 Engine time: 0.0344016682356596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.418458892963827,
    "estimated_duration": 3600.1617110863876,
    "input_throughput": 5403.874759317213,
    "output_throughput": 4740.640384970326,
    "total_throughput": 10144.51514428754,
    "itl": 174.67738987313612,
    "ttft": 193030.28220418884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 726,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.367426511645317,
    "arrivals": 81441,
    "finished_requests": 78162,
    "scheduler_time": 66.58189123590003
}
#Debug simulation 
Total elapsed time: 6.418547618668526. Arrivals time: 0.1816249922849238 Scheduler time: 6.136674320325255 Scheduler overhead time: 0.033885208424180746 Adapter cache time: 0.016148393042385578 Engine time: 0.034670138731598854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.4233601107262075,
    "estimated_duration": 3600.007169385201,
    "input_throughput": 5412.321443606316,
    "output_throughput": 4748.192766215848,
    "total_throughput": 10160.514209822164,
    "itl": 176.2503991889961,
    "ttft": 184380.63763922962,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2113032351084754,
    "arrivals": 81441,
    "finished_requests": 78284,
    "scheduler_time": 66.7086466426006
}
#Debug simulation 
Total elapsed time: 6.423449337948114. Arrivals time: 0.1802309388294816 Scheduler time: 6.144275636877865 Scheduler overhead time: 0.03349237563088536 Adapter cache time: 0.015796196181327105 Engine time: 0.03429996781051159 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.417377138976008,
    "estimated_duration": 3600.172489041135,
    "input_throughput": 5403.7505311812065,
    "output_throughput": 4740.546752121186,
    "total_throughput": 10144.297283302392,
    "itl": 174.709623077915,
    "ttft": 192953.57612381273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 727,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4064813394658318,
    "arrivals": 81441,
    "finished_requests": 78162,
    "scheduler_time": 66.58446990268942
}
#Debug simulation 
Total elapsed time: 6.417470722924918. Arrivals time: 0.18110808869823813 Scheduler time: 6.136291457805783 Scheduler overhead time: 0.03401639638468623 Adapter cache time: 0.016004575416445732 Engine time: 0.0344987022690475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.513133083004504,
    "estimated_duration": 3600.153957070099,
    "input_throughput": 5412.368813209784,
    "output_throughput": 4748.22499366439,
    "total_throughput": 10160.593806874174,
    "itl": 176.24520188414846,
    "ttft": 184222.0801335984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 714,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1348959105601146,
    "arrivals": 81441,
    "finished_requests": 78288,
    "scheduler_time": 66.71186397035841
}
#Debug simulation 
Total elapsed time: 6.513230653945357. Arrivals time: 0.19393075117841363 Scheduler time: 6.219346336089075 Scheduler overhead time: 0.033983682282269 Adapter cache time: 0.01600942714139819 Engine time: 0.034464642871171236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 270, 270, 4320, 270, 1080, 1080, 1080, 270, 4320, 1080, 270, 4320, 1080, 270, 270, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 1080, 270, 4320, 4320, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 270, 1080, 4320, 4320, 1080, 270, 270, 270, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 270, 270, 1080, 270, 4320, 4320, 270, 270, 1080, 1080, 1080, 270, 4320, 270, 4320, 1080, 270, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 1080, 4320, 4320, 1080, 4320, 270, 1080, 4320, 270, 4320, 1080, 270, 1080, 1080, 4320, 4320, 270, 270, 4320, 270, 270, 270, 1080, 270]
Prompts retrieved: 243540 . Total input tokens: 54310305 . Total output tokens: 48687351
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.440341915003955,
    "estimated_duration": 3600.004849819907,
    "input_throughput": 5403.846886754387,
    "output_throughput": 4740.550835883941,
    "total_throughput": 10144.397722638327,
    "itl": 174.7095606452895,
    "ttft": 193050.721643947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 727,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.430248805098245,
    "arrivals": 81441,
    "finished_requests": 78159,
    "scheduler_time": 66.58086648642133
}
#Debug simulation 
Total elapsed time: 6.440457001328468. Arrivals time: 0.1831940240226686 Scheduler time: 6.157225570175797 Scheduler overhead time: 0.03384420555084944 Adapter cache time: 0.01610753545537591 Engine time: 0.03451237082481384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.96221630088985,
    "estimated_duration": 3600.0161781898764,
    "input_throughput": 5406.938757088372,
    "output_throughput": 4744.460900891857,
    "total_throughput": 10151.399657980228,
    "itl": 175.31104990200132,
    "ttft": 80948.23946213997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 913,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.794225310210665,
    "arrivals": 79515,
    "finished_requests": 77993,
    "scheduler_time": 66.19608487877342
}
#Debug simulation 
Total elapsed time: 5.962311957962811. Arrivals time: 0.17967345099896193 Scheduler time: 5.6809319499880075 Scheduler overhead time: 0.03352350881323218 Adapter cache time: 0.01871488941833377 Engine time: 0.034040496684610844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.954048899002373,
    "estimated_duration": 3600.1416874990578,
    "input_throughput": 5406.733870388843,
    "output_throughput": 4744.239111284831,
    "total_throughput": 10150.972981673673,
    "itl": 175.32563682766414,
    "ttft": 81181.65852455338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 912,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9745346319256405,
    "arrivals": 79515,
    "finished_requests": 77992,
    "scheduler_time": 66.19704007740849
}
#Debug simulation 
Total elapsed time: 5.9541434701532125. Arrivals time: 0.17860679980367422 Scheduler time: 5.674130535684526 Scheduler overhead time: 0.03332469007000327 Adapter cache time: 0.018578553572297096 Engine time: 0.034109581261873245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.903317573945969,
    "estimated_duration": 3600.108742250238,
    "input_throughput": 5391.975740117998,
    "output_throughput": 4733.0975867549505,
    "total_throughput": 10125.073326872947,
    "itl": 173.40442345341813,
    "ttft": 94196.25242767765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 943,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0818297009542306,
    "arrivals": 79515,
    "finished_requests": 77793,
    "scheduler_time": 66.03420879043753
}
#Debug simulation 
Total elapsed time: 5.903442287817597. Arrivals time: 0.17407673271372914 Scheduler time: 5.62623624689877 Scheduler overhead time: 0.03384885797277093 Adapter cache time: 0.019319846760481596 Engine time: 0.03437446430325508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.951931823976338,
    "estimated_duration": 3600.026161737298,
    "input_throughput": 5406.923762633592,
    "output_throughput": 4744.447743612363,
    "total_throughput": 10151.371506245956,
    "itl": 175.31682189622006,
    "ttft": 80994.72569019994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 914,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.830560250647795,
    "arrivals": 79515,
    "finished_requests": 77993,
    "scheduler_time": 66.19635542212204
}
#Debug simulation 
Total elapsed time: 5.952022730838507. Arrivals time: 0.17691227607429028 Scheduler time: 5.6729768961668015 Scheduler overhead time: 0.03359815198928118 Adapter cache time: 0.018850131426006556 Engine time: 0.03429943462833762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.919046844355762,
    "estimated_duration": 3600.0773006441723,
    "input_throughput": 5391.607562572858,
    "output_throughput": 4732.664767212456,
    "total_throughput": 10124.272329785314,
    "itl": 173.40833438401046,
    "ttft": 94374.08157773354,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 943,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1294903860055,
    "arrivals": 79515,
    "finished_requests": 77789,
    "scheduler_time": 66.03326737787285
}
#Debug simulation 
Total elapsed time: 5.919134634081274. Arrivals time: 0.17409434309229255 Scheduler time: 5.642344717867672 Scheduler overhead time: 0.03371040662750602 Adapter cache time: 0.01908201491460204 Engine time: 0.0343866772018373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.949346973095089,
    "estimated_duration": 3600.0560401272674,
    "input_throughput": 5407.080829583935,
    "output_throughput": 4744.594197871477,
    "total_throughput": 10151.675027455412,
    "itl": 175.306161910192,
    "ttft": 80967.90805569281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 914,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7329059695405284,
    "arrivals": 79515,
    "finished_requests": 77995,
    "scheduler_time": 66.19739951976165
}
#Debug simulation 
Total elapsed time: 5.949433776084334. Arrivals time: 0.17345337197184563 Scheduler time: 5.674404362216592 Scheduler overhead time: 0.033456132747232914 Adapter cache time: 0.018570781219750643 Engine time: 0.03412456624209881 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 135, 135, 4320, 135, 1080, 1080, 1080, 135, 4320, 1080, 135, 4320, 1080, 135, 135, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 1080, 135, 4320, 4320, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 135, 1080, 4320, 4320, 1080, 135, 135, 135, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 135, 135, 1080, 135, 4320, 4320, 135, 135, 1080, 1080, 1080, 135, 4320, 135, 4320, 1080, 135, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 1080, 4320, 4320, 1080, 4320, 135, 1080, 4320, 135, 4320, 1080, 135, 1080, 1080, 4320, 4320, 135, 135, 4320, 135, 135, 135, 1080, 135]
Prompts retrieved: 237870 . Total input tokens: 53041664 . Total output tokens: 47565174
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.895840439014137,
    "estimated_duration": 3600.0823360420413,
    "input_throughput": 5391.451969217777,
    "output_throughput": 4732.569816370176,
    "total_throughput": 10124.021785587955,
    "itl": 173.39807901727812,
    "ttft": 94566.88079028597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 942,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1572965586930835,
    "arrivals": 79515,
    "finished_requests": 77786,
    "scheduler_time": 66.03227364696117
}
#Debug simulation 
Total elapsed time: 5.895958392880857. Arrivals time: 0.1737814643420279 Scheduler time: 5.619072398170829 Scheduler overhead time: 0.033875214867293835 Adapter cache time: 0.019072269555181265 Engine time: 0.034576715901494026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.59198462869972,
    "estimated_duration": 3600.120832273157,
    "input_throughput": 5440.384896090599,
    "output_throughput": 4713.60262352229,
    "total_throughput": 10153.987519612889,
    "itl": 163.46657187542584,
    "ttft": 37470.29376366183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 967,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9594916483830396,
    "arrivals": 78583,
    "finished_requests": 77781,
    "scheduler_time": 65.2497036513464
}
#Debug simulation 
Total elapsed time: 5.592069322708994. Arrivals time: 0.16590023878961802 Scheduler time: 5.318843134213239 Scheduler overhead time: 0.0346325826831162 Adapter cache time: 0.020633202977478504 Engine time: 0.03589783562347293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.664137861225754,
    "estimated_duration": 3600.1711070683796,
    "input_throughput": 5440.308923524727,
    "output_throughput": 4713.536800149008,
    "total_throughput": 10153.845723673734,
    "itl": 163.52076325349844,
    "ttft": 37517.74956901231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 968,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1546439033118143,
    "arrivals": 78583,
    "finished_requests": 77781,
    "scheduler_time": 65.25196555795004
}
#Debug simulation 
Total elapsed time: 5.664227283094078. Arrivals time: 0.16688694152981043 Scheduler time: 5.390103516168892 Scheduler overhead time: 0.03457623487338424 Adapter cache time: 0.02089740475639701 Engine time: 0.035699466709047556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.686194323003292,
    "estimated_duration": 3600.1482618098134,
    "input_throughput": 5440.090678419575,
    "output_throughput": 4713.160893954422,
    "total_throughput": 10153.251572373996,
    "itl": 163.38187213995545,
    "ttft": 37939.70238427523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 967,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1575721332803046,
    "arrivals": 78583,
    "finished_requests": 77776,
    "scheduler_time": 65.24852599530865
}
#Debug simulation 
Total elapsed time: 5.686287583783269. Arrivals time: 0.17070436291396618 Scheduler time: 5.408956777770072 Scheduler overhead time: 0.03448167396709323 Adapter cache time: 0.020734301768243313 Engine time: 0.03534866077825427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.661786896176636,
    "estimated_duration": 3600.1550200754323,
    "input_throughput": 5440.333233092175,
    "output_throughput": 4713.557862195736,
    "total_throughput": 10153.89109528791,
    "itl": 163.47417313882437,
    "ttft": 37516.01395834466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 968,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9952917291200323,
    "arrivals": 78583,
    "finished_requests": 77781,
    "scheduler_time": 65.25040021891546
}
#Debug simulation 
Total elapsed time: 5.661872320342809. Arrivals time: 0.16834637662395835 Scheduler time: 5.386569153517485 Scheduler overhead time: 0.03467810293659568 Adapter cache time: 0.020735811442136765 Engine time: 0.03547480842098594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.657993114087731,
    "estimated_duration": 3600.0858419514125,
    "input_throughput": 5439.800288035551,
    "output_throughput": 4712.915676144866,
    "total_throughput": 10152.715964180416,
    "itl": 163.39347364594565,
    "ttft": 38079.819826316205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 968,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.209533875044447,
    "arrivals": 78583,
    "finished_requests": 77772,
    "scheduler_time": 65.24753954536155
}
#Debug simulation 
Total elapsed time: 5.658083077985793. Arrivals time: 0.177881121635437 Scheduler time: 5.372744097840041 Scheduler overhead time: 0.03478882787749171 Adapter cache time: 0.020741119049489498 Engine time: 0.03577048145234585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.680687709711492,
    "estimated_duration": 3600.1131109878834,
    "input_throughput": 5440.396564269483,
    "output_throughput": 4713.612732946466,
    "total_throughput": 10154.00929721595,
    "itl": 163.45156875683185,
    "ttft": 37469.91248804122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 969,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8973587357601422,
    "arrivals": 78583,
    "finished_requests": 77781,
    "scheduler_time": 65.24948249244228
}
#Debug simulation 
Total elapsed time: 5.680777007713914. Arrivals time: 0.1675847116857767 Scheduler time: 5.405726828612387 Scheduler overhead time: 0.034624260384589434 Adapter cache time: 0.02084725722670555 Engine time: 0.035917301662266254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 66, 66, 4320, 66, 1080, 1080, 1080, 66, 4320, 1080, 66, 4320, 1080, 66, 66, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 1080, 66, 4320, 4320, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 66, 1080, 4320, 4320, 1080, 66, 66, 66, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 66, 66, 1080, 66, 4320, 4320, 66, 66, 1080, 1080, 1080, 66, 4320, 66, 4320, 1080, 66, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 1080, 4320, 4320, 1080, 4320, 66, 1080, 4320, 66, 4320, 1080, 66, 1080, 1080, 4320, 4320, 66, 66, 4320, 66, 66, 66, 1080, 66]
Prompts retrieved: 234972 . Total input tokens: 52388090 . Total output tokens: 46994386
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.640108530875295,
    "estimated_duration": 3600.057572572167,
    "input_throughput": 5439.709672756194,
    "output_throughput": 4712.9249068863,
    "total_throughput": 10152.634579642494,
    "itl": 163.39658884254703,
    "ttft": 38088.37507328511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 967,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2370885401592027,
    "arrivals": 78583,
    "finished_requests": 77771,
    "scheduler_time": 65.24678096976244
}
#Debug simulation 
Total elapsed time: 5.640194596722722. Arrivals time: 0.16896314825862646 Scheduler time: 5.364571382757276 Scheduler overhead time: 0.03438657568767667 Adapter cache time: 0.020743394270539284 Engine time: 0.03555495943874121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.588799988850951,
    "estimated_duration": 3600.1533989280124,
    "input_throughput": 5362.61455018796,
    "output_throughput": 4672.008421921226,
    "total_throughput": 10034.622972109186,
    "itl": 138.65755283881484,
    "ttft": 36404.111426970725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4720946048316697,
    "arrivals": 78101,
    "finished_requests": 77317,
    "scheduler_time": 63.937141837583845
}
#Debug simulation 
Total elapsed time: 5.588914583902806. Arrivals time: 0.16785365296527743 Scheduler time: 5.302813679911196 Scheduler overhead time: 0.03897225484251976 Adapter cache time: 0.020581987220793962 Engine time: 0.04046087944880128 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.587945118080825,
    "estimated_duration": 3600.0613473898593,
    "input_throughput": 5362.585283163883,
    "output_throughput": 4671.877331255551,
    "total_throughput": 10034.462614419435,
    "itl": 138.67880254076294,
    "ttft": 36497.92511189149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5730506469332686,
    "arrivals": 78101,
    "finished_requests": 77313,
    "scheduler_time": 63.93625834768118
}
#Debug simulation 
Total elapsed time: 5.588058663997799. Arrivals time: 0.16667751921340823 Scheduler time: 5.303454260807484 Scheduler overhead time: 0.038862436544150114 Adapter cache time: 0.02051848266273737 Engine time: 0.04036223515868187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.585912170819938,
    "estimated_duration": 3600.069167134787,
    "input_throughput": 5362.573635040716,
    "output_throughput": 4671.867183425782,
    "total_throughput": 10034.440818466499,
    "itl": 138.67882689134257,
    "ttft": 36497.82222371767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5752242394536828,
    "arrivals": 78101,
    "finished_requests": 77313,
    "scheduler_time": 63.93645523935758
}
#Debug simulation 
Total elapsed time: 5.585998693946749. Arrivals time: 0.16730235796421766 Scheduler time: 5.301357709802687 Scheduler overhead time: 0.03853142214938998 Adapter cache time: 0.020372946746647358 Engine time: 0.04032799322158098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.553963765967637,
    "estimated_duration": 3600.0350238071387,
    "input_throughput": 5362.481162637216,
    "output_throughput": 4671.886492430143,
    "total_throughput": 10034.367655067359,
    "itl": 138.65625529605467,
    "ttft": 36590.213447984956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4933745598373869,
    "arrivals": 78101,
    "finished_requests": 77311,
    "scheduler_time": 63.93474976044634
}
#Debug simulation 
Total elapsed time: 5.554050808772445. Arrivals time: 0.16766763851046562 Scheduler time: 5.268063626717776 Scheduler overhead time: 0.03901875717565417 Adapter cache time: 0.02035396872088313 Engine time: 0.040717947762459517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.575719855725765,
    "estimated_duration": 3600.0516723922947,
    "input_throughput": 5362.45636362531,
    "output_throughput": 4671.864887101335,
    "total_throughput": 10034.321250726645,
    "itl": 138.67751854388595,
    "ttft": 36589.85237424162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.599746227804574,
    "arrivals": 78101,
    "finished_requests": 77311,
    "scheduler_time": 63.935876414554166
}
#Debug simulation 
Total elapsed time: 5.575833253096789. Arrivals time: 0.16594471875578165 Scheduler time: 5.292347633745521 Scheduler overhead time: 0.03871844056993723 Adapter cache time: 0.020344650372862816 Engine time: 0.04029634315520525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.582157616037875,
    "estimated_duration": 3600.1315869674727,
    "input_throughput": 5362.647040427312,
    "output_throughput": 4672.036728015289,
    "total_throughput": 10034.683768442601,
    "itl": 138.6579202308083,
    "ttft": 36403.92040667566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4382141918479323,
    "arrivals": 78101,
    "finished_requests": 77317,
    "scheduler_time": 63.936937406599846
}
#Debug simulation 
Total elapsed time: 5.5822696359828115. Arrivals time: 0.16630456876009703 Scheduler time: 5.298680578358471 Scheduler overhead time: 0.03862720914185047 Adapter cache time: 0.020332927349954844 Engine time: 0.040231029503047466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 1080, 4320, 4320, 33, 33, 4320, 33, 1080, 1080, 1080, 33, 4320, 1080, 33, 4320, 1080, 33, 33, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 1080, 33, 4320, 4320, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 4320, 4320, 4320, 4320, 1080, 1080, 33, 1080, 4320, 4320, 1080, 33, 33, 33, 4320, 1080, 1080, 1080, 1080, 4320, 1080, 4320, 33, 33, 1080, 33, 4320, 4320, 33, 33, 1080, 1080, 1080, 33, 4320, 33, 4320, 1080, 33, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 1080, 4320, 4320, 1080, 4320, 33, 1080, 4320, 33, 4320, 1080, 33, 1080, 1080, 4320, 4320, 33, 33, 4320, 33, 33, 33, 1080, 33]
Prompts retrieved: 233586 . Total input tokens: 52087403 . Total output tokens: 46722705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.57097644591704,
    "estimated_duration": 3600.1032298674177,
    "input_throughput": 5362.678169845423,
    "output_throughput": 4671.961587229102,
    "total_throughput": 10034.639757074527,
    "itl": 138.6842022293328,
    "ttft": 36450.667381497085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 481,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6167229889705759,
    "arrivals": 78101,
    "finished_requests": 77316,
    "scheduler_time": 63.9370161382008
}
#Debug simulation 
Total elapsed time: 5.571064229123294. Arrivals time: 0.16742033790796995 Scheduler time: 5.286528495606035 Scheduler overhead time: 0.038555804174393415 Adapter cache time: 0.020351677667349577 Engine time: 0.03996461117640138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.563403162173927,
    "estimated_duration": 3600.0345795658764,
    "input_throughput": 5012.550185608513,
    "output_throughput": 4469.655400349066,
    "total_throughput": 9482.20558595758,
    "itl": 139.27510386872518,
    "ttft": 35725.698191530515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.2564164408647756,
    "arrivals": 73678,
    "finished_requests": 72970,
    "scheduler_time": 60.927108055566904
}
#Debug simulation 
Total elapsed time: 5.563493762165308. Arrivals time: 0.1614024480804801 Scheduler time: 5.263957274612039 Scheduler overhead time: 0.04065098753198981 Adapter cache time: 0.037695088889449835 Engine time: 0.041206346824765205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.5448212921619415,
    "estimated_duration": 3600.0683397638963,
    "input_throughput": 5012.503179643381,
    "output_throughput": 4469.645984846859,
    "total_throughput": 9482.149164490242,
    "itl": 139.39691361836356,
    "ttft": 35756.72958812765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.613436854784058,
    "arrivals": 73678,
    "finished_requests": 72970,
    "scheduler_time": 60.93197344507663
}
#Debug simulation 
Total elapsed time: 5.544907070230693. Arrivals time: 0.15919870091602206 Scheduler time: 5.2488993848674 Scheduler overhead time: 0.04031359730288386 Adapter cache time: 0.037200165912508965 Engine time: 0.04086136585101485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.53639449365437,
    "estimated_duration": 3600.076130590762,
    "input_throughput": 5012.7717707565935,
    "output_throughput": 4469.659089503614,
    "total_throughput": 9482.430860260207,
    "itl": 139.40271920290752,
    "ttft": 35707.50620013004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.645941586811045,
    "arrivals": 73678,
    "finished_requests": 72971,
    "scheduler_time": 60.93226801513246
}
#Debug simulation 
Total elapsed time: 5.536479563917965. Arrivals time: 0.15816014260053635 Scheduler time: 5.240915780421346 Scheduler overhead time: 0.040528930723667145 Adapter cache time: 0.03725096536800265 Engine time: 0.04113867972046137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.495116090867668,
    "estimated_duration": 3600.0692663414757,
    "input_throughput": 5012.781606377086,
    "output_throughput": 4469.7073332571,
    "total_throughput": 9482.488939634186,
    "itl": 139.2972956101987,
    "ttft": 35629.38704170988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.3574477781096705,
    "arrivals": 73678,
    "finished_requests": 72972,
    "scheduler_time": 60.92828474978389
}
#Debug simulation 
Total elapsed time: 5.49520128313452. Arrivals time: 0.1538755688816309 Scheduler time: 5.204474770929664 Scheduler overhead time: 0.04038114473223686 Adapter cache time: 0.03713007131591439 Engine time: 0.04081380274146795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.438250869046897,
    "estimated_duration": 3600.031101840397,
    "input_throughput": 5012.580583199646,
    "output_throughput": 4469.291665778868,
    "total_throughput": 9481.872248978514,
    "itl": 139.37929920219196,
    "ttft": 35876.32190833079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.85577321469763,
    "arrivals": 73678,
    "finished_requests": 72967,
    "scheduler_time": 60.93023685061086
}
#Debug simulation 
Total elapsed time: 5.43833040073514. Arrivals time: 0.15422324743121862 Scheduler time: 5.147977042943239 Scheduler overhead time: 0.04028317146003246 Adapter cache time: 0.03705586865544319 Engine time: 0.04043636051937938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.494876055978239,
    "estimated_duration": 3599.999595422417,
    "input_throughput": 5012.785285572377,
    "output_throughput": 4469.533557853634,
    "total_throughput": 9482.318843426012,
    "itl": 139.25725438861525,
    "ttft": 35774.946389948775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2375,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.101369450392451,
    "arrivals": 73678,
    "finished_requests": 72969,
    "scheduler_time": 60.926115636917885
}
#Debug simulation 
Total elapsed time: 5.494960870128125. Arrivals time: 0.15499232383444905 Scheduler time: 5.202878440264612 Scheduler overhead time: 0.04050591913983226 Adapter cache time: 0.03722467180341482 Engine time: 0.040885588619858027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 270, 270, 4320, 270, 540, 540, 540, 270, 4320, 540, 270, 4320, 540, 270, 270, 270, 270, 540, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 540, 270, 540, 270, 4320, 4320, 270, 540, 540, 270, 540, 270, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 270, 540, 4320, 4320, 540, 270, 270, 270, 4320, 540, 540, 540, 540, 4320, 540, 4320, 270, 270, 540, 270, 4320, 4320, 270, 270, 540, 540, 540, 270, 4320, 270, 4320, 540, 270, 4320, 4320, 540, 540, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 270, 4320, 270, 270, 4320, 540, 4320, 4320, 540, 4320, 270, 540, 4320, 270, 4320, 540, 270, 540, 540, 4320, 4320, 270, 270, 4320, 270, 270, 270, 540, 270]
Prompts retrieved: 220320 . Total input tokens: 49124623 . Total output tokens: 44066949
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.535098497290164,
    "estimated_duration": 3599.974200587131,
    "input_throughput": 5012.203697753495,
    "output_throughput": 4469.210639725137,
    "total_throughput": 9481.414337478633,
    "itl": 139.3935117880373,
    "ttft": 35975.416326793966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2370,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.932180660888223,
    "arrivals": 73678,
    "finished_requests": 72965,
    "scheduler_time": 60.929856817441184
}
#Debug simulation 
Total elapsed time: 5.53518416499719. Arrivals time: 0.15571032837033272 Scheduler time: 5.241857858840376 Scheduler overhead time: 0.0406247703358531 Adapter cache time: 0.03733709920197725 Engine time: 0.0410253694280982 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.130708435084671,
    "estimated_duration": 3600.116790825019,
    "input_throughput": 4948.618346327952,
    "output_throughput": 4331.606696688955,
    "total_throughput": 9280.225043016906,
    "itl": 104.76706927060675,
    "ttft": 26988.139719651776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.152359857571058,
    "arrivals": 71837,
    "finished_requests": 71302,
    "scheduler_time": 57.25159069639088
}
#Debug simulation 
Total elapsed time: 5.1307951039634645. Arrivals time: 0.15161411790177226 Scheduler time: 4.811046985909343 Scheduler overhead time: 0.04776510549709201 Adapter cache time: 0.04897528747096658 Engine time: 0.04896898800507188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.167313918936998,
    "estimated_duration": 3600.110732094446,
    "input_throughput": 4948.626674501028,
    "output_throughput": 4331.613986475262,
    "total_throughput": 9280.240660976291,
    "itl": 104.84609470995326,
    "ttft": 26988.30325876281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.632362122498124,
    "arrivals": 71837,
    "finished_requests": 71302,
    "scheduler_time": 57.25584254566399
}
#Debug simulation 
Total elapsed time: 5.167399602942169. Arrivals time: 0.1528078787960112 Scheduler time: 4.844570229295641 Scheduler overhead time: 0.04800794506445527 Adapter cache time: 0.04970661969855428 Engine time: 0.049695517867803574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.146308945957571,
    "estimated_duration": 3600.0034715935994,
    "input_throughput": 4948.634672319874,
    "output_throughput": 4331.742489430694,
    "total_throughput": 9280.377161750568,
    "itl": 104.84751880017193,
    "ttft": 26988.514254608635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.6486146925762,
    "arrivals": 71837,
    "finished_requests": 71301,
    "scheduler_time": 57.25408994469906
}
#Debug simulation 
Total elapsed time: 5.146391368936747. Arrivals time: 0.1511122602969408 Scheduler time: 4.826063886284828 Scheduler overhead time: 0.04806592548266053 Adapter cache time: 0.04901725007221103 Engine time: 0.04956047097221017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.199085460975766,
    "estimated_duration": 3600.0757961212125,
    "input_throughput": 4948.674697125782,
    "output_throughput": 4331.656021465318,
    "total_throughput": 9280.3307185911,
    "itl": 104.78254932915306,
    "ttft": 26938.33513714356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.2578290446752,
    "arrivals": 71837,
    "finished_requests": 71302,
    "scheduler_time": 57.251731696218854
}
#Debug simulation 
Total elapsed time: 5.199168636929244. Arrivals time: 0.151828043628484 Scheduler time: 4.8786305277608335 Scheduler overhead time: 0.04783062543720007 Adapter cache time: 0.049140202812850475 Engine time: 0.0492796809412539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.210148598998785,
    "estimated_duration": 3600.0801334681546,
    "input_throughput": 4948.668735003199,
    "output_throughput": 4331.650802721762,
    "total_throughput": 9280.319537724961,
    "itl": 104.8653681106358,
    "ttft": 26938.7628044369,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2336,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.748230021111481,
    "arrivals": 71837,
    "finished_requests": 71302,
    "scheduler_time": 57.25637511004224
}
#Debug simulation 
Total elapsed time: 5.210230836179107. Arrivals time: 0.15109417820349336 Scheduler time: 4.8900974546559155 Scheduler overhead time: 0.048135106917470694 Adapter cache time: 0.049101839773356915 Engine time: 0.04922611964866519 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.216674274299294,
    "estimated_duration": 3600.0465319091777,
    "input_throughput": 4948.714924124057,
    "output_throughput": 4331.691232815825,
    "total_throughput": 9280.406156939882,
    "itl": 104.73578368442708,
    "ttft": 26938.31084399368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.987747539186173,
    "arrivals": 71837,
    "finished_requests": 71302,
    "scheduler_time": 57.248669109284656
}
#Debug simulation 
Total elapsed time: 5.21675454126671. Arrivals time: 0.15162583906203508 Scheduler time: 4.895943165756762 Scheduler overhead time: 0.04816097393631935 Adapter cache time: 0.04887964203953743 Engine time: 0.04954633070155978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 135, 135, 4320, 135, 540, 540, 540, 135, 4320, 540, 135, 4320, 540, 135, 135, 135, 135, 540, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 540, 135, 540, 135, 4320, 4320, 135, 540, 540, 135, 540, 135, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 135, 540, 4320, 4320, 540, 135, 135, 135, 4320, 540, 540, 540, 540, 4320, 540, 4320, 135, 135, 540, 135, 4320, 4320, 135, 135, 540, 540, 540, 135, 4320, 135, 4320, 540, 135, 4320, 4320, 540, 540, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 540, 4320, 4320, 540, 4320, 135, 540, 4320, 135, 4320, 540, 135, 540, 540, 4320, 4320, 135, 135, 4320, 135, 135, 135, 540, 135]
Prompts retrieved: 214650 . Total input tokens: 47843602 . Total output tokens: 42937940
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.149291425012052,
    "estimated_duration": 3600.0550626122777,
    "input_throughput": 4948.70319763182,
    "output_throughput": 4331.68096842509,
    "total_throughput": 9280.38416605691,
    "itl": 104.88045384233932,
    "ttft": 26938.65836229081,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.834522546566707,
    "arrivals": 71837,
    "finished_requests": 71302,
    "scheduler_time": 57.25708326965922
}
#Debug simulation 
Total elapsed time: 5.149372474756092. Arrivals time: 0.15017704106867313 Scheduler time: 4.830802170094103 Scheduler overhead time: 0.04792327154427767 Adapter cache time: 0.04898826312273741 Engine time: 0.04900086252018809 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.188304392620921,
    "estimated_duration": 3600.0186968660328,
    "input_throughput": 4861.602528685782,
    "output_throughput": 4322.411717901992,
    "total_throughput": 9184.014246587774,
    "itl": 90.06807882262555,
    "ttft": 19397.505727627715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.700129682414792,
    "arrivals": 70891,
    "finished_requests": 70510,
    "scheduler_time": 55.980007036953914
}
#Debug simulation 
Total elapsed time: 5.188386135734618. Arrivals time: 0.1519338358193636 Scheduler time: 4.853234199341387 Scheduler overhead time: 0.05439801560714841 Adapter cache time: 0.04706162679940462 Engine time: 0.056042717304080725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.189446193166077,
    "estimated_duration": 3600.052344221804,
    "input_throughput": 4861.557090438151,
    "output_throughput": 4322.371319121376,
    "total_throughput": 9183.928409559527,
    "itl": 90.09760858671144,
    "ttft": 19397.614113489817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9430728472443346,
    "arrivals": 70891,
    "finished_requests": 70510,
    "scheduler_time": 55.98280738835991
}
#Debug simulation 
Total elapsed time: 5.189527242910117. Arrivals time: 0.1519121560268104 Scheduler time: 4.854640694800764 Scheduler overhead time: 0.054424034897238016 Adapter cache time: 0.04710185620933771 Engine time: 0.05582677433267236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.1654551778919995,
    "estimated_duration": 3600.0612773848866,
    "input_throughput": 4861.54502700951,
    "output_throughput": 4322.360593623968,
    "total_throughput": 9183.905620633479,
    "itl": 90.09811072527899,
    "ttft": 19397.612441241203,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9504240262694053,
    "arrivals": 70891,
    "finished_requests": 70510,
    "scheduler_time": 55.98294950428672
}
#Debug simulation 
Total elapsed time: 5.165538888890296. Arrivals time: 0.15159977227449417 Scheduler time: 4.829661106690764 Scheduler overhead time: 0.05473388032987714 Adapter cache time: 0.047143451403826475 Engine time: 0.05673182988539338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.149265957996249,
    "estimated_duration": 3600.035129381503,
    "input_throughput": 4861.580337691558,
    "output_throughput": 4322.3919880674575,
    "total_throughput": 9183.972325759016,
    "itl": 90.07546242981427,
    "ttft": 19397.358832332095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7570131481671183,
    "arrivals": 70891,
    "finished_requests": 70510,
    "scheduler_time": 55.98088100658045
}
#Debug simulation 
Total elapsed time: 5.149349362123758. Arrivals time: 0.15071877045556903 Scheduler time: 4.8179243840277195 Scheduler overhead time: 0.053811860736459494 Adapter cache time: 0.04696617508307099 Engine time: 0.054619196336716413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.147278290241957,
    "estimated_duration": 3600.0275008940675,
    "input_throughput": 4861.590639419674,
    "output_throughput": 4322.401147251092,
    "total_throughput": 9183.991786670766,
    "itl": 90.10420893802498,
    "ttft": 19397.46777938302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1206,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0001862556301155,
    "arrivals": 70891,
    "finished_requests": 70510,
    "scheduler_time": 55.982934502807744
}
#Debug simulation 
Total elapsed time: 5.147358736954629. Arrivals time: 0.15160943754017353 Scheduler time: 4.814331021159887 Scheduler overhead time: 0.053965114057064056 Adapter cache time: 0.047139111906290054 Engine time: 0.054833062924444675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.15611427789554,
    "estimated_duration": 3600.014602165044,
    "input_throughput": 4861.608058332432,
    "output_throughput": 4322.416634266366,
    "total_throughput": 9184.024692598798,
    "itl": 90.06097328637883,
    "ttft": 19397.510839080827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.617960856831541,
    "arrivals": 70891,
    "finished_requests": 70510,
    "scheduler_time": 55.97940472461004
}
#Debug simulation 
Total elapsed time: 5.156191966962069. Arrivals time: 0.15195548022165895 Scheduler time: 4.822320752777159 Scheduler overhead time: 0.053964479360729456 Adapter cache time: 0.04709388688206673 Engine time: 0.055411793291568756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 66, 66, 4320, 66, 540, 540, 540, 66, 4320, 540, 66, 4320, 540, 66, 66, 66, 66, 540, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 540, 66, 540, 66, 4320, 4320, 66, 540, 540, 66, 540, 66, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 66, 540, 4320, 4320, 540, 66, 66, 66, 4320, 540, 540, 540, 540, 4320, 540, 4320, 66, 66, 540, 66, 4320, 4320, 66, 66, 540, 540, 540, 66, 4320, 66, 4320, 540, 66, 4320, 4320, 540, 540, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 540, 4320, 4320, 540, 4320, 66, 540, 4320, 66, 4320, 540, 66, 540, 540, 4320, 4320, 66, 66, 4320, 66, 66, 66, 540, 66]
Prompts retrieved: 211752 . Total input tokens: 47203189 . Total output tokens: 42355653
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.15426962915808,
    "estimated_duration": 3600.057068458403,
    "input_throughput": 4861.550710776524,
    "output_throughput": 4322.365647015519,
    "total_throughput": 9183.916357792044,
    "itl": 90.11017544178414,
    "ttft": 19397.588643459774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.051867318041678,
    "arrivals": 70891,
    "finished_requests": 70510,
    "scheduler_time": 55.98381055818071
}
#Debug simulation 
Total elapsed time: 5.1543468572199345. Arrivals time: 0.1501583606004715 Scheduler time: 4.8227717275731266 Scheduler overhead time: 0.05381100159138441 Adapter cache time: 0.04728627437725663 Engine time: 0.05498138349503279 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.094449102878571,
    "estimated_duration": 3600.090738984365,
    "input_throughput": 4805.371101529653,
    "output_throughput": 4280.575440258904,
    "total_throughput": 9085.946541788557,
    "itl": 81.53061350816928,
    "ttft": 22270.458927474847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7628409405052914,
    "arrivals": 70422,
    "finished_requests": 69989,
    "scheduler_time": 54.68716111213544
}
#Debug simulation 
Total elapsed time: 5.094532134011388. Arrivals time: 0.15269921254366636 Scheduler time: 4.7508838889189065 Scheduler overhead time: 0.05820906441658735 Adapter cache time: 0.045061788987368345 Engine time: 0.06003788113594055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.077118564862758,
    "estimated_duration": 3600.0309194197375,
    "input_throughput": 4805.35872808236,
    "output_throughput": 4280.487680501313,
    "total_throughput": 9085.846408583673,
    "itl": 81.53955828569227,
    "ttft": 22321.93619268075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8806740703526952,
    "arrivals": 70422,
    "finished_requests": 69987,
    "scheduler_time": 54.68708666844483
}
#Debug simulation 
Total elapsed time: 5.077198639046401. Arrivals time: 0.1509971092455089 Scheduler time: 4.734994142781943 Scheduler overhead time: 0.058065364602953196 Adapter cache time: 0.04494229191914201 Engine time: 0.06065140152350068 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.05855803610757,
    "estimated_duration": 3600.035074700391,
    "input_throughput": 4805.353181576912,
    "output_throughput": 4280.4827398195475,
    "total_throughput": 9085.83592139646,
    "itl": 81.53940519669966,
    "ttft": 22321.867611548714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8832360983826337,
    "arrivals": 70422,
    "finished_requests": 69987,
    "scheduler_time": 54.68711101552848
}
#Debug simulation 
Total elapsed time: 5.058650132268667. Arrivals time: 0.1512518459931016 Scheduler time: 4.717155257705599 Scheduler overhead time: 0.05793040245771408 Adapter cache time: 0.04496456729248166 Engine time: 0.05984754581004381 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.091686478815973,
    "estimated_duration": 3600.087879704193,
    "input_throughput": 4805.374918076018,
    "output_throughput": 4280.57883999938,
    "total_throughput": 9085.953758075397,
    "itl": 81.53150263783319,
    "ttft": 22270.37273366756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7831496723112503,
    "arrivals": 70422,
    "finished_requests": 69989,
    "scheduler_time": 54.68716878449778
}
#Debug simulation 
Total elapsed time: 5.091763553209603. Arrivals time: 0.15046057803556323 Scheduler time: 4.751859413925558 Scheduler overhead time: 0.057774610351771116 Adapter cache time: 0.04483935469761491 Engine time: 0.0593703999184072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.103553431108594,
    "estimated_duration": 3600.0472601553574,
    "input_throughput": 4805.33691639744,
    "output_throughput": 4280.468251223735,
    "total_throughput": 9085.805167621174,
    "itl": 81.54229560085355,
    "ttft": 22321.79465232733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.914171529840679,
    "arrivals": 70422,
    "finished_requests": 69987,
    "scheduler_time": 54.687570976718874
}
#Debug simulation 
Total elapsed time: 5.103633084800094. Arrivals time: 0.15048796124756336 Scheduler time: 4.761571524199098 Scheduler overhead time: 0.05822114832699299 Adapter cache time: 0.044955873396247625 Engine time: 0.06073310738429427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.119685067795217,
    "estimated_duration": 3600.0185648477977,
    "input_throughput": 4805.3752191501235,
    "output_throughput": 4280.502370312499,
    "total_throughput": 9085.877589462623,
    "itl": 81.52684502179687,
    "ttft": 22321.81806970431,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7192789195687268,
    "arrivals": 70422,
    "finished_requests": 69987,
    "scheduler_time": 54.68564441291187
}
#Debug simulation 
Total elapsed time: 5.119761283975095. Arrivals time: 0.15114816464483738 Scheduler time: 4.777249380480498 Scheduler overhead time: 0.0588656235486269 Adapter cache time: 0.044833061285316944 Engine time: 0.060018157586455345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [42 43 43]
Adapter prompts. [4320, 540, 4320, 4320, 33, 33, 4320, 33, 540, 540, 540, 33, 4320, 540, 33, 4320, 540, 33, 33, 33, 33, 540, 540, 4320, 540, 33, 540, 540, 540, 540, 4320, 540, 33, 540, 33, 4320, 4320, 33, 540, 540, 33, 540, 33, 540, 540, 4320, 4320, 4320, 4320, 540, 540, 33, 540, 4320, 4320, 540, 33, 33, 33, 4320, 540, 540, 540, 540, 4320, 540, 4320, 33, 33, 540, 33, 4320, 4320, 33, 33, 540, 540, 540, 33, 4320, 33, 4320, 540, 33, 4320, 4320, 540, 540, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 33, 4320, 33, 33, 4320, 540, 4320, 4320, 540, 4320, 33, 540, 4320, 33, 4320, 540, 33, 540, 540, 4320, 4320, 33, 33, 4320, 33, 33, 33, 540, 33]
Prompts retrieved: 210366 . Total input tokens: 46882551 . Total output tokens: 42078605
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.10316560510546,
    "estimated_duration": 3600.0760092590363,
    "input_throughput": 4805.298542449539,
    "output_throughput": 4280.434068716134,
    "total_throughput": 9085.732611165673,
    "itl": 81.5444894220864,
    "ttft": 22372.783533743113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9329088440164874,
    "arrivals": 70422,
    "finished_requests": 69987,
    "scheduler_time": 54.68831467427419
}
#Debug simulation 
Total elapsed time: 5.103245985228568. Arrivals time: 0.1517996615730226 Scheduler time: 4.7608054163865745 Scheduler overhead time: 0.05819593649357557 Adapter cache time: 0.04507990041747689 Engine time: 0.05977737298235297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.9229915626347065,
    "estimated_duration": 3600.036888528876,
    "input_throughput": 4652.555659462836,
    "output_throughput": 4154.818815235045,
    "total_throughput": 8807.374474697881,
    "itl": 71.33254416470753,
    "ttft": 14405.860032998587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2059,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.301544264757723,
    "arrivals": 68030,
    "finished_requests": 67760,
    "scheduler_time": 51.575114305886856
}
#Debug simulation 
Total elapsed time: 4.9230760568752885. Arrivals time: 0.14996892167255282 Scheduler time: 4.560402198228985 Scheduler overhead time: 0.06375814229249954 Adapter cache time: 0.05359342787414789 Engine time: 0.0649838438257575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.952834348194301,
    "estimated_duration": 3600.0356781762266,
    "input_throughput": 4652.557223678741,
    "output_throughput": 4154.820212108967,
    "total_throughput": 8807.377435787708,
    "itl": 71.35933374473588,
    "ttft": 14406.029790208646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2058,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.701041233618506,
    "arrivals": 68030,
    "finished_requests": 67760,
    "scheduler_time": 51.57854225976785
}
#Debug simulation 
Total elapsed time: 4.952917673159391. Arrivals time: 0.1497122603468597 Scheduler time: 4.5888823084533215 Scheduler overhead time: 0.06411182368174195 Adapter cache time: 0.05386201059445739 Engine time: 0.06572069833055139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.972803223878145,
    "estimated_duration": 3600.043840783207,
    "input_throughput": 4652.546674641632,
    "output_throughput": 4154.81079162245,
    "total_throughput": 8807.357466264082,
    "itl": 71.36180730792765,
    "ttft": 14406.12981074174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2058,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.7154734334349095,
    "arrivals": 68030,
    "finished_requests": 67760,
    "scheduler_time": 51.57885694688771
}
#Debug simulation 
Total elapsed time: 4.972883706912398. Arrivals time: 0.14994742209091783 Scheduler time: 4.608315544668585 Scheduler overhead time: 0.06451100204139948 Adapter cache time: 0.05370096303522587 Engine time: 0.06570800067856908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 4.954695867840201,
    "estimated_duration": 3600.0028781893325,
    "input_throughput": 4652.5996135937285,
    "output_throughput": 4154.858067092176,
    "total_throughput": 8807.457680685904,
    "itl": 71.33742614530375,
    "ttft": 14247.933353493665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2057,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.379755430258613,
    "arrivals": 68030,
    "finished_requests": 67760,
    "scheduler_time": 51.57534935283355
}
#Debug simulation 
Total elapsed time: 4.954773134086281. Arrivals time: 0.149034412112087 Scheduler time: 4.590573627501726 Scheduler overhead time: 0.06437912629917264 Adapter cache time: 0.05378594482317567 Engine time: 0.06634963303804398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 4.943180533125997,
    "estimated_duration": 3600.046071423608,
    "input_throughput": 4652.543791856698,
    "output_throughput": 4154.808217241837,
    "total_throughput": 8807.352009098535,
    "itl": 71.36828461962928,
    "ttft": 14406.076513075406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2057,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.810392114203272,
    "arrivals": 68030,
    "finished_requests": 67760,
    "scheduler_time": 51.57964934335833
}
#Debug simulation 
Total elapsed time: 4.943257307168096. Arrivals time: 0.14844255475327373 Scheduler time: 4.58036845875904 Scheduler overhead time: 0.06425007712095976 Adapter cache time: 0.05381042184308171 Engine time: 0.06574484100565314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.9729519239626825,
    "estimated_duration": 3600.0383153351395,
    "input_throughput": 4652.553815511474,
    "output_throughput": 4154.817168552151,
    "total_throughput": 8807.370984063624,
    "itl": 71.32262202124377,
    "ttft": 14405.995009024806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2059,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.1565135572033975,
    "arrivals": 68030,
    "finished_requests": 67760,
    "scheduler_time": 51.573785461448665
}
#Debug simulation 
Total elapsed time: 4.97303520468995. Arrivals time: 0.14869445422664285 Scheduler time: 4.609348981175572 Scheduler overhead time: 0.06425125617533922 Adapter cache time: 0.053669482469558716 Engine time: 0.06643727561458945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.4-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.4   ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 135, 135, 4320, 135, 270, 270, 270, 135, 4320, 270, 135, 4320, 270, 135, 135, 135, 135, 270, 270, 4320, 270, 135, 270, 270, 270, 270, 4320, 270, 135, 270, 135, 4320, 4320, 135, 270, 270, 135, 270, 135, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 135, 270, 4320, 4320, 270, 135, 135, 135, 4320, 270, 270, 270, 270, 4320, 270, 4320, 135, 135, 270, 135, 4320, 4320, 135, 135, 270, 270, 270, 135, 4320, 135, 4320, 270, 135, 4320, 4320, 270, 270, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 135, 4320, 135, 135, 4320, 270, 4320, 4320, 270, 4320, 135, 270, 4320, 135, 4320, 270, 135, 270, 270, 4320, 4320, 135, 135, 4320, 135, 135, 135, 270, 135]
Prompts retrieved: 203040 . Total input tokens: 45261739 . Total output tokens: 40617405
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.951786964666098,
    "estimated_duration": 3600.0565813119124,
    "input_throughput": 4652.530209371401,
    "output_throughput": 4154.796087829617,
    "total_throughput": 8807.326297201018,
    "itl": 71.37295869056302,
    "ttft": 14405.846485782135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2059,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.887027026079369,
    "arrivals": 68030,
    "finished_requests": 67760,
    "scheduler_time": 51.580327150980246
}
#Debug simulation 
Total elapsed time: 4.9518628478981555. Arrivals time: 0.1487396052107215 Scheduler time: 4.588457005098462 Scheduler overhead time: 0.06411566119641066 Adapter cache time: 0.05396548844873905 Engine time: 0.06609906861558557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.844181035179645,
    "estimated_duration": 3600.0395562463914,
    "input_throughput": 4595.349784781016,
    "output_throughput": 4081.212378488217,
    "total_throughput": 8676.562163269233,
    "itl": 64.27551000668066,
    "ttft": 14006.356106835226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7888838269888447,
    "arrivals": 67080,
    "finished_requests": 66820,
    "scheduler_time": 49.45536685012349
}
#Debug simulation 
Total elapsed time: 4.844261886086315. Arrivals time: 0.14978609094396234 Scheduler time: 4.476243398617953 Scheduler overhead time: 0.06836055405437946 Adapter cache time: 0.04775287676602602 Engine time: 0.06953071570023894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.812901432160288,
    "estimated_duration": 3600.0370006771013,
    "input_throughput": 4595.353046896041,
    "output_throughput": 4081.215275630946,
    "total_throughput": 8676.568322526986,
    "itl": 64.28912226365888,
    "ttft": 14006.321498324633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.027072967754213,
    "arrivals": 67080,
    "finished_requests": 66820,
    "scheduler_time": 49.457486366154725
}
#Debug simulation 
Total elapsed time: 4.812982163392007. Arrivals time: 0.14808612316846848 Scheduler time: 4.447698128875345 Scheduler overhead time: 0.0678373402915895 Adapter cache time: 0.04732711333781481 Engine time: 0.06944199837744236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.820266536902636,
    "estimated_duration": 3600.04311216608,
    "input_throughput": 4595.345245753492,
    "output_throughput": 4081.2083472966456,
    "total_throughput": 8676.553593050137,
    "itl": 64.28917617432204,
    "ttft": 14006.360734376847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.035297015141634,
    "arrivals": 67080,
    "finished_requests": 66820,
    "scheduler_time": 49.45752684031324
}
#Debug simulation 
Total elapsed time: 4.820351047907025. Arrivals time: 0.14727423340082169 Scheduler time: 4.4553934801369905 Scheduler overhead time: 0.06791640957817435 Adapter cache time: 0.04757978254929185 Engine time: 0.06958122085779905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 4.833278797101229,
    "estimated_duration": 3600.0195191091507,
    "input_throughput": 4595.375361768535,
    "output_throughput": 4081.235093868537,
    "total_throughput": 8676.610455637072,
    "itl": 64.2788002258872,
    "ttft": 14006.380314203705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.846882432321967,
    "arrivals": 67080,
    "finished_requests": 66820,
    "scheduler_time": 49.45563621036827
}
#Debug simulation 
Total elapsed time: 4.8333641909994185. Arrivals time: 0.1513683283701539 Scheduler time: 4.464140382129699 Scheduler overhead time: 0.06822621962055564 Adapter cache time: 0.04762725764885545 Engine time: 0.06937902187928557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 4.845749770756811,
    "estimated_duration": 3600.0498828715563,
    "input_throughput": 4595.336603170685,
    "output_throughput": 4081.2006716641945,
    "total_throughput": 8676.53727483488,
    "itl": 64.29138599196675,
    "ttft": 14006.281355041572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.090754434950652,
    "arrivals": 67080,
    "finished_requests": 66820,
    "scheduler_time": 49.45805853935699
}
#Debug simulation 
Total elapsed time: 4.845832359045744. Arrivals time: 0.15685556828975677 Scheduler time: 4.470300053246319 Scheduler overhead time: 0.06857923977077007 Adapter cache time: 0.04751915391534567 Engine time: 0.06965156551450491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 4.821453087031841,
    "estimated_duration": 3600.0350015860163,
    "input_throughput": 4595.355598684927,
    "output_throughput": 4081.217541920321,
    "total_throughput": 8676.573140605247,
    "itl": 64.27045673461042,
    "ttft": 14006.259833186305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6927121142040926,
    "arrivals": 67080,
    "finished_requests": 66820,
    "scheduler_time": 49.45452101108387
}
#Debug simulation 
Total elapsed time: 4.821530980058014. Arrivals time: 0.1501965611241758 Scheduler time: 4.453390656970441 Scheduler overhead time: 0.06827097339555621 Adapter cache time: 0.04774582665413618 Engine time: 0.06941797537729144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.4,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.025-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.4-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.4    ]. Counts: [42 43 43]
Adapter prompts. [4320, 270, 4320, 4320, 66, 66, 4320, 66, 270, 270, 270, 66, 4320, 270, 66, 4320, 270, 66, 66, 66, 66, 270, 270, 4320, 270, 66, 270, 270, 270, 270, 4320, 270, 66, 270, 66, 4320, 4320, 66, 270, 270, 66, 270, 66, 270, 270, 4320, 4320, 4320, 4320, 270, 270, 66, 270, 4320, 4320, 270, 66, 66, 66, 4320, 270, 270, 270, 270, 4320, 270, 4320, 66, 66, 270, 66, 4320, 4320, 66, 66, 270, 270, 270, 66, 4320, 66, 4320, 270, 66, 4320, 4320, 270, 270, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 4320, 66, 4320, 66, 66, 4320, 270, 4320, 4320, 270, 4320, 66, 270, 4320, 66, 4320, 270, 66, 270, 270, 4320, 4320, 66, 66, 4320, 66, 66, 66, 270, 66]
Prompts retrieved: 200142 . Total input tokens: 44611233 . Total output tokens: 40046224
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 4.814772863872349,
    "estimated_duration": 3600.063762515272,
    "input_throughput": 4595.318886363702,
    "output_throughput": 4081.1849370508676,
    "total_throughput": 8676.50382341457,
    "itl": 64.29472577382916,
    "ttft": 14059.9035790929,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.137283335924196,
    "arrivals": 67080,
    "finished_requests": 66820,
    "scheduler_time": 49.45857084886818
}
#Debug simulation 
Total elapsed time: 4.814836723729968. Arrivals time: 0.14641336631029844 Scheduler time: 4.451522580347955 Scheduler overhead time: 0.06785791274160147 Adapter cache time: 0.04746370203793049 Engine time: 0.06909203762188554 
