INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_192_slots_96_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_192_slots_96_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 10.548314905259758,
    "estimated_duration": 3600.1266943705323,
    "input_throughput": 5358.816963349453,
    "output_throughput": 4728.386094472257,
    "total_throughput": 10087.20305782171,
    "itl": 181.58547035588623,
    "ttft": 2109717.640002633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5121509983646633,
    "arrivals": 926191,
    "finished_requests": 78082,
    "scheduler_time": 107.83736340263155
}
#Debug simulation 
Total elapsed time: 10.548503730911762. Arrivals time: 0.30697135580703616 Scheduler time: 10.145280309487134 Scheduler overhead time: 0.0336116086691618 Adapter cache time: 0.012851882260292768 Engine time: 0.0343484953045845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_192_slots_96_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_192_slots_96_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 10.420175370760262,
    "estimated_duration": 3600.002188473822,
    "input_throughput": 5344.645362050983,
    "output_throughput": 4715.980188666892,
    "total_throughput": 10060.625550717874,
    "itl": 179.75393861926804,
    "ttft": 2109918.5871800627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6404889006726482,
    "arrivals": 926191,
    "finished_requests": 77866,
    "scheduler_time": 108.06434274246111
}
#Debug simulation 
Total elapsed time: 10.420283590909094. Arrivals time: 0.343196967151016 Scheduler time: 9.979360170196742 Scheduler overhead time: 0.0340422042645514 Adapter cache time: 0.013049185741692781 Engine time: 0.035088243428617716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_192_slots_96_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_192_slots_96_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.493350128177553,
    "estimated_duration": 3600.0500728097036,
    "input_throughput": 5358.93101757415,
    "output_throughput": 4728.486730939927,
    "total_throughput": 10087.417748514077,
    "itl": 181.5825013156872,
    "ttft": 2109690.3033154476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4471843427326385,
    "arrivals": 926191,
    "finished_requests": 78082,
    "scheduler_time": 107.83673648037765
}
#Debug simulation 
Total elapsed time: 10.493475243914872. Arrivals time: 0.2916660774499178 Scheduler time: 10.105391434859484 Scheduler overhead time: 0.03383335052058101 Adapter cache time: 0.012663205154240131 Engine time: 0.03453145734965801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_192_slots_96_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_192_slots_96_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.348345223348588,
    "estimated_duration": 3600.0227593561813,
    "input_throughput": 5344.6148222243355,
    "output_throughput": 4715.95324109457,
    "total_throughput": 10060.568063318906,
    "itl": 179.7548112249854,
    "ttft": 2109927.4371595406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 496,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6609867678582655,
    "arrivals": 926191,
    "finished_requests": 77866,
    "scheduler_time": 108.06441575765504
}
#Debug simulation 
Total elapsed time: 10.348442464135587. Arrivals time: 0.2912401123903692 Scheduler time: 9.95906521147117 Scheduler overhead time: 0.03430199157446623 Adapter cache time: 0.013419904746115208 Engine time: 0.03491194033995271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.969823795370758,
    "estimated_duration": 3600.161197144263,
    "input_throughput": 5368.189350890771,
    "output_throughput": 4730.1098666104035,
    "total_throughput": 10098.299217501173,
    "itl": 181.30308235307604,
    "ttft": 2115079.7613791404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3374331440986238,
    "arrivals": 923294,
    "finished_requests": 78182,
    "scheduler_time": 107.86583248934163
}
#Debug simulation 
Total elapsed time: 9.969952018000185. Arrivals time: 0.28592396108433604 Scheduler time: 9.588895407505333 Scheduler overhead time: 0.033564727287739515 Adapter cache time: 0.011829471681267023 Engine time: 0.03451471822336316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.027766895014793,
    "estimated_duration": 3600.0500931296574,
    "input_throughput": 5368.207247138428,
    "output_throughput": 4730.181402891579,
    "total_throughput": 10098.388650030007,
    "itl": 181.3092778428418,
    "ttft": 2115023.8885390377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4263704071752785,
    "arrivals": 923294,
    "finished_requests": 78176,
    "scheduler_time": 107.86008758903598
}
#Debug simulation 
Total elapsed time: 10.027861048933119. Arrivals time: 0.2927681594155729 Scheduler time: 9.63960095262155 Scheduler overhead time: 0.033306831028312445 Adapter cache time: 0.011975116096436977 Engine time: 0.03486132901161909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.722513834014535,
    "estimated_duration": 3600.176671668641,
    "input_throughput": 5353.631712486685,
    "output_throughput": 4717.986518180335,
    "total_throughput": 10071.61823066702,
    "itl": 179.45197035407523,
    "ttft": 2116938.4258168265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4876615620218305,
    "arrivals": 923294,
    "finished_requests": 77982,
    "scheduler_time": 108.09680815676494
}
#Debug simulation 
Total elapsed time: 9.722647594753653. Arrivals time: 0.29082444543018937 Scheduler time: 9.335117836017162 Scheduler overhead time: 0.033943678718060255 Adapter cache time: 0.012343422509729862 Engine time: 0.03497950406745076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 10.091906551737338,
    "estimated_duration": 3600.1877143179145,
    "input_throughput": 5368.149811505464,
    "output_throughput": 4730.075026997951,
    "total_throughput": 10098.224838503415,
    "itl": 181.304204633653,
    "ttft": 2115091.684014744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3638553234538964,
    "arrivals": 923294,
    "finished_requests": 78182,
    "scheduler_time": 107.86592748359905
}
#Debug simulation 
Total elapsed time: 10.092066730838269. Arrivals time: 0.2913109208457172 Scheduler time: 9.704698308371007 Scheduler overhead time: 0.03407893097028136 Adapter cache time: 0.012057232670485973 Engine time: 0.03449762938544154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 9.743563304189593,
    "estimated_duration": 3600.1963587354962,
    "input_throughput": 5353.602437054197,
    "output_throughput": 4717.960718666433,
    "total_throughput": 10071.56315572063,
    "itl": 179.45279895649998,
    "ttft": 2116947.1960700476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5072791527025444,
    "arrivals": 923294,
    "finished_requests": 77982,
    "scheduler_time": 108.09687763295713
}
#Debug simulation 
Total elapsed time: 9.743691830895841. Arrivals time: 0.2895005573518574 Scheduler time: 9.358357890509069 Scheduler overhead time: 0.03372256038710475 Adapter cache time: 0.012265285011380911 Engine time: 0.03448764141649008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.071457229088992,
    "estimated_duration": 3600.1302983538467,
    "input_throughput": 5368.235424378095,
    "output_throughput": 4730.150463661427,
    "total_throughput": 10098.385888039522,
    "itl": 181.30178481995986,
    "ttft": 2115065.893465284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3066519788722413,
    "arrivals": 923294,
    "finished_requests": 78182,
    "scheduler_time": 107.86571486409935
}
#Debug simulation 
Total elapsed time: 10.071552348323166. Arrivals time: 0.28989647375419736 Scheduler time: 9.685627792961895 Scheduler overhead time: 0.03375277481973171 Adapter cache time: 0.011788538191467524 Engine time: 0.03515475522726774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.814605974126607,
    "estimated_duration": 3600.0114229198966,
    "input_throughput": 5353.713845820997,
    "output_throughput": 4718.126418116629,
    "total_throughput": 10071.840263937625,
    "itl": 179.45518317648373,
    "ttft": 2116797.4139083712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5260164668783542,
    "arrivals": 923294,
    "finished_requests": 77978,
    "scheduler_time": 108.09080829729149
}
#Debug simulation 
Total elapsed time: 9.8147511780262. Arrivals time: 0.3064965517260134 Scheduler time: 9.411878971382976 Scheduler overhead time: 0.03384669870138168 Adapter cache time: 0.01233970234170556 Engine time: 0.03474672278389335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.365732134785503,
    "estimated_duration": 3600.0326190701207,
    "input_throughput": 5334.443054285543,
    "output_throughput": 4732.644618203706,
    "total_throughput": 10067.08767248925,
    "itl": 182.4350925559099,
    "ttft": 2117090.771125551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2333765608049065,
    "arrivals": 921863,
    "finished_requests": 77903,
    "scheduler_time": 107.78776522496864
}
#Debug simulation 
Total elapsed time: 9.365827834699303. Arrivals time: 0.38313489127904177 Scheduler time: 8.888852599542588 Scheduler overhead time: 0.033218476455658674 Adapter cache time: 0.011209127958863974 Engine time: 0.03421197785064578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.295738658867776,
    "estimated_duration": 3600.110761111862,
    "input_throughput": 5334.327267772441,
    "output_throughput": 4732.541893999413,
    "total_throughput": 10066.869161771854,
    "itl": 182.4383117528541,
    "ttft": 2117126.739512945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3112250516400725,
    "arrivals": 921863,
    "finished_requests": 77903,
    "scheduler_time": 107.7880587758504
}
#Debug simulation 
Total elapsed time: 9.295908071566373. Arrivals time: 0.38205123227089643 Scheduler time: 8.819303532596678 Scheduler overhead time: 0.03349816985428333 Adapter cache time: 0.01123595330864191 Engine time: 0.034500989597290754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.030987024307251,
    "estimated_duration": 3600.059693582419,
    "input_throughput": 5324.073385274048,
    "output_throughput": 4725.161927265847,
    "total_throughput": 10049.235312539895,
    "itl": 180.84205506571112,
    "ttft": 2117720.141113634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.373052753843375,
    "arrivals": 921863,
    "finished_requests": 77752,
    "scheduler_time": 107.98652381241855
}
#Debug simulation 
Total elapsed time: 9.031106353271753. Arrivals time: 0.2870875336229801 Scheduler time: 8.649767914321274 Scheduler overhead time: 0.03328324528411031 Adapter cache time: 0.011479356326162815 Engine time: 0.034160316456109285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 9.425559420138597,
    "estimated_duration": 3600.0578411864026,
    "input_throughput": 5334.4056810129605,
    "output_throughput": 4732.611461149529,
    "total_throughput": 10067.01714216249,
    "itl": 182.43616698078026,
    "ttft": 2117102.674029689,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.258516255561261,
    "arrivals": 921863,
    "finished_requests": 77903,
    "scheduler_time": 107.78784764645668
}
#Debug simulation 
Total elapsed time: 9.425709293223917. Arrivals time: 0.3841737830080092 Scheduler time: 8.946792635601014 Scheduler overhead time: 0.033258304465562105 Adapter cache time: 0.01130931917577982 Engine time: 0.03490991797298193 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 9.023754984140396,
    "estimated_duration": 3600.076734604393,
    "input_throughput": 5324.048183685793,
    "output_throughput": 4725.139560634753,
    "total_throughput": 10049.187744320547,
    "itl": 180.84275410241742,
    "ttft": 2117727.822374036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3900295150093782,
    "arrivals": 921863,
    "finished_requests": 77752,
    "scheduler_time": 107.98658807323976
}
#Debug simulation 
Total elapsed time: 9.023854633793235. Arrivals time: 0.38737172493711114 Scheduler time: 8.542262328322977 Scheduler overhead time: 0.03327765269204974 Adapter cache time: 0.011535852681845427 Engine time: 0.034055409487336874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.278971547726542,
    "estimated_duration": 3600.0041286570545,
    "input_throughput": 5334.48527103882,
    "output_throughput": 4732.682072327438,
    "total_throughput": 10067.167343366258,
    "itl": 182.4339154702911,
    "ttft": 2117077.6966458214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.204990268845571,
    "arrivals": 921863,
    "finished_requests": 77903,
    "scheduler_time": 107.78766110381196
}
#Debug simulation 
Total elapsed time: 9.27909784577787. Arrivals time: 0.3799118069000542 Scheduler time: 8.805177218280733 Scheduler overhead time: 0.03308315435424447 Adapter cache time: 0.011242172680795193 Engine time: 0.03447727393358946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.063783013727516,
    "estimated_duration": 3600.094018007042,
    "input_throughput": 5324.022623889849,
    "output_throughput": 4725.116876091186,
    "total_throughput": 10049.139499981035,
    "itl": 180.84347393442064,
    "ttft": 2117735.71990137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 421,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.407257783748211,
    "arrivals": 921863,
    "finished_requests": 77752,
    "scheduler_time": 107.9866432071617
}
#Debug simulation 
Total elapsed time: 9.063871785998344. Arrivals time: 0.4463021564297378 Scheduler time: 8.5210687443614 Scheduler overhead time: 0.03442475479096174 Adapter cache time: 0.011813399381935596 Engine time: 0.03451968962326646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 8.893951881211251,
    "estimated_duration": 3600.174975632865,
    "input_throughput": 5369.245698009995,
    "output_throughput": 4727.047744952568,
    "total_throughput": 10096.293442962564,
    "itl": 181.32671636343295,
    "ttft": 2113739.4721119567,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 411,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2578604627563694,
    "arrivals": 921076,
    "finished_requests": 77993,
    "scheduler_time": 107.8651258366341
}
#Debug simulation 
Total elapsed time: 8.89407592592761. Arrivals time: 0.3770679011940956 Scheduler time: 8.423208934720606 Scheduler overhead time: 0.03305529383942485 Adapter cache time: 0.011330385226756334 Engine time: 0.034194722305983305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.075062483083457,
    "estimated_duration": 3600.0942302532035,
    "input_throughput": 5368.941412025364,
    "output_throughput": 4727.047380313458,
    "total_throughput": 10095.988792338821,
    "itl": 181.34385768351154,
    "ttft": 2113484.84362845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2946404188918001,
    "arrivals": 921076,
    "finished_requests": 77974,
    "scheduler_time": 107.86136128171759
}
#Debug simulation 
Total elapsed time: 9.075156687293202. Arrivals time: 0.3873918647877872 Scheduler time: 8.593789036385715 Scheduler overhead time: 0.03341517271474004 Adapter cache time: 0.011098382528871298 Engine time: 0.03417031932622194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.038533503189683,
    "estimated_duration": 3600.020352569692,
    "input_throughput": 5356.082497210472,
    "output_throughput": 4716.768889342349,
    "total_throughput": 10072.851386552822,
    "itl": 179.24163531565017,
    "ttft": 2115009.8171873596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3010366267524733,
    "arrivals": 921076,
    "finished_requests": 77768,
    "scheduler_time": 108.12442186571495
}
#Debug simulation 
Total elapsed time: 9.038740186020732. Arrivals time: 0.38979806285351515 Scheduler time: 8.553668051958084 Scheduler overhead time: 0.03382964711636305 Adapter cache time: 0.011324851773679256 Engine time: 0.03460093913599849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 8.954916635993868,
    "estimated_duration": 3600.1988504000365,
    "input_throughput": 5369.210091784825,
    "output_throughput": 4727.016397471773,
    "total_throughput": 10096.226489256598,
    "itl": 181.3277154500742,
    "ttft": 2113751.4770941506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 411,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.281619467283597,
    "arrivals": 921076,
    "finished_requests": 77993,
    "scheduler_time": 107.8652415992439
}
#Debug simulation 
Total elapsed time: 8.955013643018901. Arrivals time: 0.28722650557756424 Scheduler time: 8.57399978628382 Scheduler overhead time: 0.03333378257229924 Adapter cache time: 0.011333394329994917 Engine time: 0.03385481610894203 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 9.044438079930842,
    "estimated_duration": 3600.037142549752,
    "input_throughput": 5356.057517324219,
    "output_throughput": 4716.746891109425,
    "total_throughput": 10072.804408433643,
    "itl": 179.2423252821825,
    "ttft": 2115017.5213394486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3177618803456468,
    "arrivals": 921076,
    "finished_requests": 77768,
    "scheduler_time": 108.12448659219427
}
#Debug simulation 
Total elapsed time: 9.044545100070536. Arrivals time: 0.3905048780143261 Scheduler time: 8.559160101693124 Scheduler overhead time: 0.03358569787815213 Adapter cache time: 0.011217128485441208 Engine time: 0.034592142794281244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 9.028854611795396,
    "estimated_duration": 3600.081309315596,
    "input_throughput": 5371.191186700187,
    "output_throughput": 4729.369571721367,
    "total_throughput": 10100.560758421554,
    "itl": 181.33623479125936,
    "ttft": 2113407.0071138907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.204990268845571,
    "arrivals": 921076,
    "finished_requests": 77984,
    "scheduler_time": 107.86498729820762
}
#Debug simulation 
Total elapsed time: 9.02894465206191. Arrivals time: 0.38639354426413774 Scheduler time: 8.548786396626383 Scheduler overhead time: 0.03317484725266695 Adapter cache time: 0.011272513773292303 Engine time: 0.034085634630173445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 9.122757033910602,
    "estimated_duration": 3600.0528110833475,
    "input_throughput": 5356.034206119758,
    "output_throughput": 4716.726362380819,
    "total_throughput": 10072.760568500576,
    "itl": 179.24294507787224,
    "ttft": 2115025.3033105107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3333553498610875,
    "arrivals": 921076,
    "finished_requests": 77768,
    "scheduler_time": 108.12456165628396
}
#Debug simulation 
Total elapsed time: 9.122879271861166. Arrivals time: 0.4397687432356179 Scheduler time: 8.588818764314055 Scheduler overhead time: 0.03344055451452732 Adapter cache time: 0.010960044339299202 Engine time: 0.03451383439823985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 8.413648164831102,
    "estimated_duration": 3600.1015390709595,
    "input_throughput": 5356.268091531716,
    "output_throughput": 4728.669404250505,
    "total_throughput": 10084.93749578222,
    "itl": 181.8179448747368,
    "ttft": 2102774.2869217223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 851,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6044750700868278,
    "arrivals": 851723,
    "finished_requests": 77937,
    "scheduler_time": 107.73267849542354
}
#Debug simulation 
Total elapsed time: 8.413799045141786. Arrivals time: 0.3377607730217278 Scheduler time: 7.97585223801434 Scheduler overhead time: 0.03314555436372757 Adapter cache time: 0.01781029812991619 Engine time: 0.03391458326950669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 8.396058476995677,
    "estimated_duration": 3600.1212017055996,
    "input_throughput": 5356.044677291624,
    "output_throughput": 4728.903846885595,
    "total_throughput": 10084.948524177218,
    "itl": 181.81942818451375,
    "ttft": 2102846.8572485982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 844,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7581361616822027,
    "arrivals": 851723,
    "finished_requests": 77937,
    "scheduler_time": 107.72891340513932
}
#Debug simulation 
Total elapsed time: 8.39618357969448. Arrivals time: 0.3337829886004329 Scheduler time: 7.962265516631305 Scheduler overhead time: 0.03283448191359639 Adapter cache time: 0.018093477003276348 Engine time: 0.0340646686963737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 8.361609894316643,
    "estimated_duration": 3600.111482669505,
    "input_throughput": 5344.334499812009,
    "output_throughput": 4718.334163197741,
    "total_throughput": 10062.66866300975,
    "itl": 180.12178760531387,
    "ttft": 2103908.607299527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 860,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.815390489231776,
    "arrivals": 851723,
    "finished_requests": 77750,
    "scheduler_time": 107.94255106636687
}
#Debug simulation 
Total elapsed time: 8.361711559351534. Arrivals time: 0.33489775424823165 Scheduler time: 7.925729470793158 Scheduler overhead time: 0.03350714407861233 Adapter cache time: 0.018350203055888414 Engine time: 0.03391829039901495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 8.48391079902649,
    "estimated_duration": 3600.000018329361,
    "input_throughput": 5356.224972728839,
    "output_throughput": 4729.063031477583,
    "total_throughput": 10085.28800420642,
    "itl": 181.8138037721476,
    "ttft": 2102797.4515571673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 844,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6371919474238275,
    "arrivals": 851723,
    "finished_requests": 77937,
    "scheduler_time": 107.7286742431265
}
#Debug simulation 
Total elapsed time: 8.484031990170479. Arrivals time: 0.33562606386840343 Scheduler time: 8.047850117553025 Scheduler overhead time: 0.03307082038372755 Adapter cache time: 0.017816766165196896 Engine time: 0.034279980696737766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 8.296003133058548,
    "estimated_duration": 3600.149405804874,
    "input_throughput": 5344.27820383708,
    "output_throughput": 4718.284461364563,
    "total_throughput": 10062.562665201642,
    "itl": 180.1235440085969,
    "ttft": 2103924.09863481,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 860,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.853242378942672,
    "arrivals": 851723,
    "finished_requests": 77750,
    "scheduler_time": 107.94262231205994
}
#Debug simulation 
Total elapsed time: 8.296097800135612. Arrivals time: 0.33410795824602246 Scheduler time: 7.861425565555692 Scheduler overhead time: 0.0331665575504303 Adapter cache time: 0.017982823308557272 Engine time: 0.03410721197724342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 8.40523601602763,
    "estimated_duration": 3600.041467907777,
    "input_throughput": 5356.357467517366,
    "output_throughput": 4728.7483079725735,
    "total_throughput": 10085.10577548994,
    "itl": 181.81518812531525,
    "ttft": 2102749.3988237507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 851,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.544532800961698,
    "arrivals": 851723,
    "finished_requests": 77937,
    "scheduler_time": 107.73254960125465
}
#Debug simulation 
Total elapsed time: 8.405349994078279. Arrivals time: 0.3469967059791088 Scheduler time: 7.958444455172867 Scheduler overhead time: 0.03302952693775296 Adapter cache time: 0.017996222712099552 Engine time: 0.03370193159207702 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 8.261805453337729,
    "estimated_duration": 3600.185449897714,
    "input_throughput": 5344.224698354536,
    "output_throughput": 4718.23722316377,
    "total_throughput": 10062.461921518307,
    "itl": 180.12517759601886,
    "ttft": 2103939.222533322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 860,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.889207961857344,
    "arrivals": 851723,
    "finished_requests": 77750,
    "scheduler_time": 107.94270082201976
}
#Debug simulation 
Total elapsed time: 8.261897830292583. Arrivals time: 0.33770694490522146 Scheduler time: 7.823473047465086 Scheduler overhead time: 0.03330881940200925 Adapter cache time: 0.018140628468245268 Engine time: 0.03389822691679001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.81782005680725,
    "estimated_duration": 3600.0046491728476,
    "input_throughput": 5331.700336667656,
    "output_throughput": 4730.793334923355,
    "total_throughput": 10062.493671591012,
    "itl": 182.22770604924418,
    "ttft": 2104502.1803237456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 921,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.818709212162128,
    "arrivals": 839966,
    "finished_requests": 77779,
    "scheduler_time": 107.67386893231115
}
#Debug simulation 
Total elapsed time: 7.817915424704552. Arrivals time: 0.6047558919526637 Scheduler time: 7.113517189398408 Scheduler overhead time: 0.032423587050288916 Adapter cache time: 0.018658397253602743 Engine time: 0.033446886111050844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.526661502197385,
    "estimated_duration": 3600.1913806339503,
    "input_throughput": 5331.423796870527,
    "output_throughput": 4730.54796242556,
    "total_throughput": 10061.971759296088,
    "itl": 182.23643121026188,
    "ttft": 2104577.3663062295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 921,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0051224424457237,
    "arrivals": 839966,
    "finished_requests": 77779,
    "scheduler_time": 107.67418716307353
}
#Debug simulation 
Total elapsed time: 7.526752642355859. Arrivals time: 0.3284905250184238 Scheduler time: 7.098087786696851 Scheduler overhead time: 0.0327791441231966 Adapter cache time: 0.018975446466356516 Engine time: 0.033388099167495966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.386028903070837,
    "estimated_duration": 3600.103475871319,
    "input_throughput": 5318.159360785092,
    "output_throughput": 4720.037386116337,
    "total_throughput": 10038.196746901429,
    "itl": 180.3068168765633,
    "ttft": 2105969.933752123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.040240470953258,
    "arrivals": 839966,
    "finished_requests": 77581,
    "scheduler_time": 107.91132249109027
}
#Debug simulation 
Total elapsed time: 7.386123278178275. Arrivals time: 0.28120194002985954 Scheduler time: 7.002734229899943 Scheduler overhead time: 0.03385428013280034 Adapter cache time: 0.018998160026967525 Engine time: 0.03375098900869489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 7.453803080134094,
    "estimated_duration": 3600.0604016802085,
    "input_throughput": 5331.617767035734,
    "output_throughput": 4730.720071266416,
    "total_throughput": 10062.33783830215,
    "itl": 182.23032616149823,
    "ttft": 2104524.900668187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 921,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8743719405447767,
    "arrivals": 839966,
    "finished_requests": 77779,
    "scheduler_time": 107.67395871119757
}
#Debug simulation 
Total elapsed time: 7.453979574143887. Arrivals time: 0.2799486177973449 Scheduler time: 7.073445933870971 Scheduler overhead time: 0.032748869620263577 Adapter cache time: 0.019027195870876312 Engine time: 0.03355673095211387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 7.357631030958146,
    "estimated_duration": 3600.1444172838223,
    "input_throughput": 5318.098881834552,
    "output_throughput": 4719.983709103623,
    "total_throughput": 10038.082590938177,
    "itl": 180.30871834834568,
    "ttft": 2105986.2653286015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.08111045153812,
    "arrivals": 839966,
    "finished_requests": 77581,
    "scheduler_time": 107.91139392304659
}
#Debug simulation 
Total elapsed time: 7.357722763903439. Arrivals time: 0.2765793544240296 Scheduler time: 6.98035332094878 Scheduler overhead time: 0.03305125189945102 Adapter cache time: 0.019165110774338245 Engine time: 0.03340121265500784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.460066199768335,
    "estimated_duration": 3600.1366224953867,
    "input_throughput": 5332.089865715811,
    "output_throughput": 4730.839072489068,
    "total_throughput": 10062.92893820488,
    "itl": 182.22209415067005,
    "ttft": 2104526.9012311553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 921,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.753836321604843,
    "arrivals": 839966,
    "finished_requests": 77785,
    "scheduler_time": 107.67972618457769
}
#Debug simulation 
Total elapsed time: 7.460176721680909. Arrivals time: 0.28986090049147606 Scheduler time: 7.070417752023786 Scheduler overhead time: 0.03258675755932927 Adapter cache time: 0.018839381635189056 Engine time: 0.03341438015922904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.397562869824469,
    "estimated_duration": 3600.1818319092804,
    "input_throughput": 5318.043613882236,
    "output_throughput": 4719.934657019343,
    "total_throughput": 10037.97827090158,
    "itl": 180.31044594770796,
    "ttft": 2106001.4868951538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 930,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1184593261033626,
    "arrivals": 839966,
    "finished_requests": 77581,
    "scheduler_time": 107.9114596739738
}
#Debug simulation 
Total elapsed time: 7.397656999994069. Arrivals time: 0.2740128035657108 Scheduler time: 7.022638391703367 Scheduler overhead time: 0.03283025790005922 Adapter cache time: 0.01906424341723323 Engine time: 0.03392424248158932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.11957638990134,
    "estimated_duration": 3600.0892529251123,
    "input_throughput": 5364.058400582577,
    "output_throughput": 4729.059421559332,
    "total_throughput": 10093.117822141909,
    "itl": 181.61978243147038,
    "ttft": 2097494.5798446983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 873,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6718058004533507,
    "arrivals": 834363,
    "finished_requests": 78247,
    "scheduler_time": 107.73907975352986
}
#Debug simulation 
Total elapsed time: 7.119683805853128. Arrivals time: 0.369335712864995 Scheduler time: 6.651443460024893 Scheduler overhead time: 0.032710788771510124 Adapter cache time: 0.017746233846992254 Engine time: 0.033308036159724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.134702532086521,
    "estimated_duration": 3600.0649182471293,
    "input_throughput": 5363.907995693099,
    "output_throughput": 4728.67861735359,
    "total_throughput": 10092.58661304669,
    "itl": 181.6267547869826,
    "ttft": 2097514.2704387824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 873,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8464820015081265,
    "arrivals": 834363,
    "finished_requests": 78242,
    "scheduler_time": 107.73334723973035
}
#Debug simulation 
Total elapsed time: 7.13485546130687. Arrivals time: 0.36230451660230756 Scheduler time: 6.6734918830916286 Scheduler overhead time: 0.03274691058322787 Adapter cache time: 0.01813977351412177 Engine time: 0.03306957054883242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.028463967144489,
    "estimated_duration": 3600.049413059636,
    "input_throughput": 5352.532920825439,
    "output_throughput": 4717.074698585169,
    "total_throughput": 10069.607619410608,
    "itl": 179.79573739865728,
    "ttft": 2098864.6070574336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 910,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9719506490975376,
    "arrivals": 834363,
    "finished_requests": 78054,
    "scheduler_time": 107.95406641538759
}
#Debug simulation 
Total elapsed time: 7.028583904262632. Arrivals time: 0.35798122780397534 Scheduler time: 6.570601572282612 Scheduler overhead time: 0.03277986356988549 Adapter cache time: 0.018539772368967533 Engine time: 0.033399938605725765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 7.127183573320508,
    "estimated_duration": 3600.1443042021015,
    "input_throughput": 5363.976376574691,
    "output_throughput": 4728.987107580192,
    "total_throughput": 10092.963484154883,
    "itl": 181.6223380198168,
    "ttft": 2097517.136771302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 873,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7267635732050723,
    "arrivals": 834363,
    "finished_requests": 78247,
    "scheduler_time": 107.73917325768129
}
#Debug simulation 
Total elapsed time: 7.127302903216332. Arrivals time: 0.3625174453482032 Scheduler time: 6.66557529149577 Scheduler overhead time: 0.032783284317702055 Adapter cache time: 0.017926148138940334 Engine time: 0.033380245324224234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 7.332645216956735,
    "estimated_duration": 3600.0878306420436,
    "input_throughput": 5352.475802392709,
    "output_throughput": 4717.0243613116145,
    "total_throughput": 10069.500163704322,
    "itl": 179.79752633748868,
    "ttft": 2098880.3921506903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 910,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0103055539540953,
    "arrivals": 834363,
    "finished_requests": 78054,
    "scheduler_time": 107.95412909297116
}
#Debug simulation 
Total elapsed time: 7.332709609996527. Arrivals time: 0.7019218266941607 Scheduler time: 6.5298230573534966 Scheduler overhead time: 0.0332166887819767 Adapter cache time: 0.018954710569232702 Engine time: 0.03365095378831029 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 7.1638121721334755,
    "estimated_duration": 3600.027640613034,
    "input_throughput": 5364.150203222216,
    "output_throughput": 4729.140356572617,
    "total_throughput": 10093.290559794834,
    "itl": 181.61691992916712,
    "ttft": 2097469.2154564345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 873,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6103139074495436,
    "arrivals": 834363,
    "finished_requests": 78247,
    "scheduler_time": 107.73895933433884
}
#Debug simulation 
Total elapsed time: 7.163908951915801. Arrivals time: 0.36128578428179026 Scheduler time: 6.703498258721083 Scheduler overhead time: 0.03267433354631066 Adapter cache time: 0.018211612477898598 Engine time: 0.033123974688351154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.992538603954017,
    "estimated_duration": 3600.1249918041653,
    "input_throughput": 5352.420553138448,
    "output_throughput": 4716.975671305733,
    "total_throughput": 10069.396224444181,
    "itl": 179.7992363194609,
    "ttft": 2098895.5744965547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 910,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0474029209465088,
    "arrivals": 834363,
    "finished_requests": 78054,
    "scheduler_time": 107.95419288813423
}
#Debug simulation 
Total elapsed time: 6.992687940597534. Arrivals time: 0.3667317465879023 Scheduler time: 6.525837593246251 Scheduler overhead time: 0.0330138998106122 Adapter cache time: 0.018569511361420155 Engine time: 0.033301337622106075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.7558653508313,
    "estimated_duration": 3600.0771378889,
    "input_throughput": 5344.829641979136,
    "output_throughput": 4727.059823495698,
    "total_throughput": 10071.889465474833,
    "itl": 182.00304689252965,
    "ttft": 2102343.4104044163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 803,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4575716583780505,
    "arrivals": 831492,
    "finished_requests": 77816,
    "scheduler_time": 107.71339721797344
}
#Debug simulation 
Total elapsed time: 6.7559922486543655. Arrivals time: 0.27241986570879817 Scheduler time: 6.386116272304207 Scheduler overhead time: 0.032599099446088076 Adapter cache time: 0.016782183200120926 Engine time: 0.03303756797686219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.807341778185219,
    "estimated_duration": 3600.029144970004,
    "input_throughput": 5344.5308982798015,
    "output_throughput": 4726.822843580559,
    "total_throughput": 10071.35374186036,
    "itl": 182.0103514673242,
    "ttft": 2102355.5328302355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 803,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6155229289876343,
    "arrivals": 831492,
    "finished_requests": 77811,
    "scheduler_time": 107.70753738503109
}
#Debug simulation 
Total elapsed time: 6.807437658775598. Arrivals time: 0.27246641321107745 Scheduler time: 6.437698430381715 Scheduler overhead time: 0.032378481701016426 Adapter cache time: 0.016936044208705425 Engine time: 0.03292241273447871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.84754523774609,
    "estimated_duration": 3600.0407215643804,
    "input_throughput": 5330.41303812286,
    "output_throughput": 4716.965532717883,
    "total_throughput": 10047.378570840743,
    "itl": 179.9311035742016,
    "ttft": 2104165.889511295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 802,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6172825563512636,
    "arrivals": 831492,
    "finished_requests": 77626,
    "scheduler_time": 107.9608188171082
}
#Debug simulation 
Total elapsed time: 6.847666634712368. Arrivals time: 0.2835222100839019 Scheduler time: 6.465670404024422 Scheduler overhead time: 0.03265359904617071 Adapter cache time: 0.017104418948292732 Engine time: 0.033457291312515736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.754365528933704,
    "estimated_duration": 3600.12486360413,
    "input_throughput": 5344.758787265171,
    "output_throughput": 4726.997158360582,
    "total_throughput": 10071.755945625753,
    "itl": 182.00521850658643,
    "ttft": 2102362.5274260254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 803,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5052021930087127,
    "arrivals": 831492,
    "finished_requests": 77816,
    "scheduler_time": 107.71349239849411
}
#Debug simulation 
Total elapsed time: 6.754458584357053. Arrivals time: 0.27636817237362266 Scheduler time: 6.38013397809118 Scheduler overhead time: 0.03301059082150459 Adapter cache time: 0.016699072439223528 Engine time: 0.03319045668467879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.8675051759928465,
    "estimated_duration": 3600.0746215099057,
    "input_throughput": 5330.362844521166,
    "output_throughput": 4716.921115617846,
    "total_throughput": 10047.283960139011,
    "itl": 179.93267609838617,
    "ttft": 2104180.16402145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 802,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6511103248968744,
    "arrivals": 831492,
    "finished_requests": 77626,
    "scheduler_time": 107.96089099411752
}
#Debug simulation 
Total elapsed time: 6.867648923769593. Arrivals time: 0.27466848213225603 Scheduler time: 6.494217121973634 Scheduler overhead time: 0.033100991044193506 Adapter cache time: 0.016993302386254072 Engine time: 0.033290908206254244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.803323043975979,
    "estimated_duration": 3600.02045144853,
    "input_throughput": 5344.913802436243,
    "output_throughput": 4727.134256460295,
    "total_throughput": 10072.048058896537,
    "itl": 182.0004503250638,
    "ttft": 2102319.8860098673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 803,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4010103868063988,
    "arrivals": 831492,
    "finished_requests": 77816,
    "scheduler_time": 107.71327204906953
}
#Debug simulation 
Total elapsed time: 6.803450321312994. Arrivals time: 0.2758692321367562 Scheduler time: 6.429415678605437 Scheduler overhead time: 0.032658789306879044 Adapter cache time: 0.016794467344880104 Engine time: 0.03349167248234153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.726236342918128,
    "estimated_duration": 3600.1066317027617,
    "input_throughput": 5330.315449829814,
    "output_throughput": 4716.879175317171,
    "total_throughput": 10047.194625146984,
    "itl": 179.93412996918238,
    "ttft": 2104193.1652000044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 802,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6830517866462644,
    "arrivals": 831492,
    "finished_requests": 77626,
    "scheduler_time": 107.96095972525671
}
#Debug simulation 
Total elapsed time: 6.726330486126244. Arrivals time: 0.2728521153330803 Scheduler time: 6.355128331575543 Scheduler overhead time: 0.03278897795826197 Adapter cache time: 0.01703572552651167 Engine time: 0.033329442609101534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.653060824610293,
    "estimated_duration": 3600.086224921931,
    "input_throughput": 5356.551703263213,
    "output_throughput": 4731.165848776124,
    "total_throughput": 10087.717552039338,
    "itl": 181.68637266889303,
    "ttft": 2104859.045757975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 704,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1545833717286973,
    "arrivals": 830076,
    "finished_requests": 77947,
    "scheduler_time": 107.74617119848511
}
#Debug simulation 
Total elapsed time: 6.653190327808261. Arrivals time: 0.2752733784727752 Scheduler time: 6.281695007812232 Scheduler overhead time: 0.03277278970927 Adapter cache time: 0.015463986899703741 Engine time: 0.03294465318322182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.645519248209894,
    "estimated_duration": 3600.1363754092595,
    "input_throughput": 5357.7367601268425,
    "output_throughput": 4731.359655247406,
    "total_throughput": 10089.096415374248,
    "itl": 181.68917688007247,
    "ttft": 2104967.498118391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 712,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.318504037726675,
    "arrivals": 830076,
    "finished_requests": 77943,
    "scheduler_time": 107.74375771938504
}
#Debug simulation 
Total elapsed time: 6.645618840120733. Arrivals time: 0.2707805777899921 Scheduler time: 6.278635860886425 Scheduler overhead time: 0.03264615358784795 Adapter cache time: 0.015523332171142101 Engine time: 0.032938838470727205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.585217142943293,
    "estimated_duration": 3600.189880020212,
    "input_throughput": 5343.613709589116,
    "output_throughput": 4720.485742797708,
    "total_throughput": 10064.099452386823,
    "itl": 180.0056910310372,
    "ttft": 2105836.414819604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 733,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.392440866027025,
    "arrivals": 830076,
    "finished_requests": 77767,
    "scheduler_time": 107.95173839209248
}
#Debug simulation 
Total elapsed time: 6.5853757238946855. Arrivals time: 0.2771439105272293 Scheduler time: 6.21080482006073 Scheduler overhead time: 0.03291005315259099 Adapter cache time: 0.016002317424863577 Engine time: 0.03332908218726516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.654154347721487,
    "estimated_duration": 3600.129902674276,
    "input_throughput": 5356.486716125237,
    "output_throughput": 4731.108448988941,
    "total_throughput": 10087.595165114177,
    "itl": 181.68839324924787,
    "ttft": 2104877.9514923734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 704,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1981551402155133,
    "arrivals": 830076,
    "finished_requests": 77947,
    "scheduler_time": 107.74627718227428
}
#Debug simulation 
Total elapsed time: 6.654280287679285. Arrivals time: 0.322935804259032 Scheduler time: 6.23508161958307 Scheduler overhead time: 0.03219000482931733 Adapter cache time: 0.015502605587244034 Engine time: 0.03360262140631676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.565916853956878,
    "estimated_duration": 3600.030110154007,
    "input_throughput": 5343.749194135775,
    "output_throughput": 4720.475240489867,
    "total_throughput": 10064.224434625641,
    "itl": 180.00652962442163,
    "ttft": 2105817.19647337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 733,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4236278050579174,
    "arrivals": 830076,
    "finished_requests": 77764,
    "scheduler_time": 107.94598256837705
}
#Debug simulation 
Total elapsed time: 6.566013055853546. Arrivals time: 0.2802110519260168 Scheduler time: 6.188747732900083 Scheduler overhead time: 0.032661871053278446 Adapter cache time: 0.016089320182800293 Engine time: 0.033176375553011894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.611856851261109,
    "estimated_duration": 3600.0365171906365,
    "input_throughput": 5356.625664188737,
    "output_throughput": 4731.231174647014,
    "total_throughput": 10087.85683883575,
    "itl": 181.6841043048308,
    "ttft": 2104838.1719897063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 704,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.104995407611094,
    "arrivals": 830076,
    "finished_requests": 77947,
    "scheduler_time": 107.74605143121546
}
#Debug simulation 
Total elapsed time: 6.61198439123109. Arrivals time: 0.269261219073087 Scheduler time: 6.247079307213426 Scheduler overhead time: 0.03239493351429701 Adapter cache time: 0.01543911686167121 Engine time: 0.0328535046428442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.553606300614774,
    "estimated_duration": 3600.0592260540184,
    "input_throughput": 5343.705975939225,
    "output_throughput": 4720.4370630943085,
    "total_throughput": 10064.143039033534,
    "itl": 180.00782259629017,
    "ttft": 2105829.627351734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 733,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.452676929719756,
    "arrivals": 830076,
    "finished_requests": 77764,
    "scheduler_time": 107.94604934375212
}
#Debug simulation 
Total elapsed time: 6.553706143051386. Arrivals time: 0.2728016576729715 Scheduler time: 6.184313316363841 Scheduler overhead time: 0.032617314253002405 Adapter cache time: 0.015905420295894146 Engine time: 0.03302940586581826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.497404259163886,
    "estimated_duration": 3600.1608905037606,
    "input_throughput": 5338.772789487899,
    "output_throughput": 4728.750330271446,
    "total_throughput": 10067.523119759344,
    "itl": 181.718026029581,
    "ttft": 2097960.3884145417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 734,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.246398004046683,
    "arrivals": 829350,
    "finished_requests": 77813,
    "scheduler_time": 107.69854321172281
}
#Debug simulation 
Total elapsed time: 6.497559988871217. Arrivals time: 0.28538437793031335 Scheduler time: 6.116377478931099 Scheduler overhead time: 0.03233569022268057 Adapter cache time: 0.015555694699287415 Engine time: 0.03283702954649925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.472800670191646,
    "estimated_duration": 3600.0573947857783,
    "input_throughput": 5337.45684939098,
    "output_throughput": 4728.719054495239,
    "total_throughput": 10066.17590388622,
    "itl": 181.74263526485746,
    "ttft": 2097836.204731123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 731,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.378992351195779,
    "arrivals": 829350,
    "finished_requests": 77800,
    "scheduler_time": 107.68977418180285
}
#Debug simulation 
Total elapsed time: 6.472888758871704. Arrivals time: 0.2716179145500064 Scheduler time: 6.105638005305082 Scheduler overhead time: 0.03246845165267587 Adapter cache time: 0.01553102396428585 Engine time: 0.032738938461989164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.44890513876453,
    "estimated_duration": 3600.1972125894267,
    "input_throughput": 5325.931849774665,
    "output_throughput": 4717.091313946504,
    "total_throughput": 10043.02316372117,
    "itl": 179.7959779269463,
    "ttft": 2099298.2619802067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 746,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.434030096027996,
    "arrivals": 829350,
    "finished_requests": 77606,
    "scheduler_time": 107.93140353294689
}
#Debug simulation 
Total elapsed time: 6.449001615867019. Arrivals time: 0.2689574006944895 Scheduler time: 6.083664963487536 Scheduler overhead time: 0.032489250879734755 Adapter cache time: 0.01594006223604083 Engine time: 0.03288799850270152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.422937146853656,
    "estimated_duration": 3600.2020550122966,
    "input_throughput": 5338.711746259017,
    "output_throughput": 4728.696262005176,
    "total_throughput": 10067.408008264194,
    "itl": 181.71992065043486,
    "ttft": 2097979.4933349965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 734,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2874480537441344,
    "arrivals": 829350,
    "finished_requests": 77813,
    "scheduler_time": 107.69865767048984
}
#Debug simulation 
Total elapsed time: 6.423055537976325. Arrivals time: 0.2693568021059036 Scheduler time: 6.057927097193897 Scheduler overhead time: 0.032560248859226704 Adapter cache time: 0.015821595210582018 Engine time: 0.03247791947796941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.41060637217015,
    "estimated_duration": 3600.020538750842,
    "input_throughput": 5325.704893520592,
    "output_throughput": 4716.9289222700345,
    "total_throughput": 10042.633815790627,
    "itl": 179.79821727761376,
    "ttft": 2099247.1774175004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 746,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.465594296418134,
    "arrivals": 829350,
    "finished_requests": 77600,
    "scheduler_time": 107.92523430062663
}
#Debug simulation 
Total elapsed time: 6.410716004204005. Arrivals time: 0.2655724729411304 Scheduler time: 6.048765340354294 Scheduler overhead time: 0.03243450727313757 Adapter cache time: 0.015878320205956697 Engine time: 0.033081409987062216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.411370690912008,
    "estimated_duration": 3600.0795650495293,
    "input_throughput": 5338.118686756193,
    "output_throughput": 4728.806597852452,
    "total_throughput": 10066.925284608646,
    "itl": 181.7285344939498,
    "ttft": 2097877.8806018736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 732,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.188716815868352,
    "arrivals": 829350,
    "finished_requests": 77806,
    "scheduler_time": 107.69629465807084
}
#Debug simulation 
Total elapsed time: 6.411504602059722. Arrivals time: 0.2631232114508748 Scheduler time: 6.053334985859692 Scheduler overhead time: 0.032042660284787416 Adapter cache time: 0.015668625477701426 Engine time: 0.03250236716121435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.285604094620794,
    "estimated_duration": 3600.0500405734538,
    "input_throughput": 5325.66125023806,
    "output_throughput": 4716.8902678072445,
    "total_throughput": 10042.551518045306,
    "itl": 179.7995551271182,
    "ttft": 2099259.9795635524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 746,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4950206824392196,
    "arrivals": 829350,
    "finished_requests": 77600,
    "scheduler_time": 107.92530973724283
}
#Debug simulation 
Total elapsed time: 6.285696618724614. Arrivals time: 0.24930602917447686 Scheduler time: 5.941479885019362 Scheduler overhead time: 0.03191191144287586 Adapter cache time: 0.015587781090289354 Engine time: 0.03259962657466531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.811278508044779,
    "estimated_duration": 3600.0090344999735,
    "input_throughput": 5366.529032248222,
    "output_throughput": 4727.74813532211,
    "total_throughput": 10094.277167570333,
    "itl": 181.50888362753804,
    "ttft": 2089829.647207514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1884,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.7659589095694725,
    "arrivals": 771049,
    "finished_requests": 77924,
    "scheduler_time": 107.63168567135752
}
#Debug simulation 
Total elapsed time: 5.811366697773337. Arrivals time: 0.28545444179326296 Scheduler time: 5.41778136882931 Scheduler overhead time: 0.03156763082370162 Adapter cache time: 0.029475785326212645 Engine time: 0.03240645211189985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.786122158635408,
    "estimated_duration": 3600.1974402517135,
    "input_throughput": 5366.158473422298,
    "output_throughput": 4727.354341658012,
    "total_throughput": 10093.51281508031,
    "itl": 181.52739000575605,
    "ttft": 2089883.0990990018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1883,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.151632331630491,
    "arrivals": 771049,
    "finished_requests": 77922,
    "scheduler_time": 107.62606844495899
}
#Debug simulation 
Total elapsed time: 5.78620979655534. Arrivals time: 0.24202758679166436 Scheduler time: 5.435722514055669 Scheduler overhead time: 0.031543662305921316 Adapter cache time: 0.02961469581350684 Engine time: 0.032544431276619434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.749927972909063,
    "estimated_duration": 3600.1639961933015,
    "input_throughput": 5353.273078775938,
    "output_throughput": 4714.942157620698,
    "total_throughput": 10068.215236396636,
    "itl": 179.60365845627524,
    "ttft": 2091223.3503765028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1942,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.356354792080778,
    "arrivals": 771049,
    "finished_requests": 77725,
    "scheduler_time": 107.8591937492475
}
#Debug simulation 
Total elapsed time: 5.750016686040908. Arrivals time: 0.23944367980584502 Scheduler time: 5.401193515397608 Scheduler overhead time: 0.03172921948134899 Adapter cache time: 0.030209381598979235 Engine time: 0.032604186329990625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.773144335951656,
    "estimated_duration": 3600.1228284339095,
    "input_throughput": 5366.359405132909,
    "output_throughput": 4727.598699015458,
    "total_throughput": 10093.958104148367,
    "itl": 181.5142975318003,
    "ttft": 2089871.7076411208,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1884,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.87963773261506,
    "arrivals": 771049,
    "finished_requests": 77924,
    "scheduler_time": 107.63180078204593
}
#Debug simulation 
Total elapsed time: 5.773267439100891. Arrivals time: 0.2383087668567896 Scheduler time: 5.426727317739278 Scheduler overhead time: 0.03169400757178664 Adapter cache time: 0.02958194725215435 Engine time: 0.032275970093905926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.7335816621780396,
    "estimated_duration": 3600.0531144847932,
    "input_throughput": 5352.839635189054,
    "output_throughput": 4714.406832419441,
    "total_throughput": 10067.246467608496,
    "itl": 179.6077126774457,
    "ttft": 2091238.5476528893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1942,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.443879427425457,
    "arrivals": 771049,
    "finished_requests": 77719,
    "scheduler_time": 107.85326470369112
}
#Debug simulation 
Total elapsed time: 5.733673199079931. Arrivals time: 0.24216411914676428 Scheduler time: 5.38197415927425 Scheduler overhead time: 0.03179537272080779 Adapter cache time: 0.030411547515541315 Engine time: 0.03253764985129237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.800532312132418,
    "estimated_duration": 3600.0740108784266,
    "input_throughput": 5367.1124375816335,
    "output_throughput": 4728.103630249176,
    "total_throughput": 10095.21606783081,
    "itl": 181.50171748392202,
    "ttft": 2089791.5463539425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1884,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.6332547555955355,
    "arrivals": 771049,
    "finished_requests": 77932,
    "scheduler_time": 107.63755038863165
}
#Debug simulation 
Total elapsed time: 5.800619951914996. Arrivals time: 0.245023294351995 Scheduler time: 5.447480394039303 Scheduler overhead time: 0.031529733911156654 Adapter cache time: 0.029586675576865673 Engine time: 0.03228933783248067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.807629261165857,
    "estimated_duration": 3600.131913747389,
    "input_throughput": 5352.7224728666315,
    "output_throughput": 4714.303644038885,
    "total_throughput": 10067.026116905516,
    "itl": 179.61150626351946,
    "ttft": 2091267.8201182052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1942,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.522601297721084,
    "arrivals": 771049,
    "finished_requests": 77719,
    "scheduler_time": 107.85334209607144
}
#Debug simulation 
Total elapsed time: 5.807739534415305. Arrivals time: 0.23711385717615485 Scheduler time: 5.459752035327256 Scheduler overhead time: 0.03251068294048309 Adapter cache time: 0.030264031141996384 Engine time: 0.033202335238456726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.712776060216129,
    "estimated_duration": 3600.1515903280883,
    "input_throughput": 5434.0436809820985,
    "output_throughput": 4758.186862470115,
    "total_throughput": 10192.230543452213,
    "itl": 179.83596556034598,
    "ttft": 2084793.6982268551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1990,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.090370610426356,
    "arrivals": 765271,
    "finished_requests": 78650,
    "scheduler_time": 108.45456639628016
}
#Debug simulation 
Total elapsed time: 5.712893974035978. Arrivals time: 0.3189771957695484 Scheduler time: 5.284790074452758 Scheduler overhead time: 0.031432829331606627 Adapter cache time: 0.030831967014819384 Engine time: 0.032238262705504894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.703978972975165,
    "estimated_duration": 3600.1562369226567,
    "input_throughput": 5433.462525704337,
    "output_throughput": 4757.555192838139,
    "total_throughput": 10191.017718542476,
    "itl": 179.85429607991304,
    "ttft": 2084855.5364531882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1989,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.483287094353942,
    "arrivals": 765271,
    "finished_requests": 78640,
    "scheduler_time": 108.4429732004639
}
#Debug simulation 
Total elapsed time: 5.704109789803624. Arrivals time: 0.31795523734763265 Scheduler time: 5.277860463131219 Scheduler overhead time: 0.031016196589916945 Adapter cache time: 0.030526941176503897 Engine time: 0.032119815703481436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.674811244010925,
    "estimated_duration": 3600.118298627596,
    "input_throughput": 5426.346130749965,
    "output_throughput": 4750.387232141636,
    "total_throughput": 10176.7333628916,
    "itl": 178.05200231516514,
    "ttft": 2085961.7583526915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.521724843792575,
    "arrivals": 765271,
    "finished_requests": 78530,
    "scheduler_time": 108.74479309452255
}
#Debug simulation 
Total elapsed time: 5.674896235577762. Arrivals time: 0.31714791152626276 Scheduler time: 5.24872873397544 Scheduler overhead time: 0.031141411047428846 Adapter cache time: 0.03089457144960761 Engine time: 0.032335948664695024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.70107386726886,
    "estimated_duration": 3600.0656790005864,
    "input_throughput": 5433.741699243264,
    "output_throughput": 4757.94458415413,
    "total_throughput": 10191.686283397394,
    "itl": 179.84021260996673,
    "ttft": 2084831.6458669214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1989,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.199313348037931,
    "arrivals": 765271,
    "finished_requests": 78644,
    "scheduler_time": 108.44868640541954
}
#Debug simulation 
Total elapsed time: 5.701155104208738. Arrivals time: 0.3107795873656869 Scheduler time: 5.281676341313869 Scheduler overhead time: 0.03102395636960864 Adapter cache time: 0.03070894069969654 Engine time: 0.03233462106436491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.673245375044644,
    "estimated_duration": 3600.0098219597894,
    "input_throughput": 5425.841863222055,
    "output_throughput": 4749.950373938452,
    "total_throughput": 10175.792237160505,
    "itl": 178.05556062101917,
    "ttft": 2085940.8984847337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.609375232923649,
    "arrivals": 765271,
    "finished_requests": 78522,
    "scheduler_time": 108.73881008232256
}
#Debug simulation 
Total elapsed time: 5.673328194301575. Arrivals time: 0.31991091277450323 Scheduler time: 5.244154319167137 Scheduler overhead time: 0.031110402662307024 Adapter cache time: 0.0311478772200644 Engine time: 0.032403587363660336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.728234899230301,
    "estimated_duration": 3600.011280107357,
    "input_throughput": 5434.255472504129,
    "output_throughput": 4758.372312513742,
    "total_throughput": 10192.62778501787,
    "itl": 179.82918741012287,
    "ttft": 2084739.8058709241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1990,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.950200086855155,
    "arrivals": 765271,
    "finished_requests": 78650,
    "scheduler_time": 108.45442669884275
}
#Debug simulation 
Total elapsed time: 5.7283307570032775. Arrivals time: 0.245964162517339 Scheduler time: 5.373438423499465 Scheduler overhead time: 0.03117704577744007 Adapter cache time: 0.03078369190916419 Engine time: 0.03232380375266075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.707675892859697,
    "estimated_duration": 3600.0889208839003,
    "input_throughput": 5425.722649985046,
    "output_throughput": 4749.846010970643,
    "total_throughput": 10175.568660955689,
    "itl": 178.05804899494095,
    "ttft": 2085971.586011694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.68721682671437,
    "arrivals": 765271,
    "finished_requests": 78522,
    "scheduler_time": 108.73907989911758
}
#Debug simulation 
Total elapsed time: 5.70783005701378. Arrivals time: 0.2869033948518336 Scheduler time: 5.311367877759039 Scheduler overhead time: 0.03146859537810087 Adapter cache time: 0.030961371958255768 Engine time: 0.032394043169915676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 34560, 34560, 135, 1080, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 1080, 135, 135, 135, 135, 135, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 135, 1080, 1080, 34560, 135, 135, 135, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 1080, 135, 34560, 1080, 135, 1080, 1080, 135, 1080, 34560, 135, 1080, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 2289600 . Total input tokens: 510901104 . Total output tokens: 458051534
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.703412548173219,
    "estimated_duration": 3600.18594167724,
    "input_throughput": 5471.466840632969,
    "output_throughput": 4833.718391748593,
    "total_throughput": 10305.185232381562,
    "itl": 178.03181103051543,
    "ttft": 2082731.301467737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1774,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.429305257736858,
    "arrivals": 762397,
    "finished_requests": 79371,
    "scheduler_time": 110.02619681226604
}
#Debug simulation 
Total elapsed time: 5.703516236972064. Arrivals time: 0.24443953670561314 Scheduler time: 5.35203921655193 Scheduler overhead time: 0.031039889436215162 Adapter cache time: 0.028948268853127956 Engine time: 0.032435243017971516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 34560, 34560, 135, 1080, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 1080, 135, 135, 135, 135, 135, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 135, 1080, 1080, 34560, 135, 135, 135, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 1080, 135, 34560, 1080, 135, 1080, 1080, 135, 1080, 34560, 135, 1080, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 2289600 . Total input tokens: 510901104 . Total output tokens: 458051534
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.688392810989171,
    "estimated_duration": 3600.1368651867597,
    "input_throughput": 5471.021168796102,
    "output_throughput": 4833.272081476086,
    "total_throughput": 10304.293250272187,
    "itl": 178.04713947549388,
    "ttft": 2082777.3414692178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1774,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.781997150413068,
    "arrivals": 762397,
    "finished_requests": 79365,
    "scheduler_time": 110.01426980180531
}
#Debug simulation 
Total elapsed time: 5.688478131312877. Arrivals time: 0.2446408187970519 Scheduler time: 5.335393979214132 Scheduler overhead time: 0.030966325663030148 Adapter cache time: 0.030479361303150654 Engine time: 0.032379478216171265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 34560, 34560, 135, 1080, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 1080, 135, 135, 135, 135, 135, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 135, 1080, 1080, 34560, 135, 135, 135, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 1080, 135, 34560, 1080, 135, 1080, 1080, 135, 1080, 34560, 135, 1080, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 2289600 . Total input tokens: 510901104 . Total output tokens: 458051534
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.618296350818127,
    "estimated_duration": 3600.0471981665532,
    "input_throughput": 5464.479190722508,
    "output_throughput": 4827.776149393672,
    "total_throughput": 10292.25534011618,
    "itl": 175.96765121350262,
    "ttft": 2083752.66709512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1768,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.77450915073971,
    "arrivals": 762397,
    "finished_requests": 79270,
    "scheduler_time": 110.39847249265466
}
#Debug simulation 
Total elapsed time: 5.6183845601044595. Arrivals time: 0.23872340004891157 Scheduler time: 5.272101333364844 Scheduler overhead time: 0.031177558936178684 Adapter cache time: 0.029133334755897522 Engine time: 0.03253216343000531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 34560, 34560, 135, 1080, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 1080, 135, 135, 135, 135, 135, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 135, 1080, 1080, 34560, 135, 135, 135, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 1080, 135, 34560, 1080, 135, 1080, 1080, 135, 1080, 34560, 135, 1080, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 2289600 . Total input tokens: 510901104 . Total output tokens: 458051534
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.7302668751217425,
    "estimated_duration": 3600.084592416523,
    "input_throughput": 5471.2592147115565,
    "output_throughput": 4833.519478029734,
    "total_throughput": 10304.77869274129,
    "itl": 178.03557221775182,
    "ttft": 2082714.1765998076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1774,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.529485243616942,
    "arrivals": 762397,
    "finished_requests": 79367,
    "scheduler_time": 110.0201584749539
}
#Debug simulation 
Total elapsed time: 5.730351488105953. Arrivals time: 0.2908020317554474 Scheduler time: 5.332265968434513 Scheduler overhead time: 0.0312257194891572 Adapter cache time: 0.029090278316289186 Engine time: 0.032366632018238306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 34560, 34560, 135, 1080, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 1080, 135, 135, 135, 135, 135, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 135, 1080, 1080, 34560, 135, 135, 135, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 1080, 135, 34560, 1080, 135, 1080, 1080, 135, 1080, 34560, 135, 1080, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 2289600 . Total input tokens: 510901104 . Total output tokens: 458051534
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.657950494904071,
    "estimated_duration": 3600.125110726573,
    "input_throughput": 5464.360930509369,
    "output_throughput": 4827.671668469417,
    "total_throughput": 10292.032598978785,
    "itl": 175.97134725334172,
    "ttft": 2083783.031011016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1768,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.852350744530471,
    "arrivals": 762397,
    "finished_requests": 79270,
    "scheduler_time": 110.39854345895286
}
#Debug simulation 
Total elapsed time: 5.658058819826692. Arrivals time: 0.23831656062975526 Scheduler time: 5.3117377338930964 Scheduler overhead time: 0.03129966091364622 Adapter cache time: 0.029128726571798325 Engine time: 0.03276449628174305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 34560, 34560, 135, 1080, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 1080, 135, 135, 135, 135, 135, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 135, 1080, 1080, 34560, 135, 135, 135, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 1080, 135, 34560, 1080, 135, 1080, 1080, 135, 1080, 34560, 135, 1080, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 2289600 . Total input tokens: 510901104 . Total output tokens: 458051534
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.628108149394393,
    "estimated_duration": 3600.0608563762066,
    "input_throughput": 5471.65694855174,
    "output_throughput": 4833.886340887305,
    "total_throughput": 10305.543289439045,
    "itl": 178.0258715906326,
    "ttft": 2082682.235627004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1774,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.304349223156308,
    "arrivals": 762397,
    "finished_requests": 79371,
    "scheduler_time": 110.02606754557064
}
#Debug simulation 
Total elapsed time: 5.628189700189978. Arrivals time: 0.2439542692154646 Scheduler time: 5.277707951609045 Scheduler overhead time: 0.030903782695531845 Adapter cache time: 0.028603772167116404 Engine time: 0.03246171632781625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 1080, 1080, 1080, 1080, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 34560, 1080, 135, 135, 1080, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 34560, 34560, 135, 1080, 34560, 135, 1080, 135, 1080, 135, 135, 1080, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 1080, 135, 135, 135, 135, 135, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 135, 1080, 1080, 34560, 135, 135, 135, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 1080, 34560, 34560, 1080, 34560, 1080, 135, 34560, 1080, 135, 1080, 1080, 135, 1080, 34560, 135, 1080, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 1080, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 1080, 135, 34560, 34560, 1080, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 135, 34560, 1080, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 2289600 . Total input tokens: 510901104 . Total output tokens: 458051534
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.692659585736692,
    "estimated_duration": 3600.194095512815,
    "input_throughput": 5464.2562256627025,
    "output_throughput": 4827.579163485169,
    "total_throughput": 10291.835389147871,
    "itl": 175.97461870218763,
    "ttft": 2083809.9010232517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1768,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.921263819485774,
    "arrivals": 762397,
    "finished_requests": 79270,
    "scheduler_time": 110.39861517030404
}
#Debug simulation 
Total elapsed time: 5.692760671023279. Arrivals time: 0.2456973628140986 Scheduler time: 5.3385067488998175 Scheduler overhead time: 0.031755753327161074 Adapter cache time: 0.029286082834005356 Engine time: 0.032697205897420645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 34560, 34560, 66, 1080, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 1080, 66, 66, 66, 66, 66, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 66, 1080, 1080, 34560, 66, 66, 66, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 1080, 66, 34560, 1080, 66, 1080, 1080, 66, 1080, 34560, 66, 1080, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 2285184 . Total input tokens: 509916820 . Total output tokens: 457184416
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.68477019527927,
    "estimated_duration": 3600.113224181599,
    "input_throughput": 5579.58479335503,
    "output_throughput": 4882.191449407981,
    "total_throughput": 10461.776242763011,
    "itl": 175.30274042718776,
    "ttft": 2074476.8150795163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.8233286844381515,
    "arrivals": 760968,
    "finished_requests": 80641,
    "scheduler_time": 111.26053236194765
}
#Debug simulation 
Total elapsed time: 5.6848792331293225. Arrivals time: 0.24797952082008123 Scheduler time: 5.331408618018031 Scheduler overhead time: 0.0312087326310575 Adapter cache time: 0.02706144656985998 Engine time: 0.032519098836928606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 34560, 34560, 66, 1080, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 1080, 66, 66, 66, 66, 66, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 66, 1080, 1080, 34560, 66, 66, 66, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 1080, 66, 34560, 1080, 66, 1080, 1080, 66, 1080, 34560, 66, 1080, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 2285184 . Total input tokens: 509916820 . Total output tokens: 457184416
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.7611577729694545,
    "estimated_duration": 3600.0801216233217,
    "input_throughput": 5579.241383923171,
    "output_throughput": 4882.010790380667,
    "total_throughput": 10461.252174303838,
    "itl": 175.31821322239605,
    "ttft": 2074472.9533319487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.128269298938004,
    "arrivals": 760968,
    "finished_requests": 80636,
    "scheduler_time": 111.25101758386762
}
#Debug simulation 
Total elapsed time: 5.761244466993958. Arrivals time: 0.2935920120216906 Scheduler time: 5.3610721048898995 Scheduler overhead time: 0.03145931055769324 Adapter cache time: 0.0273142303340137 Engine time: 0.03294279519468546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 34560, 34560, 66, 1080, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 1080, 66, 66, 66, 66, 66, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 66, 1080, 1080, 34560, 66, 66, 66, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 1080, 66, 34560, 1080, 66, 1080, 1080, 66, 1080, 34560, 66, 1080, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 2285184 . Total input tokens: 509916820 . Total output tokens: 457184416
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.6374173308722675,
    "estimated_duration": 3600.1722930183523,
    "input_throughput": 5572.418308675021,
    "output_throughput": 4876.161075413985,
    "total_throughput": 10448.579384089006,
    "itl": 173.63267944419482,
    "ttft": 2075223.8126366015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1584,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.165938043352161,
    "arrivals": 760968,
    "finished_requests": 80552,
    "scheduler_time": 111.56497678798277
}
#Debug simulation 
Total elapsed time: 5.637517726048827. Arrivals time: 0.23539512744173408 Scheduler time: 5.296391041949391 Scheduler overhead time: 0.031254202127456665 Adapter cache time: 0.026937831193208694 Engine time: 0.03270582668483257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 34560, 34560, 66, 1080, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 1080, 66, 66, 66, 66, 66, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 66, 1080, 1080, 34560, 66, 66, 66, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 1080, 66, 34560, 1080, 66, 1080, 1080, 66, 1080, 34560, 66, 1080, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 2285184 . Total input tokens: 509916820 . Total output tokens: 457184416
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.738888388965279,
    "estimated_duration": 3600.012561795626,
    "input_throughput": 5579.536364167919,
    "output_throughput": 4882.138519881582,
    "total_throughput": 10461.6748840495,
    "itl": 175.30572166241333,
    "ttft": 2074413.8865898103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1579,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.919458145093994,
    "arrivals": 760968,
    "finished_requests": 80638,
    "scheduler_time": 111.25518192602053
}
#Debug simulation 
Total elapsed time: 5.738973106723279. Arrivals time: 0.24646848952397704 Scheduler time: 5.386533601675183 Scheduler overhead time: 0.03134263260290027 Adapter cache time: 0.027030331548303366 Engine time: 0.032824303954839706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 34560, 34560, 66, 1080, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 1080, 66, 66, 66, 66, 66, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 66, 1080, 1080, 34560, 66, 66, 66, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 1080, 66, 34560, 1080, 66, 1080, 1080, 66, 1080, 34560, 66, 1080, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 2285184 . Total input tokens: 509916820 . Total output tokens: 457184416
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.71793974423781,
    "estimated_duration": 3600.009788371924,
    "input_throughput": 5572.349848824219,
    "output_throughput": 4876.131186281777,
    "total_throughput": 10448.481035105997,
    "itl": 173.64041523194769,
    "ttft": 2075200.5567353894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.229835552740783,
    "arrivals": 760968,
    "finished_requests": 80547,
    "scheduler_time": 111.55740823049004
}
#Debug simulation 
Total elapsed time: 5.718037838116288. Arrivals time: 0.251222085673362 Scheduler time: 5.360357618425041 Scheduler overhead time: 0.03145955828949809 Adapter cache time: 0.02711424231529236 Engine time: 0.03293990809470415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 34560, 34560, 66, 1080, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 1080, 66, 66, 66, 66, 66, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 66, 1080, 1080, 34560, 66, 66, 66, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 1080, 66, 34560, 1080, 66, 1080, 1080, 66, 1080, 34560, 66, 1080, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 2285184 . Total input tokens: 509916820 . Total output tokens: 457184416
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.744390433188528,
    "estimated_duration": 3600.0017790577012,
    "input_throughput": 5579.75752035817,
    "output_throughput": 4882.3425872307835,
    "total_throughput": 10462.100107588953,
    "itl": 175.29751872168043,
    "ttft": 2074433.7721817843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.712319264765698,
    "arrivals": 760968,
    "finished_requests": 80641,
    "scheduler_time": 111.26040160514626
}
#Debug simulation 
Total elapsed time: 5.744490789249539. Arrivals time: 0.2517956760711968 Scheduler time: 5.386607695836574 Scheduler overhead time: 0.03122672252357006 Adapter cache time: 0.027133385185152292 Engine time: 0.03291098400950432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 1080, 1080, 1080, 1080, 34560, 1080, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 34560, 1080, 66, 66, 1080, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 34560, 34560, 66, 1080, 34560, 66, 1080, 66, 1080, 66, 66, 1080, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 1080, 66, 66, 66, 66, 66, 34560, 66, 1080, 66, 34560, 34560, 66, 34560, 66, 1080, 1080, 34560, 66, 66, 66, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 1080, 34560, 34560, 1080, 34560, 1080, 66, 34560, 1080, 66, 1080, 1080, 66, 1080, 34560, 66, 1080, 34560, 66, 1080, 34560, 34560, 1080, 66, 34560, 1080, 66, 66, 1080, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 1080, 66, 34560, 34560, 1080, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 66, 34560, 1080, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 2285184 . Total input tokens: 509916820 . Total output tokens: 457184416
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.987070557195693,
    "estimated_duration": 3600.0709968981564,
    "input_throughput": 5572.255107547674,
    "output_throughput": 4876.048282137975,
    "total_throughput": 10448.303389685649,
    "itl": 173.64306462629435,
    "ttft": 2075224.232555034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1583,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.29107764672481,
    "arrivals": 760968,
    "finished_requests": 80547,
    "scheduler_time": 111.55744876677514
}
#Debug simulation 
Total elapsed time: 5.987134360242635. Arrivals time: 0.5237351967953146 Scheduler time: 5.357158496044576 Scheduler overhead time: 0.03151402622461319 Adapter cache time: 0.027180959936231375 Engine time: 0.03264594916254282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 34560, 34560, 33, 1080, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 1080, 33, 33, 33, 33, 33, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 33, 1080, 1080, 34560, 33, 33, 33, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 1080, 33, 34560, 1080, 33, 1080, 1080, 33, 1080, 34560, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 2283072 . Total input tokens: 509442026 . Total output tokens: 456770431
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.052333862055093,
    "estimated_duration": 3600.0935285207192,
    "input_throughput": 5608.561788753482,
    "output_throughput": 4931.1324440214,
    "total_throughput": 10539.694232774882,
    "itl": 174.0140427517143,
    "ttft": 2068895.6559951156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.299985280225632,
    "arrivals": 760248,
    "finished_requests": 81418,
    "scheduler_time": 112.28374920771843
}
#Debug simulation 
Total elapsed time: 6.052445513661951. Arrivals time: 0.5324789918959141 Scheduler time: 5.41455724555999 Scheduler overhead time: 0.03151803370565176 Adapter cache time: 0.025990914553403854 Engine time: 0.032983805518597364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 34560, 34560, 33, 1080, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 1080, 33, 33, 33, 33, 33, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 33, 1080, 1080, 34560, 33, 33, 33, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 1080, 33, 34560, 1080, 33, 1080, 1080, 33, 1080, 34560, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 2283072 . Total input tokens: 509442026 . Total output tokens: 456770431
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.038121194578707,
    "estimated_duration": 3600.1763402597317,
    "input_throughput": 5608.258621727223,
    "output_throughput": 4930.666256953222,
    "total_throughput": 10538.924878680446,
    "itl": 174.02628251733236,
    "ttft": 2068971.0154498594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.575702571347307,
    "arrivals": 760248,
    "finished_requests": 81415,
    "scheduler_time": 112.27800713731109
}
#Debug simulation 
Total elapsed time: 6.0381834278814495. Arrivals time: 0.531175056938082 Scheduler time: 5.4026042385958135 Scheduler overhead time: 0.031057247426360846 Adapter cache time: 0.02588674472644925 Engine time: 0.03270386718213558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 34560, 34560, 33, 1080, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 1080, 33, 33, 33, 33, 33, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 33, 1080, 1080, 34560, 33, 33, 33, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 1080, 33, 34560, 1080, 33, 1080, 1080, 33, 1080, 34560, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 2283072 . Total input tokens: 509442026 . Total output tokens: 456770431
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.047808336094022,
    "estimated_duration": 3600.0237152940454,
    "input_throughput": 5602.42698244354,
    "output_throughput": 4926.362547184339,
    "total_throughput": 10528.789529627878,
    "itl": 172.2492296620531,
    "ttft": 2069719.6310748532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.5979112578927905,
    "arrivals": 760248,
    "finished_requests": 81328,
    "scheduler_time": 112.61017220205156
}
#Debug simulation 
Total elapsed time: 6.047877573408186. Arrivals time: 0.5315958978608251 Scheduler time: 5.409787111449987 Scheduler overhead time: 0.03175480663776398 Adapter cache time: 0.02634685579687357 Engine time: 0.03317875321954489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 34560, 34560, 33, 1080, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 1080, 33, 33, 33, 33, 33, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 33, 1080, 1080, 34560, 33, 33, 33, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 1080, 33, 34560, 1080, 33, 1080, 1080, 33, 1080, 34560, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 2283072 . Total input tokens: 509442026 . Total output tokens: 456770431
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.091827435884625,
    "estimated_duration": 3600.1730456076443,
    "input_throughput": 5608.4379123482,
    "output_throughput": 4931.023530010261,
    "total_throughput": 10539.461442358463,
    "itl": 174.01778647049997,
    "ttft": 2068925.770666548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.378759627859033,
    "arrivals": 760248,
    "finished_requests": 81418,
    "scheduler_time": 112.28388320709279
}
#Debug simulation 
Total elapsed time: 6.091891343705356. Arrivals time: 0.5255765123292804 Scheduler time: 5.46091746352613 Scheduler overhead time: 0.03158083511516452 Adapter cache time: 0.026063333731144667 Engine time: 0.03295326232910156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 34560, 34560, 33, 1080, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 1080, 33, 33, 33, 33, 33, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 33, 1080, 1080, 34560, 33, 33, 33, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 1080, 33, 34560, 1080, 33, 1080, 1080, 33, 1080, 34560, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 2283072 . Total input tokens: 509442026 . Total output tokens: 456770431
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.0010751048102975,
    "estimated_duration": 3600.084084334615,
    "input_throughput": 5602.333036542869,
    "output_throughput": 4926.279938063689,
    "total_throughput": 10528.612974606558,
    "itl": 172.25167260088395,
    "ttft": 2069742.864270293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.658147321585537,
    "arrivals": 760248,
    "finished_requests": 81328,
    "scheduler_time": 112.61041429457757
}
#Debug simulation 
Total elapsed time: 6.001167800743133. Arrivals time: 0.5284748431295156 Scheduler time: 5.3666220917366445 Scheduler overhead time: 0.0318530322983861 Adapter cache time: 0.026389511302113533 Engine time: 0.03284809412434697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 34560, 34560, 33, 1080, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 1080, 33, 33, 33, 33, 33, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 33, 1080, 1080, 34560, 33, 33, 33, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 1080, 33, 34560, 1080, 33, 1080, 1080, 33, 1080, 34560, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 2283072 . Total input tokens: 509442026 . Total output tokens: 456770431
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.063439669087529,
    "estimated_duration": 3600.18695122962,
    "input_throughput": 5608.7923414930765,
    "output_throughput": 4931.392241710188,
    "total_throughput": 10540.184583203263,
    "itl": 174.01096898264296,
    "ttft": 2068861.7920535894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.201020664337444,
    "arrivals": 760248,
    "finished_requests": 81424,
    "scheduler_time": 112.28969000397944
}
#Debug simulation 
Total elapsed time: 6.063541354145855. Arrivals time: 0.5272030611522496 Scheduler time: 5.431765386369079 Scheduler overhead time: 0.031127406284213066 Adapter cache time: 0.02579905977472663 Engine time: 0.032809797674417496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 1080, 1080, 1080, 1080, 34560, 1080, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 34560, 1080, 33, 33, 1080, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 34560, 34560, 33, 1080, 34560, 33, 1080, 33, 1080, 33, 33, 1080, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 1080, 33, 33, 33, 33, 33, 34560, 33, 1080, 33, 34560, 34560, 33, 34560, 33, 1080, 1080, 34560, 33, 33, 33, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 1080, 34560, 34560, 1080, 34560, 1080, 33, 34560, 1080, 33, 1080, 1080, 33, 1080, 34560, 33, 1080, 34560, 33, 1080, 34560, 34560, 1080, 33, 34560, 1080, 33, 33, 1080, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 1080, 33, 34560, 34560, 1080, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 33, 34560, 1080, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 2283072 . Total input tokens: 509442026 . Total output tokens: 456770431
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.976440050173551,
    "estimated_duration": 3600.1245564775677,
    "input_throughput": 5602.270055826518,
    "output_throughput": 4926.224557450393,
    "total_throughput": 10528.494613276911,
    "itl": 172.25333553582675,
    "ttft": 2069752.2644055663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.713353233821681,
    "arrivals": 760248,
    "finished_requests": 81328,
    "scheduler_time": 112.61072453049803
}
#Debug simulation 
Total elapsed time: 5.97650035424158. Arrivals time: 0.517997411545366 Scheduler time: 5.353222307749093 Scheduler overhead time: 0.03138322336599231 Adapter cache time: 0.026026082690805197 Engine time: 0.033013971988111734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 540, 540, 34560, 540, 270, 34560, 270, 540, 540, 270, 34560, 34560, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 270, 540, 540, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 34560, 34560, 270, 540, 34560, 270, 540, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 540, 270, 270, 270, 270, 270, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 270, 540, 540, 34560, 270, 270, 270, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 270, 34560, 540, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 540, 270, 34560, 540, 270, 540, 540, 270, 540, 34560, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 2263680 . Total input tokens: 505155231 . Total output tokens: 452887769
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.845220190938562,
    "estimated_duration": 3600.126033973904,
    "input_throughput": 5650.36162846388,
    "output_throughput": 4983.721633821571,
    "total_throughput": 10634.083262285452,
    "itl": 172.618668389249,
    "ttft": 2066915.5725036007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.722332588888367,
    "arrivals": 753816,
    "finished_requests": 82060,
    "scheduler_time": 113.3863542749231
}
#Debug simulation 
Total elapsed time: 5.845304094720632. Arrivals time: 0.2519393595866859 Scheduler time: 5.487750803586096 Scheduler overhead time: 0.031524261459708214 Adapter cache time: 0.026096796616911888 Engine time: 0.033047594130039215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 540, 540, 34560, 540, 270, 34560, 270, 540, 540, 270, 34560, 34560, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 270, 540, 540, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 34560, 34560, 270, 540, 34560, 270, 540, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 540, 270, 270, 270, 270, 270, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 270, 540, 540, 34560, 270, 270, 270, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 270, 34560, 540, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 540, 270, 34560, 540, 270, 540, 540, 270, 540, 34560, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 2263680 . Total input tokens: 505155231 . Total output tokens: 452887769
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.79979916010052,
    "estimated_duration": 3600.0544569184854,
    "input_throughput": 5650.089531537485,
    "output_throughput": 4983.644612797656,
    "total_throughput": 10633.734144335142,
    "itl": 172.63418582838452,
    "ttft": 2066971.1131784979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1542,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.029059160821982,
    "arrivals": 753816,
    "finished_requests": 82055,
    "scheduler_time": 113.37455108310935
}
#Debug simulation 
Total elapsed time: 5.799884312320501. Arrivals time: 0.25232521584257483 Scheduler time: 5.4420558623969555 Scheduler overhead time: 0.03154541039839387 Adapter cache time: 0.02580616157501936 Engine time: 0.033185725565999746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 540, 540, 34560, 540, 270, 34560, 270, 540, 540, 270, 34560, 34560, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 270, 540, 540, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 34560, 34560, 270, 540, 34560, 270, 540, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 540, 270, 270, 270, 270, 270, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 270, 540, 540, 34560, 270, 270, 270, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 270, 34560, 540, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 540, 270, 34560, 540, 270, 540, 540, 270, 540, 34560, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 2263680 . Total input tokens: 505155231 . Total output tokens: 452887769
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.119470453821123,
    "estimated_duration": 3600.1102602268784,
    "input_throughput": 5643.518540101493,
    "output_throughput": 4978.511407834071,
    "total_throughput": 10622.029947935564,
    "itl": 170.6845274095065,
    "ttft": 2067966.6019376214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1546,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.050958822090126,
    "arrivals": 753816,
    "finished_requests": 81968,
    "scheduler_time": 113.7559581320441
}
#Debug simulation 
Total elapsed time: 6.119537740945816. Arrivals time: 0.5270931660197675 Scheduler time: 5.484899442177266 Scheduler overhead time: 0.03216729033738375 Adapter cache time: 0.026402538176625967 Engine time: 0.03354041092097759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 540, 540, 34560, 540, 270, 34560, 270, 540, 540, 270, 34560, 34560, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 270, 540, 540, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 34560, 34560, 270, 540, 34560, 270, 540, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 540, 270, 270, 270, 270, 270, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 270, 540, 540, 34560, 270, 270, 270, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 270, 34560, 540, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 540, 270, 34560, 540, 270, 540, 540, 270, 540, 34560, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 2263680 . Total input tokens: 505155231 . Total output tokens: 452887769
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.849899545777589,
    "estimated_duration": 3600.022213358206,
    "input_throughput": 5650.203191670157,
    "output_throughput": 4983.736470687937,
    "total_throughput": 10633.939662358094,
    "itl": 172.62463282355148,
    "ttft": 2066924.7278042617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1542,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.809643474819501,
    "arrivals": 753816,
    "finished_requests": 82056,
    "scheduler_time": 113.38039829984938
}
#Debug simulation 
Total elapsed time: 5.849979144986719. Arrivals time: 0.2577372887171805 Scheduler time: 5.485884764231741 Scheduler overhead time: 0.031759215984493494 Adapter cache time: 0.026332874782383442 Engine time: 0.033204348757863045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_192_slots_96_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 540, 540, 34560, 540, 270, 34560, 270, 540, 540, 270, 34560, 34560, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 270, 540, 540, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 34560, 34560, 270, 540, 34560, 270, 540, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 540, 270, 270, 270, 270, 270, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 270, 540, 540, 34560, 270, 270, 270, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 270, 34560, 540, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 540, 270, 34560, 540, 270, 540, 540, 270, 540, 34560, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 2263680 . Total input tokens: 505155231 . Total output tokens: 452887769
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.834688515868038,
    "estimated_duration": 3600.1783133629983,
    "input_throughput": 5643.411862292236,
    "output_throughput": 4978.41730046354,
    "total_throughput": 10621.829162755777,
    "itl": 170.6876734444597,
    "ttft": 2067993.535635511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1546,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.118614359181324,
    "arrivals": 753816,
    "finished_requests": 81968,
    "scheduler_time": 113.75604670021941
}
#Debug simulation 
Total elapsed time: 5.834786574821919. Arrivals time: 0.2615484190173447 Scheduler time: 5.466127331368625 Scheduler overhead time: 0.032115004025399685 Adapter cache time: 0.02627777773886919 Engine time: 0.033532270696014166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_192_slots_96_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_192_slots_96_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 540, 540, 34560, 540, 270, 34560, 270, 540, 540, 270, 34560, 34560, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 270, 540, 540, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 34560, 34560, 270, 540, 34560, 270, 540, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 540, 270, 270, 270, 270, 270, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 270, 540, 540, 34560, 270, 270, 270, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 270, 34560, 540, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 540, 270, 34560, 540, 270, 540, 540, 270, 540, 34560, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 2263680 . Total input tokens: 505155231 . Total output tokens: 452887769
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.047525762114674,
    "estimated_duration": 3600.0162718731144,
    "input_throughput": 5650.533904230356,
    "output_throughput": 4983.873584178172,
    "total_throughput": 10634.407488408528,
    "itl": 172.6140100541272,
    "ttft": 2066874.3393205304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1543,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.61364760503393,
    "arrivals": 753816,
    "finished_requests": 82060,
    "scheduler_time": 113.3862701273668
}
#Debug simulation 
Total elapsed time: 6.047613120172173. Arrivals time: 0.5174982347525656 Scheduler time: 5.4245923943817616 Scheduler overhead time: 0.03168333973735571 Adapter cache time: 0.025920278392732143 Engine time: 0.032992427702993155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_192_slots_96_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_192_slots_96_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 540, 540, 540, 540, 34560, 540, 270, 34560, 270, 540, 540, 270, 34560, 34560, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 270, 540, 540, 34560, 540, 270, 270, 540, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 34560, 34560, 270, 540, 34560, 270, 540, 270, 540, 270, 270, 540, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 540, 270, 270, 270, 270, 270, 34560, 270, 540, 270, 34560, 34560, 270, 34560, 270, 540, 540, 34560, 270, 270, 270, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 270, 34560, 540, 540, 540, 34560, 540, 270, 540, 34560, 34560, 540, 34560, 540, 270, 34560, 540, 270, 540, 540, 270, 540, 34560, 270, 540, 34560, 270, 540, 34560, 34560, 540, 270, 34560, 540, 270, 270, 540, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 540, 270, 34560, 34560, 540, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 270, 34560, 540, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 2263680 . Total input tokens: 505155231 . Total output tokens: 452887769
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.808909812010825,
    "estimated_duration": 3600.049449598437,
    "input_throughput": 5643.270539593874,
    "output_throughput": 4978.218008088491,
    "total_throughput": 10621.488547682364,
    "itl": 170.69069063911812,
    "ttft": 2067952.7158828792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1546,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.179856453165356,
    "arrivals": 753816,
    "finished_requests": 81963,
    "scheduler_time": 113.75001846035694
}
#Debug simulation 
Total elapsed time: 5.809037692844868. Arrivals time: 0.2533196844160557 Scheduler time: 5.448545563966036 Scheduler overhead time: 0.03183082211762667 Adapter cache time: 0.02658623829483986 Engine time: 0.033606682904064655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 540, 540, 34560, 540, 135, 34560, 135, 540, 540, 135, 34560, 34560, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 135, 540, 540, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 34560, 34560, 135, 540, 34560, 135, 540, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 540, 135, 135, 135, 135, 135, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 135, 540, 540, 34560, 135, 135, 135, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 135, 34560, 540, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 540, 135, 34560, 540, 135, 540, 540, 135, 540, 34560, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 2255040 . Total input tokens: 503262017 . Total output tokens: 451178001
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.938013296108693,
    "estimated_duration": 3600.183520437331,
    "input_throughput": 5761.619062541641,
    "output_throughput": 5075.511538861863,
    "total_throughput": 10837.130601403505,
    "itl": 169.33606778187672,
    "ttft": 2051078.207618943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6205570010725374,
    "arrivals": 750836,
    "finished_requests": 83667,
    "scheduler_time": 115.41586794666829
}
#Debug simulation 
Total elapsed time: 5.938100709114224. Arrivals time: 0.2577470331452787 Scheduler time: 5.576225661672652 Scheduler overhead time: 0.032284729182720184 Adapter cache time: 0.022675184067338705 Engine time: 0.03384783584624529 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 540, 540, 34560, 540, 135, 34560, 135, 540, 540, 135, 34560, 34560, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 135, 540, 540, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 34560, 34560, 135, 540, 34560, 135, 540, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 540, 135, 135, 135, 135, 135, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 135, 540, 540, 34560, 135, 135, 135, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 135, 34560, 540, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 540, 135, 34560, 540, 135, 540, 540, 135, 540, 34560, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 2255040 . Total input tokens: 503262017 . Total output tokens: 451178001
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.891449176706374,
    "estimated_duration": 3600.0683809399375,
    "input_throughput": 5761.2988991572265,
    "output_throughput": 5075.199431414586,
    "total_throughput": 10836.498330571812,
    "itl": 169.34614414901503,
    "ttft": 2051116.7167713288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.851718011002527,
    "arrivals": 750836,
    "finished_requests": 83661,
    "scheduler_time": 115.40537383565909
}
#Debug simulation 
Total elapsed time: 5.891529839951545. Arrivals time: 0.2643709359690547 Scheduler time: 5.522393868304789 Scheduler overhead time: 0.032274887431412935 Adapter cache time: 0.023198497015982866 Engine time: 0.03400410385802388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 540, 540, 34560, 540, 135, 34560, 135, 540, 540, 135, 34560, 34560, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 135, 540, 540, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 34560, 34560, 135, 540, 34560, 135, 540, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 540, 135, 135, 135, 135, 135, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 135, 540, 540, 34560, 135, 135, 135, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 135, 34560, 540, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 540, 135, 34560, 540, 135, 540, 540, 135, 540, 34560, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 2255040 . Total input tokens: 503262017 . Total output tokens: 451178001
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.870320779271424,
    "estimated_duration": 3600.067319283931,
    "input_throughput": 5754.08656639241,
    "output_throughput": 5069.387981230871,
    "total_throughput": 10823.47454762328,
    "itl": 167.6868602893931,
    "ttft": 2051880.541534776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8527190498075825,
    "arrivals": 750836,
    "finished_requests": 83559,
    "scheduler_time": 115.7253928551755
}
#Debug simulation 
Total elapsed time: 5.870399357285351. Arrivals time: 0.25829282496124506 Scheduler time: 5.507613928988576 Scheduler overhead time: 0.03230920433998108 Adapter cache time: 0.022998278960585594 Engine time: 0.0339043946005404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 540, 540, 34560, 540, 135, 34560, 135, 540, 540, 135, 34560, 34560, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 135, 540, 540, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 34560, 34560, 135, 540, 34560, 135, 540, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 540, 135, 135, 135, 135, 135, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 135, 540, 540, 34560, 135, 135, 135, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 135, 34560, 540, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 540, 135, 34560, 540, 135, 540, 540, 135, 540, 34560, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 2255040 . Total input tokens: 503262017 . Total output tokens: 451178001
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.9352565803565085,
    "estimated_duration": 3600.0656975846996,
    "input_throughput": 5761.442635315132,
    "output_throughput": 5075.364600223415,
    "total_throughput": 10836.807235538547,
    "itl": 169.3395749130866,
    "ttft": 2051080.642181074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.692087124558122,
    "arrivals": 750836,
    "finished_requests": 83663,
    "scheduler_time": 115.4099196725644
}
#Debug simulation 
Total elapsed time: 5.935338407289237. Arrivals time: 0.2598508670926094 Scheduler time: 5.570567321963608 Scheduler overhead time: 0.03238721704110503 Adapter cache time: 0.023057664278894663 Engine time: 0.033988694194704294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 540, 540, 34560, 540, 135, 34560, 135, 540, 540, 135, 34560, 34560, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 135, 540, 540, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 34560, 34560, 135, 540, 34560, 135, 540, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 540, 135, 135, 135, 135, 135, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 135, 540, 540, 34560, 135, 135, 135, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 135, 34560, 540, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 540, 135, 34560, 540, 135, 540, 540, 135, 540, 34560, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 2255040 . Total input tokens: 503262017 . Total output tokens: 451178001
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.926976785995066,
    "estimated_duration": 3600.117046697787,
    "input_throughput": 5754.007086797625,
    "output_throughput": 5069.317959186901,
    "total_throughput": 10823.325045984526,
    "itl": 167.6890704907851,
    "ttft": 2051900.3605101875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9023917954415084,
    "arrivals": 750836,
    "finished_requests": 83559,
    "scheduler_time": 115.72544752343933
}
#Debug simulation 
Total elapsed time: 5.927056972868741. Arrivals time: 0.249205875210464 Scheduler time: 5.572343967389315 Scheduler overhead time: 0.032631011214107275 Adapter cache time: 0.023319712840020657 Engine time: 0.03409717511385679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 540, 540, 34560, 540, 135, 34560, 135, 540, 540, 135, 34560, 34560, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 135, 540, 540, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 34560, 34560, 135, 540, 34560, 135, 540, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 540, 135, 135, 135, 135, 135, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 135, 540, 540, 34560, 135, 135, 135, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 135, 34560, 540, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 540, 135, 34560, 540, 135, 540, 540, 135, 540, 34560, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 2255040 . Total input tokens: 503262017 . Total output tokens: 451178001
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.963715407066047,
    "estimated_duration": 3600.096975826571,
    "input_throughput": 5761.677293495008,
    "output_throughput": 5075.632996193007,
    "total_throughput": 10837.310289688014,
    "itl": 169.33409030036395,
    "ttft": 2051045.7715670054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.534239448574283,
    "arrivals": 750836,
    "finished_requests": 83666,
    "scheduler_time": 115.41500362575862
}
#Debug simulation 
Total elapsed time: 5.963811242952943. Arrivals time: 0.25985108222812414 Scheduler time: 5.598873682785779 Scheduler overhead time: 0.032252770848572254 Adapter cache time: 0.023182806558907032 Engine time: 0.03429802227765322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 540, 540, 540, 540, 34560, 540, 135, 34560, 135, 540, 540, 135, 34560, 34560, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 135, 540, 540, 34560, 540, 135, 135, 540, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 34560, 34560, 135, 540, 34560, 135, 540, 135, 540, 135, 135, 540, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 540, 135, 135, 135, 135, 135, 34560, 135, 540, 135, 34560, 34560, 135, 34560, 135, 540, 540, 34560, 135, 135, 135, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 135, 34560, 540, 540, 540, 34560, 540, 135, 540, 34560, 34560, 540, 34560, 540, 135, 34560, 540, 135, 540, 540, 135, 540, 34560, 135, 540, 34560, 135, 540, 34560, 34560, 540, 135, 34560, 540, 135, 135, 540, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 540, 135, 34560, 34560, 540, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 135, 34560, 540, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 2255040 . Total input tokens: 503262017 . Total output tokens: 451178001
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.9160072901286185,
    "estimated_duration": 3600.1649056861133,
    "input_throughput": 5753.930595590913,
    "output_throughput": 5069.250569932413,
    "total_throughput": 10823.181165523325,
    "itl": 167.69119598567505,
    "ttft": 2051919.186447103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.950178234279214,
    "arrivals": 750836,
    "finished_requests": 83559,
    "scheduler_time": 115.72552007297516
}
#Debug simulation 
Total elapsed time: 5.916118927299976. Arrivals time: 0.25490238098427653 Scheduler time: 5.55573206814006 Scheduler overhead time: 0.03251477563753724 Adapter cache time: 0.023082489613443613 Engine time: 0.03444588091224432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 540, 540, 34560, 540, 66, 34560, 66, 540, 540, 66, 34560, 34560, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 66, 540, 540, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 34560, 34560, 66, 540, 34560, 66, 540, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 540, 66, 66, 66, 66, 66, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 66, 540, 540, 34560, 66, 66, 66, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 66, 34560, 540, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 540, 66, 34560, 540, 66, 540, 540, 66, 540, 34560, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 2250624 . Total input tokens: 502286914 . Total output tokens: 450313199
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.069450973067433,
    "estimated_duration": 3600.0170227619337,
    "input_throughput": 5838.851279617972,
    "output_throughput": 5159.473658752278,
    "total_throughput": 10998.324938370251,
    "itl": 166.78065395943233,
    "ttft": 2045457.254354675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 975,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9839755503345025,
    "arrivals": 749460,
    "finished_requests": 85212,
    "scheduler_time": 117.35122374966126
}
#Debug simulation 
Total elapsed time: 6.069574221968651. Arrivals time: 0.3360040965490043 Scheduler time: 5.630311897024512 Scheduler overhead time: 0.0326789291575551 Adapter cache time: 0.020839679054915905 Engine time: 0.034158088732510805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 540, 540, 34560, 540, 66, 34560, 66, 540, 540, 66, 34560, 34560, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 66, 540, 540, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 34560, 34560, 66, 540, 34560, 66, 540, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 540, 66, 66, 66, 66, 66, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 66, 540, 540, 34560, 66, 66, 66, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 66, 34560, 540, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 540, 66, 34560, 540, 66, 540, 540, 66, 540, 34560, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 2250624 . Total input tokens: 502286914 . Total output tokens: 450313199
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.172227126080543,
    "estimated_duration": 3600.0168991963155,
    "input_throughput": 5838.672314202872,
    "output_throughput": 5159.005504709217,
    "total_throughput": 10997.67781891209,
    "itl": 166.78773376032157,
    "ttft": 2045521.806232392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 975,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1710797068732814,
    "arrivals": 749460,
    "finished_requests": 85208,
    "scheduler_time": 117.34554851509569
}
#Debug simulation 
Total elapsed time: 6.1723091141320765. Arrivals time: 0.38238581269979477 Scheduler time: 5.686762547586113 Scheduler overhead time: 0.032672216184437275 Adapter cache time: 0.020653555635362864 Engine time: 0.034330550115555525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 540, 540, 34560, 540, 66, 34560, 66, 540, 540, 66, 34560, 34560, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 66, 540, 540, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 34560, 34560, 66, 540, 34560, 66, 540, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 540, 66, 66, 66, 66, 66, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 66, 540, 540, 34560, 66, 66, 66, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 66, 34560, 540, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 540, 66, 34560, 540, 66, 540, 540, 66, 540, 34560, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 2250624 . Total input tokens: 502286914 . Total output tokens: 450313199
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.091923373285681,
    "estimated_duration": 3600.123023212193,
    "input_throughput": 5830.974626325348,
    "output_throughput": 5151.29757522926,
    "total_throughput": 10982.272201554608,
    "itl": 164.9921869277083,
    "ttft": 2046694.2873854607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 974,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.175504290275244,
    "arrivals": 749460,
    "finished_requests": 85082,
    "scheduler_time": 117.6955476885692
}
#Debug simulation 
Total elapsed time: 6.0920030903071165. Arrivals time: 0.33762379735708237 Scheduler time: 5.650168692693114 Scheduler overhead time: 0.03294757753610611 Adapter cache time: 0.020936372224241495 Engine time: 0.034634086303412914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 540, 540, 34560, 540, 66, 34560, 66, 540, 540, 66, 34560, 34560, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 66, 540, 540, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 34560, 34560, 66, 540, 34560, 66, 540, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 540, 66, 66, 66, 66, 66, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 66, 540, 540, 34560, 66, 66, 66, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 66, 34560, 540, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 540, 66, 34560, 540, 66, 540, 540, 66, 540, 34560, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 2250624 . Total input tokens: 502286914 . Total output tokens: 450313199
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.115802388638258,
    "estimated_duration": 3600.061220443127,
    "input_throughput": 5838.779596479384,
    "output_throughput": 5159.410316281713,
    "total_throughput": 10998.189912761096,
    "itl": 166.78254124002933,
    "ttft": 2045475.6946184717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 975,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.02807134541912,
    "arrivals": 749460,
    "finished_requests": 85212,
    "scheduler_time": 117.35132563565892
}
#Debug simulation 
Total elapsed time: 6.1158846956677735. Arrivals time: 0.25709175504744053 Scheduler time: 5.754842315334827 Scheduler overhead time: 0.03276897827163339 Adapter cache time: 0.021112500224262476 Engine time: 0.034527262672781944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 540, 540, 34560, 540, 66, 34560, 66, 540, 540, 66, 34560, 34560, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 66, 540, 540, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 34560, 34560, 66, 540, 34560, 66, 540, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 540, 66, 66, 66, 66, 66, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 66, 540, 540, 34560, 66, 66, 66, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 66, 34560, 540, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 540, 66, 34560, 540, 66, 540, 540, 66, 540, 34560, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 2250624 . Total input tokens: 502286914 . Total output tokens: 450313199
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.157252490054816,
    "estimated_duration": 3600.1671059547984,
    "input_throughput": 5830.903228152423,
    "output_throughput": 5151.234499455716,
    "total_throughput": 10982.137727608138,
    "itl": 164.99410271765493,
    "ttft": 2046711.7422010517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 974,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.219518115520485,
    "arrivals": 749460,
    "finished_requests": 85082,
    "scheduler_time": 117.69561660597165
}
#Debug simulation 
Total elapsed time: 6.157355273142457. Arrivals time: 0.3802306572906673 Scheduler time: 5.67312498902902 Scheduler overhead time: 0.03307573730126023 Adapter cache time: 0.020854360423982143 Engine time: 0.0343435425311327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 540, 540, 34560, 540, 66, 34560, 66, 540, 540, 66, 34560, 34560, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 66, 540, 540, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 34560, 34560, 66, 540, 34560, 66, 540, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 540, 66, 66, 66, 66, 66, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 66, 540, 540, 34560, 66, 66, 66, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 66, 34560, 540, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 540, 66, 34560, 540, 66, 540, 540, 66, 540, 34560, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 2250624 . Total input tokens: 502286914 . Total output tokens: 450313199
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.080718145240098,
    "estimated_duration": 3600.1356203843784,
    "input_throughput": 5838.823371258353,
    "output_throughput": 5159.59479271416,
    "total_throughput": 10998.418163972514,
    "itl": 166.77818617296006,
    "ttft": 2045471.5516945843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 975,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9152990375295547,
    "arrivals": 749460,
    "finished_requests": 85219,
    "scheduler_time": 117.35710178475775
}
#Debug simulation 
Total elapsed time: 6.080825748387724. Arrivals time: 0.33698557037860155 Scheduler time: 5.640196337830275 Scheduler overhead time: 0.032585108652710915 Adapter cache time: 0.0212131948210299 Engine time: 0.034358961042016745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 540, 540, 540, 540, 34560, 540, 66, 34560, 66, 540, 540, 66, 34560, 34560, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 66, 540, 540, 34560, 540, 66, 66, 540, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 34560, 34560, 66, 540, 34560, 66, 540, 66, 540, 66, 66, 540, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 540, 66, 66, 66, 66, 66, 34560, 66, 540, 66, 34560, 34560, 66, 34560, 66, 540, 540, 34560, 66, 66, 66, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 66, 34560, 540, 540, 540, 34560, 540, 66, 540, 34560, 34560, 540, 34560, 540, 66, 34560, 540, 66, 540, 540, 66, 540, 34560, 66, 540, 34560, 66, 540, 34560, 34560, 540, 66, 34560, 540, 66, 66, 540, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 540, 66, 34560, 34560, 540, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 66, 34560, 540, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 2250624 . Total input tokens: 502286914 . Total output tokens: 450313199
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.127168246079236,
    "estimated_duration": 3600.0148147226023,
    "input_throughput": 5830.880449201005,
    "output_throughput": 5151.289079189235,
    "total_throughput": 10982.16952839024,
    "itl": 164.99567236066142,
    "ttft": 2046658.0389485282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 974,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.254226160571017,
    "arrivals": 749460,
    "finished_requests": 85078,
    "scheduler_time": 117.68970065346855
}
#Debug simulation 
Total elapsed time: 6.127267424017191. Arrivals time: 0.33752203127369285 Scheduler time: 5.685210585128516 Scheduler overhead time: 0.03319712355732918 Adapter cache time: 0.02103573316708207 Engine time: 0.034588065929710865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 540, 540, 34560, 540, 33, 34560, 33, 540, 540, 33, 34560, 34560, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 33, 540, 540, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 34560, 34560, 33, 540, 34560, 33, 540, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 540, 33, 33, 33, 33, 33, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 33, 540, 540, 34560, 33, 33, 33, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 33, 34560, 540, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 540, 33, 34560, 540, 33, 540, 540, 33, 540, 34560, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 2248512 . Total input tokens: 501813378 . Total output tokens: 449886057
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.05703666433692,
    "estimated_duration": 3600.129784370081,
    "input_throughput": 5897.933483449464,
    "output_throughput": 5176.183947840807,
    "total_throughput": 11074.117431290271,
    "itl": 165.62739742602648,
    "ttft": 2041618.7421366998,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 868,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6565033617336864,
    "arrivals": 748703,
    "finished_requests": 85703,
    "scheduler_time": 117.77992495016319
}
#Debug simulation 
Total elapsed time: 6.057119490113109. Arrivals time: 0.26299825916066766 Scheduler time: 5.690881115850061 Scheduler overhead time: 0.03291136911138892 Adapter cache time: 0.019852631259709597 Engine time: 0.03489056648686528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 540, 540, 34560, 540, 33, 34560, 33, 540, 540, 33, 34560, 34560, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 33, 540, 540, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 34560, 34560, 33, 540, 34560, 33, 540, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 540, 33, 33, 33, 33, 33, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 33, 540, 540, 34560, 33, 33, 33, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 33, 34560, 540, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 540, 33, 34560, 540, 33, 540, 540, 33, 540, 34560, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 2248512 . Total input tokens: 501813378 . Total output tokens: 449886057
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.038229576777667,
    "estimated_duration": 3600.0356505343657,
    "input_throughput": 5897.132712240996,
    "output_throughput": 5175.588190976482,
    "total_throughput": 11072.720903217478,
    "itl": 165.63621665969706,
    "ttft": 2041653.616824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 867,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8220041746436655,
    "arrivals": 748703,
    "finished_requests": 85694,
    "scheduler_time": 117.7710986121271
}
#Debug simulation 
Total elapsed time: 6.03833403903991. Arrivals time: 0.26190051436424255 Scheduler time: 5.673717864323407 Scheduler overhead time: 0.03286138828843832 Adapter cache time: 0.01977974036708474 Engine time: 0.03449384216219187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 540, 540, 34560, 540, 33, 34560, 33, 540, 540, 33, 34560, 34560, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 33, 540, 540, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 34560, 34560, 33, 540, 34560, 33, 540, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 540, 33, 33, 33, 33, 33, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 33, 540, 540, 34560, 33, 33, 33, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 33, 34560, 540, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 540, 33, 34560, 540, 33, 540, 540, 33, 540, 34560, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 2248512 . Total input tokens: 501813378 . Total output tokens: 449886057
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.9975252179428935,
    "estimated_duration": 3600.142282365772,
    "input_throughput": 5887.960068642184,
    "output_throughput": 5167.005785053596,
    "total_throughput": 11054.96585369578,
    "itl": 164.17165442294407,
    "ttft": 2042609.511118379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 865,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.821165153626335,
    "arrivals": 748703,
    "finished_requests": 85568,
    "scheduler_time": 118.05361883354452
}
#Debug simulation 
Total elapsed time: 5.99760579271242. Arrivals time: 0.2510751988738775 Scheduler time: 5.642466329969466 Scheduler overhead time: 0.03326805029064417 Adapter cache time: 0.02027628757059574 Engine time: 0.03477495722472668 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 540, 540, 34560, 540, 33, 34560, 33, 540, 540, 33, 34560, 34560, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 33, 540, 540, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 34560, 34560, 33, 540, 34560, 33, 540, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 540, 33, 33, 33, 33, 33, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 33, 540, 540, 34560, 33, 33, 33, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 33, 34560, 540, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 540, 33, 34560, 540, 33, 540, 540, 33, 540, 34560, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 2248512 . Total input tokens: 501813378 . Total output tokens: 449886057
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.036096925847232,
    "estimated_duration": 3600.02146773903,
    "input_throughput": 5897.733163612104,
    "output_throughput": 5175.943301166661,
    "total_throughput": 11073.676464778764,
    "itl": 165.6302937309712,
    "ttft": 2041616.1798522177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 869,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7103088235226176,
    "arrivals": 748703,
    "finished_requests": 85698,
    "scheduler_time": 117.77469363122798
}
#Debug simulation 
Total elapsed time: 6.0361790410242975. Arrivals time: 0.26522220950573683 Scheduler time: 5.6676293183118105 Scheduler overhead time: 0.033058939035981894 Adapter cache time: 0.019936442375183105 Engine time: 0.03475964907556772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 540, 540, 34560, 540, 33, 34560, 33, 540, 540, 33, 34560, 34560, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 33, 540, 540, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 34560, 34560, 33, 540, 34560, 33, 540, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 540, 33, 33, 33, 33, 33, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 33, 540, 540, 34560, 33, 33, 33, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 33, 34560, 540, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 540, 33, 34560, 540, 33, 540, 540, 33, 540, 34560, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 2248512 . Total input tokens: 501813378 . Total output tokens: 449886057
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.036187568679452,
    "estimated_duration": 3600.0057312968092,
    "input_throughput": 5888.071181587896,
    "output_throughput": 5167.201218121151,
    "total_throughput": 11055.272399709047,
    "itl": 164.17408119356932,
    "ttft": 2042593.3548154677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 865,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8573822441138357,
    "arrivals": 748703,
    "finished_requests": 85567,
    "scheduler_time": 118.04788772955457
}
#Debug simulation 
Total elapsed time: 6.0362722598947585. Arrivals time: 0.26368931867182255 Scheduler time: 5.669003666378558 Scheduler overhead time: 0.03313677525147796 Adapter cache time: 0.020208258647471666 Engine time: 0.03451702231541276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 540, 540, 34560, 540, 33, 34560, 33, 540, 540, 33, 34560, 34560, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 33, 540, 540, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 34560, 34560, 33, 540, 34560, 33, 540, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 540, 33, 33, 33, 33, 33, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 33, 540, 540, 34560, 33, 33, 33, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 33, 34560, 540, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 540, 33, 34560, 540, 33, 540, 540, 33, 540, 34560, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 2248512 . Total input tokens: 501813378 . Total output tokens: 449886057
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.023547722958028,
    "estimated_duration": 3600.0680511931187,
    "input_throughput": 5898.034619918628,
    "output_throughput": 5176.272707907311,
    "total_throughput": 11074.307327825938,
    "itl": 165.62471554942994,
    "ttft": 2041593.5616895955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 868,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5953636559750333,
    "arrivals": 748703,
    "finished_requests": 85703,
    "scheduler_time": 117.77981928591706
}
#Debug simulation 
Total elapsed time: 6.023631596006453. Arrivals time: 0.2617014041170478 Scheduler time: 5.659381679724902 Scheduler overhead time: 0.032730838283896446 Adapter cache time: 0.019814501982182264 Engine time: 0.034408241510391235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 540, 540, 540, 540, 34560, 540, 33, 34560, 33, 540, 540, 33, 34560, 34560, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 33, 540, 540, 34560, 540, 33, 33, 540, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 34560, 34560, 33, 540, 34560, 33, 540, 33, 540, 33, 33, 540, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 540, 33, 33, 33, 33, 33, 34560, 33, 540, 33, 34560, 34560, 33, 34560, 33, 540, 540, 34560, 33, 33, 33, 34560, 34560, 540, 34560, 540, 34560, 34560, 540, 540, 33, 34560, 540, 540, 540, 34560, 540, 33, 540, 34560, 34560, 540, 34560, 540, 33, 34560, 540, 33, 540, 540, 33, 540, 34560, 33, 540, 34560, 33, 540, 34560, 34560, 540, 33, 34560, 540, 33, 33, 540, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 540, 33, 34560, 34560, 540, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 540, 540, 540, 540, 540, 540, 33, 34560, 540, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 2248512 . Total input tokens: 501813378 . Total output tokens: 449886057
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.024006793741137,
    "estimated_duration": 3600.042281647605,
    "input_throughput": 5888.011401438009,
    "output_throughput": 5167.148756788096,
    "total_throughput": 11055.160158226105,
    "itl": 164.17446030839588,
    "ttft": 2042609.2846815286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 865,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8914615202322844,
    "arrivals": 748703,
    "finished_requests": 85567,
    "scheduler_time": 118.04837246846935
}
#Debug simulation 
Total elapsed time: 6.024089647922665. Arrivals time: 0.2620307640172541 Scheduler time: 5.658365996088833 Scheduler overhead time: 0.033308373764157295 Adapter cache time: 0.01991015626117587 Engine time: 0.034694228786975145 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 270, 270, 34560, 270, 135, 34560, 135, 270, 270, 135, 34560, 34560, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 135, 270, 270, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 34560, 34560, 135, 270, 34560, 135, 270, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 270, 135, 135, 135, 135, 135, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 135, 270, 270, 34560, 135, 135, 135, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 135, 34560, 270, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 270, 135, 34560, 270, 135, 270, 270, 135, 270, 34560, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 2237760 . Total input tokens: 499386486 . Total output tokens: 447738155
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.091484295669943,
    "estimated_duration": 3600.142774374602,
    "input_throughput": 5959.973907903515,
    "output_throughput": 5266.297807673344,
    "total_throughput": 11226.27171557686,
    "itl": 163.50726037837035,
    "ttft": 2034994.5405017943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.494297511305245,
    "arrivals": 745105,
    "finished_requests": 86702,
    "scheduler_time": 119.78527661248289
}
#Debug simulation 
Total elapsed time: 6.091568560805172. Arrivals time: 0.26703313551843166 Scheduler time: 5.722957471385598 Scheduler overhead time: 0.03318798867985606 Adapter cache time: 0.017592365853488445 Engine time: 0.03503613779321313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 270, 270, 34560, 270, 135, 34560, 135, 270, 270, 135, 34560, 34560, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 135, 270, 270, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 34560, 34560, 135, 270, 34560, 135, 270, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 270, 135, 135, 135, 135, 135, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 135, 270, 270, 34560, 135, 135, 135, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 135, 34560, 270, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 270, 135, 34560, 270, 135, 270, 270, 135, 270, 34560, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 2237760 . Total input tokens: 499386486 . Total output tokens: 447738155
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.4125081822276115,
    "estimated_duration": 3600.107144671897,
    "input_throughput": 5959.385967651608,
    "output_throughput": 5266.00465990514,
    "total_throughput": 11225.390627556748,
    "itl": 163.51252329818095,
    "ttft": 2035041.0981713745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.657532462303066,
    "arrivals": 745105,
    "finished_requests": 86698,
    "scheduler_time": 119.77964561379682
}
#Debug simulation 
Total elapsed time: 6.412603065837175. Arrivals time: 0.2708028219640255 Scheduler time: 6.0400312026031315 Scheduler overhead time: 0.03336316207423806 Adapter cache time: 0.01760087627917528 Engine time: 0.03492127312347293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 270, 270, 34560, 270, 135, 34560, 135, 270, 270, 135, 34560, 34560, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 135, 270, 270, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 34560, 34560, 135, 270, 34560, 135, 270, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 270, 135, 135, 135, 135, 135, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 135, 270, 270, 34560, 135, 135, 135, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 135, 34560, 270, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 270, 135, 34560, 270, 135, 270, 270, 135, 270, 34560, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 2237760 . Total input tokens: 499386486 . Total output tokens: 447738155
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.1118164849467576,
    "estimated_duration": 3600.0206565097255,
    "input_throughput": 5950.6777999350425,
    "output_throughput": 5259.325377970606,
    "total_throughput": 11210.003177905648,
    "itl": 161.97380401062614,
    "ttft": 2035706.6705721018,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6656250276416413,
    "arrivals": 745105,
    "finished_requests": 86568,
    "scheduler_time": 120.06645664104379
}
#Debug simulation 
Total elapsed time: 6.111949035897851. Arrivals time: 0.26371218729764223 Scheduler time: 5.7459089565090835 Scheduler overhead time: 0.03342211525887251 Adapter cache time: 0.017769579775631428 Engine time: 0.035176250617951155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 270, 270, 34560, 270, 135, 34560, 135, 270, 270, 135, 34560, 34560, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 135, 270, 270, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 34560, 34560, 135, 270, 34560, 135, 270, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 270, 135, 135, 135, 135, 135, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 135, 270, 270, 34560, 135, 135, 135, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 135, 34560, 270, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 270, 135, 34560, 270, 135, 270, 270, 135, 270, 34560, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 2237760 . Total input tokens: 499386486 . Total output tokens: 447738155
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.150352772325277,
    "estimated_duration": 3600.169907559462,
    "input_throughput": 5959.97620971882,
    "output_throughput": 5266.372278760805,
    "total_throughput": 11226.348488479625,
    "itl": 163.50841730746197,
    "ttft": 2035013.184660535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5398570105922156,
    "arrivals": 745105,
    "finished_requests": 86703,
    "scheduler_time": 119.78552372128144
}
#Debug simulation 
Total elapsed time: 6.150456449948251. Arrivals time: 0.2692765574902296 Scheduler time: 5.779299959540367 Scheduler overhead time: 0.03338617226108909 Adapter cache time: 0.017561320681124926 Engine time: 0.03500462183728814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 270, 270, 34560, 270, 135, 34560, 135, 270, 270, 135, 34560, 34560, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 135, 270, 270, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 34560, 34560, 135, 270, 34560, 135, 270, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 270, 135, 135, 135, 135, 135, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 135, 270, 270, 34560, 135, 135, 135, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 135, 34560, 270, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 270, 135, 34560, 270, 135, 270, 270, 135, 270, 34560, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 2237760 . Total input tokens: 499386486 . Total output tokens: 447738155
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.155300748068839,
    "estimated_duration": 3600.016794082531,
    "input_throughput": 5950.684184366303,
    "output_throughput": 5259.331020655773,
    "total_throughput": 11210.015205022077,
    "itl": 161.9809753830208,
    "ttft": 2035692.1036430243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.701842118129137,
    "arrivals": 745105,
    "finished_requests": 86568,
    "scheduler_time": 120.06453126934157
}
#Debug simulation 
Total elapsed time: 6.155386186204851. Arrivals time: 0.263558535836637 Scheduler time: 5.788780944887549 Scheduler overhead time: 0.03366276202723384 Adapter cache time: 0.0176677368581295 Engine time: 0.03572504688054323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 270, 270, 34560, 270, 135, 34560, 135, 270, 270, 135, 34560, 34560, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 135, 270, 270, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 34560, 34560, 135, 270, 34560, 135, 270, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 270, 135, 135, 135, 135, 135, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 135, 270, 270, 34560, 135, 135, 135, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 135, 34560, 270, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 270, 135, 34560, 270, 135, 270, 270, 135, 270, 34560, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 2237760 . Total input tokens: 499386486 . Total output tokens: 447738155
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.113344537559897,
    "estimated_duration": 3600.02777996658,
    "input_throughput": 5960.16428523204,
    "output_throughput": 5266.466027152714,
    "total_throughput": 11226.630312384754,
    "itl": 163.50477366551067,
    "ttft": 2034967.4084874291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4398810406401257,
    "arrivals": 745105,
    "finished_requests": 86702,
    "scheduler_time": 119.78337628747367
}
#Debug simulation 
Total elapsed time: 6.113440576940775. Arrivals time: 0.26585674565285444 Scheduler time: 5.745966472197324 Scheduler overhead time: 0.03337102336809039 Adapter cache time: 0.017609226517379284 Engine time: 0.034873830154538155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 270, 270, 270, 270, 34560, 270, 135, 34560, 135, 270, 270, 135, 34560, 34560, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 135, 270, 270, 34560, 270, 135, 135, 270, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 34560, 34560, 135, 270, 34560, 135, 270, 135, 270, 135, 135, 270, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 270, 135, 135, 135, 135, 135, 34560, 135, 270, 135, 34560, 34560, 135, 34560, 135, 270, 270, 34560, 135, 135, 135, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 135, 34560, 270, 270, 270, 34560, 270, 135, 270, 34560, 34560, 270, 34560, 270, 135, 34560, 270, 135, 270, 270, 135, 270, 34560, 135, 270, 34560, 135, 270, 34560, 34560, 270, 135, 34560, 270, 135, 135, 270, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 270, 135, 34560, 34560, 270, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 135, 34560, 270, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 2237760 . Total input tokens: 499386486 . Total output tokens: 447738155
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.153050283901393,
    "estimated_duration": 3600.0530473031413,
    "input_throughput": 5950.624259841947,
    "output_throughput": 5259.278058189595,
    "total_throughput": 11209.902318031542,
    "itl": 161.98167077119786,
    "ttft": 2035707.7910299222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 816,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.733532072305699,
    "arrivals": 745105,
    "finished_requests": 86568,
    "scheduler_time": 120.06498538407163
}
#Debug simulation 
Total elapsed time: 6.153135921806097. Arrivals time: 0.2713982593268156 Scheduler time: 5.778724723029882 Scheduler overhead time: 0.033681278582662344 Adapter cache time: 0.01792940078303218 Engine time: 0.03537614177912474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 270, 270, 34560, 270, 66, 34560, 66, 270, 270, 66, 34560, 34560, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 66, 270, 270, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 34560, 34560, 66, 270, 34560, 66, 270, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 270, 66, 66, 66, 66, 66, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 66, 270, 270, 34560, 66, 66, 66, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 66, 34560, 270, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 270, 66, 34560, 270, 66, 270, 270, 66, 270, 34560, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 2233344 . Total input tokens: 498395742 . Total output tokens: 446887936
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.178033822681755,
    "estimated_duration": 3600.00954246834,
    "input_throughput": 6003.733530434129,
    "output_throughput": 5315.278133090749,
    "total_throughput": 11319.011663524878,
    "itl": 162.2106083439267,
    "ttft": 2025468.11213869,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9770750825805916,
    "arrivals": 743742,
    "finished_requests": 87298,
    "scheduler_time": 120.78159599964717
}
#Debug simulation 
Total elapsed time: 6.178151765838265. Arrivals time: 0.2735061962157488 Scheduler time: 5.803981943521649 Scheduler overhead time: 0.033551549538969994 Adapter cache time: 0.015646921005100012 Engine time: 0.03551507042720914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 270, 270, 34560, 270, 66, 34560, 66, 270, 270, 66, 34560, 34560, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 66, 270, 270, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 34560, 34560, 66, 270, 34560, 66, 270, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 270, 66, 66, 66, 66, 66, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 66, 270, 270, 34560, 66, 66, 66, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 66, 34560, 270, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 270, 66, 34560, 270, 66, 270, 270, 66, 270, 34560, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 2233344 . Total input tokens: 498395742 . Total output tokens: 446887936
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.114180263131857,
    "estimated_duration": 3600.1603157940995,
    "input_throughput": 6003.482096388988,
    "output_throughput": 5315.0555312921715,
    "total_throughput": 11318.537627681158,
    "itl": 162.2162567745647,
    "ttft": 2025539.730094367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.105634096162399,
    "arrivals": 743742,
    "finished_requests": 87298,
    "scheduler_time": 120.78234637757275
}
#Debug simulation 
Total elapsed time: 6.1142925708554685. Arrivals time: 0.26733740884810686 Scheduler time: 5.746954502072185 Scheduler overhead time: 0.033363066613674164 Adapter cache time: 0.015752512495964766 Engine time: 0.03498999681323767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 270, 270, 34560, 270, 66, 34560, 66, 270, 270, 66, 34560, 34560, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 66, 270, 270, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 34560, 34560, 66, 270, 34560, 66, 270, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 270, 66, 66, 66, 66, 66, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 66, 270, 270, 34560, 66, 66, 66, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 66, 34560, 270, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 270, 66, 34560, 270, 66, 270, 270, 66, 270, 34560, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 2233344 . Total input tokens: 498395742 . Total output tokens: 446887936
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.1526157250627875,
    "estimated_duration": 3600.0615671640658,
    "input_throughput": 5988.6767483766935,
    "output_throughput": 5304.634002424462,
    "total_throughput": 11293.310750801156,
    "itl": 160.2805958218258,
    "ttft": 2026713.2033772396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.100650052428254,
    "arrivals": 743742,
    "finished_requests": 87105,
    "scheduler_time": 121.14603817292046
}
#Debug simulation 
Total elapsed time: 6.152719662990421. Arrivals time: 0.26694498443976045 Scheduler time: 5.7847287259064615 Scheduler overhead time: 0.03368876222521067 Adapter cache time: 0.015846802853047848 Engine time: 0.03543982142582536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 270, 270, 34560, 270, 66, 34560, 66, 270, 270, 66, 34560, 34560, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 66, 270, 270, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 34560, 34560, 66, 270, 34560, 66, 270, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 270, 66, 66, 66, 66, 66, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 66, 270, 270, 34560, 66, 66, 66, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 66, 34560, 270, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 270, 66, 34560, 270, 66, 270, 270, 66, 270, 34560, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 2233344 . Total input tokens: 498395742 . Total output tokens: 446887936
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.162854330148548,
    "estimated_duration": 3600.0483137017723,
    "input_throughput": 6003.668872370155,
    "output_throughput": 5315.220889445304,
    "total_throughput": 11318.889761815459,
    "itl": 162.21228014225935,
    "ttft": 2025484.8993110158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.015743126105502,
    "arrivals": 743742,
    "finished_requests": 87298,
    "scheduler_time": 120.7816991894876
}
#Debug simulation 
Total elapsed time: 6.1629369109869. Arrivals time: 0.27262154035270214 Scheduler time: 5.789749193470925 Scheduler overhead time: 0.0333652151748538 Adapter cache time: 0.01582828164100647 Engine time: 0.0353877916932106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 270, 270, 34560, 270, 66, 34560, 66, 270, 270, 66, 34560, 34560, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 66, 270, 270, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 34560, 34560, 66, 270, 34560, 66, 270, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 270, 66, 66, 66, 66, 66, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 66, 270, 270, 34560, 66, 66, 66, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 66, 34560, 270, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 270, 66, 34560, 270, 66, 270, 270, 66, 270, 34560, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 2233344 . Total input tokens: 498395742 . Total output tokens: 446887936
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.167550433892757,
    "estimated_duration": 3600.0875114535256,
    "input_throughput": 5988.633590547183,
    "output_throughput": 5304.5957741981765,
    "total_throughput": 11293.229364745359,
    "itl": 160.28001240751965,
    "ttft": 2026724.1694143475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1283158854395183,
    "arrivals": 743742,
    "finished_requests": 87105,
    "scheduler_time": 121.14626807846415
}
#Debug simulation 
Total elapsed time: 6.167631491087377. Arrivals time: 0.25979462498798966 Scheduler time: 5.806293697562069 Scheduler overhead time: 0.03388541005551815 Adapter cache time: 0.015817774459719658 Engine time: 0.035656193271279335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 270, 270, 34560, 270, 66, 34560, 66, 270, 270, 66, 34560, 34560, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 66, 270, 270, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 34560, 34560, 66, 270, 34560, 66, 270, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 270, 66, 66, 66, 66, 66, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 66, 270, 270, 34560, 66, 66, 66, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 66, 34560, 270, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 270, 66, 34560, 270, 66, 270, 270, 66, 270, 34560, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 2233344 . Total input tokens: 498395742 . Total output tokens: 446887936
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.225761860143393,
    "estimated_duration": 3600.1390565718925,
    "input_throughput": 6003.685596683946,
    "output_throughput": 5315.263577130072,
    "total_throughput": 11318.949173814017,
    "itl": 162.20916801973277,
    "ttft": 2025506.1979722872,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9315724905067737,
    "arrivals": 743742,
    "finished_requests": 87301,
    "scheduler_time": 120.78746308371778
}
#Debug simulation 
Total elapsed time: 6.225843095220625. Arrivals time: 0.2763588107191026 Scheduler time: 5.8480290630832314 Scheduler overhead time: 0.033717216458171606 Adapter cache time: 0.015911332331597805 Engine time: 0.03589979372918606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 270, 270, 270, 270, 34560, 270, 66, 34560, 66, 270, 270, 66, 34560, 34560, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 66, 270, 270, 34560, 270, 66, 66, 270, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 34560, 34560, 66, 270, 34560, 66, 270, 66, 270, 66, 66, 270, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 270, 66, 66, 66, 66, 66, 34560, 66, 270, 66, 34560, 34560, 66, 34560, 66, 270, 270, 34560, 66, 66, 66, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 66, 34560, 270, 270, 270, 34560, 270, 66, 270, 34560, 34560, 270, 34560, 270, 66, 34560, 270, 66, 270, 270, 66, 270, 34560, 66, 270, 34560, 66, 270, 34560, 34560, 270, 66, 34560, 270, 66, 66, 270, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 270, 66, 34560, 34560, 270, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 66, 34560, 270, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 2233344 . Total input tokens: 498395742 . Total output tokens: 446887936
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.184833214152604,
    "estimated_duration": 3600.113482205036,
    "input_throughput": 5988.590389321545,
    "output_throughput": 5304.557507532585,
    "total_throughput": 11293.14789685413,
    "itl": 160.28110154916791,
    "ttft": 2026735.152831948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1542211654409775,
    "arrivals": 743742,
    "finished_requests": 87105,
    "scheduler_time": 121.14633354999647
}
#Debug simulation 
Total elapsed time: 6.184927389957011. Arrivals time: 0.2712941439822316 Scheduler time: 5.811980033293366 Scheduler overhead time: 0.03403705172240734 Adapter cache time: 0.01590352226048708 Engine time: 0.035560918506234884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 270, 270, 34560, 270, 33, 34560, 33, 270, 270, 33, 34560, 34560, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 33, 270, 270, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 34560, 34560, 33, 270, 34560, 33, 270, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 270, 33, 33, 33, 33, 33, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 33, 270, 270, 34560, 33, 33, 33, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 33, 34560, 270, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 270, 33, 34560, 270, 33, 270, 270, 33, 270, 34560, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 2231232 . Total input tokens: 497902069 . Total output tokens: 446468704
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.254614430014044,
    "estimated_duration": 3600.142421604937,
    "input_throughput": 6068.405479986703,
    "output_throughput": 5350.372775367311,
    "total_throughput": 11418.778255354015,
    "itl": 160.76259391817706,
    "ttft": 2018976.6061134818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6863287469069699,
    "arrivals": 743017,
    "finished_requests": 88372,
    "scheduler_time": 121.75375614775504
}
#Debug simulation 
Total elapsed time: 6.254721323959529. Arrivals time: 0.2741042971611023 Scheduler time: 5.8795776115730405 Scheduler overhead time: 0.03394981101155281 Adapter cache time: 0.015202118083834648 Engine time: 0.03570271609351039 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 270, 270, 34560, 270, 33, 34560, 33, 270, 270, 33, 34560, 34560, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 33, 270, 270, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 34560, 34560, 33, 270, 34560, 33, 270, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 270, 33, 33, 33, 33, 33, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 33, 270, 270, 34560, 33, 33, 33, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 33, 34560, 270, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 270, 33, 34560, 270, 33, 270, 270, 33, 270, 34560, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 2231232 . Total input tokens: 497902069 . Total output tokens: 446468704
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.213849054649472,
    "estimated_duration": 3600.077670730697,
    "input_throughput": 6068.434627846742,
    "output_throughput": 5350.450118508261,
    "total_throughput": 11418.884746355003,
    "itl": 160.76514575829148,
    "ttft": 2018943.12930557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7995151709509136,
    "arrivals": 743017,
    "finished_requests": 88371,
    "scheduler_time": 121.74897864967778
}
#Debug simulation 
Total elapsed time: 6.213957147672772. Arrivals time: 0.2687312145717442 Scheduler time: 5.844656013417989 Scheduler overhead time: 0.033955967985093594 Adapter cache time: 0.015151596162468195 Engine time: 0.03544626571238041 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 270, 270, 34560, 270, 33, 34560, 33, 270, 270, 33, 34560, 34560, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 33, 270, 270, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 34560, 34560, 33, 270, 34560, 33, 270, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 270, 33, 33, 33, 33, 33, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 33, 270, 270, 34560, 33, 33, 33, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 33, 34560, 270, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 270, 33, 34560, 270, 33, 270, 270, 33, 270, 34560, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 2231232 . Total input tokens: 497902069 . Total output tokens: 446468704
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.292001717258245,
    "estimated_duration": 3600.176201092828,
    "input_throughput": 6059.2312102330725,
    "output_throughput": 5343.9048328134695,
    "total_throughput": 11403.136043046541,
    "itl": 159.3638672648895,
    "ttft": 2019404.0946390135,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8027722011320404,
    "arrivals": 743017,
    "finished_requests": 88243,
    "scheduler_time": 122.02736150106813
}
#Debug simulation 
Total elapsed time: 6.292108362074941. Arrivals time: 0.31387859443202615 Scheduler time: 5.876194713637233 Scheduler overhead time: 0.034136712085455656 Adapter cache time: 0.015494529623538256 Engine time: 0.03611032851040363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 270, 270, 34560, 270, 33, 34560, 33, 270, 270, 33, 34560, 34560, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 33, 270, 270, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 34560, 34560, 33, 270, 34560, 33, 270, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 270, 33, 33, 33, 33, 33, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 33, 270, 270, 34560, 33, 33, 33, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 33, 34560, 270, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 270, 33, 34560, 270, 33, 270, 270, 33, 270, 34560, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 2231232 . Total input tokens: 497902069 . Total output tokens: 446468704
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.211802215781063,
    "estimated_duration": 3600.1885478813547,
    "input_throughput": 6068.341340860235,
    "output_throughput": 5350.346445420334,
    "total_throughput": 11418.687786280569,
    "itl": 160.76330119278796,
    "ttft": 2018983.439614554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7255594183131964,
    "arrivals": 743017,
    "finished_requests": 88373,
    "scheduler_time": 121.75473673452473
}
#Debug simulation 
Total elapsed time: 6.2118883659131825. Arrivals time: 0.2704631509259343 Scheduler time: 5.841228025965393 Scheduler overhead time: 0.033561933785676956 Adapter cache time: 0.015102574601769447 Engine time: 0.035560185089707375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 270, 270, 34560, 270, 33, 34560, 33, 270, 270, 33, 34560, 34560, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 33, 270, 270, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 34560, 34560, 33, 270, 34560, 33, 270, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 270, 33, 33, 33, 33, 33, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 33, 270, 270, 34560, 33, 33, 33, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 33, 34560, 270, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 270, 33, 34560, 270, 33, 270, 270, 33, 270, 34560, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 2231232 . Total input tokens: 497902069 . Total output tokens: 446468704
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.160098178777844,
    "estimated_duration": 3600.014154132386,
    "input_throughput": 6059.261176781984,
    "output_throughput": 5343.7334344693145,
    "total_throughput": 11402.994611251299,
    "itl": 159.36443481539223,
    "ttft": 2019410.573428571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8256593902595377,
    "arrivals": 743017,
    "finished_requests": 88237,
    "scheduler_time": 122.02125974111941
}
#Debug simulation 
Total elapsed time: 6.16019846405834. Arrivals time: 0.26948655024170876 Scheduler time: 5.790269929915667 Scheduler overhead time: 0.03379700053483248 Adapter cache time: 0.01507753785699606 Engine time: 0.03537000762298703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 270, 270, 34560, 270, 33, 34560, 33, 270, 270, 33, 34560, 34560, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 33, 270, 270, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 34560, 34560, 33, 270, 34560, 33, 270, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 270, 33, 33, 33, 33, 33, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 33, 270, 270, 34560, 33, 33, 33, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 33, 34560, 270, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 270, 33, 34560, 270, 33, 270, 270, 33, 270, 34560, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 2231232 . Total input tokens: 497902069 . Total output tokens: 446468704
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.142830727156252,
    "estimated_duration": 3600.103505890648,
    "input_throughput": 6068.471077082304,
    "output_throughput": 5350.430610809522,
    "total_throughput": 11418.901687891825,
    "itl": 160.7610176607473,
    "ttft": 2018959.9007168307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6475177124910771,
    "arrivals": 743017,
    "finished_requests": 88372,
    "scheduler_time": 121.75365146780861
}
#Debug simulation 
Total elapsed time: 6.142915314994752. Arrivals time: 0.26610651705414057 Scheduler time: 5.776457016821951 Scheduler overhead time: 0.03374695824459195 Adapter cache time: 0.015001526102423668 Engine time: 0.03569911886006594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 270, 270, 270, 270, 34560, 270, 33, 34560, 33, 270, 270, 33, 34560, 34560, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 33, 270, 270, 34560, 270, 33, 33, 270, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 34560, 34560, 33, 270, 34560, 33, 270, 33, 270, 33, 33, 270, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 270, 33, 33, 33, 33, 33, 34560, 33, 270, 33, 34560, 34560, 33, 34560, 33, 270, 270, 34560, 33, 33, 33, 34560, 34560, 270, 34560, 270, 34560, 34560, 270, 270, 33, 34560, 270, 270, 270, 34560, 270, 33, 270, 34560, 34560, 270, 34560, 270, 33, 34560, 270, 33, 270, 270, 33, 270, 34560, 33, 270, 34560, 33, 270, 34560, 34560, 270, 33, 34560, 270, 33, 33, 270, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 270, 33, 34560, 34560, 270, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 270, 270, 270, 270, 270, 270, 33, 34560, 270, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 2231232 . Total input tokens: 497902069 . Total output tokens: 446468704
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.259495771024376,
    "estimated_duration": 3600.038567186675,
    "input_throughput": 6059.220086924389,
    "output_throughput": 5343.6971968424095,
    "total_throughput": 11402.9172837668,
    "itl": 159.36541650853454,
    "ttft": 2019421.451248461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8496783634647693,
    "arrivals": 743017,
    "finished_requests": 88237,
    "scheduler_time": 122.02133499147234
}
#Debug simulation 
Total elapsed time: 6.2595927328802645. Arrivals time: 0.2725404864177108 Scheduler time: 5.885398541111499 Scheduler overhead time: 0.03398514539003372 Adapter cache time: 0.01536379475146532 Engine time: 0.03606002265587449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 135, 135, 34560, 135, 66, 34560, 66, 135, 135, 66, 34560, 34560, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 66, 135, 135, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 34560, 34560, 66, 135, 34560, 66, 135, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 135, 66, 66, 66, 66, 66, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 66, 135, 135, 34560, 66, 66, 66, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 66, 34560, 135, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 135, 66, 34560, 135, 66, 135, 135, 66, 135, 34560, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 2224704 . Total input tokens: 496499022 . Total output tokens: 445175971
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.311449713073671,
    "estimated_duration": 3600.1237461132737,
    "input_throughput": 6139.710065206335,
    "output_throughput": 5424.356321385616,
    "total_throughput": 11564.06638659195,
    "itl": 158.72200710638813,
    "ttft": 2011882.1632575088,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3435541195864895,
    "arrivals": 740854,
    "finished_requests": 89498,
    "scheduler_time": 123.22411204062615
}
#Debug simulation 
Total elapsed time: 6.31153206108138. Arrivals time: 0.26928553683683276 Scheduler time: 5.9434792986139655 Scheduler overhead time: 0.034139650873839855 Adapter cache time: 0.012593755032867193 Engine time: 0.035811340902000666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 135, 135, 34560, 135, 66, 34560, 66, 135, 135, 66, 34560, 34560, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 66, 135, 135, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 34560, 34560, 66, 135, 34560, 66, 135, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 135, 66, 66, 66, 66, 66, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 66, 135, 135, 34560, 66, 66, 66, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 66, 34560, 135, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 135, 66, 34560, 135, 66, 135, 135, 66, 135, 34560, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 2224704 . Total input tokens: 496499022 . Total output tokens: 445175971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.2858304982073605,
    "estimated_duration": 3600.079882989893,
    "input_throughput": 6139.223772347067,
    "output_throughput": 5423.852146242728,
    "total_throughput": 11563.075918589795,
    "itl": 158.72624620886688,
    "ttft": 2011930.6151768952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4348020796757248,
    "arrivals": 740854,
    "finished_requests": 89490,
    "scheduler_time": 123.21965095605537
}
#Debug simulation 
Total elapsed time: 6.285914826206863. Arrivals time: 0.312946277204901 Scheduler time: 5.874455174431205 Scheduler overhead time: 0.033970055636018515 Adapter cache time: 0.012510626576840878 Engine time: 0.03581513185054064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 135, 135, 34560, 135, 66, 34560, 66, 135, 135, 66, 34560, 34560, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 66, 135, 135, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 34560, 34560, 66, 135, 34560, 66, 135, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 135, 66, 66, 66, 66, 66, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 66, 135, 135, 34560, 66, 66, 66, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 66, 34560, 135, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 135, 66, 34560, 135, 66, 135, 135, 66, 135, 34560, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 2224704 . Total input tokens: 496499022 . Total output tokens: 445175971
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.347673390060663,
    "estimated_duration": 3600.011889688378,
    "input_throughput": 6128.964757921931,
    "output_throughput": 5415.7540578810895,
    "total_throughput": 11544.71881580302,
    "itl": 157.31466924911462,
    "ttft": 2013014.53695223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.430180242788055,
    "arrivals": 740854,
    "finished_requests": 89338,
    "scheduler_time": 123.48800639243383
}
#Debug simulation 
Total elapsed time: 6.3478101817891. Arrivals time: 0.2767279166728258 Scheduler time: 5.971187568735331 Scheduler overhead time: 0.03460128791630268 Adapter cache time: 0.012749713379889727 Engine time: 0.036088384222239256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 135, 135, 34560, 135, 66, 34560, 66, 135, 135, 66, 34560, 34560, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 66, 135, 135, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 34560, 34560, 66, 135, 34560, 66, 135, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 135, 66, 66, 66, 66, 66, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 66, 135, 135, 34560, 66, 66, 66, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 66, 34560, 135, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 135, 66, 34560, 135, 66, 135, 135, 66, 135, 34560, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 2224704 . Total input tokens: 496499022 . Total output tokens: 445175971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.22311739390716,
    "estimated_duration": 3600.156156166662,
    "input_throughput": 6139.654793067468,
    "output_throughput": 5424.307489148805,
    "total_throughput": 11563.962282216273,
    "itl": 158.7233021308918,
    "ttft": 2011921.9535787613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3718784006359017,
    "arrivals": 740854,
    "finished_requests": 89498,
    "scheduler_time": 123.22426170643165
}
#Debug simulation 
Total elapsed time: 6.223203267902136. Arrivals time: 0.27248829090967774 Scheduler time: 5.852715319022536 Scheduler overhead time: 0.03406678466126323 Adapter cache time: 0.012415105011314154 Engine time: 0.035445199348032475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 135, 135, 34560, 135, 66, 34560, 66, 135, 135, 66, 34560, 34560, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 66, 135, 135, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 34560, 34560, 66, 135, 34560, 66, 135, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 135, 66, 66, 66, 66, 66, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 66, 135, 135, 34560, 66, 66, 66, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 66, 34560, 135, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 135, 66, 34560, 135, 66, 135, 135, 66, 135, 34560, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 2224704 . Total input tokens: 496499022 . Total output tokens: 445175971
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.32273961417377,
    "estimated_duration": 3600.0263138505015,
    "input_throughput": 6128.940201106615,
    "output_throughput": 5415.732358674543,
    "total_throughput": 11544.672559781158,
    "itl": 157.3140110908343,
    "ttft": 2013020.4937069828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4495463258959393,
    "arrivals": 740854,
    "finished_requests": 89338,
    "scheduler_time": 123.48832937260636
}
#Debug simulation 
Total elapsed time: 6.322847294155508. Arrivals time: 0.26938762282952666 Scheduler time: 5.953559231944382 Scheduler overhead time: 0.034586642403155565 Adapter cache time: 0.012685264460742474 Engine time: 0.03621981665492058 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 135, 135, 34560, 135, 66, 34560, 66, 135, 135, 66, 34560, 34560, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 66, 135, 135, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 34560, 34560, 66, 135, 34560, 66, 135, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 135, 66, 66, 66, 66, 66, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 66, 135, 135, 34560, 66, 66, 66, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 66, 34560, 135, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 135, 66, 34560, 135, 66, 135, 135, 66, 135, 34560, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 2224704 . Total input tokens: 496499022 . Total output tokens: 445175971
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.2375289420597255,
    "estimated_duration": 3600.092085911278,
    "input_throughput": 6139.764059508763,
    "output_throughput": 5424.404024670069,
    "total_throughput": 11564.168084178833,
    "itl": 158.7208336677918,
    "ttft": 2011868.3401344675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3126320794620454,
    "arrivals": 740854,
    "finished_requests": 89498,
    "scheduler_time": 123.22399895152061
}
#Debug simulation 
Total elapsed time: 6.237607412040234. Arrivals time: 0.25891739316284657 Scheduler time: 5.879854302853346 Scheduler overhead time: 0.03417934011667967 Adapter cache time: 0.012583754025399685 Engine time: 0.03587400633841753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_192_slots_96_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 135, 135, 135, 135, 34560, 135, 66, 34560, 66, 135, 135, 66, 34560, 34560, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 66, 135, 135, 34560, 135, 66, 66, 135, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 34560, 34560, 66, 135, 34560, 66, 135, 66, 135, 66, 66, 135, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 135, 66, 66, 66, 66, 66, 34560, 66, 135, 66, 34560, 34560, 66, 34560, 66, 135, 135, 34560, 66, 66, 66, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 66, 34560, 135, 135, 135, 34560, 135, 66, 135, 34560, 34560, 135, 34560, 135, 66, 34560, 135, 66, 135, 135, 66, 135, 34560, 66, 135, 34560, 66, 135, 34560, 34560, 135, 66, 34560, 135, 66, 66, 135, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 135, 66, 34560, 34560, 135, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 66, 34560, 135, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 2224704 . Total input tokens: 496499022 . Total output tokens: 445175971
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.241102108731866,
    "estimated_duration": 3600.0100065336546,
    "input_throughput": 6128.9679639655005,
    "output_throughput": 5415.756890846224,
    "total_throughput": 11544.724854811724,
    "itl": 157.32000756582454,
    "ttft": 2013005.0602446906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.467529117353261,
    "arrivals": 740854,
    "finished_requests": 89338,
    "scheduler_time": 123.48641062310325
}
#Debug simulation 
Total elapsed time: 6.241184170823544. Arrivals time: 0.2603980805724859 Scheduler time: 5.8814248950220644 Scheduler overhead time: 0.034391671884804964 Adapter cache time: 0.01255125505849719 Engine time: 0.03605989459902048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.3024468482472,
    "estimated_duration": 3600.0554350100665,
    "input_throughput": 6160.016810946132,
    "output_throughput": 5465.358618830541,
    "total_throughput": 11625.375429776674,
    "itl": 158.03992312463185,
    "ttft": 2016476.267809147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.114017538791525,
    "arrivals": 740135,
    "finished_requests": 89664,
    "scheduler_time": 124.06866421566588
}
#Debug simulation 
Total elapsed time: 6.3025267091579735. Arrivals time: 0.2587411548011005 Scheduler time: 5.94515219097957 Scheduler overhead time: 0.03424796089529991 Adapter cache time: 0.01162748271599412 Engine time: 0.03650517761707306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.34733951324597,
    "estimated_duration": 3600.1534913898427,
    "input_throughput": 6159.849310046717,
    "output_throughput": 5465.232537183904,
    "total_throughput": 11625.08184723062,
    "itl": 158.04231805762936,
    "ttft": 2016506.4619420527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1842683241120604,
    "arrivals": 740135,
    "finished_requests": 89665,
    "scheduler_time": 124.07049416674549
}
#Debug simulation 
Total elapsed time: 6.347418913152069. Arrivals time: 0.26612622290849686 Scheduler time: 5.982642504386604 Scheduler overhead time: 0.034557098522782326 Adapter cache time: 0.011731528211385012 Engine time: 0.03602811275050044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.3351544588804245,
    "estimated_duration": 3600.0766723223846,
    "input_throughput": 6144.988291521295,
    "output_throughput": 5451.991939754544,
    "total_throughput": 11596.980231275838,
    "itl": 156.11263727398097,
    "ttft": 2018428.7680909073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1799869505874878,
    "arrivals": 740135,
    "finished_requests": 89438,
    "scheduler_time": 124.39951420014118
}
#Debug simulation 
Total elapsed time: 6.335238147061318. Arrivals time: 0.26370674883946776 Scheduler time: 5.972155891824514 Scheduler overhead time: 0.03470663167536259 Adapter cache time: 0.011639342177659273 Engine time: 0.03649117145687342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.364782738965005,
    "estimated_duration": 3600.076772922614,
    "input_throughput": 6159.980300085866,
    "output_throughput": 5465.326225259068,
    "total_throughput": 11625.306525344935,
    "itl": 158.04064145226994,
    "ttft": 2016486.9090945343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1349581736465923,
    "arrivals": 740135,
    "finished_requests": 89664,
    "scheduler_time": 124.06877206208753
}
#Debug simulation 
Total elapsed time: 6.364900889806449. Arrivals time: 0.2716432139277458 Scheduler time: 5.994515280704945 Scheduler overhead time: 0.03429635800421238 Adapter cache time: 0.01161029189825058 Engine time: 0.036368006374686956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.309666340239346,
    "estimated_duration": 3600.0090557130216,
    "input_throughput": 6144.9678758151085,
    "output_throughput": 5452.0096189348615,
    "total_throughput": 11596.97749474997,
    "itl": 156.12368288997865,
    "ttft": 2018415.3698365476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1959576814621726,
    "arrivals": 740135,
    "finished_requests": 89437,
    "scheduler_time": 124.39473014305548
}
#Debug simulation 
Total elapsed time: 6.309769899118692. Arrivals time: 0.2629200811497867 Scheduler time: 5.9474784336052835 Scheduler overhead time: 0.0346276112832129 Adapter cache time: 0.011617443058639765 Engine time: 0.03664812119677663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.3565848399885,
    "estimated_duration": 3600.029700411881,
    "input_throughput": 6160.0608454599105,
    "output_throughput": 5465.397687621551,
    "total_throughput": 11625.45853308146,
    "itl": 158.03896079302476,
    "ttft": 2016464.3676115328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0883783073443902,
    "arrivals": 740135,
    "finished_requests": 89664,
    "scheduler_time": 124.06856884888194
}
#Debug simulation 
Total elapsed time: 6.356684667989612. Arrivals time: 0.2609301032498479 Scheduler time: 5.997339099179953 Scheduler overhead time: 0.03432460129261017 Adapter cache time: 0.011575077194720507 Engine time: 0.036185321398079395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.338012596126646,
    "estimated_duration": 3600.0234532977984,
    "input_throughput": 6144.94330022634,
    "output_throughput": 5451.987814696164,
    "total_throughput": 11596.931114922503,
    "itl": 156.12420967770566,
    "ttft": 2018422.2232061343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2102936131134658,
    "arrivals": 740135,
    "finished_requests": 89437,
    "scheduler_time": 124.39479179619119
}
#Debug simulation 
Total elapsed time: 6.338091789279133. Arrivals time: 0.2589021949097514 Scheduler time: 5.979655754286796 Scheduler overhead time: 0.03469919692724943 Adapter cache time: 0.01169348880648613 Engine time: 0.0365687208250165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.4008573750033975,
    "estimated_duration": 3600.1276759722755,
    "input_throughput": 6225.761699950501,
    "output_throughput": 5500.260485804085,
    "total_throughput": 11726.022185754586,
    "itl": 156.5411249947636,
    "ttft": 2005645.5507538647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7865453501907167,
    "arrivals": 738702,
    "finished_requests": 90619,
    "scheduler_time": 125.01883862856249
}
#Debug simulation 
Total elapsed time: 6.400936232879758. Arrivals time: 0.30668837716802955 Scheduler time: 5.997264721430838 Scheduler overhead time: 0.03445874247699976 Adapter cache time: 0.00969478627666831 Engine time: 0.03647642722353339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.41346869757399,
    "estimated_duration": 3600.097207122526,
    "input_throughput": 6225.586063525758,
    "output_throughput": 5500.24203813841,
    "total_throughput": 11725.828101664167,
    "itl": 156.54274300362462,
    "ttft": 2005647.2416416449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8370869392878416,
    "arrivals": 738702,
    "finished_requests": 90618,
    "scheduler_time": 125.01668995344559
}
#Debug simulation 
Total elapsed time: 6.413547486998141. Arrivals time: 0.26827798690646887 Scheduler time: 6.0479344935156405 Scheduler overhead time: 0.03452645195648074 Adapter cache time: 0.009829370770603418 Engine time: 0.036519547924399376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.420223555061966,
    "estimated_duration": 3600.0499204139687,
    "input_throughput": 6209.979720900443,
    "output_throughput": 5487.380018810516,
    "total_throughput": 11697.359739710959,
    "itl": 154.77603954969447,
    "ttft": 2006829.9808000384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8388419232144992,
    "arrivals": 738702,
    "finished_requests": 90392,
    "scheduler_time": 125.32953198563045
}
#Debug simulation 
Total elapsed time: 6.420304487925023. Arrivals time: 0.3352575902827084 Scheduler time: 5.986912443302572 Scheduler overhead time: 0.03503177547827363 Adapter cache time: 0.009799183811992407 Engine time: 0.03663803031668067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.446785011328757,
    "estimated_duration": 3600.172072421503,
    "input_throughput": 6225.684925366494,
    "output_throughput": 5500.192657925172,
    "total_throughput": 11725.877583291665,
    "itl": 156.54182810732652,
    "ttft": 2005661.3403447138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8011305512650893,
    "arrivals": 738702,
    "finished_requests": 90619,
    "scheduler_time": 125.0202610932595
}
#Debug simulation 
Total elapsed time: 6.446871242020279. Arrivals time: 0.34642501920461655 Scheduler time: 6.003166676964611 Scheduler overhead time: 0.0345728206448257 Adapter cache time: 0.009829510003328323 Engine time: 0.03643673891201615 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.438762079924345,
    "estimated_duration": 3600.0610307984234,
    "input_throughput": 6209.960555874749,
    "output_throughput": 5487.363083847154,
    "total_throughput": 11697.323639721903,
    "itl": 154.7764288959191,
    "ttft": 2006835.21760446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8499082564190072,
    "arrivals": 738702,
    "finished_requests": 90392,
    "scheduler_time": 125.32957603688904
}
#Debug simulation 
Total elapsed time: 6.438868558965623. Arrivals time: 0.33739974722266197 Scheduler time: 6.00313530722633 Scheduler overhead time: 0.03507495857775211 Adapter cache time: 0.009805432986468077 Engine time: 0.036721750162541866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.4380543432198465,
    "estimated_duration": 3600.0908044789476,
    "input_throughput": 6225.825463100779,
    "output_throughput": 5500.316818499236,
    "total_throughput": 11726.142281600016,
    "itl": 156.5403208476261,
    "ttft": 2005636.8054409847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7684429257898602,
    "arrivals": 738702,
    "finished_requests": 90619,
    "scheduler_time": 125.01809889793712
}
#Debug simulation 
Total elapsed time: 6.43813434522599. Arrivals time: 0.2600730722770095 Scheduler time: 6.0807577688246965 Scheduler overhead time: 0.03465266339480877 Adapter cache time: 0.009653964545577765 Engine time: 0.03651195578277111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.366883134935051,
    "estimated_duration": 3600.071146421515,
    "input_throughput": 6209.9431068805925,
    "output_throughput": 5487.347665236113,
    "total_throughput": 11697.290772116707,
    "itl": 154.77677295992424,
    "ttft": 2006840.200003921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 257,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8599685593321961,
    "arrivals": 738702,
    "finished_requests": 90392,
    "scheduler_time": 125.32963135707425
}
#Debug simulation 
Total elapsed time: 6.366964181885123. Arrivals time: 0.26878347573801875 Scheduler time: 6.0000874302349985 Scheduler overhead time: 0.034872645512223244 Adapter cache time: 0.00973881920799613 Engine time: 0.036920733749866486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 34.201273147016764,
    "estimated_duration": 3600.0434588619805,
    "input_throughput": 5401.556459584262,
    "output_throughput": 4770.474355725943,
    "total_throughput": 10172.030815310205,
    "itl": 180.01040077149642,
    "ttft": 2059788.9982690448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2119531465973765,
    "arrivals": 644931,
    "finished_requests": 78747,
    "scheduler_time": 108.48651334739532
}
#Debug simulation 
Total elapsed time: 34.20141306100413. Arrivals time: 0.38447852386161685 Scheduler time: 33.69791361363605 Scheduler overhead time: 0.04439342534169555 Adapter cache time: 0.013195847161114216 Engine time: 0.044313314370810986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 46.05726887891069,
    "estimated_duration": 3600.0174117736306,
    "input_throughput": 5381.625360099296,
    "output_throughput": 4750.832299884289,
    "total_throughput": 10132.457659983584,
    "itl": 180.95242328472835,
    "ttft": 2050921.7393628191,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 379,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2333349147858148,
    "arrivals": 644931,
    "finished_requests": 78320,
    "scheduler_time": 107.94377921237853
}
#Debug simulation 
Total elapsed time: 46.05740443896502. Arrivals time: 0.3993092430755496 Scheduler time: 45.53362911101431 Scheduler overhead time: 0.04711939347907901 Adapter cache time: 0.013076025526970625 Engine time: 0.046570537611842155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 32.194466771092266,
    "estimated_duration": 3600.090375205862,
    "input_throughput": 5394.356801081558,
    "output_throughput": 4767.705032687828,
    "total_throughput": 10162.061833769387,
    "itl": 177.89966478327474,
    "ttft": 2061607.6742574065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 388,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2665461769327593,
    "arrivals": 644931,
    "finished_requests": 78662,
    "scheduler_time": 108.86630554397485
}
#Debug simulation 
Total elapsed time: 32.194570552092046. Arrivals time: 0.3893538126721978 Scheduler time: 31.6848642132245 Scheduler overhead time: 0.044461775571107864 Adapter cache time: 0.013551660813391209 Engine time: 0.045153530314564705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 34.37083074776456,
    "estimated_duration": 3600.067832713393,
    "input_throughput": 5401.519888958191,
    "output_throughput": 4770.442057769761,
    "total_throughput": 10171.961946727952,
    "itl": 180.01160991724544,
    "ttft": 2059801.3306595627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2359515222231843,
    "arrivals": 644931,
    "finished_requests": 78747,
    "scheduler_time": 108.48663170649763
}
#Debug simulation 
Total elapsed time: 34.370955772697926. Arrivals time: 0.3246589903719723 Scheduler time: 33.92463714489713 Scheduler overhead time: 0.045620855409651995 Adapter cache time: 0.013175501022487879 Engine time: 0.04540690593421459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 32.69772887416184,
    "estimated_duration": 3600.113402681069,
    "input_throughput": 5390.214926437723,
    "output_throughput": 4767.4367666357875,
    "total_throughput": 10157.65169307351,
    "itl": 177.84275977393307,
    "ttft": 2061433.396257814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 393,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2989120121859061,
    "arrivals": 644931,
    "finished_requests": 78640,
    "scheduler_time": 108.8590665675303
}
#Debug simulation 
Total elapsed time: 32.697861979249865. Arrivals time: 0.3949120440520346 Scheduler time: 32.18284146254882 Scheduler overhead time: 0.04449613718315959 Adapter cache time: 0.01381006557494402 Engine time: 0.04419251065701246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_192_slots_96_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 34.67967463796958,
    "estimated_duration": 3600.0157355664664,
    "input_throughput": 5401.598056331878,
    "output_throughput": 4770.511092584896,
    "total_throughput": 10172.109148916774,
    "itl": 180.00948978394152,
    "ttft": 2059775.855989019,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1840599167812564,
    "arrivals": 644931,
    "finished_requests": 78747,
    "scheduler_time": 108.48642616500143
}
#Debug simulation 
Total elapsed time: 34.67976282117888. Arrivals time: 0.6520931092090905 Scheduler time: 33.907715141773224 Scheduler overhead time: 0.044717291835695505 Adapter cache time: 0.013368800282478333 Engine time: 0.04459827905520797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_192_slots_96_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 32.61127792298794,
    "estimated_duration": 3600.140252649727,
    "input_throughput": 5388.682284175308,
    "output_throughput": 4767.900635917272,
    "total_throughput": 10156.58292009258,
    "itl": 177.99390940848437,
    "ttft": 2061570.6450838712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 392,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3111247153580214,
    "arrivals": 644931,
    "finished_requests": 78630,
    "scheduler_time": 108.83076889185125
}
#Debug simulation 
Total elapsed time: 32.61140617309138. Arrivals time: 0.37931204540655017 Scheduler time: 32.11136910971254 Scheduler overhead time: 0.04469892429187894 Adapter cache time: 0.013126363046467304 Engine time: 0.04552223300561309 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 32.73807473061606,
    "estimated_duration": 3600.032167806935,
    "input_throughput": 5332.730682708046,
    "output_throughput": 4741.642075492545,
    "total_throughput": 10074.37275820059,
    "itl": 181.90055276702148,
    "ttft": 2036771.1998074027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0313843697053378,
    "arrivals": 576087,
    "finished_requests": 77901,
    "scheduler_time": 107.49327790046348
}
#Debug simulation 
Total elapsed time: 32.738196389749646. Arrivals time: 0.3266034028492868 Scheduler time: 32.28946964023635 Scheduler overhead time: 0.046247697435319424 Adapter cache time: 0.012498309835791588 Engine time: 0.04573478735983372 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 24.30985199799761,
    "estimated_duration": 3600.1206069739915,
    "input_throughput": 5346.081173701924,
    "output_throughput": 4741.550593314142,
    "total_throughput": 10087.631767016066,
    "itl": 181.58758277371106,
    "ttft": 2043411.8375253347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 555,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8061636329907975,
    "arrivals": 576087,
    "finished_requests": 77968,
    "scheduler_time": 107.58071909883618
}
#Debug simulation 
Total elapsed time: 24.30995903816074. Arrivals time: 0.3633420947007835 Scheduler time: 23.8292996911332 Scheduler overhead time: 0.042703107465058565 Adapter cache time: 0.015086904633790255 Engine time: 0.04290129942819476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 27.06066402979195,
    "estimated_duration": 3600.007173862404,
    "input_throughput": 5335.0413130965835,
    "output_throughput": 4730.320573703077,
    "total_throughput": 10065.36188679966,
    "itl": 179.25332662148574,
    "ttft": 2045998.6848947317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5904501250945127,
    "arrivals": 576087,
    "finished_requests": 77802,
    "scheduler_time": 107.93980662653387
}
#Debug simulation 
Total elapsed time: 27.060780357103795. Arrivals time: 0.37532394006848335 Scheduler time: 26.564316295087337 Scheduler overhead time: 0.04435288114473224 Adapter cache time: 0.015066650696098804 Engine time: 0.04458400793373585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 24.166248148307204,
    "estimated_duration": 3600.09608313056,
    "input_throughput": 5341.517158419299,
    "output_throughput": 4742.428425730666,
    "total_throughput": 10083.945584149966,
    "itl": 181.68178557090164,
    "ttft": 2044204.2210481276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 551,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7239250370394363,
    "arrivals": 576087,
    "finished_requests": 77946,
    "scheduler_time": 107.56104689557266
}
#Debug simulation 
Total elapsed time: 24.166349042207003. Arrivals time: 0.30940548656508327 Scheduler time: 23.74034310458228 Scheduler overhead time: 0.04227339196950197 Adapter cache time: 0.015053112525492907 Engine time: 0.042503926903009415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_192_slots_96_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 27.03468320798129,
    "estimated_duration": 3600.0276143344004,
    "input_throughput": 5335.011021450451,
    "output_throughput": 4730.293715579868,
    "total_throughput": 10065.30473703032,
    "itl": 179.2541655293208,
    "ttft": 2046006.4497212472,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 487,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.610822238493715,
    "arrivals": 576087,
    "finished_requests": 77802,
    "scheduler_time": 107.93987498514657
}
#Debug simulation 
Total elapsed time: 27.034779188223183. Arrivals time: 0.37386873504146934 Scheduler time: 26.53980870405212 Scheduler overhead time: 0.044562208000570536 Adapter cache time: 0.014573147054761648 Engine time: 0.04460301389917731 
