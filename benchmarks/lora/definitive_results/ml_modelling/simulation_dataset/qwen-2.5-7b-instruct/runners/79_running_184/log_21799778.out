INFO 06-01 00:47:10 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:10 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 110.151640904136,
    "estimated_duration": 3600.072774206302,
    "input_throughput": 6350.072466254531,
    "output_throughput": 5612.6684840294,
    "total_throughput": 11962.740950283931,
    "itl": 152.31932209050723,
    "ttft": 1938077.881636222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8390971745923201,
    "arrivals": 540089,
    "finished_requests": 92518,
    "scheduler_time": 141.685931632651
}
#Debug simulation 
Total elapsed time: 110.15201037703082. Arrivals time: 0.7702928851358593 Scheduler time: 109.14049458689988 Scheduler overhead time: 0.09359014499932528 Adapter cache time: 0.02134228404611349 Engine time: 0.09608829161152244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 114.70633335504681,
    "estimated_duration": 3600.116881208686,
    "input_throughput": 6338.200050976985,
    "output_throughput": 5589.659909387124,
    "total_throughput": 11927.859960364109,
    "itl": 152.59956247086592,
    "ttft": 1929348.2222516686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9358857423043823,
    "arrivals": 540089,
    "finished_requests": 92232,
    "scheduler_time": 141.19127242627022
}
#Debug simulation 
Total elapsed time: 114.70655539399013. Arrivals time: 0.8408663985319436 Scheduler time: 113.62861633859575 Scheduler overhead time: 0.09306993847712874 Adapter cache time: 0.021462623961269855 Engine time: 0.09392117336392403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 113.46734378114343,
    "estimated_duration": 3600.125471988538,
    "input_throughput": 6333.38731591647,
    "output_throughput": 5598.692089158434,
    "total_throughput": 11932.079405074905,
    "itl": 152.64122857967007,
    "ttft": 1934248.265288821,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8428989599645174,
    "arrivals": 540089,
    "finished_requests": 92249,
    "scheduler_time": 141.37256038561551
}
#Debug simulation 
Total elapsed time: 113.46752069238573. Arrivals time: 0.7829664153978229 Scheduler time: 112.44510880624875 Scheduler overhead time: 0.09483145363628864 Adapter cache time: 0.020100848749279976 Engine time: 0.09601368196308613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 89.79196625202894,
    "estimated_duration": 3600.032008015325,
    "input_throughput": 6283.798296691061,
    "output_throughput": 5572.602675568934,
    "total_throughput": 11856.400972259995,
    "itl": 154.45825935514475,
    "ttft": 1912244.9153757668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 372,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1385014407429879,
    "arrivals": 483089,
    "finished_requests": 91652,
    "scheduler_time": 139.85386812994696
}
#Debug simulation 
Total elapsed time: 89.79214049736038. Arrivals time: 0.6867261463776231 Scheduler time: 88.8986678365618 Scheduler overhead time: 0.07962459465488791 Adapter cache time: 0.020489437505602837 Engine time: 0.08101852796971798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 89.31744681717828,
    "estimated_duration": 3600.0233314719558,
    "input_throughput": 6284.529825741282,
    "output_throughput": 5573.127214094083,
    "total_throughput": 11857.657039835365,
    "itl": 154.43207526008703,
    "ttft": 1912771.0441910585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.229806386106653,
    "arrivals": 483089,
    "finished_requests": 91666,
    "scheduler_time": 139.86809475203083
}
#Debug simulation 
Total elapsed time: 89.31763446331024. Arrivals time: 0.6736709815450013 Scheduler time: 88.43655164260417 Scheduler overhead time: 0.07993781520053744 Adapter cache time: 0.020781816449016333 Engine time: 0.08143776515498757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 88.90307447267696,
    "estimated_duration": 3600.026599146145,
    "input_throughput": 6284.524121395679,
    "output_throughput": 5573.122155474806,
    "total_throughput": 11857.646276870484,
    "itl": 154.4321752182087,
    "ttft": 1912772.4788573394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2320557271130452,
    "arrivals": 483089,
    "finished_requests": 91666,
    "scheduler_time": 139.86813841966259
}
#Debug simulation 
Total elapsed time: 88.90324867982417. Arrivals time: 0.6863433974795043 Scheduler time: 88.00759714562446 Scheduler overhead time: 0.08192118816077709 Adapter cache time: 0.02038711030036211 Engine time: 0.08181406650692225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 89.293537016958,
    "estimated_duration": 3600.165727225629,
    "input_throughput": 6282.610500108919,
    "output_throughput": 5573.616749988696,
    "total_throughput": 11856.227250097616,
    "itl": 154.48161493033427,
    "ttft": 1912174.5133295367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 355,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.111725078858437,
    "arrivals": 483089,
    "finished_requests": 91685,
    "scheduler_time": 139.84056201110837
}
#Debug simulation 
Total elapsed time: 89.29373581986874. Arrivals time: 1.0607209927402437 Scheduler time: 88.02604577457532 Scheduler overhead time: 0.07977403001859784 Adapter cache time: 0.021452571731060743 Engine time: 0.08049316145479679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 89.0093416259624,
    "estimated_duration": 3600.0421948705152,
    "input_throughput": 6284.496896240891,
    "output_throughput": 5573.098012180835,
    "total_throughput": 11857.594908421726,
    "itl": 154.43268258327728,
    "ttft": 1912777.1786964026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2476491966284857,
    "arrivals": 483089,
    "finished_requests": 91666,
    "scheduler_time": 139.86822077455818
}
#Debug simulation 
Total elapsed time: 89.00962178315967. Arrivals time: 0.7067464510910213 Scheduler time: 88.09372364124283 Scheduler overhead time: 0.08237584959715605 Adapter cache time: 0.020689927972853184 Engine time: 0.0805350043810904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 87.60609908727929,
    "estimated_duration": 3600.1188719239167,
    "input_throughput": 6284.75331091183,
    "output_throughput": 5573.335968564106,
    "total_throughput": 11858.089279475937,
    "itl": 154.428451937489,
    "ttft": 1912774.982262766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.124258910883215,
    "arrivals": 483089,
    "finished_requests": 91671,
    "scheduler_time": 139.8750366627672
}
#Debug simulation 
Total elapsed time: 87.6063150074333. Arrivals time: 0.6960747418925166 Scheduler time: 86.70145068317652 Scheduler overhead time: 0.08041053684428334 Adapter cache time: 0.021273414604365826 Engine time: 0.08202283596619964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 89.64828457683325,
    "estimated_duration": 3600.058009593511,
    "input_throughput": 6284.469289025308,
    "output_throughput": 5573.073530074976,
    "total_throughput": 11857.542819100285,
    "itl": 154.43290308250687,
    "ttft": 1912782.0608209553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 376,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.260113407373431,
    "arrivals": 483089,
    "finished_requests": 91666,
    "scheduler_time": 139.868373311448
}
#Debug simulation 
Total elapsed time: 89.64847419271246. Arrivals time: 0.6664874996058643 Scheduler time: 88.77663827873766 Scheduler overhead time: 0.07957794424146414 Adapter cache time: 0.020249349530786276 Engine time: 0.08033361379057169 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 97.34446723712608,
    "estimated_duration": 3600.1253351351547,
    "input_throughput": 6291.3115215611515,
    "output_throughput": 5570.778551588144,
    "total_throughput": 11862.090073149295,
    "itl": 154.19010828879766,
    "ttft": 1903351.554055037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 469,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4353687519044753,
    "arrivals": 473564,
    "finished_requests": 91807,
    "scheduler_time": 139.8403854933162
}
#Debug simulation 
Total elapsed time: 97.344693581108. Arrivals time: 0.6870929524302483 Scheduler time: 96.44256228301674 Scheduler overhead time: 0.08267789101228118 Adapter cache time: 0.022171592339873314 Engine time: 0.08376689627766609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 98.3200554731302,
    "estimated_duration": 3600.1576426412917,
    "input_throughput": 6290.893690806249,
    "output_throughput": 5572.095444487945,
    "total_throughput": 11862.989135294194,
    "itl": 154.24292109265087,
    "ttft": 1902673.8497293007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3810921113123231,
    "arrivals": 473564,
    "finished_requests": 91793,
    "scheduler_time": 139.8426951399066
}
#Debug simulation 
Total elapsed time: 98.32025484368205. Arrivals time: 0.7024612561799586 Scheduler time: 97.39900287194178 Scheduler overhead time: 0.08599592419341207 Adapter cache time: 0.02104052994400263 Engine time: 0.08563278568908572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 97.34469103813171,
    "estimated_duration": 3600.1611129403,
    "input_throughput": 6290.887626832596,
    "output_throughput": 5572.090073384628,
    "total_throughput": 11862.977700217223,
    "itl": 154.24305654368675,
    "ttft": 1902674.915716605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3845523342490276,
    "arrivals": 473564,
    "finished_requests": 91793,
    "scheduler_time": 139.842705215953
}
#Debug simulation 
Total elapsed time: 97.34488590434194. Arrivals time: 0.6934004938229918 Scheduler time: 96.4328548591584 Scheduler overhead time: 0.0852780076675117 Adapter cache time: 0.02285175956785679 Engine time: 0.08503808453679085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 100.2879330557771,
    "estimated_duration": 3600.0832167083436,
    "input_throughput": 6286.407740511258,
    "output_throughput": 5571.30149295238,
    "total_throughput": 11857.709233463638,
    "itl": 154.2851437827568,
    "ttft": 1902057.0884231557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 447,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3962073983135599,
    "arrivals": 473564,
    "finished_requests": 91780,
    "scheduler_time": 139.82753679884007
}
#Debug simulation 
Total elapsed time: 100.28818172076717. Arrivals time: 0.6643310291692615 Scheduler time: 99.40038761030883 Scheduler overhead time: 0.0873240171931684 Adapter cache time: 0.021414426621049643 Engine time: 0.08859070343896747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 98.23889440530911,
    "estimated_duration": 3600.0067168298892,
    "input_throughput": 6288.0927122085905,
    "output_throughput": 5571.338216185818,
    "total_throughput": 11859.43092839441,
    "itl": 154.26918433704427,
    "ttft": 1902657.3782304449,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.478997779078786,
    "arrivals": 473564,
    "finished_requests": 91812,
    "scheduler_time": 139.82230934551134
}
#Debug simulation 
Total elapsed time: 98.23908044910058. Arrivals time: 0.6688242754898965 Scheduler time: 97.35418963199481 Scheduler overhead time: 0.08381288778036833 Adapter cache time: 0.022525771986693144 Engine time: 0.08402095176279545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 97.49014734709635,
    "estimated_duration": 3600.0834858263597,
    "input_throughput": 6291.294657240741,
    "output_throughput": 5570.842753774772,
    "total_throughput": 11862.137411015514,
    "itl": 154.19012350814657,
    "ttft": 1903338.5668170184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 470,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4053236386040096,
    "arrivals": 473564,
    "finished_requests": 91806,
    "scheduler_time": 139.83935705082115
}
#Debug simulation 
Total elapsed time: 97.49035963322967. Arrivals time: 0.7186597790569067 Scheduler time: 96.55393925262615 Scheduler overhead time: 0.08417310286313295 Adapter cache time: 0.02247867127880454 Engine time: 0.08472062600776553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 98.01015840703622,
    "estimated_duration": 3600.025719502055,
    "input_throughput": 6288.059520622288,
    "output_throughput": 5571.3088079754625,
    "total_throughput": 11859.368328597751,
    "itl": 154.26974265481908,
    "ttft": 1902663.6246361174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4971063243225224,
    "arrivals": 473564,
    "finished_requests": 91812,
    "scheduler_time": 139.82244835178375
}
#Debug simulation 
Total elapsed time: 98.01035659899935. Arrivals time: 0.7344920681789517 Scheduler time: 97.05694001493976 Scheduler overhead time: 0.08532702550292015 Adapter cache time: 0.023241817951202393 Engine time: 0.08421464264392853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 116.85574543988332,
    "estimated_duration": 3600.139057502144,
    "input_throughput": 6312.595051750016,
    "output_throughput": 5576.709032436601,
    "total_throughput": 11889.304084186617,
    "itl": 153.53258342182738,
    "ttft": 1888131.0525629532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4812760680634682,
    "arrivals": 468761,
    "finished_requests": 91858,
    "scheduler_time": 140.40357815390058
}
#Debug simulation 
Total elapsed time: 116.85593653703108. Arrivals time: 1.0606519342400134 Scheduler time: 115.55116580240428 Scheduler overhead time: 0.0944807156920433 Adapter cache time: 0.023036981467157602 Engine time: 0.09762216405943036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 115.89094370696694,
    "estimated_duration": 3600.069657199157,
    "input_throughput": 6312.100365768812,
    "output_throughput": 5576.625152197542,
    "total_throughput": 11888.725517966353,
    "itl": 153.53697703757612,
    "ttft": 1888172.718409455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5795692259073313,
    "arrivals": 468761,
    "finished_requests": 91851,
    "scheduler_time": 140.39822323452265
}
#Debug simulation 
Total elapsed time: 115.89113637711853. Arrivals time: 0.7222524569369853 Scheduler time: 114.92582596512511 Scheduler overhead time: 0.09487086068838835 Adapter cache time: 0.024269826244562864 Engine time: 0.09501011250540614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 114.74349454976618,
    "estimated_duration": 3600.0722793602445,
    "input_throughput": 6312.095768265574,
    "output_throughput": 5576.621090387573,
    "total_throughput": 11888.716858653146,
    "itl": 153.53706450043688,
    "ttft": 1888173.5693525788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5823313110321855,
    "arrivals": 468761,
    "finished_requests": 91851,
    "scheduler_time": 140.39822254839675
}
#Debug simulation 
Total elapsed time: 114.74369983095676. Arrivals time: 0.7268517673946917 Scheduler time: 113.77226390270516 Scheduler overhead time: 0.09538963157683611 Adapter cache time: 0.023508616723120213 Engine time: 0.0963883213698864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 114.72592624882236,
    "estimated_duration": 3600.004887457549,
    "input_throughput": 6312.213930367327,
    "output_throughput": 5576.725484441926,
    "total_throughput": 11888.939414809252,
    "itl": 153.5350711205267,
    "ttft": 1888151.2458528224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.515419760912186,
    "arrivals": 468761,
    "finished_requests": 91851,
    "scheduler_time": 140.39707321716517
}
#Debug simulation 
Total elapsed time: 114.7261469126679. Arrivals time: 0.786595971789211 Scheduler time: 113.69581291684881 Scheduler overhead time: 0.096026960760355 Adapter cache time: 0.023209013510495424 Engine time: 0.09610607475042343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 115.01124166278169,
    "estimated_duration": 3600.092868235309,
    "input_throughput": 6311.997448871013,
    "output_throughput": 5576.777248482839,
    "total_throughput": 11888.774697353852,
    "itl": 153.54140538082257,
    "ttft": 1887939.9460936207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 479,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5858453187719028,
    "arrivals": 468761,
    "finished_requests": 91793,
    "scheduler_time": 140.39772477389027
}
#Debug simulation 
Total elapsed time: 115.01143871899694. Arrivals time: 0.7445987104438245 Scheduler time: 114.02254725107923 Scheduler overhead time: 0.09714061813428998 Adapter cache time: 0.022701077163219452 Engine time: 0.0957369557581842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 83.79718529479578,
    "estimated_duration": 3600.1032901423077,
    "input_throughput": 6312.657767966891,
    "output_throughput": 5576.764437557675,
    "total_throughput": 11889.422205524565,
    "itl": 153.5321791315633,
    "ttft": 1888117.5166206534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4471843427326385,
    "arrivals": 468761,
    "finished_requests": 91858,
    "scheduler_time": 140.4032671111798
}
#Debug simulation 
Total elapsed time: 83.79737734282389. Arrivals time: 0.4987919647246599 Scheduler time: 83.14181433897465 Scheduler overhead time: 0.06065264856442809 Adapter cache time: 0.015543953981250525 Engine time: 0.05814553378149867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 114.84214031789452,
    "estimated_duration": 3600.0803172038823,
    "input_throughput": 6312.081675346989,
    "output_throughput": 5576.608639551924,
    "total_throughput": 11888.690314898913,
    "itl": 153.5410660471006,
    "ttft": 1888172.341166744,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 484,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6230755378305903,
    "arrivals": 468761,
    "finished_requests": 91851,
    "scheduler_time": 140.39646546445542
}
#Debug simulation 
Total elapsed time: 114.84237439185381. Arrivals time: 0.7443501153029501 Scheduler time: 113.85526652634144 Scheduler overhead time: 0.09534035949036479 Adapter cache time: 0.02469835849478841 Engine time: 0.0946176927536726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 115.27165567781776,
    "estimated_duration": 3600.056120171638,
    "input_throughput": 6341.25864652121,
    "output_throughput": 5628.548090254195,
    "total_throughput": 11969.806736775405,
    "itl": 152.36640444517363,
    "ttft": 1894618.9858788033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2119531465973765,
    "arrivals": 466310,
    "finished_requests": 92389,
    "scheduler_time": 141.46460643468728
}
#Debug simulation 
Total elapsed time: 115.27182649681345. Arrivals time: 0.7917991452850401 Scheduler time: 114.23922249581665 Scheduler overhead time: 0.09287314908578992 Adapter cache time: 0.02318528387695551 Engine time: 0.09691762458533049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 112.38569379085675,
    "estimated_duration": 3600.1104649199187,
    "input_throughput": 6355.84025072658,
    "output_throughput": 5643.487386838264,
    "total_throughput": 11999.327637564844,
    "itl": 152.0213048215228,
    "ttft": 1893901.046184714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.271128611851023,
    "arrivals": 466310,
    "finished_requests": 92592,
    "scheduler_time": 141.82660129083217
}
#Debug simulation 
Total elapsed time: 112.38587115099654. Arrivals time: 0.6931234137155116 Scheduler time: 111.47545493021607 Scheduler overhead time: 0.08531415369361639 Adapter cache time: 0.019603187683969736 Engine time: 0.08593811746686697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 109.71417231438681,
    "estimated_duration": 3600.113134144178,
    "input_throughput": 6355.835538329399,
    "output_throughput": 5643.483202599359,
    "total_throughput": 11999.318740928758,
    "itl": 152.02139996765092,
    "ttft": 1893901.9620696162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2736449571140183,
    "arrivals": 466310,
    "finished_requests": 92592,
    "scheduler_time": 141.82661493187487
}
#Debug simulation 
Total elapsed time: 109.71436354098842. Arrivals time: 0.6435820059850812 Scheduler time: 108.85176543332636 Scheduler overhead time: 0.08486386062577367 Adapter cache time: 0.020393161568790674 Engine time: 0.08725595427677035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 110.70902923401445,
    "estimated_duration": 3600.0565488280968,
    "input_throughput": 6355.935160920886,
    "output_throughput": 5643.567461909373,
    "total_throughput": 11999.50262283026,
    "itl": 152.02019089310602,
    "ttft": 1893886.28653284,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2200541970459744,
    "arrivals": 466310,
    "finished_requests": 92591,
    "scheduler_time": 141.82461130153465
}
#Debug simulation 
Total elapsed time: 110.70924541121349. Arrivals time: 0.6239539063535631 Scheduler time: 109.86125892074779 Scheduler overhead time: 0.08809439837932587 Adapter cache time: 0.02168601518496871 Engine time: 0.08696231059730053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 110.64393335115165,
    "estimated_duration": 3600.1310698410666,
    "input_throughput": 6355.803873832335,
    "output_throughput": 5643.455087010744,
    "total_throughput": 11999.258960843079,
    "itl": 152.02195050963104,
    "ttft": 1893908.9291118523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2893641804158738,
    "arrivals": 466310,
    "finished_requests": 92592,
    "scheduler_time": 141.8267820358392
}
#Debug simulation 
Total elapsed time: 110.64414803311229. Arrivals time: 0.7108920384198427 Scheduler time: 109.70941846305504 Scheduler overhead time: 0.08733919449150562 Adapter cache time: 0.021518724039196968 Engine time: 0.08812116924673319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 113.61022477084771,
    "estimated_duration": 3600.031413439567,
    "input_throughput": 6341.302166079897,
    "output_throughput": 5628.586718536463,
    "total_throughput": 11969.88888461636,
    "itl": 152.36603677364758,
    "ttft": 1894610.2161831867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 396,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1840599167812564,
    "arrivals": 466310,
    "finished_requests": 92389,
    "scheduler_time": 141.4644556297864
}
#Debug simulation 
Total elapsed time: 113.61038957163692. Arrivals time: 0.6476822183467448 Scheduler time: 112.73976067407057 Scheduler overhead time: 0.0877254894003272 Adapter cache time: 0.020050487015396357 Engine time: 0.0879363096319139 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 112.4752917168662,
    "estimated_duration": 3600.147725432611,
    "input_throughput": 6355.77446957414,
    "output_throughput": 5643.4289783368795,
    "total_throughput": 11999.203447911019,
    "itl": 152.02272351793184,
    "ttft": 1893915.568991655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 390,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3059636802226327,
    "arrivals": 466310,
    "finished_requests": 92592,
    "scheduler_time": 141.82678835511086
}
#Debug simulation 
Total elapsed time: 112.47546009393409. Arrivals time: 0.7168145654723048 Scheduler time: 111.53520057257265 Scheduler overhead time: 0.08818887965753675 Adapter cache time: 0.02118892828002572 Engine time: 0.08764076605439186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 118.55376378586516,
    "estimated_duration": 3600.079788052781,
    "input_throughput": 6501.998671718555,
    "output_throughput": 5689.978613246134,
    "total_throughput": 12191.977284964689,
    "itl": 149.91819786740538,
    "ttft": 1894566.6554815874,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8599970560451027,
    "arrivals": 465116,
    "finished_requests": 93933,
    "scheduler_time": 143.20102958565076
}
#Debug simulation 
Total elapsed time: 118.55392780900002. Arrivals time: 0.6061004553921521 Scheduler time: 117.72186401626095 Scheduler overhead time: 0.09017466707155108 Adapter cache time: 0.020553498063236475 Engine time: 0.08778581814840436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 118.50710699893534,
    "estimated_duration": 3600.039570958533,
    "input_throughput": 6474.38688952913,
    "output_throughput": 5677.233151789546,
    "total_throughput": 12151.620041318676,
    "itl": 150.28056403337578,
    "ttft": 1891433.2035174328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8812693322612948,
    "arrivals": 465116,
    "finished_requests": 93660,
    "scheduler_time": 142.94268336165678
}
#Debug simulation 
Total elapsed time: 118.50726084504277. Arrivals time: 0.6224782872013748 Scheduler time: 117.65682257711887 Scheduler overhead time: 0.09167758142575622 Adapter cache time: 0.020373468287289143 Engine time: 0.08868118468672037 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 117.58219968201593,
    "estimated_duration": 3600.0410981901277,
    "input_throughput": 6474.3841429248705,
    "output_throughput": 5677.230743358753,
    "total_throughput": 12151.614886283623,
    "itl": 150.28060637636426,
    "ttft": 1891433.7271239248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8827918856777295,
    "arrivals": 465116,
    "finished_requests": 93660,
    "scheduler_time": 142.9426880398212
}
#Debug simulation 
Total elapsed time: 117.58235274581239. Arrivals time: 0.6773543846793473 Scheduler time: 116.67487444542348 Scheduler overhead time: 0.0925523629412055 Adapter cache time: 0.020678516011685133 Engine time: 0.08888343023136258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 117.80005065491423,
    "estimated_duration": 3600.0948433511408,
    "input_throughput": 6501.971480898814,
    "output_throughput": 5689.954818227,
    "total_throughput": 12191.926299125815,
    "itl": 149.92355783382146,
    "ttft": 1894599.7432644812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8810636647115492,
    "arrivals": 465116,
    "finished_requests": 93933,
    "scheduler_time": 143.19810488999246
}
#Debug simulation 
Total elapsed time: 117.80022081406787. Arrivals time: 0.636652119923383 Scheduler time: 116.93619794258848 Scheduler overhead time: 0.09158143121749163 Adapter cache time: 0.01967993611469865 Engine time: 0.0894852033816278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 115.57654573069885,
    "estimated_duration": 3600.056960293398,
    "input_throughput": 6474.355616334592,
    "output_throughput": 5677.2057290822195,
    "total_throughput": 12151.561345416812,
    "itl": 150.28056640985977,
    "ttft": 1891440.1933821742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8929779423773332,
    "arrivals": 465116,
    "finished_requests": 93660,
    "scheduler_time": 142.94294632719277
}
#Debug simulation 
Total elapsed time: 115.5766898128204. Arrivals time: 0.6537622879259288 Scheduler time: 114.69674079865217 Scheduler overhead time: 0.08962632529437542 Adapter cache time: 0.020584885496646166 Engine time: 0.08902379684150219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 117.55915606301278,
    "estimated_duration": 3600.0048940241277,
    "input_throughput": 6476.473417773009,
    "output_throughput": 5679.365612513239,
    "total_throughput": 12155.839030286248,
    "itl": 150.2187602289795,
    "ttft": 1891362.7038023334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8103036299184907,
    "arrivals": 465116,
    "finished_requests": 93692,
    "scheduler_time": 142.99813995225068
}
#Debug simulation 
Total elapsed time: 117.55929230386391. Arrivals time: 0.6131559447385371 Scheduler time: 116.7172377104871 Scheduler overhead time: 0.09171885438263416 Adapter cache time: 0.01948325941339135 Engine time: 0.09052323829382658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 117.76360012684017,
    "estimated_duration": 3600.0664183678555,
    "input_throughput": 6474.338606943551,
    "output_throughput": 5677.190813958926,
    "total_throughput": 12151.529420902476,
    "itl": 150.28054418364147,
    "ttft": 1891443.6167505789,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9055533210188196,
    "arrivals": 465116,
    "finished_requests": 93660,
    "scheduler_time": 142.94305934691948
}
#Debug simulation 
Total elapsed time: 117.76382363308221. Arrivals time: 0.6407085587270558 Scheduler time: 116.89703268790618 Scheduler overhead time: 0.08997832471504807 Adapter cache time: 0.01950166979804635 Engine time: 0.08913360768929124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 115.9779033199884,
    "estimated_duration": 3600.0223425569156,
    "input_throughput": 6468.779019704605,
    "output_throughput": 5705.901254326797,
    "total_throughput": 12174.680274031401,
    "itl": 149.93819373420106,
    "ttft": 1884243.5631281442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6702468159212722,
    "arrivals": 464531,
    "finished_requests": 94132,
    "scheduler_time": 143.63756827334984
}
#Debug simulation 
Total elapsed time: 115.97804339788854. Arrivals time: 0.6652559661306441 Scheduler time: 115.08049821387976 Scheduler overhead time: 0.09367996267974377 Adapter cache time: 0.01916696270927787 Engine time: 0.09207475604489446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 116.0348619162105,
    "estimated_duration": 3600.1212004625527,
    "input_throughput": 6468.601111820328,
    "output_throughput": 5705.639298299413,
    "total_throughput": 12174.24041011974,
    "itl": 149.94849574815152,
    "ttft": 1884310.0693103187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7152931217127503,
    "arrivals": 464531,
    "finished_requests": 94131,
    "scheduler_time": 143.63313396855114
}
#Debug simulation 
Total elapsed time: 116.03501728223637. Arrivals time: 0.6881843274459243 Scheduler time: 115.11845494620502 Scheduler overhead time: 0.09188099158927798 Adapter cache time: 0.01938834832981229 Engine time: 0.08987507689744234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 114.85811937740073,
    "estimated_duration": 3600.1223556359414,
    "input_throughput": 6468.599036236465,
    "output_throughput": 5705.637467527558,
    "total_throughput": 12174.236503764023,
    "itl": 149.94853155633885,
    "ttft": 1884310.4964775536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.716443257071081,
    "arrivals": 464531,
    "finished_requests": 94131,
    "scheduler_time": 143.63313900657437
}
#Debug simulation 
Total elapsed time: 114.858253586106. Arrivals time: 0.7595161362551153 Scheduler time: 113.8669961313717 Scheduler overhead time: 0.09261860465630889 Adapter cache time: 0.019984304904937744 Engine time: 0.09199961880221963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 116.0984873501584,
    "estimated_duration": 3600.0445530681127,
    "input_throughput": 6468.739110507168,
    "output_throughput": 5705.866051711432,
    "total_throughput": 12174.6051622186,
    "itl": 149.9387714734886,
    "ttft": 1884247.787366813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6854656634666039,
    "arrivals": 464531,
    "finished_requests": 94132,
    "scheduler_time": 143.63854545758636
}
#Debug simulation 
Total elapsed time: 116.09862987417728. Arrivals time: 0.690610283985734 Scheduler time: 115.17921441374347 Scheduler overhead time: 0.09212943213060498 Adapter cache time: 0.019611109048128128 Engine time: 0.08961451565846801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 114.47402670700103,
    "estimated_duration": 3600.1381085174708,
    "input_throughput": 6468.570732023902,
    "output_throughput": 5705.612501754478,
    "total_throughput": 12174.18323377838,
    "itl": 149.94908028683292,
    "ttft": 1884317.843530439,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7256232834793657,
    "arrivals": 464531,
    "finished_requests": 94131,
    "scheduler_time": 143.6334658686685
}
#Debug simulation 
Total elapsed time: 114.47420706320554. Arrivals time: 0.6695864610373974 Scheduler time: 113.57861662842333 Scheduler overhead time: 0.08961524767801166 Adapter cache time: 0.020418759901076555 Engine time: 0.08848454803228378 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 118.07637384533882,
    "estimated_duration": 3600.0060687219107,
    "input_throughput": 6468.8082618337685,
    "output_throughput": 5705.927047865418,
    "total_throughput": 12174.735309699186,
    "itl": 149.93790669206427,
    "ttft": 1884236.1358146265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6548210145835773,
    "arrivals": 464531,
    "finished_requests": 94132,
    "scheduler_time": 143.63739710051368
}
#Debug simulation 
Total elapsed time: 118.07652436103672. Arrivals time: 0.6450734855607152 Scheduler time: 117.19711857149377 Scheduler overhead time: 0.09415220888331532 Adapter cache time: 0.01961908768862486 Engine time: 0.09224337246268988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 116.55009423568845,
    "estimated_duration": 3600.063729501925,
    "input_throughput": 6468.704653520647,
    "output_throughput": 5705.835658315397,
    "total_throughput": 12174.540311836045,
    "itl": 149.94115895896138,
    "ttft": 1884256.6351903758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7350548174604797,
    "arrivals": 464531,
    "finished_requests": 94132,
    "scheduler_time": 143.63765213541868
}
#Debug simulation 
Total elapsed time: 116.5502366698347. Arrivals time: 0.6173702403903008 Scheduler time: 115.70061333756894 Scheduler overhead time: 0.09324809862300754 Adapter cache time: 0.020289655309170485 Engine time: 0.09091254370287061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 56.22988992230967,
    "estimated_duration": 3600.01277281495,
    "input_throughput": 6327.58421637159,
    "output_throughput": 5580.692699679126,
    "total_throughput": 11908.276916050716,
    "itl": 153.2489769626175,
    "ttft": 1857073.220876368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 330,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0099609554978077,
    "arrivals": 406769,
    "finished_requests": 92321,
    "scheduler_time": 139.87114418761678
}
#Debug simulation 
Total elapsed time: 56.23005417129025. Arrivals time: 0.6763027715496719 Scheduler time: 55.371162813622504 Scheduler overhead time: 0.07199308788403869 Adapter cache time: 0.01747784623876214 Engine time: 0.07044264394789934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 53.12279127910733,
    "estimated_duration": 3600.0887788433306,
    "input_throughput": 6315.459533557641,
    "output_throughput": 5565.3916419353745,
    "total_throughput": 11880.851175493015,
    "itl": 153.52122820908767,
    "ttft": 1857320.2174766064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9187843170738824,
    "arrivals": 406769,
    "finished_requests": 92098,
    "scheduler_time": 139.58491091157344
}
#Debug simulation 
Total elapsed time: 53.122951415833086. Arrivals time: 0.5886713163927197 Scheduler time: 52.351736365351826 Scheduler overhead time: 0.0722997491247952 Adapter cache time: 0.015637619886547327 Engine time: 0.07071120850741863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 52.726975391153246,
    "estimated_duration": 3600.0926896162696,
    "input_throughput": 6315.452673087545,
    "output_throughput": 5565.385596262414,
    "total_throughput": 11880.838269349959,
    "itl": 153.52132654768383,
    "ttft": 1857322.1455454833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9206631018407689,
    "arrivals": 406769,
    "finished_requests": 92098,
    "scheduler_time": 139.5849935686727
}
#Debug simulation 
Total elapsed time: 52.727152715902776. Arrivals time: 0.6215075943619013 Scheduler time: 51.92167919315398 Scheduler overhead time: 0.07063890714198351 Adapter cache time: 0.016372171230614185 Engine time: 0.07278787763789296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 45.61657558986917,
    "estimated_duration": 3600.1574316880556,
    "input_throughput": 6339.078896696827,
    "output_throughput": 5586.107658233806,
    "total_throughput": 11925.186554930633,
    "itl": 153.08326766596502,
    "ttft": 1856920.8977933866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0150774495978871,
    "arrivals": 406769,
    "finished_requests": 92415,
    "scheduler_time": 140.00618070892952
}
#Debug simulation 
Total elapsed time: 45.61671518906951. Arrivals time: 0.45708515029400587 Scheduler time: 45.01510635390878 Scheduler overhead time: 0.05633393255993724 Adapter cache time: 0.013452798593789339 Engine time: 0.05381561769172549 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 59.23536630021408,
    "estimated_duration": 3600.1057194099662,
    "input_throughput": 6315.429815690612,
    "output_throughput": 5565.365453568889,
    "total_throughput": 11880.7952692595,
    "itl": 153.52152389573706,
    "ttft": 1857327.1711141956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.932735465336596,
    "arrivals": 406769,
    "finished_requests": 92098,
    "scheduler_time": 139.5851188378909
}
#Debug simulation 
Total elapsed time: 59.235512405168265. Arrivals time: 0.7447800552472472 Scheduler time: 58.28286315780133 Scheduler overhead time: 0.08297877851873636 Adapter cache time: 0.018365458585321903 Engine time: 0.0820138156414032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 57.71873733494431,
    "estimated_duration": 3600.0191482160017,
    "input_throughput": 6339.611279931623,
    "output_throughput": 5593.011362169538,
    "total_throughput": 11932.622642101162,
    "itl": 152.96254691979865,
    "ttft": 1858982.075308801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 328,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.980736496727915,
    "arrivals": 406769,
    "finished_requests": 92492,
    "scheduler_time": 140.17765403771733
}
#Debug simulation 
Total elapsed time: 57.7188831968233. Arrivals time: 0.6436017197556794 Scheduler time: 56.88457693811506 Scheduler overhead time: 0.07508849585428834 Adapter cache time: 0.01810019137337804 Engine time: 0.07434319658204913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 53.81956895533949,
    "estimated_duration": 3600.0730844200675,
    "input_throughput": 6315.236515167139,
    "output_throughput": 5565.300073131015,
    "total_throughput": 11880.536588298153,
    "itl": 153.5218676252097,
    "ttft": 1857321.772325364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9439275523275186,
    "arrivals": 406769,
    "finished_requests": 92096,
    "scheduler_time": 139.5839939112149
}
#Debug simulation 
Total elapsed time: 53.81975082727149. Arrivals time: 0.5758659010753036 Scheduler time: 53.06029917066917 Scheduler overhead time: 0.07137463008984923 Adapter cache time: 0.01599893532693386 Engine time: 0.07233616523444653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 47.898092987947166,
    "estimated_duration": 3600.1401476649808,
    "input_throughput": 6298.231477101383,
    "output_throughput": 5590.931512223193,
    "total_throughput": 11889.162989324575,
    "itl": 154.11167928043932,
    "ttft": 1843766.0610533357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.331312168610758,
    "arrivals": 397242,
    "finished_requests": 91843,
    "scheduler_time": 139.42624232785664
}
#Debug simulation 
Total elapsed time: 47.89820759696886. Arrivals time: 0.5528249572962523 Scheduler time: 47.17840123362839 Scheduler overhead time: 0.06390509521588683 Adapter cache time: 0.01876409398391843 Engine time: 0.06273137126117945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 47.39511290518567,
    "estimated_duration": 3600.0841360842755,
    "input_throughput": 6289.071906143358,
    "output_throughput": 5571.868667996716,
    "total_throughput": 11860.940574140073,
    "itl": 154.53180616093562,
    "ttft": 1852265.1491638103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3506531299790423,
    "arrivals": 397242,
    "finished_requests": 91666,
    "scheduler_time": 139.1202092533327
}
#Debug simulation 
Total elapsed time: 47.395253017079085. Arrivals time: 0.5474761589430273 Scheduler time: 46.66707430873066 Scheduler overhead time: 0.06995854619890451 Adapter cache time: 0.01836662506684661 Engine time: 0.06948742317035794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 46.808883916120976,
    "estimated_duration": 3600.0862985387316,
    "input_throughput": 6289.068128502924,
    "output_throughput": 5571.865321156882,
    "total_throughput": 11860.933449659806,
    "itl": 154.53188430612056,
    "ttft": 1852266.0623247393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 414,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.353097111880787,
    "arrivals": 397242,
    "finished_requests": 91666,
    "scheduler_time": 139.12020620173982
}
#Debug simulation 
Total elapsed time: 46.809050280135125. Arrivals time: 0.5865975893102586 Scheduler time: 46.04469776991755 Scheduler overhead time: 0.06907119555398822 Adapter cache time: 0.019040900748223066 Engine time: 0.06717574968934059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 47.53736671991646,
    "estimated_duration": 3600.0250418837845,
    "input_throughput": 6298.237855627659,
    "output_throughput": 5590.886942683036,
    "total_throughput": 11889.124798310695,
    "itl": 154.1147724942336,
    "ttft": 1843704.428308614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.357875222864093,
    "arrivals": 397242,
    "finished_requests": 91837,
    "scheduler_time": 139.42040951526022
}
#Debug simulation 
Total elapsed time: 47.53749145800248. Arrivals time: 0.5642221504822373 Scheduler time: 46.80535886390135 Scheduler overhead time: 0.06405383814126253 Adapter cache time: 0.01784692844375968 Engine time: 0.06371587095782161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 49.30649342387915,
    "estimated_duration": 3600.173881009225,
    "input_throughput": 6284.086199097123,
    "output_throughput": 5576.787584040737,
    "total_throughput": 11860.87378313786,
    "itl": 154.6297354262866,
    "ttft": 1840333.8560083467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0744590076059155,
    "arrivals": 397242,
    "finished_requests": 91721,
    "scheduler_time": 139.1433575425347
}
#Debug simulation 
Total elapsed time: 49.30663696769625. Arrivals time: 0.5898692733608186 Scheduler time: 48.53309649089351 Scheduler overhead time: 0.07089039357379079 Adapter cache time: 0.01849882397800684 Engine time: 0.07105134287849069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 48.911184849217534,
    "estimated_duration": 3600.037283206996,
    "input_throughput": 6292.335389321442,
    "output_throughput": 5580.638871079389,
    "total_throughput": 11872.97426040083,
    "itl": 154.59131559323157,
    "ttft": 1846600.0962105053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 392,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1720997156016482,
    "arrivals": 397242,
    "finished_requests": 91747,
    "scheduler_time": 139.18646239417424
}
#Debug simulation 
Total elapsed time: 48.91131375916302. Arrivals time: 0.5566116687841713 Scheduler time: 48.175072212237865 Scheduler overhead time: 0.06962922448292375 Adapter cache time: 0.018217442091554403 Engine time: 0.06912439502775669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 48.020539335906506,
    "estimated_duration": 3600.1132441153686,
    "input_throughput": 6282.74569889479,
    "output_throughput": 5576.9984549287665,
    "total_throughput": 11859.744153823556,
    "itl": 154.61444361075363,
    "ttft": 1840589.4351993368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 330,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.107610505744819,
    "arrivals": 397242,
    "finished_requests": 91689,
    "scheduler_time": 139.15866732346535
}
#Debug simulation 
Total elapsed time: 48.02068900503218. Arrivals time: 0.5773648447357118 Scheduler time: 47.26175527460873 Scheduler overhead time: 0.07169561926275492 Adapter cache time: 0.016980899963527918 Engine time: 0.07015227666124701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 47.51491134893149,
    "estimated_duration": 3600.0920576274593,
    "input_throughput": 6334.633013531652,
    "output_throughput": 5598.472393865007,
    "total_throughput": 11933.105407396659,
    "itl": 153.69742032744855,
    "ttft": 1848168.460507044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2088926588534437,
    "arrivals": 392500,
    "finished_requests": 92451,
    "scheduler_time": 139.7085428117331
}
#Debug simulation 
Total elapsed time: 47.51509081572294. Arrivals time: 0.6149736475199461 Scheduler time: 46.72089155158028 Scheduler overhead time: 0.06934953620657325 Adapter cache time: 0.018923440482467413 Engine time: 0.06864140648394823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 55.12034279014915,
    "estimated_duration": 3600.1580560558496,
    "input_throughput": 6344.857265801566,
    "output_throughput": 5602.893452433212,
    "total_throughput": 11947.750718234778,
    "itl": 153.4446833815305,
    "ttft": 1849491.862977813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2586299322871553,
    "arrivals": 392500,
    "finished_requests": 92553,
    "scheduler_time": 139.93740422714512
}
#Debug simulation 
Total elapsed time: 55.12047462724149. Arrivals time: 0.6063372809439898 Scheduler time: 54.32124117016792 Scheduler overhead time: 0.0755667774938047 Adapter cache time: 0.0190289537422359 Engine time: 0.07493853056803346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 53.75933260889724,
    "estimated_duration": 3600.1484583080833,
    "input_throughput": 6343.372298244739,
    "output_throughput": 5602.501183931444,
    "total_throughput": 11945.873482176183,
    "itl": 153.4437595168678,
    "ttft": 1849420.5928778767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 387,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2658633905462993,
    "arrivals": 392500,
    "finished_requests": 92552,
    "scheduler_time": 139.93150435145262
}
#Debug simulation 
Total elapsed time: 53.75946165714413. Arrivals time: 0.6286488207988441 Scheduler time: 52.94119072658941 Scheduler overhead time: 0.07484319107607007 Adapter cache time: 0.019822913222014904 Engine time: 0.072111114859581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 48.487548479344696,
    "estimated_duration": 3600.109890722227,
    "input_throughput": 6335.017733423875,
    "output_throughput": 5597.816347755181,
    "total_throughput": 11932.834081179057,
    "itl": 153.6945645943577,
    "ttft": 1848185.1676605768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 395,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2358216391573653,
    "arrivals": 392500,
    "finished_requests": 92446,
    "scheduler_time": 139.70831838981053
}
#Debug simulation 
Total elapsed time: 48.48767533292994. Arrivals time: 0.8599400636740029 Scheduler time: 47.446462363470346 Scheduler overhead time: 0.07053159410133958 Adapter cache time: 0.019357575103640556 Engine time: 0.06861383654177189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 53.33611797215417,
    "estimated_duration": 3600.165428609174,
    "input_throughput": 6343.342397136035,
    "output_throughput": 5602.47477510834,
    "total_throughput": 11945.817172244375,
    "itl": 153.4444471240593,
    "ttft": 1849425.6476913632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2787048626504889,
    "arrivals": 392500,
    "finished_requests": 92552,
    "scheduler_time": 139.93157969142263
}
#Debug simulation 
Total elapsed time: 53.33628376899287. Arrivals time: 0.5956352660432458 Scheduler time: 52.552234860602766 Scheduler overhead time: 0.07280058739706874 Adapter cache time: 0.019179022405296564 Engine time: 0.07273400668054819 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 49.18784733489156,
    "estimated_duration": 3600.131095235516,
    "input_throughput": 6336.710635396349,
    "output_throughput": 5598.808061927364,
    "total_throughput": 11935.518697323712,
    "itl": 153.67380748921968,
    "ttft": 1848218.5368522205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1541594138322357,
    "arrivals": 392500,
    "finished_requests": 92448,
    "scheduler_time": 139.72089033048755
}
#Debug simulation 
Total elapsed time: 49.18797526927665. Arrivals time: 0.5857290043495595 Scheduler time: 48.418750521261245 Scheduler overhead time: 0.07045586593449116 Adapter cache time: 0.019219472538679838 Engine time: 0.07074205158278346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 53.50310793425888,
    "estimated_duration": 3600.0073355397335,
    "input_throughput": 6343.086519454668,
    "output_throughput": 5602.286640056598,
    "total_throughput": 11945.373159511266,
    "itl": 153.44385812824586,
    "ttft": 1849429.8316238015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 386,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.295178608670833,
    "arrivals": 392500,
    "finished_requests": 92546,
    "scheduler_time": 139.92485546166913
}
#Debug simulation 
Total elapsed time: 53.5032718940638. Arrivals time: 0.6095485873520374 Scheduler time: 52.70391495153308 Scheduler overhead time: 0.07363052340224385 Adapter cache time: 0.019164813682436943 Engine time: 0.07321375096216798 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 46.202412992250174,
    "estimated_duration": 3600.0368994962064,
    "input_throughput": 6339.721407631659,
    "output_throughput": 5617.993805238582,
    "total_throughput": 11957.71521287024,
    "itl": 153.60795369525925,
    "ttft": 1843120.0418739454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7108126488584328,
    "arrivals": 390109,
    "finished_requests": 92518,
    "scheduler_time": 140.1002654828617
}
#Debug simulation 
Total elapsed time: 46.20254998933524. Arrivals time: 0.5781417964026332 Scheduler time: 45.45642384327948 Scheduler overhead time: 0.0640461165457964 Adapter cache time: 0.017719397321343422 Engine time: 0.0643803752027452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 48.43825642904267,
    "estimated_duration": 3600.019991678585,
    "input_throughput": 6355.969148196581,
    "output_throughput": 5626.054590479146,
    "total_throughput": 11982.023738675727,
    "itl": 153.29868693144837,
    "ttft": 1840344.0476887678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.422971761561936,
    "arrivals": 390109,
    "finished_requests": 92702,
    "scheduler_time": 140.31359792737155
}
#Debug simulation 
Total elapsed time: 48.43839439190924. Arrivals time: 0.573569412343204 Scheduler time: 47.69061876554042 Scheduler overhead time: 0.06711382186040282 Adapter cache time: 0.017677819821983576 Engine time: 0.06697110924869776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 46.16089183697477,
    "estimated_duration": 3600.024085537453,
    "input_throughput": 6355.961920344755,
    "output_throughput": 5626.048192668206,
    "total_throughput": 11982.01011301296,
    "itl": 153.29884677239443,
    "ttft": 1840346.0748520496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.425450486466297,
    "arrivals": 390109,
    "finished_requests": 92702,
    "scheduler_time": 140.31364680574904
}
#Debug simulation 
Total elapsed time: 46.161014538723975. Arrivals time: 0.598141472786665 Scheduler time: 45.3957374538295 Scheduler overhead time: 0.06359799858182669 Adapter cache time: 0.0182420052587986 Engine time: 0.06315831374377012 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 47.78422306384891,
    "estimated_duration": 3600.138713557847,
    "input_throughput": 6365.064188694572,
    "output_throughput": 5634.226793432157,
    "total_throughput": 11999.29098212673,
    "itl": 153.12112770189583,
    "ttft": 1838252.4879001668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3440208742790845,
    "arrivals": 390109,
    "finished_requests": 92829,
    "scheduler_time": 140.47341238449343
}
#Debug simulation 
Total elapsed time: 47.7843624050729. Arrivals time: 0.5938110835850239 Scheduler time: 47.019076854456216 Scheduler overhead time: 0.06555305980145931 Adapter cache time: 0.017022754997015 Engine time: 0.06621896661818027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 47.68315521394834,
    "estimated_duration": 3600.157553681494,
    "input_throughput": 6364.387018720763,
    "output_throughput": 5629.022812981279,
    "total_throughput": 11993.409831702042,
    "itl": 153.14180454105036,
    "ttft": 1840424.3652402232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4429302627779594,
    "arrivals": 390109,
    "finished_requests": 92777,
    "scheduler_time": 140.41394394240297
}
#Debug simulation 
Total elapsed time: 47.68327623512596. Arrivals time: 0.5565589349716902 Scheduler time: 46.95068142656237 Scheduler overhead time: 0.0675739198923111 Adapter cache time: 0.01776566170156002 Engine time: 0.06844594702124596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 47.63785869674757,
    "estimated_duration": 3600.0836441248234,
    "input_throughput": 6353.983201843316,
    "output_throughput": 5631.578597649118,
    "total_throughput": 11985.561799492434,
    "itl": 153.24133048967258,
    "ttft": 1842231.3309032428,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 544,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6265873604267627,
    "arrivals": 390109,
    "finished_requests": 92730,
    "scheduler_time": 140.44720053571174
}
#Debug simulation 
Total elapsed time: 47.638117311988026. Arrivals time: 0.5771523420698941 Scheduler time: 46.891459740232676 Scheduler overhead time: 0.06400163006037474 Adapter cache time: 0.019504867028445005 Engine time: 0.06377738201990724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 38.97774506313726,
    "estimated_duration": 3600.1199610637136,
    "input_throughput": 6370.4057775962865,
    "output_throughput": 5633.787545792575,
    "total_throughput": 12004.193323388861,
    "itl": 153.00362753965874,
    "ttft": 1840315.3278682705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 432,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.449533515423537,
    "arrivals": 390109,
    "finished_requests": 92863,
    "scheduler_time": 140.53609931664838
}
#Debug simulation 
Total elapsed time: 38.97787853796035. Arrivals time: 0.5060648410581052 Scheduler time: 38.32830626470968 Scheduler overhead time: 0.054515611845999956 Adapter cache time: 0.014535303227603436 Engine time: 0.053715023677796125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 67.27756373304874,
    "estimated_duration": 3600.0188463448344,
    "input_throughput": 6430.551613224117,
    "output_throughput": 5676.62306011231,
    "total_throughput": 12107.174673336427,
    "itl": 151.19232529285725,
    "ttft": 1849427.5119081084,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1966507078777122,
    "arrivals": 388923,
    "finished_requests": 93459,
    "scheduler_time": 142.06825011267358
}
#Debug simulation 
Total elapsed time: 67.27770297368988. Arrivals time: 0.5481943651102483 Scheduler time: 66.56076347036287 Scheduler overhead time: 0.0642637275159359 Adapter cache time: 0.0172884538769722 Engine time: 0.06428523687645793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 66.88203590223566,
    "estimated_duration": 3600.1583926148583,
    "input_throughput": 6413.655312323412,
    "output_throughput": 5665.125746088771,
    "total_throughput": 12078.781058412183,
    "itl": 151.68565927078464,
    "ttft": 1848552.8681957184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.284723194304393,
    "arrivals": 388923,
    "finished_requests": 93238,
    "scheduler_time": 141.69654087030344
}
#Debug simulation 
Total elapsed time: 66.88218477414921. Arrivals time: 0.5554785002022982 Scheduler time: 66.15341722080484 Scheduler overhead time: 0.06694510160014033 Adapter cache time: 0.017242785077542067 Engine time: 0.06651681708171964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 68.10907981637865,
    "estimated_duration": 3600.1608422411728,
    "input_throughput": 6413.65094833538,
    "output_throughput": 5665.121891416241,
    "total_throughput": 12078.772839751622,
    "itl": 151.68575229803923,
    "ttft": 1848553.7613549074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2871680224873199,
    "arrivals": 388923,
    "finished_requests": 93238,
    "scheduler_time": 141.69654566842078
}
#Debug simulation 
Total elapsed time: 68.10922313621268. Arrivals time: 0.5784494471736252 Scheduler time: 67.35677286796272 Scheduler overhead time: 0.06767159607261419 Adapter cache time: 0.017008699011057615 Engine time: 0.06645479425787926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 66.78652197262272,
    "estimated_duration": 3600.0499969506313,
    "input_throughput": 6449.170711425091,
    "output_throughput": 5691.516789310019,
    "total_throughput": 12140.68750073511,
    "itl": 150.76285523272227,
    "ttft": 1849553.727846923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2234528426593168,
    "arrivals": 388923,
    "finished_requests": 93690,
    "scheduler_time": 142.48482598328985
}
#Debug simulation 
Total elapsed time: 66.78666280955076. Arrivals time: 0.5906003131531179 Scheduler time: 66.02247347356752 Scheduler overhead time: 0.06664157193154097 Adapter cache time: 0.017733794637024403 Engine time: 0.06666631950065494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 67.87612690776587,
    "estimated_duration": 3600.009403589411,
    "input_throughput": 6413.5599137544195,
    "output_throughput": 5665.062980020486,
    "total_throughput": 12078.622893774906,
    "itl": 151.6855831056475,
    "ttft": 1848511.1387876337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 394,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3031387533620051,
    "arrivals": 388923,
    "finished_requests": 93235,
    "scheduler_time": 141.69000206359925
}
#Debug simulation 
Total elapsed time: 67.87626486690715. Arrivals time: 0.5636996999382973 Scheduler time: 67.14117284025997 Scheduler overhead time: 0.06541732093319297 Adapter cache time: 0.018211382441222668 Engine time: 0.06454200763255358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 66.61404025927186,
    "estimated_duration": 3600.156545734814,
    "input_throughput": 6430.52092482683,
    "output_throughput": 5676.773701469315,
    "total_throughput": 12107.294626296145,
    "itl": 151.19148623909527,
    "ttft": 1849412.2331538259,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.169109665306746,
    "arrivals": 388923,
    "finished_requests": 93465,
    "scheduler_time": 142.07464862245118
}
#Debug simulation 
Total elapsed time: 66.61419136496261. Arrivals time: 0.565653317142278 Scheduler time: 65.87837285362184 Scheduler overhead time: 0.06488095410168171 Adapter cache time: 0.017944408114999533 Engine time: 0.06482875207439065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 60.179946108255535,
    "estimated_duration": 3600.018521942107,
    "input_throughput": 6431.309133240156,
    "output_throughput": 5674.674692778852,
    "total_throughput": 12105.983826019008,
    "itl": 151.77852922486056,
    "ttft": 1846354.0825442264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 385,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2898200834170015,
    "arrivals": 388923,
    "finished_requests": 93403,
    "scheduler_time": 141.7115681857336
}
#Debug simulation 
Total elapsed time: 60.180099718272686. Arrivals time: 0.5916653582826257 Scheduler time: 59.418366057332605 Scheduler overhead time: 0.06605993211269379 Adapter cache time: 0.016867663711309433 Engine time: 0.06480834493413568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 85.6074623260647,
    "estimated_duration": 3600.001897165021,
    "input_throughput": 6328.738053705427,
    "output_throughput": 5600.707881814698,
    "total_throughput": 11929.445935520125,
    "itl": 153.9354750654257,
    "ttft": 1834518.506509268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7375775462877927,
    "arrivals": 388315,
    "finished_requests": 92147,
    "scheduler_time": 139.84684837599863
}
#Debug simulation 
Total elapsed time: 85.6076284549199. Arrivals time: 0.6253992416895926 Scheduler time: 84.76677651004866 Scheduler overhead time: 0.08682624530047178 Adapter cache time: 0.018369876313954592 Engine time: 0.0836180318146944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 67.49349081004038,
    "estimated_duration": 3600.1410918244624,
    "input_throughput": 6317.589622153891,
    "output_throughput": 5599.431101680531,
    "total_throughput": 11917.020723834421,
    "itl": 154.19176601029034,
    "ttft": 1854420.8826082957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8183294468815473,
    "arrivals": 388315,
    "finished_requests": 91952,
    "scheduler_time": 139.7350554575663
}
#Debug simulation 
Total elapsed time: 67.49364327592775. Arrivals time: 0.6431545685045421 Scheduler time: 66.64489475311711 Scheduler overhead time: 0.08151224907487631 Adapter cache time: 0.01816049637272954 Engine time: 0.08053820906206965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 66.57779099186882,
    "estimated_duration": 3600.1429571216895,
    "input_throughput": 6317.586348899871,
    "output_throughput": 5599.428200517041,
    "total_throughput": 11917.014549416912,
    "itl": 154.19182458864574,
    "ttft": 1854421.3299942988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8199063151329795,
    "arrivals": 388315,
    "finished_requests": 91952,
    "scheduler_time": 139.73507194388773
}
#Debug simulation 
Total elapsed time: 66.57791599724442. Arrivals time: 0.6340083507820964 Scheduler time: 65.73844603961334 Scheduler overhead time: 0.08172072982415557 Adapter cache time: 0.017684745602309704 Engine time: 0.08163029979914427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 83.05813938286155,
    "estimated_duration": 3600.117577404145,
    "input_throughput": 6329.009403195434,
    "output_throughput": 5602.799232613967,
    "total_throughput": 11931.808635809402,
    "itl": 153.9079570400619,
    "ttft": 1835626.7909079806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7604956330917797,
    "arrivals": 388315,
    "finished_requests": 92148,
    "scheduler_time": 139.86322588919947
}
#Debug simulation 
Total elapsed time: 83.05834415694699. Arrivals time: 0.6249441187828779 Scheduler time: 82.22510815272108 Scheduler overhead time: 0.08353696716949344 Adapter cache time: 0.017723402474075556 Engine time: 0.08210042724385858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 84.67969096498564,
    "estimated_duration": 3600.1455759857895,
    "input_throughput": 6331.243700821383,
    "output_throughput": 5600.715741745769,
    "total_throughput": 11931.959442567153,
    "itl": 153.89941068190564,
    "ttft": 1834539.532452163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8016889597103035,
    "arrivals": 388315,
    "finished_requests": 92147,
    "scheduler_time": 139.8647035322187
}
#Debug simulation 
Total elapsed time: 84.67983402730897. Arrivals time: 0.6419306942261755 Scheduler time: 83.8266209368594 Scheduler overhead time: 0.08481345418840647 Adapter cache time: 0.01901388168334961 Engine time: 0.08172260457649827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 85.58189451508224,
    "estimated_duration": 3600.140165413567,
    "input_throughput": 6328.853309349581,
    "output_throughput": 5600.517222552031,
    "total_throughput": 11929.370531901612,
    "itl": 153.9350817502805,
    "ttft": 1834525.4480128004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206021210714253,
    "arrivals": 388315,
    "finished_requests": 92149,
    "scheduler_time": 139.85266656349935
}
#Debug simulation 
Total elapsed time: 85.5820482429117. Arrivals time: 0.6629594648256898 Scheduler time: 84.70887693250552 Scheduler overhead time: 0.0828665322624147 Adapter cache time: 0.017811967059969902 Engine time: 0.08361050300300121 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 85.78918478824198,
    "estimated_duration": 3600.1613098501084,
    "input_throughput": 6331.216031247499,
    "output_throughput": 5600.691264814325,
    "total_throughput": 11931.907296061823,
    "itl": 153.8997686008609,
    "ttft": 1834544.1320212232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.812000770196322,
    "arrivals": 388315,
    "finished_requests": 92147,
    "scheduler_time": 139.86490456368497
}
#Debug simulation 
Total elapsed time: 85.78934002108872. Arrivals time: 0.6479735071770847 Scheduler time: 84.93008990306407 Scheduler overhead time: 0.08547320542857051 Adapter cache time: 0.017566825728863478 Engine time: 0.08286655740812421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 15.061001067981124,
    "estimated_duration": 3600.134951409153,
    "input_throughput": 6295.1070740082205,
    "output_throughput": 5576.797334261439,
    "total_throughput": 11871.90440826966,
    "itl": 154.5588967621909,
    "ttft": 1798457.0466987027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.049747296168935,
    "arrivals": 339810,
    "finished_requests": 91596,
    "scheduler_time": 138.52378508639944
}
#Debug simulation 
Total elapsed time: 15.061135374940932. Arrivals time: 0.4644056377001107 Scheduler time: 14.482084239367396 Scheduler overhead time: 0.04152452200651169 Adapter cache time: 0.013996066991239786 Engine time: 0.04102846560999751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 15.266364513896406,
    "estimated_duration": 3600.061163956459,
    "input_throughput": 6295.034991876069,
    "output_throughput": 5576.574976280822,
    "total_throughput": 11871.609968156892,
    "itl": 154.5591264364746,
    "ttft": 1798470.1463232075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1179297931189704,
    "arrivals": 339810,
    "finished_requests": 91592,
    "scheduler_time": 138.51952142637595
}
#Debug simulation 
Total elapsed time: 15.266485455911607. Arrivals time: 0.4757131398655474 Scheduler time: 14.672699415124953 Scheduler overhead time: 0.04275254998356104 Adapter cache time: 0.014059299603104591 Engine time: 0.0427387123927474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 15.22605646820739,
    "estimated_duration": 3600.063367420629,
    "input_throughput": 6295.031138920541,
    "output_throughput": 5576.571563067804,
    "total_throughput": 11871.602701988344,
    "itl": 154.55920940456713,
    "ttft": 1798471.1824805045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.120144898891455,
    "arrivals": 339810,
    "finished_requests": 91592,
    "scheduler_time": 138.51953510101035
}
#Debug simulation 
Total elapsed time: 15.226177549920976. Arrivals time: 0.45975200505927205 Scheduler time: 14.64979468472302 Scheduler overhead time: 0.04228012869134545 Adapter cache time: 0.014039159286767244 Engine time: 0.04223703593015671 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 15.553362761158496,
    "estimated_duration": 3600.1660119344165,
    "input_throughput": 6295.126370526064,
    "output_throughput": 5576.753942302854,
    "total_throughput": 11871.880312828918,
    "itl": 154.55950303629254,
    "ttft": 1798460.891648145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0738014987274098,
    "arrivals": 339810,
    "finished_requests": 91597,
    "scheduler_time": 138.52417644945785
}
#Debug simulation 
Total elapsed time: 15.553522909991443. Arrivals time: 0.7430040184408426 Scheduler time: 14.693445997312665 Scheduler overhead time: 0.042810259852558374 Adapter cache time: 0.013632765505462885 Engine time: 0.04244762612506747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 15.168781577143818,
    "estimated_duration": 3600.0746030540495,
    "input_throughput": 6295.051213876124,
    "output_throughput": 5576.5838805031535,
    "total_throughput": 11871.635094379277,
    "itl": 154.55937270243206,
    "ttft": 1798467.7965673218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1337263078242594,
    "arrivals": 339810,
    "finished_requests": 91593,
    "scheduler_time": 138.51956426887256
}
#Debug simulation 
Total elapsed time: 15.168939928989857. Arrivals time: 0.45907227881252766 Scheduler time: 14.594021318946034 Scheduler overhead time: 0.04198902752250433 Adapter cache time: 0.013995463494211435 Engine time: 0.041843660175800323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 15.107024838216603,
    "estimated_duration": 3600.041713738164,
    "input_throughput": 6295.398443166028,
    "output_throughput": 5576.89371303219,
    "total_throughput": 11872.292156198218,
    "itl": 154.5560620669324,
    "ttft": 1798413.0846962784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0255872511514468,
    "arrivals": 339810,
    "finished_requests": 91595,
    "scheduler_time": 138.5215242406875
}
#Debug simulation 
Total elapsed time: 15.107158202212304. Arrivals time: 0.4443035935983062 Scheduler time: 14.546834043227136 Scheduler overhead time: 0.041813460644334555 Adapter cache time: 0.014217004179954529 Engine time: 0.041732702404260635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 15.307603684719652,
    "estimated_duration": 3600.093545002778,
    "input_throughput": 6295.018092365295,
    "output_throughput": 5576.554539219483,
    "total_throughput": 11871.572631584779,
    "itl": 154.5599785841917,
    "ttft": 1798477.5816171751,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 343,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1485652546212117,
    "arrivals": 339810,
    "finished_requests": 91593,
    "scheduler_time": 138.51981291200397
}
#Debug simulation 
Total elapsed time: 15.307696724776179. Arrivals time: 0.6459340080618858 Scheduler time: 14.547366065438837 Scheduler overhead time: 0.04108539130538702 Adapter cache time: 0.013547430280596018 Engine time: 0.041762749664485455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 13.51224883692339,
    "estimated_duration": 3600.09977533237,
    "input_throughput": 6237.332407801641,
    "output_throughput": 5581.855852354747,
    "total_throughput": 11819.188260156388,
    "itl": 155.33235828576863,
    "ttft": 1795745.8082398186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1446224162308536,
    "arrivals": 335110,
    "finished_requests": 91417,
    "scheduler_time": 138.31179217240265
}
#Debug simulation 
Total elapsed time: 13.51241406518966. Arrivals time: 0.4803351159207523 Scheduler time: 12.919438671786338 Scheduler overhead time: 0.04034904297441244 Adapter cache time: 0.013382762670516968 Engine time: 0.04082553740590811 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.941498004831374,
    "estimated_duration": 3600.0317185100575,
    "input_throughput": 6236.842549068578,
    "output_throughput": 5581.386379649995,
    "total_throughput": 11818.228928718572,
    "itl": 155.33324915509928,
    "ttft": 1795826.728144227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2187932586297443,
    "arrivals": 335110,
    "finished_requests": 91410,
    "scheduler_time": 138.306790609815
}
#Debug simulation 
Total elapsed time: 12.94162047514692. Arrivals time: 0.43597838608548045 Scheduler time: 12.393737980164587 Scheduler overhead time: 0.040564259979873896 Adapter cache time: 0.013615183997899294 Engine time: 0.03993666497990489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 13.026708777062595,
    "estimated_duration": 3600.034786317412,
    "input_throughput": 6236.837234277867,
    "output_throughput": 5581.381623413125,
    "total_throughput": 11818.218857690992,
    "itl": 155.33333055835502,
    "ttft": 1795828.1857427172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2212389330938527,
    "arrivals": 335110,
    "finished_requests": 91410,
    "scheduler_time": 138.30682400950005
}
#Debug simulation 
Total elapsed time: 13.026825826149434. Arrivals time: 0.5046108602546155 Scheduler time: 12.409280460327864 Scheduler overhead time: 0.04058592300862074 Adapter cache time: 0.014147911220788956 Engine time: 0.04034983832389116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 13.027313048951328,
    "estimated_duration": 3600.141285047957,
    "input_throughput": 6237.260491208995,
    "output_throughput": 5581.791493422546,
    "total_throughput": 11819.05198463154,
    "itl": 155.3334292875996,
    "ttft": 1795769.3628084757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1689446297800172,
    "arrivals": 335110,
    "finished_requests": 91417,
    "scheduler_time": 138.31270898171434
}
#Debug simulation 
Total elapsed time: 13.027410948649049. Arrivals time: 0.4317340785637498 Scheduler time: 12.483014815486968 Scheduler overhead time: 0.04040806647390127 Adapter cache time: 0.013803018722683191 Engine time: 0.040574231185019016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 13.015978180337697,
    "estimated_duration": 3600.0540180696953,
    "input_throughput": 6236.803916636488,
    "output_throughput": 5581.351807263633,
    "total_throughput": 11818.155723900121,
    "itl": 155.33376552439992,
    "ttft": 1795836.3973097783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.236580895036464,
    "arrivals": 335110,
    "finished_requests": 91410,
    "scheduler_time": 138.3071101547805
}
#Debug simulation 
Total elapsed time: 13.016067251097411. Arrivals time: 0.41741474671289325 Scheduler time: 12.48603568924591 Scheduler overhead time: 0.04075382649898529 Adapter cache time: 0.01356205390766263 Engine time: 0.04037772258743644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 12.967793434858322,
    "estimated_duration": 3600.008508742098,
    "input_throughput": 6237.078869529015,
    "output_throughput": 5581.753196194889,
    "total_throughput": 11818.832065723904,
    "itl": 155.32982568344747,
    "ttft": 1795778.4692849747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.118278810293411,
    "arrivals": 335110,
    "finished_requests": 91413,
    "scheduler_time": 138.30887314390282
}
#Debug simulation 
Total elapsed time: 12.967893495690078. Arrivals time: 0.48098542587831616 Scheduler time: 12.375901467632502 Scheduler overhead time: 0.039777752477675676 Adapter cache time: 0.013451314065605402 Engine time: 0.039951430168002844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.802272283937782,
    "estimated_duration": 3600.102452620523,
    "input_throughput": 6237.1219418090395,
    "output_throughput": 5581.636707414588,
    "total_throughput": 11818.758649223628,
    "itl": 155.33550434846697,
    "ttft": 1795803.79023251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2521743645519048,
    "arrivals": 335110,
    "finished_requests": 91414,
    "scheduler_time": 138.3085830989961
}
#Debug simulation 
Total elapsed time: 12.802370303310454. Arrivals time: 0.3122969130054116 Scheduler time: 12.382401639595628 Scheduler overhead time: 0.03936228668317199 Adapter cache time: 0.011034513358026743 Engine time: 0.039621809497475624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.701008860953152,
    "estimated_duration": 3600.123785752372,
    "input_throughput": 6340.6903091332015,
    "output_throughput": 5571.40953857736,
    "total_throughput": 11912.099847710562,
    "itl": 154.112277231766,
    "ttft": 1794684.2264521348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.156864367206585,
    "arrivals": 332795,
    "finished_requests": 91682,
    "scheduler_time": 138.6069023577462
}
#Debug simulation 
Total elapsed time: 11.701124057173729. Arrivals time: 0.33750455221161246 Scheduler time: 11.255957744084299 Scheduler overhead time: 0.03884013928472996 Adapter cache time: 0.011981940362602472 Engine time: 0.0391091532073915 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 12.21124521503225,
    "estimated_duration": 3600.128136671669,
    "input_throughput": 6340.531540386612,
    "output_throughput": 5571.321419282371,
    "total_throughput": 11911.852959668982,
    "itl": 154.11454071326457,
    "ttft": 1794699.7490016762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2340222223568769,
    "arrivals": 332795,
    "finished_requests": 91682,
    "scheduler_time": 138.60417549879884
}
#Debug simulation 
Total elapsed time: 12.211346692871302. Arrivals time: 0.7995490664616227 Scheduler time: 11.29908919846639 Scheduler overhead time: 0.039551930502057076 Adapter cache time: 0.015173952095210552 Engine time: 0.04010014096274972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.847562160342932,
    "estimated_duration": 3600.1589998446693,
    "input_throughput": 6340.477184753471,
    "output_throughput": 5571.273657876052,
    "total_throughput": 11911.750842629523,
    "itl": 154.11473253689144,
    "ttft": 1794710.655395153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.236110988445587,
    "arrivals": 332795,
    "finished_requests": 91682,
    "scheduler_time": 138.60540856120895
}
#Debug simulation 
Total elapsed time: 11.84767308505252. Arrivals time: 0.4177036280743778 Scheduler time: 11.319331753533334 Scheduler overhead time: 0.03945844620466232 Adapter cache time: 0.013637043535709381 Engine time: 0.039686895441263914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 11.888025015126914,
    "estimated_duration": 3600.1552982799003,
    "input_throughput": 6340.634808422437,
    "output_throughput": 5571.3607714598575,
    "total_throughput": 11911.995579882296,
    "itl": 154.1128418220794,
    "ttft": 1794696.6905512062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1833564028702683,
    "arrivals": 332795,
    "finished_requests": 91682,
    "scheduler_time": 138.6071998553132
}
#Debug simulation 
Total elapsed time: 11.88813040824607. Arrivals time: 0.41488950373604894 Scheduler time: 11.359707477036864 Scheduler overhead time: 0.04135752376168966 Adapter cache time: 0.013967362698167562 Engine time: 0.04018619190901518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 12.143407818861306,
    "estimated_duration": 3600.003612065954,
    "input_throughput": 6340.901971178854,
    "output_throughput": 5571.595520841531,
    "total_throughput": 11912.497492020384,
    "itl": 154.1161058708413,
    "ttft": 1794620.5651289579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2512414566800047,
    "arrivals": 332795,
    "finished_requests": 91682,
    "scheduler_time": 138.59933347349008
}
#Debug simulation 
Total elapsed time: 12.143491524737328. Arrivals time: 0.7542578708380461 Scheduler time: 11.278039928991348 Scheduler overhead time: 0.03952478850260377 Adapter cache time: 0.014058192726224661 Engine time: 0.0397907686419785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 11.799244096968323,
    "estimated_duration": 3600.0945952056327,
    "input_throughput": 6340.741721175839,
    "output_throughput": 5571.454713082151,
    "total_throughput": 11912.19643425799,
    "itl": 154.11164245506396,
    "ttft": 1794671.4814566297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1302390114730192,
    "arrivals": 332795,
    "finished_requests": 91682,
    "scheduler_time": 138.60656129320034
}
#Debug simulation 
Total elapsed time: 11.799352622125298. Arrivals time: 0.41325877560302615 Scheduler time: 11.275045342277735 Scheduler overhead time: 0.03952509490773082 Adapter cache time: 0.013822536449879408 Engine time: 0.03987796511501074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 11.8280297992751,
    "estimated_duration": 3600.037022904177,
    "input_throughput": 6340.843123214625,
    "output_throughput": 5571.543812574253,
    "total_throughput": 11912.386935788878,
    "itl": 154.1170505425866,
    "ttft": 1794642.0101515613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 378,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2675894489139339,
    "arrivals": 332795,
    "finished_requests": 91682,
    "scheduler_time": 138.6001149423004
}
#Debug simulation 
Total elapsed time: 11.828144466038793. Arrivals time: 0.40980574395507574 Scheduler time: 11.307439758908004 Scheduler overhead time: 0.0392944710329175 Adapter cache time: 0.013954653404653072 Engine time: 0.03979311138391495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.725756325293332,
    "estimated_duration": 3600.005069463736,
    "input_throughput": 6336.348577252963,
    "output_throughput": 5572.329653132471,
    "total_throughput": 11908.678230385434,
    "itl": 153.97973808977713,
    "ttft": 1787259.7464680767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1966507078777122,
    "arrivals": 331579,
    "finished_requests": 91895,
    "scheduler_time": 138.50511102431543
}
#Debug simulation 
Total elapsed time: 10.725870064925402. Arrivals time: 0.46307032741606236 Scheduler time: 10.153599477838725 Scheduler overhead time: 0.03871125029399991 Adapter cache time: 0.013537423219531775 Engine time: 0.03928035404533148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.863282249774784,
    "estimated_duration": 3600.1528809151628,
    "input_throughput": 6336.139534774812,
    "output_throughput": 5572.158645357463,
    "total_throughput": 11908.298180132275,
    "itl": 153.98246428345945,
    "ttft": 1787321.5421147398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.274935852782806,
    "arrivals": 331579,
    "finished_requests": 91897,
    "scheduler_time": 138.50843889485702
}
#Debug simulation 
Total elapsed time: 10.863387101795524. Arrivals time: 0.4057554602622986 Scheduler time: 10.347407650202513 Scheduler overhead time: 0.03908234462141991 Adapter cache time: 0.01382507011294365 Engine time: 0.03947503911331296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.77317210379988,
    "estimated_duration": 3600.155317509628,
    "input_throughput": 6336.135246459126,
    "output_throughput": 5572.154874106026,
    "total_throughput": 11908.290120565152,
    "itl": 153.9825691482878,
    "ttft": 1787322.5470797527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2773629709519518,
    "arrivals": 331579,
    "finished_requests": 91897,
    "scheduler_time": 138.50844837113877
}
#Debug simulation 
Total elapsed time: 10.773290077690035. Arrivals time: 0.4024859080091119 Scheduler time: 10.26124079246074 Scheduler overhead time: 0.03884360194206238 Adapter cache time: 0.013690526597201824 Engine time: 0.039266202598810196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 11.083159209229052,
    "estimated_duration": 3600.0478069507303,
    "input_throughput": 6336.287244841075,
    "output_throughput": 5572.30239033727,
    "total_throughput": 11908.589635178345,
    "itl": 153.9806760278631,
    "ttft": 1787294.6081159657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 391,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2230442473408765,
    "arrivals": 331579,
    "finished_requests": 91896,
    "scheduler_time": 138.50591048618816
}
#Debug simulation 
Total elapsed time: 11.083252737298608. Arrivals time: 0.6831823978573084 Scheduler time: 10.290603116620332 Scheduler overhead time: 0.03892185399308801 Adapter cache time: 0.0135828354395926 Engine time: 0.03928435826674104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 10.76803783327341,
    "estimated_duration": 3600.1650695725903,
    "input_throughput": 6336.662224965244,
    "output_throughput": 5572.4925419549545,
    "total_throughput": 11909.154766920197,
    "itl": 153.98189951613162,
    "ttft": 1787289.1426293592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2869094166345942,
    "arrivals": 331579,
    "finished_requests": 91896,
    "scheduler_time": 138.50879454089042
}
#Debug simulation 
Total elapsed time: 10.768155538942665. Arrivals time: 0.40287786070257425 Scheduler time: 10.254810622893274 Scheduler overhead time: 0.03966972650960088 Adapter cache time: 0.013881588354706764 Engine time: 0.039269988890737295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.809662896674126,
    "estimated_duration": 3600.070474471824,
    "input_throughput": 6336.828726484017,
    "output_throughput": 5572.638964225647,
    "total_throughput": 11909.467690709664,
    "itl": 153.9775453608936,
    "ttft": 1787249.8399633004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.163129564716942,
    "arrivals": 331579,
    "finished_requests": 91896,
    "scheduler_time": 138.50876138098945
}
#Debug simulation 
Total elapsed time: 10.809748965781182. Arrivals time: 0.4041114947758615 Scheduler time: 10.296050048898906 Scheduler overhead time: 0.03883926570415497 Adapter cache time: 0.013933117501437664 Engine time: 0.03911887388676405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.800376504659653,
    "estimated_duration": 3600.0078357525867,
    "input_throughput": 6336.84842945098,
    "output_throughput": 5572.636759499193,
    "total_throughput": 11909.485188950173,
    "itl": 153.9825098803684,
    "ttft": 1787259.2734270787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 389,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3033831626549386,
    "arrivals": 331579,
    "finished_requests": 91895,
    "scheduler_time": 138.5020757522604
}
#Debug simulation 
Total elapsed time: 10.800480601843446. Arrivals time: 0.40079009160399437 Scheduler time: 10.289158531930298 Scheduler overhead time: 0.03943375311791897 Adapter cache time: 0.013674615416675806 Engine time: 0.03966142563149333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.1289321500808,
    "estimated_duration": 3600.0316195392693,
    "input_throughput": 6331.066615164041,
    "output_throughput": 5574.085486106967,
    "total_throughput": 11905.152101271007,
    "itl": 153.96732850319503,
    "ttft": 1790268.297769865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1446224162308536,
    "arrivals": 330904,
    "finished_requests": 91609,
    "scheduler_time": 138.47081412138783
}
#Debug simulation 
Total elapsed time: 10.1290496699512. Arrivals time: 0.4027868816629052 Scheduler time: 9.617918086238205 Scheduler overhead time: 0.03855411568656564 Adapter cache time: 0.013411326333880424 Engine time: 0.03863225830718875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.193941500037909,
    "estimated_duration": 3600.020634160068,
    "input_throughput": 6332.017595593161,
    "output_throughput": 5574.986656898041,
    "total_throughput": 11907.004252491202,
    "itl": 153.9735112494333,
    "ttft": 1790269.2249495494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2182547802454853,
    "arrivals": 330904,
    "finished_requests": 91618,
    "scheduler_time": 138.46799883831588
}
#Debug simulation 
Total elapsed time: 10.194171933457255. Arrivals time: 0.45687689539045095 Scheduler time: 9.628093344625086 Scheduler overhead time: 0.03914671391248703 Adapter cache time: 0.013180546928197145 Engine time: 0.03900953894481063 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.186733582057059,
    "estimated_duration": 3600.0250876034474,
    "input_throughput": 6332.009762513903,
    "output_throughput": 5574.979760310707,
    "total_throughput": 11906.989522824611,
    "itl": 153.9736216340757,
    "ttft": 1790271.8772420508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2202188992127845,
    "arrivals": 330904,
    "finished_requests": 91618,
    "scheduler_time": 138.46809546272485
}
#Debug simulation 
Total elapsed time: 10.186824024189264. Arrivals time: 0.3852586098946631 Scheduler time: 9.692185111809522 Scheduler overhead time: 0.03910667169839144 Adapter cache time: 0.013516858220100403 Engine time: 0.039013553876429796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 10.155903686303645,
    "estimated_duration": 3600.0916294219733,
    "input_throughput": 6331.065246708603,
    "output_throughput": 5574.0811806009415,
    "total_throughput": 11905.146427309544,
    "itl": 153.9681218599396,
    "ttft": 1790267.105525723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1726219876459816,
    "arrivals": 330904,
    "finished_requests": 91610,
    "scheduler_time": 138.47234501251936
}
#Debug simulation 
Total elapsed time: 10.156023541931063. Arrivals time: 0.4421855960972607 Scheduler time: 9.605215513147414 Scheduler overhead time: 0.03862324124202132 Adapter cache time: 0.01343913096934557 Engine time: 0.0387973734177649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 10.138748518191278,
    "estimated_duration": 3600.0567286915957,
    "input_throughput": 6330.934959539769,
    "output_throughput": 5574.010776017149,
    "total_throughput": 11904.945735556917,
    "itl": 153.9708847528956,
    "ttft": 1790292.6201370277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2386901199258913,
    "arrivals": 330904,
    "finished_requests": 91608,
    "scheduler_time": 138.46886601117643
}
#Debug simulation 
Total elapsed time: 10.13884390797466. Arrivals time: 0.4003473282791674 Scheduler time: 9.629687754437327 Scheduler overhead time: 0.03874215483665466 Adapter cache time: 0.013632834423333406 Engine time: 0.03885564813390374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 10.17055864771828,
    "estimated_duration": 3600.1505881846756,
    "input_throughput": 6331.871526378123,
    "output_throughput": 5574.844303976782,
    "total_throughput": 11906.715830354906,
    "itl": 153.96974013085816,
    "ttft": 1790272.6742159578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 373,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1152887599985088,
    "arrivals": 330904,
    "finished_requests": 91619,
    "scheduler_time": 138.47635006940706
}
#Debug simulation 
Total elapsed time: 10.170703914947808. Arrivals time: 0.39028809405863285 Scheduler time: 9.672162882518023 Scheduler overhead time: 0.03871179185807705 Adapter cache time: 0.013130195904523134 Engine time: 0.038764836732298136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 10.121438866946846,
    "estimated_duration": 3600.0853300059007,
    "input_throughput": 6330.88466265955,
    "output_throughput": 5573.96649261286,
    "total_throughput": 11904.851155272409,
    "itl": 153.97187353340894,
    "ttft": 1790298.6664771934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2554153735190654,
    "arrivals": 330904,
    "finished_requests": 91608,
    "scheduler_time": 138.46943943860208
}
#Debug simulation 
Total elapsed time: 10.121554078999907. Arrivals time: 0.3987095579504967 Scheduler time: 9.613659539725631 Scheduler overhead time: 0.03873288817703724 Adapter cache time: 0.013549655675888062 Engine time: 0.039175847079604864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 9.879586358089,
    "estimated_duration": 3600.0460530852956,
    "input_throughput": 6297.865267742679,
    "output_throughput": 5573.982028036174,
    "total_throughput": 11871.847295778853,
    "itl": 154.54196041726237,
    "ttft": 1783797.4285688673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3741589970258181,
    "arrivals": 325545,
    "finished_requests": 91445,
    "scheduler_time": 138.27979856546162
}
#Debug simulation 
Total elapsed time: 9.879684345796704. Arrivals time: 0.39466780284419656 Scheduler time: 9.37456428585574 Scheduler overhead time: 0.03878638381138444 Adapter cache time: 0.015002882108092308 Engine time: 0.038864392787218094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.82187232002616,
    "estimated_duration": 3600.172697630035,
    "input_throughput": 6296.932648515309,
    "output_throughput": 5574.223706882371,
    "total_throughput": 11871.15635539768,
    "itl": 154.56050413014918,
    "ttft": 1783738.6381461984,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4706826832145505,
    "arrivals": 325545,
    "finished_requests": 91439,
    "scheduler_time": 138.28018006728564
}
#Debug simulation 
Total elapsed time: 9.82197915809229. Arrivals time: 0.38107510562986135 Scheduler time: 9.332197264768183 Scheduler overhead time: 0.03830021992325783 Adapter cache time: 0.014364874456077814 Engine time: 0.03843014780431986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.873864807188511,
    "estimated_duration": 3600.0011501766685,
    "input_throughput": 6297.108821447876,
    "output_throughput": 5574.308219044651,
    "total_throughput": 11871.417040492528,
    "itl": 154.5609668226037,
    "ttft": 1783720.5947791177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4734640016593126,
    "arrivals": 325545,
    "finished_requests": 91436,
    "scheduler_time": 138.2733424436541
}
#Debug simulation 
Total elapsed time: 9.873965546954423. Arrivals time: 0.3828036575578153 Scheduler time: 9.381147692911327 Scheduler overhead time: 0.03854099661111832 Adapter cache time: 0.014786075334995985 Engine time: 0.038923879619687796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 9.927097546868026,
    "estimated_duration": 3600.10888342187,
    "input_throughput": 6297.755633004605,
    "output_throughput": 5573.937525169158,
    "total_throughput": 11871.693158173763,
    "itl": 154.5438988510445,
    "ttft": 1783803.7891096422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 449,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4054562614508874,
    "arrivals": 325545,
    "finished_requests": 91446,
    "scheduler_time": 138.28119922129957
}
#Debug simulation 
Total elapsed time: 9.927202912047505. Arrivals time: 0.3880345686338842 Scheduler time: 9.429784514475614 Scheduler overhead time: 0.03854631772264838 Adapter cache time: 0.014468844514340162 Engine time: 0.03867870336398482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_160_slots_64_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 9.920432001817971,
    "estimated_duration": 3600.020224471195,
    "input_throughput": 6297.075456938558,
    "output_throughput": 5574.278684211477,
    "total_throughput": 11871.354141150036,
    "itl": 154.5615238702946,
    "ttft": 1783729.5709724487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4915725469030485,
    "arrivals": 325545,
    "finished_requests": 91436,
    "scheduler_time": 138.27348432879654
}
#Debug simulation 
Total elapsed time: 9.920528877992183. Arrivals time: 0.3963739457540214 Scheduler time: 9.413591561373323 Scheduler overhead time: 0.038913825526833534 Adapter cache time: 0.014851280953735113 Engine time: 0.039067598059773445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_160_slots_64_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_160_slots_64_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 9.906656610779464,
    "estimated_duration": 3600.060563859225,
    "input_throughput": 6297.404890237985,
    "output_throughput": 5574.4578859214835,
    "total_throughput": 11871.862776159467,
    "itl": 154.54543497514487,
    "ttft": 1783724.4544570947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3485126830008702,
    "arrivals": 325545,
    "finished_requests": 91448,
    "scheduler_time": 138.2800687460104
}
#Debug simulation 
Total elapsed time: 9.906761853955686. Arrivals time: 0.3968010200187564 Scheduler time: 9.399309238418937 Scheduler overhead time: 0.038649654015898705 Adapter cache time: 0.015420542098581791 Engine time: 0.03888282552361488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_160_slots_64_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_160_slots_64_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.937758683227003,
    "estimated_duration": 3600.0177511954907,
    "input_throughput": 6297.113116309457,
    "output_throughput": 5574.054459408227,
    "total_throughput": 11871.167575717684,
    "itl": 154.55171658103865,
    "ttft": 1783775.1981658253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5106013825163245,
    "arrivals": 325545,
    "finished_requests": 91442,
    "scheduler_time": 138.2734514001273
}
#Debug simulation 
Total elapsed time: 9.937937093898654. Arrivals time: 0.3833048390224576 Scheduler time: 9.444216876290739 Scheduler overhead time: 0.03872759221121669 Adapter cache time: 0.014609983190894127 Engine time: 0.03914631297811866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 9.096551183145493,
    "estimated_duration": 3600.055059441752,
    "input_throughput": 6307.803804401069,
    "output_throughput": 5576.973870814447,
    "total_throughput": 11884.777675215517,
    "itl": 154.38259495212534,
    "ttft": 1778001.5542873542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4476107028802068,
    "arrivals": 323217,
    "finished_requests": 91940,
    "scheduler_time": 138.29178925339318
}
#Debug simulation 
Total elapsed time: 9.096650939900428. Arrivals time: 0.3884087661281228 Scheduler time: 8.598827259615064 Scheduler overhead time: 0.03811916150152683 Adapter cache time: 0.014792356640100479 Engine time: 0.03884255886077881 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.208082881756127,
    "estimated_duration": 3600.044720561335,
    "input_throughput": 6307.541367558167,
    "output_throughput": 5576.663502354944,
    "total_throughput": 11884.20486991311,
    "itl": 154.38788858087017,
    "ttft": 1778045.044685571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5421841241605632,
    "arrivals": 323217,
    "finished_requests": 91934,
    "scheduler_time": 138.28823376452118
}
#Debug simulation 
Total elapsed time: 9.208176997955889. Arrivals time: 0.3694256376475096 Scheduler time: 8.730245355982333 Scheduler overhead time: 0.03825433785095811 Adapter cache time: 0.014445173554122448 Engine time: 0.038248177617788315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.169718159828335,
    "estimated_duration": 3600.047689435661,
    "input_throughput": 6307.536165877733,
    "output_throughput": 5576.658903412228,
    "total_throughput": 11884.19506928996,
    "itl": 154.38798341406047,
    "ttft": 1778046.2824436917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.545142881255606,
    "arrivals": 323217,
    "finished_requests": 91934,
    "scheduler_time": 138.2882495983084
}
#Debug simulation 
Total elapsed time: 9.169815857894719. Arrivals time: 0.3916025925427675 Scheduler time: 8.667937479447573 Scheduler overhead time: 0.03834097879007459 Adapter cache time: 0.015254459343850613 Engine time: 0.039033280685544014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 9.196102555841208,
    "estimated_duration": 3600.1458693891022,
    "input_throughput": 6307.727748779628,
    "output_throughput": 5576.875417941579,
    "total_throughput": 11884.603166721208,
    "itl": 154.3852549194251,
    "ttft": 1778015.0796087834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4796690404391803,
    "arrivals": 323217,
    "finished_requests": 91941,
    "scheduler_time": 138.29432038317833
}
#Debug simulation 
Total elapsed time: 9.196198283694685. Arrivals time: 0.45483766542747617 Scheduler time: 8.63099215272814 Scheduler overhead time: 0.03853125870227814 Adapter cache time: 0.015169057063758373 Engine time: 0.03894257918000221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 9.168194802943617,
    "estimated_duration": 3600.072331841322,
    "input_throughput": 6307.492990949399,
    "output_throughput": 5576.620731320597,
    "total_throughput": 11884.113722269996,
    "itl": 154.38875850657402,
    "ttft": 1778058.173283027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5643832105770752,
    "arrivals": 323217,
    "finished_requests": 91934,
    "scheduler_time": 138.28856874493718
}
#Debug simulation 
Total elapsed time: 9.1683330652304. Arrivals time: 0.37461248552426696 Scheduler time: 8.684773938264698 Scheduler overhead time: 0.03839093167334795 Adapter cache time: 0.014450469054281712 Engine time: 0.0383649617433548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 9.198881800286472,
    "estimated_duration": 3600.0164608514824,
    "input_throughput": 6307.8714352958705,
    "output_throughput": 5577.0336659103095,
    "total_throughput": 11884.90510120618,
    "itl": 154.38186858637184,
    "ttft": 1777986.743557824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4142937894887158,
    "arrivals": 323217,
    "finished_requests": 91940,
    "scheduler_time": 138.29136068086189
}
#Debug simulation 
Total elapsed time: 9.19896519323811. Arrivals time: 0.3798385006375611 Scheduler time: 8.709206314757466 Scheduler overhead time: 0.03871954698115587 Adapter cache time: 0.014817509334534407 Engine time: 0.038707245606929064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 9.220031479373574,
    "estimated_duration": 3600.121347345383,
    "input_throughput": 6307.455724164375,
    "output_throughput": 5576.725633096804,
    "total_throughput": 11884.181357261179,
    "itl": 154.38975918778564,
    "ttft": 1778044.0962997414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.584503816403448,
    "arrivals": 323217,
    "finished_requests": 91936,
    "scheduler_time": 138.2898939936212
}
#Debug simulation 
Total elapsed time: 9.220140176359564. Arrivals time: 0.3763418635353446 Scheduler time: 8.734676416032016 Scheduler overhead time: 0.03849003976210952 Adapter cache time: 0.014414871577173471 Engine time: 0.03853962477296591 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.263210467994213,
    "estimated_duration": 3600.048703363708,
    "input_throughput": 6310.825455992367,
    "output_throughput": 5575.956786707944,
    "total_throughput": 11886.78224270031,
    "itl": 154.46519767406528,
    "ttft": 1781196.3917671056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5639092371496555,
    "arrivals": 321998,
    "finished_requests": 91621,
    "scheduler_time": 138.29628704066866
}
#Debug simulation 
Total elapsed time: 8.263323022052646. Arrivals time: 0.3593051587231457 Scheduler time: 7.794821565039456 Scheduler overhead time: 0.038233544677495956 Adapter cache time: 0.014897105749696493 Engine time: 0.03849838627502322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.430087298620492,
    "estimated_duration": 3600.010055010516,
    "input_throughput": 6311.10237272202,
    "output_throughput": 5576.263314059371,
    "total_throughput": 11887.365686781392,
    "itl": 154.46285894820045,
    "ttft": 1781197.1156968707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6865426750737293,
    "arrivals": 321998,
    "finished_requests": 91630,
    "scheduler_time": 138.29153683205686
}
#Debug simulation 
Total elapsed time: 8.430154980625957. Arrivals time: 0.36667100386694074 Scheduler time: 7.953605506569147 Scheduler overhead time: 0.038435479160398245 Adapter cache time: 0.015442635398358107 Engine time: 0.03855288168415427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.265549605246633,
    "estimated_duration": 3600.0137221390382,
    "input_throughput": 6311.095943962215,
    "output_throughput": 5576.257633838177,
    "total_throughput": 11887.353577800392,
    "itl": 154.46300688161188,
    "ttft": 1781198.7099148482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6901951693184776,
    "arrivals": 321998,
    "finished_requests": 91630,
    "scheduler_time": 138.29155146631476
}
#Debug simulation 
Total elapsed time: 8.265649940352887. Arrivals time: 0.36085262009873986 Scheduler time: 7.796291663777083 Scheduler overhead time: 0.03816422447562218 Adapter cache time: 0.014632738661020994 Engine time: 0.03818045323714614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 8.296984137967229,
    "estimated_duration": 3600.112162911432,
    "input_throughput": 6310.71421442236,
    "output_throughput": 5575.858498743624,
    "total_throughput": 11886.572713165984,
    "itl": 154.46654102904947,
    "ttft": 1781220.199291146,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5977855001483043,
    "arrivals": 321998,
    "finished_requests": 91621,
    "scheduler_time": 138.2977280006907
}
#Debug simulation 
Total elapsed time: 8.297188919968903. Arrivals time: 0.3823942872695625 Scheduler time: 7.804965854156762 Scheduler overhead time: 0.03838376607745886 Adapter cache time: 0.015367565676569939 Engine time: 0.038432972971349955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 8.24096815334633,
    "estimated_duration": 3600.0482929003665,
    "input_throughput": 6311.035339388652,
    "output_throughput": 5576.20408581435,
    "total_throughput": 11887.239425203,
    "itl": 154.46383634343383,
    "ttft": 1781225.4332004467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7108187902905088,
    "arrivals": 321998,
    "finished_requests": 91630,
    "scheduler_time": 138.2922032277689
}
#Debug simulation 
Total elapsed time: 8.241061427164823. Arrivals time: 0.3713907743804157 Scheduler time: 7.759770192671567 Scheduler overhead time: 0.038635211531072855 Adapter cache time: 0.01525262650102377 Engine time: 0.03844726737588644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.236656533088535,
    "estimated_duration": 3600.0075469245744,
    "input_throughput": 6310.897603369942,
    "output_throughput": 5576.02053283156,
    "total_throughput": 11886.9181362015,
    "itl": 154.46378270499582,
    "ttft": 1781177.60395812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 511,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5279157006949944,
    "arrivals": 321998,
    "finished_requests": 91621,
    "scheduler_time": 138.2959150475659
}
#Debug simulation 
Total elapsed time: 8.236742148641497. Arrivals time: 0.36205214308574796 Scheduler time: 7.7663854020647705 Scheduler overhead time: 0.03813593043014407 Adapter cache time: 0.014649213291704655 Engine time: 0.03807119093835354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.239914359990507,
    "estimated_duration": 3600.069763151677,
    "input_throughput": 6310.9977013639245,
    "output_throughput": 5576.170830207943,
    "total_throughput": 11887.168531571868,
    "itl": 154.46441879944294,
    "ttft": 1781235.2743761921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 518,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.732574195340274,
    "arrivals": 321998,
    "finished_requests": 91630,
    "scheduler_time": 138.29231918390067
}
#Debug simulation 
Total elapsed time: 8.240018913988024. Arrivals time: 0.36320206010714173 Scheduler time: 7.767973640467972 Scheduler overhead time: 0.0381418289616704 Adapter cache time: 0.014746713917702436 Engine time: 0.03832829277962446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.871511960402131,
    "estimated_duration": 3600.0959308806237,
    "input_throughput": 6332.757081399001,
    "output_throughput": 5573.544812482762,
    "total_throughput": 11906.301893881764,
    "itl": 154.19671516444382,
    "ttft": 1781269.1534846365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 493,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.508820457758864,
    "arrivals": 321377,
    "finished_requests": 91436,
    "scheduler_time": 138.4087595409734
}
#Debug simulation 
Total elapsed time: 7.871599227190018. Arrivals time: 0.3065987816080451 Scheduler time: 7.459412841126323 Scheduler overhead time: 0.03797052334994078 Adapter cache time: 0.012256148271262646 Engine time: 0.037965791299939156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.870847309008241,
    "estimated_duration": 3600.058493331324,
    "input_throughput": 6332.591551562287,
    "output_throughput": 5573.534440389814,
    "total_throughput": 11906.125991952102,
    "itl": 154.20017887252698,
    "ttft": 1781265.6522976607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 493,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6044367019692485,
    "arrivals": 321377,
    "finished_requests": 91434,
    "scheduler_time": 138.4040414508669
}
#Debug simulation 
Total elapsed time: 7.870944547001272. Arrivals time: 0.36142982821911573 Scheduler time: 7.403021463192999 Scheduler overhead time: 0.037920926697552204 Adapter cache time: 0.013213150203227997 Engine time: 0.03792620403692126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 8.0811358098872,
    "estimated_duration": 3600.0622522443246,
    "input_throughput": 6332.584939548649,
    "output_throughput": 5573.528620926261,
    "total_throughput": 11906.11356047491,
    "itl": 154.2003193331067,
    "ttft": 1781267.2514018177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 493,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6080367431975997,
    "arrivals": 321377,
    "finished_requests": 91434,
    "scheduler_time": 138.40406516795602
}
#Debug simulation 
Total elapsed time: 8.08127315202728. Arrivals time: 0.47596819372847676 Scheduler time: 7.494761339854449 Scheduler overhead time: 0.038197843823581934 Adapter cache time: 0.016052445396780968 Engine time: 0.038578766863793135 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.990354967769235,
    "estimated_duration": 3600.13273159649,
    "input_throughput": 6332.692347676281,
    "output_throughput": 5573.487839461403,
    "total_throughput": 11906.180187137685,
    "itl": 154.19767955905368,
    "ttft": 1781284.8083915324,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 493,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5406958322925437,
    "arrivals": 321377,
    "finished_requests": 91436,
    "scheduler_time": 138.40911480784865
}
#Debug simulation 
Total elapsed time: 7.990465431008488. Arrivals time: 0.3756341082043946 Scheduler time: 7.506311185657978 Scheduler overhead time: 0.03822058532387018 Adapter cache time: 0.014705951325595379 Engine time: 0.03807857260107994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.949185973964632,
    "estimated_duration": 3600.145357036943,
    "input_throughput": 6332.270711081199,
    "output_throughput": 5573.559678855516,
    "total_throughput": 11905.830389936715,
    "itl": 154.20321530321087,
    "ttft": 1781356.0947939684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6206412936188326,
    "arrivals": 321377,
    "finished_requests": 91435,
    "scheduler_time": 138.40648466907737
}
#Debug simulation 
Total elapsed time: 7.949307051952928. Arrivals time: 0.3619023282080889 Scheduler time: 7.478910914622247 Scheduler overhead time: 0.03824284486472607 Adapter cache time: 0.014387077186256647 Engine time: 0.03828873857855797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 8.007997365668416,
    "estimated_duration": 3600.0483345956,
    "input_throughput": 6332.460256415098,
    "output_throughput": 5573.710443599922,
    "total_throughput": 11906.17070001502,
    "itl": 154.1961098301903,
    "ttft": 1781309.8339407083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.468114694796953,
    "arrivals": 321377,
    "finished_requests": 91436,
    "scheduler_time": 138.40751795152306
}
#Debug simulation 
Total elapsed time: 8.008094750810415. Arrivals time: 0.4253711658529937 Scheduler time: 7.4729710216633976 Scheduler overhead time: 0.03841490065678954 Adapter cache time: 0.015277363825589418 Engine time: 0.03847684757784009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.908406383357942,
    "estimated_duration": 3600.1645334049863,
    "input_throughput": 6332.236982080044,
    "output_throughput": 5573.52999114799,
    "total_throughput": 11905.766973228034,
    "itl": 154.2038481465033,
    "ttft": 1781363.081883851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 491,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6411391608044497,
    "arrivals": 321377,
    "finished_requests": 91435,
    "scheduler_time": 138.40656072412312
}
#Debug simulation 
Total elapsed time: 7.908515110146254. Arrivals time: 0.37609408563002944 Scheduler time: 7.423120636958629 Scheduler overhead time: 0.03811542363837361 Adapter cache time: 0.015158053953200579 Engine time: 0.038572171702980995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.859119074884802,
    "estimated_duration": 3600.0952062245365,
    "input_throughput": 6304.888815372157,
    "output_throughput": 5574.4434106358285,
    "total_throughput": 11879.332226007986,
    "itl": 154.55397458278054,
    "ttft": 1777943.1859827377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.83629264635968,
    "arrivals": 318402,
    "finished_requests": 91413,
    "scheduler_time": 138.1966590425613
}
#Debug simulation 
Total elapsed time: 7.859223295934498. Arrivals time: 0.4020368177443743 Scheduler time: 7.347547841258347 Scheduler overhead time: 0.038268566597253084 Adapter cache time: 0.015525993425399065 Engine time: 0.03830415662378073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.787520332727581,
    "estimated_duration": 3600.0841097678367,
    "input_throughput": 6304.885471540764,
    "output_throughput": 5574.376150143383,
    "total_throughput": 11879.261621684147,
    "itl": 154.55751553687594,
    "ttft": 1777971.4697142295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9558339230436885,
    "arrivals": 318402,
    "finished_requests": 91412,
    "scheduler_time": 138.1921979347564
}
#Debug simulation 
Total elapsed time: 7.787622096948326. Arrivals time: 0.3479756386950612 Scheduler time: 7.3308569067157805 Scheduler overhead time: 0.03811756754294038 Adapter cache time: 0.01512668002396822 Engine time: 0.03798902127891779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.797218170017004,
    "estimated_duration": 3600.087946278333,
    "input_throughput": 6304.878752605102,
    "output_throughput": 5574.370209690557,
    "total_throughput": 11879.248962295658,
    "itl": 154.55767022696287,
    "ttft": 1777973.055157875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9596613170951727,
    "arrivals": 318402,
    "finished_requests": 91412,
    "scheduler_time": 138.19220705117937
}
#Debug simulation 
Total elapsed time: 7.797416269779205. Arrivals time: 0.37856444250792265 Scheduler time: 7.308231664355844 Scheduler overhead time: 0.038126864936202765 Adapter cache time: 0.016690250020474195 Engine time: 0.03831891668960452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.8449805388227105,
    "estimated_duration": 3600.151066689978,
    "input_throughput": 6304.790987804019,
    "output_throughput": 5574.356916764396,
    "total_throughput": 11879.147904568415,
    "itl": 154.55548037146067,
    "ttft": 1777976.5007037483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8773836219031252,
    "arrivals": 318402,
    "finished_requests": 91413,
    "scheduler_time": 138.19743451384622
}
#Debug simulation 
Total elapsed time: 7.845065933652222. Arrivals time: 0.43249756935983896 Scheduler time: 7.302194930613041 Scheduler overhead time: 0.03823158238083124 Adapter cache time: 0.016552741173654795 Engine time: 0.03810687456279993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.8384304801002145,
    "estimated_duration": 3600.1234231634094,
    "input_throughput": 6304.816622107717,
    "output_throughput": 5574.315277881823,
    "total_throughput": 11879.131899989541,
    "itl": 154.55883847598216,
    "ttft": 1777984.897927615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9838060440868182,
    "arrivals": 318402,
    "finished_requests": 91412,
    "scheduler_time": 138.19279032916086
}
#Debug simulation 
Total elapsed time: 7.838526329025626. Arrivals time: 0.3642447032034397 Scheduler time: 7.364843411836773 Scheduler overhead time: 0.03809692244976759 Adapter cache time: 0.015735498629510403 Engine time: 0.038076817989349365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.764631978236139,
    "estimated_duration": 3600.0042445663776,
    "input_throughput": 6305.025343861505,
    "output_throughput": 5574.499816295975,
    "total_throughput": 11879.525160157478,
    "itl": 154.55102450935456,
    "ttft": 1777922.7992390508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7940301769412785,
    "arrivals": 318402,
    "finished_requests": 91412,
    "scheduler_time": 138.19438774579817
}
#Debug simulation 
Total elapsed time: 7.76473437435925. Arrivals time: 0.34667967492714524 Scheduler time: 7.309396010823548 Scheduler overhead time: 0.03790465975180268 Adapter cache time: 0.015051372349262238 Engine time: 0.03827625885605812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.8045804323628545,
    "estimated_duration": 3600.149471248532,
    "input_throughput": 6304.771005001715,
    "output_throughput": 5574.274946156705,
    "total_throughput": 11879.04595115842,
    "itl": 154.55943740608666,
    "ttft": 1777992.3849368156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 600,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.009459816515441,
    "arrivals": 318402,
    "finished_requests": 91412,
    "scheduler_time": 138.19294720841955
}
#Debug simulation 
Total elapsed time: 7.804679938126355. Arrivals time: 0.3917702669277787 Scheduler time: 7.303020097780973 Scheduler overhead time: 0.037965136114507914 Adapter cache time: 0.016274306923151016 Engine time: 0.03814690234139562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.701262080576271,
    "estimated_duration": 3600.145538992319,
    "input_throughput": 6317.006008141972,
    "output_throughput": 5573.896050218212,
    "total_throughput": 11890.902058360183,
    "itl": 154.31835426035698,
    "ttft": 1768185.336495254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9617726438609273,
    "arrivals": 317160,
    "finished_requests": 91697,
    "scheduler_time": 138.2342422734197
}
#Debug simulation 
Total elapsed time: 7.701326988637447. Arrivals time: 0.34660332556813955 Scheduler time: 7.24590769642964 Scheduler overhead time: 0.037331117782741785 Adapter cache time: 0.016344374511390924 Engine time: 0.03780872607603669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.3365448419936,
    "estimated_duration": 3600.148358522142,
    "input_throughput": 6317.001060849511,
    "output_throughput": 5573.891684907513,
    "total_throughput": 11890.892745757024,
    "itl": 154.32296891696984,
    "ttft": 1768204.3041951994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.09231822596165,
    "arrivals": 317160,
    "finished_requests": 91697,
    "scheduler_time": 138.2299598137678
}
#Debug simulation 
Total elapsed time: 7.3366468427702785. Arrivals time: 0.3921204577200115 Scheduler time: 6.836314055137336 Scheduler overhead time: 0.03748860443010926 Adapter cache time: 0.015517567284405231 Engine time: 0.037883794400841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.372784596867859,
    "estimated_duration": 3600.153624421394,
    "input_throughput": 6316.991821051817,
    "output_throughput": 5573.8835320465205,
    "total_throughput": 11890.875353098338,
    "itl": 154.3232362685917,
    "ttft": 1768205.0256088723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0959120047092528,
    "arrivals": 317160,
    "finished_requests": 91697,
    "scheduler_time": 138.23005016226838
}
#Debug simulation 
Total elapsed time: 7.372914790175855. Arrivals time: 0.413403345271945 Scheduler time: 6.85057988576591 Scheduler overhead time: 0.037526852916926146 Adapter cache time: 0.016149365808814764 Engine time: 0.03784664999693632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.3636384243145585,
    "estimated_duration": 3600.0481413788066,
    "input_throughput": 6317.176911775917,
    "output_throughput": 5574.046849360872,
    "total_throughput": 11891.22376113679,
    "itl": 154.32062870000973,
    "ttft": 1768170.3675853272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.008964780999801,
    "arrivals": 317160,
    "finished_requests": 91697,
    "scheduler_time": 138.2289039788845
}
#Debug simulation 
Total elapsed time: 7.363743822090328. Arrivals time: 0.37083213310688734 Scheduler time: 6.882599802222103 Scheduler overhead time: 0.037732959259301424 Adapter cache time: 0.016707955859601498 Engine time: 0.03835383662953973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.336431805044413,
    "estimated_duration": 3600.01469366784,
    "input_throughput": 6317.1631049176785,
    "output_throughput": 5573.9605827985115,
    "total_throughput": 11891.12368771619,
    "itl": 154.32386235963807,
    "ttft": 1768175.7328747637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1215657771378784,
    "arrivals": 317160,
    "finished_requests": 91695,
    "scheduler_time": 138.22361423193922
}
#Debug simulation 
Total elapsed time: 7.336531430017203. Arrivals time: 0.3528387392871082 Scheduler time: 6.87485783547163 Scheduler overhead time: 0.03748892340809107 Adapter cache time: 0.01604986237362027 Engine time: 0.03783274255692959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.375378866214305,
    "estimated_duration": 3600.065417128631,
    "input_throughput": 6317.1465973356835,
    "output_throughput": 5574.020101002795,
    "total_throughput": 11891.166698338478,
    "itl": 154.31632021702117,
    "ttft": 1768171.015870713,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9166222390322634,
    "arrivals": 317160,
    "finished_requests": 91697,
    "scheduler_time": 138.23256392189828
}
#Debug simulation 
Total elapsed time: 7.375490024220198. Arrivals time: 0.3464903165586293 Scheduler time: 6.920209250412881 Scheduler overhead time: 0.037695491686463356 Adapter cache time: 0.015769397374242544 Engine time: 0.03779292246326804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.312518895603716,
    "estimated_duration": 3600.0862515194153,
    "input_throughput": 6317.110038794678,
    "output_throughput": 5573.9878430775925,
    "total_throughput": 11891.09788187227,
    "itl": 154.32669459071028,
    "ttft": 1768193.2241174458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1499861328676344,
    "arrivals": 317160,
    "finished_requests": 91697,
    "scheduler_time": 138.22550489990965
}
#Debug simulation 
Total elapsed time: 7.3126141410321. Arrivals time: 0.3362220451235771 Scheduler time: 6.8682646178640425 Scheduler overhead time: 0.03745444491505623 Adapter cache time: 0.015618061181157827 Engine time: 0.03770637745037675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 270, 33, 33, 33, 17280, 33, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 33, 17280, 270, 17280, 17280, 17280, 270, 17280, 33, 270, 270, 270, 17280, 33, 33, 17280, 33, 17280, 33, 33, 270, 33, 270, 17280, 33, 33, 270, 33, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 33, 17280, 17280, 17280, 270, 270, 33, 33, 33, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 33, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 270, 33, 33, 270, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 17280, 270, 17280, 33, 33, 33, 270, 270, 270, 17280, 17280, 270, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 949179 . Total input tokens: 211564065 . Total output tokens: 189683248
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.1124801291152835,
    "estimated_duration": 3600.150643028589,
    "input_throughput": 6286.045291972047,
    "output_throughput": 5574.782277198344,
    "total_throughput": 11860.827569170391,
    "itl": 154.60675404612047,
    "ttft": 1773272.8450360822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0444058129471143,
    "arrivals": 316555,
    "finished_requests": 91589,
    "scheduler_time": 138.05040007763392
}
#Debug simulation 
Total elapsed time: 7.1126129371114075. Arrivals time: 0.33350291941314936 Scheduler time: 6.670685806777328 Scheduler overhead time: 0.03724893229082227 Adapter cache time: 0.0160779501311481 Engine time: 0.03758944245055318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 270, 33, 33, 33, 17280, 33, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 33, 17280, 270, 17280, 17280, 17280, 270, 17280, 33, 270, 270, 270, 17280, 33, 33, 17280, 33, 17280, 33, 33, 270, 33, 270, 17280, 33, 33, 270, 33, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 33, 17280, 17280, 17280, 270, 270, 33, 33, 33, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 33, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 270, 33, 33, 270, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 17280, 270, 17280, 33, 33, 33, 270, 270, 270, 17280, 17280, 270, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 949179 . Total input tokens: 211564065 . Total output tokens: 189683248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.178935714997351,
    "estimated_duration": 3600.0461104727196,
    "input_throughput": 6285.653657094033,
    "output_throughput": 5574.549709688246,
    "total_throughput": 11860.20336678228,
    "itl": 154.61235392601276,
    "ttft": 1773309.5535382358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.182855871566578,
    "arrivals": 316555,
    "finished_requests": 91583,
    "scheduler_time": 138.04180240592754
}
#Debug simulation 
Total elapsed time: 7.17902130400762. Arrivals time: 0.40587629098445177 Scheduler time: 6.664490425027907 Scheduler overhead time: 0.03754006512463093 Adapter cache time: 0.015971585176885128 Engine time: 0.037783550564199686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 270, 33, 33, 33, 17280, 33, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 33, 17280, 270, 17280, 17280, 17280, 270, 17280, 33, 270, 270, 270, 17280, 33, 33, 17280, 33, 17280, 33, 33, 270, 33, 270, 17280, 33, 33, 270, 33, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 33, 17280, 17280, 17280, 270, 270, 33, 33, 33, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 33, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 270, 33, 33, 270, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 17280, 270, 17280, 33, 33, 33, 270, 270, 270, 17280, 17280, 270, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 949179 . Total input tokens: 211564065 . Total output tokens: 189683248
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.126910260878503,
    "estimated_duration": 3600.049826881619,
    "input_throughput": 6285.647168278513,
    "output_throughput": 5574.543954960633,
    "total_throughput": 11860.191123239145,
    "itl": 154.61264428682767,
    "ttft": 1773311.2393289704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1861809534952097,
    "arrivals": 316555,
    "finished_requests": 91583,
    "scheduler_time": 138.04183808471836
}
#Debug simulation 
Total elapsed time: 7.127065199892968. Arrivals time: 0.3858815128915012 Scheduler time: 6.631927529815584 Scheduler overhead time: 0.037251430563628674 Adapter cache time: 0.01701522432267666 Engine time: 0.03753495588898659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 270, 33, 33, 33, 17280, 33, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 33, 17280, 270, 17280, 17280, 17280, 270, 17280, 33, 270, 270, 270, 17280, 33, 33, 17280, 33, 17280, 33, 33, 270, 33, 270, 17280, 33, 33, 270, 33, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 33, 17280, 17280, 17280, 270, 270, 33, 33, 33, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 33, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 270, 33, 33, 270, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 17280, 270, 17280, 33, 33, 33, 270, 270, 270, 17280, 17280, 270, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 949179 . Total input tokens: 211564065 . Total output tokens: 189683248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.176068305037916,
    "estimated_duration": 3600.0461510589535,
    "input_throughput": 6285.978859838628,
    "output_throughput": 5574.815754541516,
    "total_throughput": 11860.794614380144,
    "itl": 154.60800074814486,
    "ttft": 1773259.2243353392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0896961389621573,
    "arrivals": 316555,
    "finished_requests": 91587,
    "scheduler_time": 138.04483646372867
}
#Debug simulation 
Total elapsed time: 7.176159111317247. Arrivals time: 0.34845448518171906 Scheduler time: 6.719322717748582 Scheduler overhead time: 0.037356574553996325 Adapter cache time: 0.01605683332309127 Engine time: 0.03748616995289922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 270, 33, 33, 33, 17280, 33, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 33, 17280, 270, 17280, 17280, 17280, 270, 17280, 33, 270, 270, 270, 17280, 33, 33, 17280, 33, 17280, 33, 33, 270, 33, 270, 17280, 33, 33, 270, 33, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 33, 17280, 17280, 17280, 270, 270, 33, 33, 33, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 33, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 270, 33, 33, 270, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 17280, 270, 17280, 33, 33, 33, 270, 270, 270, 17280, 17280, 270, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 949179 . Total input tokens: 211564065 . Total output tokens: 189683248
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 7.485753894317895,
    "estimated_duration": 3600.0825258793934,
    "input_throughput": 6285.590076708726,
    "output_throughput": 5574.493322232336,
    "total_throughput": 11860.083398941062,
    "itl": 154.61367932380006,
    "ttft": 1773323.0433997565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.214852816797796,
    "arrivals": 316555,
    "finished_requests": 91583,
    "scheduler_time": 138.0420883064153
}
#Debug simulation 
Total elapsed time: 7.48582434700802. Arrivals time: 0.6749054933898151 Scheduler time: 6.702525936998427 Scheduler overhead time: 0.03749515721574426 Adapter cache time: 0.015929386485368013 Engine time: 0.03756777523085475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 270, 33, 33, 33, 17280, 33, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 33, 17280, 270, 17280, 17280, 17280, 270, 17280, 33, 270, 270, 270, 17280, 33, 33, 17280, 33, 17280, 33, 33, 270, 33, 270, 17280, 33, 33, 270, 33, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 33, 17280, 17280, 17280, 270, 270, 33, 33, 33, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 33, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 270, 33, 33, 270, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 17280, 270, 17280, 33, 33, 33, 270, 270, 270, 17280, 17280, 270, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 949179 . Total input tokens: 211564065 . Total output tokens: 189683248
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.183773933909833,
    "estimated_duration": 3600.0940066582,
    "input_throughput": 6286.016964597713,
    "output_throughput": 5574.80025879376,
    "total_throughput": 11860.817223391474,
    "itl": 154.60463767213412,
    "ttft": 1773262.1688959796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9973535969946192,
    "arrivals": 316555,
    "finished_requests": 91588,
    "scheduler_time": 138.04983201437628
}
#Debug simulation 
Total elapsed time: 7.18389101093635. Arrivals time: 0.415169695392251 Scheduler time: 6.659667531494051 Scheduler overhead time: 0.03740441985428333 Adapter cache time: 0.016486556734889746 Engine time: 0.03779647871851921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_160_slots_64_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 270, 33, 33, 33, 17280, 33, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 33, 17280, 270, 17280, 17280, 17280, 270, 17280, 33, 270, 270, 270, 17280, 33, 33, 17280, 33, 17280, 33, 33, 270, 33, 270, 17280, 33, 33, 270, 33, 33, 33, 33, 33, 17280, 270, 270, 17280, 270, 33, 17280, 17280, 17280, 270, 270, 33, 33, 33, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 33, 33, 17280, 270, 17280, 17280, 33, 17280, 33, 270, 33, 33, 270, 270, 17280, 17280, 17280, 33, 17280, 270, 270, 33, 33, 33, 33, 17280, 270, 17280, 270, 17280, 17280, 17280, 33, 270, 270, 17280, 270, 17280, 33, 33, 33, 270, 270, 270, 17280, 17280, 270, 17280, 33, 270, 270, 33, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 33, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 949179 . Total input tokens: 211564065 . Total output tokens: 189683248
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.187910606153309,
    "estimated_duration": 3600.1288362204136,
    "input_throughput": 6285.651439007156,
    "output_throughput": 5574.573553614704,
    "total_throughput": 11860.22499262186,
    "itl": 154.61504271812504,
    "ttft": 1773313.1076961344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 668,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.243273172527555,
    "arrivals": 316555,
    "finished_requests": 91585,
    "scheduler_time": 138.04298323628714
}
#Debug simulation 
Total elapsed time: 7.188009440898895. Arrivals time: 0.4014168060384691 Scheduler time: 6.677208709064871 Scheduler overhead time: 0.03753359103575349 Adapter cache time: 0.016493133269250393 Engine time: 0.037866903468966484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 135, 66, 66, 66, 17280, 66, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 66, 17280, 135, 17280, 17280, 17280, 135, 17280, 66, 135, 135, 135, 17280, 66, 66, 17280, 66, 17280, 66, 66, 135, 66, 135, 17280, 66, 66, 135, 66, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 66, 17280, 17280, 17280, 135, 135, 66, 66, 66, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 66, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 135, 66, 66, 135, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 17280, 135, 17280, 66, 66, 66, 135, 135, 135, 17280, 17280, 135, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 943773 . Total input tokens: 210368094 . Total output tokens: 188601248
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.803411113098264,
    "estimated_duration": 3600.1085578118386,
    "input_throughput": 6346.834167102977,
    "output_throughput": 5595.141278807179,
    "total_throughput": 11941.975445910155,
    "itl": 153.84338655480883,
    "ttft": 1764958.7495910244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.800346285698531,
    "arrivals": 314729,
    "finished_requests": 92094,
    "scheduler_time": 138.66070605106216
}
#Debug simulation 
Total elapsed time: 6.803539340849966. Arrivals time: 0.36002286383882165 Scheduler time: 6.3322623944841325 Scheduler overhead time: 0.03683093888685107 Adapter cache time: 0.019715999718755484 Engine time: 0.037512924056500196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 135, 66, 66, 66, 17280, 66, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 66, 17280, 135, 17280, 17280, 17280, 135, 17280, 66, 135, 135, 135, 17280, 66, 66, 17280, 66, 17280, 66, 66, 135, 66, 135, 17280, 66, 66, 135, 66, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 66, 17280, 17280, 17280, 135, 135, 66, 66, 66, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 66, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 135, 66, 66, 135, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 17280, 135, 17280, 66, 66, 66, 135, 135, 135, 17280, 17280, 135, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 943773 . Total input tokens: 210368094 . Total output tokens: 188601248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.727521202061325,
    "estimated_duration": 3600.0494730256837,
    "input_throughput": 6346.25667541125,
    "output_throughput": 5594.49450650822,
    "total_throughput": 11940.751181919471,
    "itl": 153.85177831086247,
    "ttft": 1765038.0803123326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9859563547209884,
    "arrivals": 314729,
    "finished_requests": 92082,
    "scheduler_time": 138.65201754603552
}
#Debug simulation 
Total elapsed time: 6.727613137103617. Arrivals time: 0.34734614565968513 Scheduler time: 6.271211220417172 Scheduler overhead time: 0.03643549419939518 Adapter cache time: 0.01808786392211914 Engine time: 0.03731973748654127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 135, 66, 66, 66, 17280, 66, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 66, 17280, 135, 17280, 17280, 17280, 135, 17280, 66, 135, 135, 135, 17280, 66, 66, 17280, 66, 17280, 66, 66, 135, 66, 135, 17280, 66, 66, 135, 66, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 66, 17280, 17280, 17280, 135, 135, 66, 66, 66, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 66, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 135, 66, 66, 135, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 17280, 135, 17280, 66, 66, 66, 135, 135, 135, 17280, 17280, 135, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 943773 . Total input tokens: 210368094 . Total output tokens: 188601248
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.778050555847585,
    "estimated_duration": 3600.0548651462686,
    "input_throughput": 6346.247170061321,
    "output_throughput": 5594.486127138983,
    "total_throughput": 11940.733297200304,
    "itl": 153.85202894167395,
    "ttft": 1765040.2530927027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.99121521327642,
    "arrivals": 314729,
    "finished_requests": 92082,
    "scheduler_time": 138.65203361972863
}
#Debug simulation 
Total elapsed time: 6.778137825895101. Arrivals time: 0.3352452926337719 Scheduler time: 6.333783523179591 Scheduler overhead time: 0.03670784179121256 Adapter cache time: 0.01774640381336212 Engine time: 0.037363949697464705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 135, 66, 66, 66, 17280, 66, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 66, 17280, 135, 17280, 17280, 17280, 135, 17280, 66, 135, 135, 135, 17280, 66, 66, 17280, 66, 17280, 66, 66, 135, 66, 135, 17280, 66, 66, 135, 66, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 66, 17280, 17280, 17280, 135, 135, 66, 66, 66, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 66, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 135, 66, 66, 135, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 17280, 135, 17280, 66, 66, 66, 135, 135, 135, 17280, 17280, 135, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 943773 . Total input tokens: 210368094 . Total output tokens: 188601248
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.725934900343418,
    "estimated_duration": 3600.0808481552535,
    "input_throughput": 6346.776631893751,
    "output_throughput": 5594.967960322696,
    "total_throughput": 11941.744592216448,
    "itl": 153.84706554917514,
    "ttft": 1764957.8807171725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8629691638704116,
    "arrivals": 314729,
    "finished_requests": 92090,
    "scheduler_time": 138.65776382407353
}
#Debug simulation 
Total elapsed time: 6.726049717981368. Arrivals time: 0.32716940995305777 Scheduler time: 6.290610528551042 Scheduler overhead time: 0.03660939261317253 Adapter cache time: 0.017377992160618305 Engine time: 0.03714427864179015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 135, 66, 66, 66, 17280, 66, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 66, 17280, 135, 17280, 17280, 17280, 135, 17280, 66, 135, 135, 135, 17280, 66, 66, 17280, 66, 17280, 66, 66, 135, 66, 135, 17280, 66, 66, 135, 66, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 66, 17280, 17280, 17280, 135, 135, 66, 66, 66, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 66, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 135, 66, 66, 135, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 17280, 135, 17280, 66, 66, 66, 135, 135, 135, 17280, 17280, 135, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 943773 . Total input tokens: 210368094 . Total output tokens: 188601248
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.742913676891476,
    "estimated_duration": 3600.096274740546,
    "input_throughput": 6346.17417325778,
    "output_throughput": 5594.421777359688,
    "total_throughput": 11940.59595061747,
    "itl": 153.85316761166683,
    "ttft": 1765056.4917674891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.02906710298732,
    "arrivals": 314729,
    "finished_requests": 92082,
    "scheduler_time": 138.65228316018582
}
#Debug simulation 
Total elapsed time: 6.743074795696884. Arrivals time: 0.33405472384765744 Scheduler time: 6.299535451456904 Scheduler overhead time: 0.03667838405817747 Adapter cache time: 0.018234984017908573 Engine time: 0.03734193416312337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 135, 66, 66, 66, 17280, 66, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 66, 17280, 135, 17280, 17280, 17280, 135, 17280, 66, 135, 135, 135, 17280, 66, 66, 17280, 66, 17280, 66, 66, 135, 66, 135, 17280, 66, 66, 135, 66, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 66, 17280, 17280, 17280, 135, 135, 66, 66, 66, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 66, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 135, 66, 66, 135, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 17280, 135, 17280, 66, 66, 66, 135, 135, 135, 17280, 17280, 135, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 943773 . Total input tokens: 210368094 . Total output tokens: 188601248
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.062556912191212,
    "estimated_duration": 3600.0349997808307,
    "input_throughput": 6346.963849348981,
    "output_throughput": 5595.255602022288,
    "total_throughput": 11942.21945137127,
    "itl": 153.84112753944703,
    "ttft": 1764913.046260218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7358960198354305,
    "arrivals": 314729,
    "finished_requests": 92094,
    "scheduler_time": 138.66031850811783
}
#Debug simulation 
Total elapsed time: 7.06274305190891. Arrivals time: 0.6227073078043759 Scheduler time: 6.32982524856925 Scheduler overhead time: 0.036721233278512955 Adapter cache time: 0.018838380463421345 Engine time: 0.03735388955101371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_160_slots_64_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 135, 66, 66, 66, 17280, 66, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 66, 17280, 135, 17280, 17280, 17280, 135, 17280, 66, 135, 135, 135, 17280, 66, 66, 17280, 66, 17280, 66, 66, 135, 66, 135, 17280, 66, 66, 135, 66, 66, 66, 66, 66, 17280, 135, 135, 17280, 135, 66, 17280, 17280, 17280, 135, 135, 66, 66, 66, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 66, 66, 17280, 135, 17280, 17280, 66, 17280, 66, 135, 66, 66, 135, 135, 17280, 17280, 17280, 66, 17280, 135, 135, 66, 66, 66, 66, 17280, 135, 17280, 135, 17280, 17280, 17280, 66, 135, 135, 17280, 135, 17280, 66, 66, 66, 135, 135, 135, 17280, 17280, 135, 17280, 66, 135, 135, 66, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 66, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 943773 . Total input tokens: 210368094 . Total output tokens: 188601248
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.799482135102153,
    "estimated_duration": 3600.1376052325027,
    "input_throughput": 6346.101317570197,
    "output_throughput": 5594.357551980099,
    "total_throughput": 11940.458869550297,
    "itl": 153.8547754967006,
    "ttft": 1765073.0468506136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 915,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0681765305623725,
    "arrivals": 314729,
    "finished_requests": 92082,
    "scheduler_time": 138.65246390042896
}
#Debug simulation 
Total elapsed time: 6.7996322847902775. Arrivals time: 0.34383193496614695 Scheduler time: 6.3458843301050365 Scheduler overhead time: 0.036705443635582924 Adapter cache time: 0.018244258128106594 Engine time: 0.03753068624064326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 135, 33, 33, 33, 17280, 33, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 33, 17280, 135, 17280, 17280, 17280, 135, 17280, 33, 135, 135, 135, 17280, 33, 33, 17280, 33, 17280, 33, 33, 135, 33, 135, 17280, 33, 33, 135, 33, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 33, 17280, 17280, 17280, 135, 135, 33, 33, 33, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 33, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 135, 33, 33, 135, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 17280, 135, 17280, 33, 33, 33, 135, 135, 135, 17280, 17280, 135, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 942024 . Total input tokens: 209985148 . Total output tokens: 188245389
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.730784409213811,
    "estimated_duration": 3600.048404852327,
    "input_throughput": 6326.631877866173,
    "output_throughput": 5621.612190747796,
    "total_throughput": 11948.244068613969,
    "itl": 153.64485079860918,
    "ttft": 1759606.3062831017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5432653152081706,
    "arrivals": 314142,
    "finished_requests": 92545,
    "scheduler_time": 139.0717950781827
}
#Debug simulation 
Total elapsed time: 6.730876258108765. Arrivals time: 0.3311756057664752 Scheduler time: 6.292303423397243 Scheduler overhead time: 0.03639836749061942 Adapter cache time: 0.016529262997210026 Engine time: 0.03740087989717722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 135, 33, 33, 33, 17280, 33, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 33, 17280, 135, 17280, 17280, 17280, 135, 17280, 33, 135, 135, 135, 17280, 33, 33, 17280, 33, 17280, 33, 33, 135, 33, 135, 17280, 33, 33, 135, 33, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 33, 17280, 17280, 17280, 135, 135, 33, 33, 33, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 33, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 135, 33, 33, 135, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 17280, 135, 17280, 33, 33, 33, 135, 135, 135, 17280, 17280, 135, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 942024 . Total input tokens: 209985148 . Total output tokens: 188245389
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.776085658930242,
    "estimated_duration": 3600.0667614164145,
    "input_throughput": 6326.325457097718,
    "output_throughput": 5621.251588133876,
    "total_throughput": 11947.577045231594,
    "itl": 153.65058395709238,
    "ttft": 1759610.9837896123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7168139359378323,
    "arrivals": 314142,
    "finished_requests": 92540,
    "scheduler_time": 139.0667057677044
}
#Debug simulation 
Total elapsed time: 6.776196522638202. Arrivals time: 0.37588210171088576 Scheduler time: 6.292452177032828 Scheduler overhead time: 0.036363129038363695 Adapter cache time: 0.01697651157155633 Engine time: 0.037447316106408834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 135, 33, 33, 33, 17280, 33, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 33, 17280, 135, 17280, 17280, 17280, 135, 17280, 33, 135, 135, 135, 17280, 33, 33, 17280, 33, 17280, 33, 33, 135, 33, 135, 17280, 33, 33, 135, 33, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 33, 17280, 17280, 17280, 135, 135, 33, 33, 33, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 33, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 135, 33, 33, 135, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 17280, 135, 17280, 33, 33, 33, 135, 135, 135, 17280, 17280, 135, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 942024 . Total input tokens: 209985148 . Total output tokens: 188245389
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.901607836596668,
    "estimated_duration": 3600.0706767373954,
    "input_throughput": 6326.318576789797,
    "output_throughput": 5621.245474641598,
    "total_throughput": 11947.564051431395,
    "itl": 153.65075025064846,
    "ttft": 1759612.354287281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7207207402214264,
    "arrivals": 314142,
    "finished_requests": 92540,
    "scheduler_time": 139.0667142843627
}
#Debug simulation 
Total elapsed time: 6.901688226033002. Arrivals time: 0.5446050320751965 Scheduler time: 6.249172612093389 Scheduler overhead time: 0.036254351027309895 Adapter cache time: 0.017256165854632854 Engine time: 0.03726195218041539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 135, 33, 33, 33, 17280, 33, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 33, 17280, 135, 17280, 17280, 17280, 135, 17280, 33, 135, 135, 135, 17280, 33, 33, 17280, 33, 17280, 33, 33, 135, 33, 135, 17280, 33, 33, 135, 33, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 33, 17280, 17280, 17280, 135, 135, 33, 33, 33, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 33, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 135, 33, 33, 135, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 17280, 135, 17280, 33, 33, 33, 135, 135, 135, 17280, 17280, 135, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 942024 . Total input tokens: 209985148 . Total output tokens: 188245389
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.766773843206465,
    "estimated_duration": 3600.1086361479533,
    "input_throughput": 6326.526030717249,
    "output_throughput": 5621.5181388677065,
    "total_throughput": 11948.044169584955,
    "itl": 153.64706046990332,
    "ttft": 1759628.26323051,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.604041628048268,
    "arrivals": 314142,
    "finished_requests": 92545,
    "scheduler_time": 139.0721551384115
}
#Debug simulation 
Total elapsed time: 6.766864690929651. Arrivals time: 0.36946467217057943 Scheduler time: 6.287400881294161 Scheduler overhead time: 0.03661951143294573 Adapter cache time: 0.01858766283839941 Engine time: 0.03765025408938527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 135, 33, 33, 33, 17280, 33, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 33, 17280, 135, 17280, 17280, 17280, 135, 17280, 33, 135, 135, 135, 17280, 33, 33, 17280, 33, 17280, 33, 33, 135, 33, 135, 17280, 33, 33, 135, 33, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 33, 17280, 17280, 17280, 135, 135, 33, 33, 33, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 33, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 135, 33, 33, 135, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 17280, 135, 17280, 33, 33, 33, 135, 135, 135, 17280, 17280, 135, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 942024 . Total input tokens: 209985148 . Total output tokens: 188245389
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.790540207177401,
    "estimated_duration": 3600.1679378396348,
    "input_throughput": 6326.26043930246,
    "output_throughput": 5621.178886489334,
    "total_throughput": 11947.439325791795,
    "itl": 153.65373773483668,
    "ttft": 1759637.9978472604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7554287852719423,
    "arrivals": 314142,
    "finished_requests": 92541,
    "scheduler_time": 139.06829631367646
}
#Debug simulation 
Total elapsed time: 6.790635171812028. Arrivals time: 0.3676148052327335 Scheduler time: 6.313322649337351 Scheduler overhead time: 0.03679687064141035 Adapter cache time: 0.018268082290887833 Engine time: 0.037442405708134174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_160_slots_64_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 135, 33, 33, 33, 17280, 33, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 17280, 17280, 135, 135, 135, 17280, 17280, 33, 17280, 135, 17280, 17280, 17280, 135, 17280, 33, 135, 135, 135, 17280, 33, 33, 17280, 33, 17280, 33, 33, 135, 33, 135, 17280, 33, 33, 135, 33, 33, 33, 33, 33, 17280, 135, 135, 17280, 135, 33, 17280, 17280, 17280, 135, 135, 33, 33, 33, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 33, 33, 17280, 135, 17280, 17280, 33, 17280, 33, 135, 33, 33, 135, 135, 17280, 17280, 17280, 33, 17280, 135, 135, 33, 33, 33, 33, 17280, 135, 17280, 135, 17280, 17280, 17280, 33, 135, 135, 17280, 135, 17280, 33, 33, 33, 135, 135, 135, 17280, 17280, 135, 17280, 33, 135, 135, 33, 17280, 17280, 17280, 17280, 17280, 17280, 135, 135, 135, 135, 135, 17280, 33, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 942024 . Total input tokens: 209985148 . Total output tokens: 188245389
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.783107972238213,
    "estimated_duration": 3600.1300501059427,
    "input_throughput": 6326.663115775425,
    "output_throughput": 5621.595808574869,
    "total_throughput": 11948.258924350293,
    "itl": 153.64261911551597,
    "ttft": 1759600.1164674985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 831,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4847317950636567,
    "arrivals": 314142,
    "finished_requests": 92547,
    "scheduler_time": 139.0770886639868
}
#Debug simulation 
Total elapsed time: 6.7832570699974895. Arrivals time: 0.4129674630239606 Scheduler time: 6.2618752918206155 Scheduler overhead time: 0.03649910306558013 Adapter cache time: 0.01754639483988285 Engine time: 0.037262617610394955 
