INFO 06-01 00:47:13 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:14 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_256_slots_96_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_256_slots_96_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.4266147050075233,
    "estimated_duration": 3599.967636358328,
    "input_throughput": 865.392779794953,
    "output_throughput": 760.4829477771166,
    "total_throughput": 1625.8757275720695,
    "itl": 21.89206915885675,
    "ttft": 5146.032090610851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 18.678844192254843,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.426731513813138. Arrivals time: 0.0445933947339654 Scheduler time: 0.9290542872622609 Scheduler overhead time: 0.13829202251508832 Adapter cache time: 0.10556952422484756 Engine time: 0.14134675450623035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_256_slots_96_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_256_slots_96_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.409896841738373,
    "estimated_duration": 3599.9701561809848,
    "input_throughput": 865.3921740576166,
    "output_throughput": 760.4824154721032,
    "total_throughput": 1625.8745895297197,
    "itl": 21.90940846895321,
    "ttft": 5146.167327786362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6244,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 20.950189076809465,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4100147588178515. Arrivals time: 0.04362645745277405 Scheduler time: 0.9178983033634722 Scheduler overhead time: 0.13900522934272885 Adapter cache time: 0.1058729668147862 Engine time: 0.1362082208506763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.2878315597772598,
    "estimated_duration": 3599.772507175637,
    "input_throughput": 725.3461141767096,
    "output_throughput": 655.0763958831869,
    "total_throughput": 1380.4225100598965,
    "itl": 21.202978096010394,
    "ttft": 2724.2259111752783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4457,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.640593874707085,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2879173788242042. Arrivals time: 0.038661559112370014 Scheduler time: 0.8060629805549979 Scheduler overhead time: 0.1418480989523232 Adapter cache time: 0.09205253887921572 Engine time: 0.1396655347198248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.2799280830658972,
    "estimated_duration": 3599.782747201307,
    "input_throughput": 725.3440508402946,
    "output_throughput": 655.0745324376458,
    "total_throughput": 1380.4185832779403,
    "itl": 21.208922016781507,
    "ttft": 2724.3531449555207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.534610754876297,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2800163999199867. Arrivals time: 0.03924802038818598 Scheduler time: 0.8001089091412723 Scheduler overhead time: 0.14038523053750396 Adapter cache time: 0.0916737481020391 Engine time: 0.1395956319756806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.3010602407157421,
    "estimated_duration": 3599.790903761298,
    "input_throughput": 725.3424073247619,
    "output_throughput": 655.0730481417893,
    "total_throughput": 1380.415455466551,
    "itl": 21.209264997641462,
    "ttft": 2724.2824045025504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.560501530020264,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.30118504492566. Arrivals time: 0.03933696821331978 Scheduler time: 0.8172197337262332 Scheduler overhead time: 0.1414997335523367 Adapter cache time: 0.09281445574015379 Engine time: 0.1408203192986548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.2961533898487687,
    "estimated_duration": 3599.791856023759,
    "input_throughput": 725.3422154480163,
    "output_throughput": 655.0728748535832,
    "total_throughput": 1380.4150903015995,
    "itl": 21.204582727179904,
    "ttft": 2724.123080956107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.902922392567683,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.296215693000704. Arrivals time: 0.03942400682717562 Scheduler time: 0.8110362119041383 Scheduler overhead time: 0.14201991213485599 Adapter cache time: 0.09254016866907477 Engine time: 0.14169670082628727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.2942117829807103,
    "estimated_duration": 3599.7865144923485,
    "input_throughput": 725.3432917446833,
    "output_throughput": 655.0738468813196,
    "total_throughput": 1380.417138626003,
    "itl": 21.21080426490343,
    "ttft": 2724.3620167862073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4457,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.761804170290764,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2942917249165475. Arrivals time: 0.03915004013106227 Scheduler time: 0.8096160870045424 Scheduler overhead time: 0.1420476082712412 Adapter cache time: 0.0940818265080452 Engine time: 0.13990842597559094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.3020275286398828,
    "estimated_duration": 3599.7898907743897,
    "input_throughput": 725.3426114373309,
    "output_throughput": 655.0732324804429,
    "total_throughput": 1380.4158439177738,
    "itl": 21.199504770684356,
    "ttft": 2724.203799103635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.320674063790335,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3021204266697168. Arrivals time: 0.039165085181593895 Scheduler time: 0.8179025119170547 Scheduler overhead time: 0.14148150756955147 Adapter cache time: 0.0935367876663804 Engine time: 0.14053857000544667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_256_slots_96_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.3173525999300182,
    "estimated_duration": 3599.787280924909,
    "input_throughput": 725.3431373114702,
    "output_throughput": 655.0737074092102,
    "total_throughput": 1380.4168447206803,
    "itl": 21.211225443286953,
    "ttft": 2724.1982900445587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.934453276953572,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3174508181400597. Arrivals time: 0.03971165930852294 Scheduler time: 0.8282387014478445 Scheduler overhead time: 0.14135291893035173 Adapter cache time: 0.09516142308712006 Engine time: 0.14350314904004335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.0346343661658466,
    "estimated_duration": 3599.4900189694895,
    "input_throughput": 468.4596959884758,
    "output_throughput": 410.2558952011688,
    "total_throughput": 878.7155911896446,
    "itl": 19.619232997945662,
    "ttft": 3215.0273176161927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.152417338890391,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0347541458904743. Arrivals time: 0.03053115727379918 Scheduler time: 0.5582749149762094 Scheduler overhead time: 0.15079216891899705 Adapter cache time: 0.07057085074484348 Engine time: 0.15063970582559705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.0244582388550043,
    "estimated_duration": 3599.5097987309764,
    "input_throughput": 468.4571217432116,
    "output_throughput": 410.25364079314954,
    "total_throughput": 878.7107625363611,
    "itl": 19.62249953036936,
    "ttft": 3215.0778645452174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.888091420049653,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0245239571668208. Arrivals time: 0.030647987499833107 Scheduler time: 0.5542411496862769 Scheduler overhead time: 0.1475750571116805 Adapter cache time: 0.07082654163241386 Engine time: 0.14779949467629194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.0345724397338927,
    "estimated_duration": 3599.4995411651284,
    "input_throughput": 468.45845671484255,
    "output_throughput": 410.25480990115653,
    "total_throughput": 878.7132666159991,
    "itl": 19.62422308174567,
    "ttft": 3214.9975601837727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3643,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.909074062741597,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0346624776721. Arrivals time: 0.030704551376402378 Scheduler time: 0.5562685653567314 Scheduler overhead time: 0.1471009235829115 Adapter cache time: 0.07022112840786576 Engine time: 0.15672367811203003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.0300432778894901,
    "estimated_duration": 3599.495849709469,
    "input_throughput": 468.4589371414616,
    "output_throughput": 410.25523063714377,
    "total_throughput": 878.7141677786053,
    "itl": 19.62037084417584,
    "ttft": 3215.0798124081994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.384831870797061,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0300983637571335. Arrivals time: 0.030640164855867624 Scheduler time: 0.5597441419959068 Scheduler overhead time: 0.14806158002465963 Adapter cache time: 0.07142147421836853 Engine time: 0.14696863014250994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.0303951352834702,
    "estimated_duration": 3599.499307401874,
    "input_throughput": 468.4584871380665,
    "output_throughput": 410.25483654444537,
    "total_throughput": 878.7133236825118,
    "itl": 19.625503036876758,
    "ttft": 3215.181361403221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.067926522958286,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.030489391181618. Arrivals time: 0.030375200789421797 Scheduler time: 0.5583684393204749 Scheduler overhead time: 0.14762822771444917 Adapter cache time: 0.07087082043290138 Engine time: 0.14968618657439947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.0315663199871778,
    "estimated_duration": 3599.5066019112924,
    "input_throughput": 468.45753779271877,
    "output_throughput": 410.25400515056276,
    "total_throughput": 878.7115429432815,
    "itl": 19.61754100432998,
    "ttft": 3214.99861150178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.895743274624037,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0316715501248837. Arrivals time: 0.030940250027924776 Scheduler time: 0.5586945242248476 Scheduler overhead time: 0.14799628825858235 Adapter cache time: 0.07044900953769684 Engine time: 0.14989218348637223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_256_slots_96_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.028650812804699,
    "estimated_duration": 3599.50233667142,
    "input_throughput": 468.4580928927248,
    "output_throughput": 410.2544912821379,
    "total_throughput": 878.7125841748626,
    "itl": 19.62521428745687,
    "ttft": 3215.183188683514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3644,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.218453805296866,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0287150009535253. Arrivals time: 0.03103042533621192 Scheduler time: 0.558077649679035 Scheduler overhead time: 0.14740257011726499 Adapter cache time: 0.07145335571840405 Engine time: 0.1473282682709396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 17.90700483089313,
    "estimated_duration": 3600.0044005180175,
    "input_throughput": 4649.449872225574,
    "output_throughput": 4126.566344714014,
    "total_throughput": 8776.016216939588,
    "itl": 208.23328585907714,
    "ttft": 2216759.124395255,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8034067734424637,
    "arrivals": 1724092,
    "finished_requests": 68054,
    "scheduler_time": 94.50269711183485
}
#Debug simulation 
Total elapsed time: 17.907167112920433. Arrivals time: 0.31608955515548587 Scheduler time: 17.480004027020186 Scheduler overhead time: 0.036438554525375366 Adapter cache time: 0.02401224384084344 Engine time: 0.036175139248371124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 18.588831791188568,
    "estimated_duration": 3600.2111259550393,
    "input_throughput": 4662.940981147795,
    "output_throughput": 4132.4126501201745,
    "total_throughput": 8795.35363126797,
    "itl": 208.07195283045962,
    "ttft": 2218211.189693257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.82432586348849,
    "arrivals": 1724092,
    "finished_requests": 68220,
    "scheduler_time": 94.53358367761416
}
#Debug simulation 
Total elapsed time: 18.588958860374987. Arrivals time: 0.44406689889729023 Scheduler time: 18.03335295105353 Scheduler overhead time: 0.036607093177735806 Adapter cache time: 0.024088731035590172 Engine time: 0.036386581137776375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 11.527953326236457,
    "estimated_duration": 3600.013104073967,
    "input_throughput": 4452.426570853579,
    "output_throughput": 3950.118954819177,
    "total_throughput": 8402.545525672756,
    "itl": 177.3463990350572,
    "ttft": 2237821.396627252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.568816767986802,
    "arrivals": 1724092,
    "finished_requests": 65181,
    "scheduler_time": 97.69231169141085
}
#Debug simulation 
Total elapsed time: 11.528029187116772. Arrivals time: 0.7404258139431477 Scheduler time: 10.667868048418313 Scheduler overhead time: 0.03533841576427221 Adapter cache time: 0.033153264317661524 Engine time: 0.03573998948559165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 18.509099398273975,
    "estimated_duration": 3600.0952404413265,
    "input_throughput": 4663.091079207687,
    "output_throughput": 4132.545670701811,
    "total_throughput": 8795.636749909498,
    "itl": 208.06593644165883,
    "ttft": 2218165.8787197596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 866,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.70869338836984,
    "arrivals": 1724092,
    "finished_requests": 68220,
    "scheduler_time": 94.53333063898923
}
#Debug simulation 
Total elapsed time: 18.509245231281966. Arrivals time: 0.4257390573620796 Scheduler time: 17.973051096778363 Scheduler overhead time: 0.03593836538493633 Adapter cache time: 0.023900679778307676 Engine time: 0.03623385215178132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 11.53617392713204,
    "estimated_duration": 3600.0704000910423,
    "input_throughput": 4452.355709375752,
    "output_throughput": 3950.0560876921677,
    "total_throughput": 8402.41179706792,
    "itl": 177.34905077848137,
    "ttft": 2237842.8043070394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.626034740805594,
    "arrivals": 1724092,
    "finished_requests": 65181,
    "scheduler_time": 97.69238973571254
}
#Debug simulation 
Total elapsed time: 11.536281414795667. Arrivals time: 0.2918758993037045 Scheduler time: 11.122966254595667 Scheduler overhead time: 0.03579800110310316 Adapter cache time: 0.03458129661157727 Engine time: 0.03539775172248483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 18.095770357642323,
    "estimated_duration": 3600.1760638633723,
    "input_throughput": 4649.427334405843,
    "output_throughput": 4126.603181750338,
    "total_throughput": 8776.030516156181,
    "itl": 208.22972052553928,
    "ttft": 2216799.0303378343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7388860701303326,
    "arrivals": 1724092,
    "finished_requests": 68058,
    "scheduler_time": 94.50871029077297
}
#Debug simulation 
Total elapsed time: 18.095996672753245. Arrivals time: 0.4675886742770672 Scheduler time: 17.517150414176285 Scheduler overhead time: 0.03634574916213751 Adapter cache time: 0.02448257803916931 Engine time: 0.03582834778353572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 11.61971485009417,
    "estimated_duration": 3600.1303496966625,
    "input_throughput": 4452.2815684578045,
    "output_throughput": 3949.990311100313,
    "total_throughput": 8402.271879558119,
    "itl": 177.35182875343702,
    "ttft": 2237865.3079897994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.685893543139094,
    "arrivals": 1724092,
    "finished_requests": 65181,
    "scheduler_time": 97.69248053904684
}
#Debug simulation 
Total elapsed time: 11.619792277924716. Arrivals time: 0.2974853916093707 Scheduler time: 11.200163327623159 Scheduler overhead time: 0.035808492451906204 Adapter cache time: 0.03476151172071695 Engine time: 0.035837441217154264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 18.512138061225414,
    "estimated_duration": 3600.0776310879937,
    "input_throughput": 4695.506245205973,
    "output_throughput": 4113.536017145413,
    "total_throughput": 8809.042262351386,
    "itl": 207.56761741134036,
    "ttft": 2217294.1483958657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.537144339720305,
    "arrivals": 1601458,
    "finished_requests": 68345,
    "scheduler_time": 94.34698691735277
}
#Debug simulation 
Total elapsed time: 18.512217386160046. Arrivals time: 0.5044198399409652 Scheduler time: 17.89536233851686 Scheduler overhead time: 0.03734841290861368 Adapter cache time: 0.02296681748703122 Engine time: 0.03768722526729107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 16.79603081708774,
    "estimated_duration": 3600.167893498058,
    "input_throughput": 4696.655683902238,
    "output_throughput": 4113.494825267928,
    "total_throughput": 8810.150509170166,
    "itl": 207.7474377781888,
    "ttft": 2217393.3324560123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 880,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8706811161199632,
    "arrivals": 1601458,
    "finished_requests": 68315,
    "scheduler_time": 94.25839809592892
}
#Debug simulation 
Total elapsed time: 16.796163900289685. Arrivals time: 0.5142047540284693 Scheduler time: 16.168704287614673 Scheduler overhead time: 0.03694180212914944 Adapter cache time: 0.024124133866280317 Engine time: 0.03739563003182411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 10.486787508241832,
    "estimated_duration": 3600.019876894877,
    "input_throughput": 4488.975214753208,
    "output_throughput": 3937.962979312175,
    "total_throughput": 8426.938194065384,
    "itl": 177.19932618585003,
    "ttft": 2237870.296804926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.3804807210526775,
    "arrivals": 1601458,
    "finished_requests": 65281,
    "scheduler_time": 97.47218882379164
}
#Debug simulation 
Total elapsed time: 10.486950549297035. Arrivals time: 0.475841814186424 Scheduler time: 9.893632885534316 Scheduler overhead time: 0.03462513955309987 Adapter cache time: 0.03219477133825421 Engine time: 0.03504800470545888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 18.67982393410057,
    "estimated_duration": 3600.13867549362,
    "input_throughput": 4695.426627609628,
    "output_throughput": 4113.466267509684,
    "total_throughput": 8808.892895119312,
    "itl": 207.57082292841326,
    "ttft": 2217314.9034081553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5980615274584635,
    "arrivals": 1601458,
    "finished_requests": 68345,
    "scheduler_time": 94.34711413515242
}
#Debug simulation 
Total elapsed time: 18.679949474986643. Arrivals time: 0.5439085750840604 Scheduler time: 18.02300555817783 Scheduler overhead time: 0.03730815835297108 Adapter cache time: 0.022850502282381058 Engine time: 0.03823054302483797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 10.509775892831385,
    "estimated_duration": 3600.0760626412757,
    "input_throughput": 4488.905156115941,
    "output_throughput": 3937.9015202248024,
    "total_throughput": 8426.806676340744,
    "itl": 177.20191155865137,
    "ttft": 2237891.6493420755,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.436566909793742,
    "arrivals": 1601458,
    "finished_requests": 65281,
    "scheduler_time": 97.47228838149869
}
#Debug simulation 
Total elapsed time: 10.509880094788969. Arrivals time: 0.48017825093120337 Scheduler time: 9.910794449038804 Scheduler overhead time: 0.03538830718025565 Adapter cache time: 0.03337726043537259 Engine time: 0.03466722369194031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 18.466018163599074,
    "estimated_duration": 3600.019087103977,
    "input_throughput": 4695.582604146278,
    "output_throughput": 4113.602912009305,
    "total_throughput": 8809.185516155583,
    "itl": 207.5646401411522,
    "ttft": 2217273.1325128344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 829,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4787516944738526,
    "arrivals": 1601458,
    "finished_requests": 68345,
    "scheduler_time": 94.34683557846226
}
#Debug simulation 
Total elapsed time: 18.46616996685043. Arrivals time: 0.5196198537014425 Scheduler time: 17.83612052956596 Scheduler overhead time: 0.0373230199329555 Adapter cache time: 0.021923867519944906 Engine time: 0.03651131270453334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 10.545306080952287,
    "estimated_duration": 3600.133873408071,
    "input_throughput": 4488.833073505052,
    "output_throughput": 3937.8382856022986,
    "total_throughput": 8426.671359107351,
    "itl": 177.20461828683304,
    "ttft": 2237913.4587031985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4942878977582,
    "arrivals": 1601458,
    "finished_requests": 65281,
    "scheduler_time": 97.47237816038512
}
#Debug simulation 
Total elapsed time: 10.545423746109009. Arrivals time: 0.483432506211102 Scheduler time: 9.942648782860488 Scheduler overhead time: 0.0349973626434803 Adapter cache time: 0.03307436406612396 Engine time: 0.035508225671947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 16.661782789044082,
    "estimated_duration": 3600.0811105202692,
    "input_throughput": 4642.300683493421,
    "output_throughput": 4120.639381332205,
    "total_throughput": 8762.940064825627,
    "itl": 208.72218184375734,
    "ttft": 2217771.7042514607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 754,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3076077589253403,
    "arrivals": 1509498,
    "finished_requests": 67960,
    "scheduler_time": 94.23372206836537
}
#Debug simulation 
Total elapsed time: 16.661963262129575. Arrivals time: 0.4331786590628326 Scheduler time: 16.12425897596404 Scheduler overhead time: 0.03545997245237231 Adapter cache time: 0.019147363025695086 Engine time: 0.035595616325736046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 16.655748405959457,
    "estimated_duration": 3600.2169872454033,
    "input_throughput": 4641.520513680348,
    "output_throughput": 4119.787238532073,
    "total_throughput": 8761.307752212422,
    "itl": 208.75359911729146,
    "ttft": 2217851.76528162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4574399125552797,
    "arrivals": 1509498,
    "finished_requests": 67952,
    "scheduler_time": 94.22208824939842
}
#Debug simulation 
Total elapsed time: 16.655865263659507. Arrivals time: 0.4862340781837702 Scheduler time: 16.062950875610113 Scheduler overhead time: 0.03618722967803478 Adapter cache time: 0.02019193023443222 Engine time: 0.035946912597864866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 9.096661826130003,
    "estimated_duration": 3600.1448457742686,
    "input_throughput": 4414.593767985443,
    "output_throughput": 3928.979417759486,
    "total_throughput": 8343.573185744928,
    "itl": 178.33892811478984,
    "ttft": 2237244.08394372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1044,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4124951818026177,
    "arrivals": 1509498,
    "finished_requests": 64704,
    "scheduler_time": 97.13459710940693
}
#Debug simulation 
Total elapsed time: 9.096788680180907. Arrivals time: 0.46333232056349516 Scheduler time: 8.522443625610322 Scheduler overhead time: 0.03360214829444885 Adapter cache time: 0.027896236162632704 Engine time: 0.03420938178896904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 16.764910776168108,
    "estimated_duration": 3600.114541046534,
    "input_throughput": 4641.652594515049,
    "output_throughput": 4119.904472730575,
    "total_throughput": 8761.557067245625,
    "itl": 208.74843962638184,
    "ttft": 2217805.3902356317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3552910829451674,
    "arrivals": 1509498,
    "finished_requests": 67952,
    "scheduler_time": 94.22179088012003
}
#Debug simulation 
Total elapsed time: 16.765061990823597. Arrivals time: 0.5155041026882827 Scheduler time: 16.14449132140726 Scheduler overhead time: 0.03626723540946841 Adapter cache time: 0.01925487443804741 Engine time: 0.03524407418444753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 9.099932013079524,
    "estimated_duration": 3600.186944488908,
    "input_throughput": 4414.5421460207635,
    "output_throughput": 3928.9334743165805,
    "total_throughput": 8343.475620337344,
    "itl": 178.34080851942565,
    "ttft": 2237259.476891964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1044,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.454496946465222,
    "arrivals": 1509498,
    "finished_requests": 64704,
    "scheduler_time": 97.13469405942847
}
#Debug simulation 
Total elapsed time: 9.100034719333053. Arrivals time: 0.4640085236169398 Scheduler time: 8.526010521221906 Scheduler overhead time: 0.03351539187133312 Adapter cache time: 0.026948457583785057 Engine time: 0.034213587176054716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 16.665861893910915,
    "estimated_duration": 3600.0278581923817,
    "input_throughput": 4642.36935332818,
    "output_throughput": 4120.700334649258,
    "total_throughput": 8763.069687977439,
    "itl": 208.71947199468312,
    "ttft": 2217748.2766072997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 754,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2544979223561974,
    "arrivals": 1509498,
    "finished_requests": 67960,
    "scheduler_time": 94.23357957697922
}
#Debug simulation 
Total elapsed time: 16.665966065134853. Arrivals time: 0.500223490409553 Scheduler time: 16.060316407121718 Scheduler overhead time: 0.035843264777213335 Adapter cache time: 0.02047604089602828 Engine time: 0.03494987869635224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 9.180692092049867,
    "estimated_duration": 3600.0358519428783,
    "input_throughput": 4414.448259292547,
    "output_throughput": 3928.913650225622,
    "total_throughput": 8343.36190951817,
    "itl": 178.3434340432483,
    "ttft": 2237297.2818882903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1044,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5001455709338645,
    "arrivals": 1509498,
    "finished_requests": 64701,
    "scheduler_time": 97.12944000384397
}
#Debug simulation 
Total elapsed time: 9.180821448098868. Arrivals time: 0.5609286548569798 Scheduler time: 8.50963070942089 Scheduler overhead time: 0.03320184350013733 Adapter cache time: 0.027581518981605768 Engine time: 0.034097802359610796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 13.379035220947117,
    "estimated_duration": 3600.0577369594344,
    "input_throughput": 4645.265221252877,
    "output_throughput": 4094.1360047321045,
    "total_throughput": 8739.401225984982,
    "itl": 209.70146582898676,
    "ttft": 2221286.7293163105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 739,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2617004427663474,
    "arrivals": 1494384,
    "finished_requests": 67302,
    "scheduler_time": 93.75673547499156
}
#Debug simulation 
Total elapsed time: 13.379142020829022. Arrivals time: 0.3106056754477322 Scheduler time: 12.970004727132618 Scheduler overhead time: 0.0327464216388762 Adapter cache time: 0.018699683248996735 Engine time: 0.03309178212657571 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 13.563757780008018,
    "estimated_duration": 3600.05335559942,
    "input_throughput": 4645.505315633132,
    "output_throughput": 4095.201805014708,
    "total_throughput": 8740.70712064784,
    "itl": 209.6846696784327,
    "ttft": 2221709.0548185855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4235833394876725,
    "arrivals": 1494384,
    "finished_requests": 67319,
    "scheduler_time": 93.75688933202375
}
#Debug simulation 
Total elapsed time: 13.563866097014397. Arrivals time: 0.7259945082478225 Scheduler time: 12.73917241813615 Scheduler overhead time: 0.03272666363045573 Adapter cache time: 0.018810715060681105 Engine time: 0.03363793017342687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.416927054990083,
    "estimated_duration": 3600.1910634746046,
    "input_throughput": 4449.130259364911,
    "output_throughput": 3928.365675779019,
    "total_throughput": 8377.49593514393,
    "itl": 178.36628537778992,
    "ttft": 2242359.424177289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 958,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.131192206125675,
    "arrivals": 1494384,
    "finished_requests": 64423,
    "scheduler_time": 97.19890351859812
}
#Debug simulation 
Total elapsed time: 8.417102723382413. Arrivals time: 0.2684768531471491 Scheduler time: 8.041404421441257 Scheduler overhead time: 0.03313274309039116 Adapter cache time: 0.02509080059826374 Engine time: 0.03363799350336194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 13.329346094746143,
    "estimated_duration": 3600.1902972600037,
    "input_throughput": 4645.414997292899,
    "output_throughput": 4095.2013040035235,
    "total_throughput": 8740.616301296423,
    "itl": 209.6803992177515,
    "ttft": 2221711.989474419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3283806302910492,
    "arrivals": 1494384,
    "finished_requests": 67321,
    "scheduler_time": 93.7626929068954
}
#Debug simulation 
Total elapsed time: 13.329451186582446. Arrivals time: 0.41961070662364364 Scheduler time: 12.810986430384219 Scheduler overhead time: 0.032919782679528 Adapter cache time: 0.019106497522443533 Engine time: 0.03309715259820223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 8.454944069031626,
    "estimated_duration": 3600.033746736888,
    "input_throughput": 4449.115515797029,
    "output_throughput": 3928.3640084815015,
    "total_throughput": 8377.47952427853,
    "itl": 178.36728507319535,
    "ttft": 2242333.425457682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 958,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1686668344773357,
    "arrivals": 1494384,
    "finished_requests": 64421,
    "scheduler_time": 97.19368423006492
}
#Debug simulation 
Total elapsed time: 8.455077377147973. Arrivals time: 0.2699749427847564 Scheduler time: 8.0770756425336 Scheduler overhead time: 0.03371040849015117 Adapter cache time: 0.025090401526540518 Engine time: 0.03385957656428218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 13.608490781858563,
    "estimated_duration": 3600.086250766045,
    "input_throughput": 4645.549254949461,
    "output_throughput": 4095.319659872816,
    "total_throughput": 8740.868914822277,
    "itl": 209.67505601396118,
    "ttft": 2221674.0506323054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2245974194071767,
    "arrivals": 1494384,
    "finished_requests": 67321,
    "scheduler_time": 93.76242962379173
}
#Debug simulation 
Total elapsed time: 13.608602777589113. Arrivals time: 0.7308070724830031 Scheduler time: 12.778183004818857 Scheduler overhead time: 0.032900338526815176 Adapter cache time: 0.019807009026408195 Engine time: 0.03314372571185231 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.487089204136282,
    "estimated_duration": 3600.17493296438,
    "input_throughput": 4443.739345417604,
    "output_throughput": 3924.3601389021132,
    "total_throughput": 8368.099484319717,
    "itl": 177.58447599332814,
    "ttft": 2243114.609660845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 946,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1731746443361386,
    "arrivals": 1494384,
    "finished_requests": 64368,
    "scheduler_time": 97.3001677528325
}
#Debug simulation 
Total elapsed time: 8.487206532154232. Arrivals time: 0.2676245793700218 Scheduler time: 8.112227131146938 Scheduler overhead time: 0.03346165316179395 Adapter cache time: 0.024552234914153814 Engine time: 0.0339585212059319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 12.42213957477361,
    "estimated_duration": 3600.1506524556626,
    "input_throughput": 4632.010048975408,
    "output_throughput": 4090.7379778550458,
    "total_throughput": 8722.748026830453,
    "itl": 209.32679539200666,
    "ttft": 2217753.8519493504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0903131291061072,
    "arrivals": 1486681,
    "finished_requests": 67662,
    "scheduler_time": 93.7174496294503
}
#Debug simulation 
Total elapsed time: 12.42221505381167. Arrivals time: 0.7264613062143326 Scheduler time: 11.60036343941465 Scheduler overhead time: 0.031654349993914366 Adapter cache time: 0.018022279255092144 Engine time: 0.03204917022958398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 12.046915675047785,
    "estimated_duration": 3600.1184666971003,
    "input_throughput": 4636.1238260341615,
    "output_throughput": 4092.835593134863,
    "total_throughput": 8728.959419169025,
    "itl": 209.20143051893095,
    "ttft": 2216572.510866503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 765,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.49781506459695,
    "arrivals": 1486681,
    "finished_requests": 67703,
    "scheduler_time": 93.77759627335539
}
#Debug simulation 
Total elapsed time: 12.04702885914594. Arrivals time: 0.4945868877694011 Scheduler time: 11.45678130723536 Scheduler overhead time: 0.03144252812489867 Adapter cache time: 0.01888265088200569 Engine time: 0.0316430414095521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.156472973059863,
    "estimated_duration": 3600.0899412154486,
    "input_throughput": 4446.558630864493,
    "output_throughput": 3930.0343133158626,
    "total_throughput": 8376.592944180356,
    "itl": 178.00274225183767,
    "ttft": 2238483.9432261037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.538825561273835,
    "arrivals": 1486681,
    "finished_requests": 64949,
    "scheduler_time": 97.18248479262574
}
#Debug simulation 
Total elapsed time: 8.156628009863198. Arrivals time: 0.2653463464230299 Scheduler time: 7.78760016663 Scheduler overhead time: 0.033443115185946226 Adapter cache time: 0.021629642695188522 Engine time: 0.03326817415654659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 12.473392445128411,
    "estimated_duration": 3600.172859453597,
    "input_throughput": 4632.45950988342,
    "output_throughput": 4094.2259095406594,
    "total_throughput": 8726.685419424079,
    "itl": 209.52377228096043,
    "ttft": 2217088.3815756673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7497585329250356,
    "arrivals": 1486681,
    "finished_requests": 67708,
    "scheduler_time": 93.70413166959557
}
#Debug simulation 
Total elapsed time: 12.473513317294419. Arrivals time: 0.42691840836778283 Scheduler time: 11.951368352863938 Scheduler overhead time: 0.03298290818929672 Adapter cache time: 0.016111623030155897 Engine time: 0.03256244072690606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 8.28534003533423,
    "estimated_duration": 3600.1200854973426,
    "input_throughput": 4446.521399240646,
    "output_throughput": 3930.0014066184804,
    "total_throughput": 8376.522805859127,
    "itl": 178.00404897927706,
    "ttft": 2238496.368156118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.568880716226994,
    "arrivals": 1486681,
    "finished_requests": 64949,
    "scheduler_time": 97.1825739195908
}
#Debug simulation 
Total elapsed time: 8.28544284729287. Arrivals time: 0.44567920733243227 Scheduler time: 7.736532129347324 Scheduler overhead time: 0.03298335475847125 Adapter cache time: 0.02177069801837206 Engine time: 0.03330844175070524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 11.970099317841232,
    "estimated_duration": 3600.1024022111365,
    "input_throughput": 4632.072129325504,
    "output_throughput": 4090.792803825441,
    "total_throughput": 8722.864933150944,
    "itl": 209.32440368677942,
    "ttft": 2217736.471686804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 683,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0422043514181505,
    "arrivals": 1486681,
    "finished_requests": 67662,
    "scheduler_time": 93.71730816251204
}
#Debug simulation 
Total elapsed time: 11.970223611686379. Arrivals time: 0.3092426797375083 Scheduler time: 11.56600451702252 Scheduler overhead time: 0.031876434572041035 Adapter cache time: 0.017442545387893915 Engine time: 0.03201599279418588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.126685512252152,
    "estimated_duration": 3600.156145143167,
    "input_throughput": 4446.4768622871525,
    "output_throughput": 3929.9620431983676,
    "total_throughput": 8376.43890548552,
    "itl": 178.0056593879181,
    "ttft": 2238510.625079036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 776,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.60484629914166,
    "arrivals": 1486681,
    "finished_requests": 64949,
    "scheduler_time": 97.18266798253197
}
#Debug simulation 
Total elapsed time: 8.126790442038327. Arrivals time: 0.26552643068134785 Scheduler time: 7.755930075887591 Scheduler overhead time: 0.03290032362565398 Adapter cache time: 0.023836787790060043 Engine time: 0.03338460810482502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 12.589246985968202,
    "estimated_duration": 3600.1273095770403,
    "input_throughput": 4633.37614634515,
    "output_throughput": 4110.256312502036,
    "total_throughput": 8743.632458847185,
    "itl": 209.23771699440852,
    "ttft": 2216873.0471971026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.362696538316132,
    "arrivals": 1482883,
    "finished_requests": 67912,
    "scheduler_time": 94.05819210352956
}
#Debug simulation 
Total elapsed time: 12.589386748149991. Arrivals time: 0.47828236129134893 Scheduler time: 12.01584915490821 Scheduler overhead time: 0.03208070807158947 Adapter cache time: 0.017137103248387575 Engine time: 0.03241010056808591 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 13.19131640298292,
    "estimated_duration": 3600.0478291701083,
    "input_throughput": 4633.292609294341,
    "output_throughput": 4110.034283464196,
    "total_throughput": 8743.326892758538,
    "itl": 209.24466330235535,
    "ttft": 2216852.61526537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5183368213428237,
    "arrivals": 1482883,
    "finished_requests": 67909,
    "scheduler_time": 94.05247626819926
}
#Debug simulation 
Total elapsed time: 13.191393444314599. Arrivals time: 0.9408779665827751 Scheduler time: 12.153728541452438 Scheduler overhead time: 0.03247837536036968 Adapter cache time: 0.017602527514100075 Engine time: 0.03300498938187957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.098070189356804,
    "estimated_duration": 3600.0940632085762,
    "input_throughput": 4433.516102569477,
    "output_throughput": 3937.4368422380703,
    "total_throughput": 8370.952944807548,
    "itl": 179.25799235910603,
    "ttft": 2238786.112138817,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3471253307908766,
    "arrivals": 1482883,
    "finished_requests": 65024,
    "scheduler_time": 97.06105062763658
}
#Debug simulation 
Total elapsed time: 8.098172534257174. Arrivals time: 0.4416703670285642 Scheduler time: 7.555271165911108 Scheduler overhead time: 0.032708853017538786 Adapter cache time: 0.019963834900408983 Engine time: 0.03335314290598035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 12.757567565888166,
    "estimated_duration": 3600.1772656052476,
    "input_throughput": 4633.311853658322,
    "output_throughput": 4110.1992786214405,
    "total_throughput": 8743.511132279764,
    "itl": 209.24028571646687,
    "ttft": 2216893.0523482226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.412510633866747,
    "arrivals": 1482883,
    "finished_requests": 67912,
    "scheduler_time": 94.05833403612596
}
#Debug simulation 
Total elapsed time: 12.757673597894609. Arrivals time: 0.49084975849837065 Scheduler time: 12.168989869765937 Scheduler overhead time: 0.03278716979548335 Adapter cache time: 0.017824475187808275 Engine time: 0.03343930887058377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 8.118192732334137,
    "estimated_duration": 3600.1229476243166,
    "input_throughput": 4433.48053169477,
    "output_throughput": 3937.405251493988,
    "total_throughput": 8370.885783188758,
    "itl": 179.25922661402345,
    "ttft": 2238797.750981239,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.375922947879886,
    "arrivals": 1482883,
    "finished_requests": 65024,
    "scheduler_time": 97.06113742631102
}
#Debug simulation 
Total elapsed time: 8.118345029186457. Arrivals time: 0.2654807814396918 Scheduler time: 7.7514978516846895 Scheduler overhead time: 0.03266018256545067 Adapter cache time: 0.020072360057383776 Engine time: 0.033482535276561975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 12.61489081569016,
    "estimated_duration": 3600.072787978109,
    "input_throughput": 4633.446316891922,
    "output_throughput": 4110.318560617386,
    "total_throughput": 8743.764877509308,
    "itl": 209.23492338309603,
    "ttft": 2216852.67412839,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3083188276644346,
    "arrivals": 1482883,
    "finished_requests": 67912,
    "scheduler_time": 94.05804821516905
}
#Debug simulation 
Total elapsed time: 12.615006957668811. Arrivals time: 0.48130516707897186 Scheduler time: 12.036750784143806 Scheduler overhead time: 0.03236006526276469 Adapter cache time: 0.01783340098336339 Engine time: 0.0330400294624269 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.08411696087569,
    "estimated_duration": 3600.154600858517,
    "input_throughput": 4433.441551702756,
    "output_throughput": 3937.3706330888403,
    "total_throughput": 8370.812184791597,
    "itl": 179.26061237602536,
    "ttft": 2238810.772195682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4074871482700195,
    "arrivals": 1482883,
    "finished_requests": 65024,
    "scheduler_time": 97.06122646014444
}
#Debug simulation 
Total elapsed time: 8.084226205945015. Arrivals time: 0.44190195901319385 Scheduler time: 7.541254992131144 Scheduler overhead time: 0.0327331256121397 Adapter cache time: 0.01979024149477482 Engine time: 0.03333293739706278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 11.702355535700917,
    "estimated_duration": 3600.1460543105272,
    "input_throughput": 4659.449574251388,
    "output_throughput": 4097.757084698413,
    "total_throughput": 8757.2066589498,
    "itl": 208.86826693022542,
    "ttft": 2217164.692760031,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 571,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.747538501785627,
    "arrivals": 1481051,
    "finished_requests": 67523,
    "scheduler_time": 93.85254745790033
}
#Debug simulation 
Total elapsed time: 11.702483389992267. Arrivals time: 0.29388605151325464 Scheduler time: 11.316961197182536 Scheduler overhead time: 0.03165486268699169 Adapter cache time: 0.014513167086988688 Engine time: 0.03181102126836777 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 11.728728686925024,
    "estimated_duration": 3600.0022980696012,
    "input_throughput": 4663.210634338166,
    "output_throughput": 4098.480161502036,
    "total_throughput": 8761.690795840203,
    "itl": 208.81629366089052,
    "ttft": 2217711.253212457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8223586165416121,
    "arrivals": 1481051,
    "finished_requests": 67549,
    "scheduler_time": 93.87990433733775
}
#Debug simulation 
Total elapsed time: 11.728830892127007. Arrivals time: 0.3080767160281539 Scheduler time: 11.32820267835632 Scheduler overhead time: 0.03170824469998479 Adapter cache time: 0.014921481721103191 Engine time: 0.0321241389028728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.541786470916122,
    "estimated_duration": 3600.084204515239,
    "input_throughput": 4459.863183161841,
    "output_throughput": 3926.664821414502,
    "total_throughput": 8386.528004576343,
    "itl": 177.51627283201915,
    "ttft": 2236934.2757194256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 786,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5716214822232635,
    "arrivals": 1481051,
    "finished_requests": 64525,
    "scheduler_time": 97.27336952678779
}
#Debug simulation 
Total elapsed time: 7.541913980618119. Arrivals time: 0.2626767251640558 Scheduler time: 7.177303411997855 Scheduler overhead time: 0.03280544700101018 Adapter cache time: 0.02044672193005681 Engine time: 0.03349383268505335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 11.850840800907463,
    "estimated_duration": 3600.1546140062924,
    "input_throughput": 4663.418880589959,
    "output_throughput": 4098.8786822071215,
    "total_throughput": 8762.29756279708,
    "itl": 208.8137237812655,
    "ttft": 2217725.2073149844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7430911247641685,
    "arrivals": 1481051,
    "finished_requests": 67554,
    "scheduler_time": 93.88572148211377
}
#Debug simulation 
Total elapsed time: 11.850963874720037. Arrivals time: 0.41638288786634803 Scheduler time: 11.34209579275921 Scheduler overhead time: 0.03209952637553215 Adapter cache time: 0.014272384345531464 Engine time: 0.0322978082112968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 7.552760669030249,
    "estimated_duration": 3600.1171074642357,
    "input_throughput": 4459.822422640317,
    "output_throughput": 3926.628934011818,
    "total_throughput": 8386.451356652135,
    "itl": 177.51774018124038,
    "ttft": 2236947.048238491,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 786,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6044432204775547,
    "arrivals": 1481051,
    "finished_requests": 64525,
    "scheduler_time": 97.27345073756473
}
#Debug simulation 
Total elapsed time: 7.5528624788858. Arrivals time: 0.2719751736149192 Scheduler time: 7.177742721978575 Scheduler overhead time: 0.03317915461957455 Adapter cache time: 0.020979759749025106 Engine time: 0.0336908339522779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 11.74153746291995,
    "estimated_duration": 3600.1056835608388,
    "input_throughput": 4659.501824237634,
    "output_throughput": 4097.803035995428,
    "total_throughput": 8757.304860233062,
    "itl": 208.86626804517758,
    "ttft": 2217148.926246939,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 571,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7073187183891185,
    "arrivals": 1481051,
    "finished_requests": 67523,
    "scheduler_time": 93.85239649153631
}
#Debug simulation 
Total elapsed time: 11.741670514922589. Arrivals time: 0.29535993840545416 Scheduler time: 11.353862397838384 Scheduler overhead time: 0.03198165679350495 Adapter cache time: 0.014931540470570326 Engine time: 0.03171626478433609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.557410564273596,
    "estimated_duration": 3600.151283504077,
    "input_throughput": 4459.780085789224,
    "output_throughput": 3926.5916587374404,
    "total_throughput": 8386.371744526665,
    "itl": 177.51925190562739,
    "ttft": 2236960.696938897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 786,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.638522496595995,
    "arrivals": 1481051,
    "finished_requests": 64525,
    "scheduler_time": 97.27354750132302
}
#Debug simulation 
Total elapsed time: 7.557574794162065. Arrivals time: 0.27194400737062097 Scheduler time: 7.182765329256654 Scheduler overhead time: 0.03297419147565961 Adapter cache time: 0.020836300682276487 Engine time: 0.033672972582280636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 11.337724232580513,
    "estimated_duration": 3600.03640875003,
    "input_throughput": 4660.521754507897,
    "output_throughput": 4107.947898542174,
    "total_throughput": 8768.469653050071,
    "itl": 208.4199106651626,
    "ttft": 2213015.4951314586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0597082516667786,
    "arrivals": 1480152,
    "finished_requests": 67980,
    "scheduler_time": 94.1194538745227
}
#Debug simulation 
Total elapsed time: 11.337832205928862. Arrivals time: 0.2924897996708751 Scheduler time: 10.95211736811325 Scheduler overhead time: 0.03200709214434028 Adapter cache time: 0.01609944086521864 Engine time: 0.03149834694340825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 11.360719217918813,
    "estimated_duration": 3600.102993939392,
    "input_throughput": 4660.244728621352,
    "output_throughput": 4107.748868544999,
    "total_throughput": 8767.99359716635,
    "itl": 208.43688159471066,
    "ttft": 2213070.5318680317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 672,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1927730961539846,
    "arrivals": 1480152,
    "finished_requests": 67976,
    "scheduler_time": 94.11364034083323
}
#Debug simulation 
Total elapsed time: 11.360847659874707. Arrivals time: 0.29410677775740623 Scheduler time: 10.97232614690438 Scheduler overhead time: 0.032193486113101244 Adapter cache time: 0.016643329057842493 Engine time: 0.03197895409539342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.8667408879846334,
    "estimated_duration": 3600.082993307635,
    "input_throughput": 4451.359046385908,
    "output_throughput": 3932.5026746099306,
    "total_throughput": 8383.86172099584,
    "itl": 178.3475512858614,
    "ttft": 2233907.460744225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.010035564750445,
    "arrivals": 1480152,
    "finished_requests": 64915,
    "scheduler_time": 97.16989075217204
}
#Debug simulation 
Total elapsed time: 7.8668433260172606. Arrivals time: 0.3815393913537264 Scheduler time: 7.385832242202014 Scheduler overhead time: 0.033081000205129385 Adapter cache time: 0.017417121678590775 Engine time: 0.033602009527385235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 11.34988573519513,
    "estimated_duration": 3600.0798319954547,
    "input_throughput": 4660.465540482265,
    "output_throughput": 4107.898349521564,
    "total_throughput": 8768.36389000383,
    "itl": 208.42210448080073,
    "ttft": 2213033.0865534837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.103012009162906,
    "arrivals": 1480152,
    "finished_requests": 67980,
    "scheduler_time": 94.11957336239749
}
#Debug simulation 
Total elapsed time: 11.350046048406512. Arrivals time: 0.2930256654508412 Scheduler time: 10.96264769230038 Scheduler overhead time: 0.0317556532099843 Adapter cache time: 0.016796192154288292 Engine time: 0.032073479145765305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 7.729339119978249,
    "estimated_duration": 3600.1075941784566,
    "input_throughput": 4451.328628598101,
    "output_throughput": 3932.4758023602067,
    "total_throughput": 8383.80443095831,
    "itl": 178.3485751860282,
    "ttft": 2233918.293108892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0345575531013345,
    "arrivals": 1480152,
    "finished_requests": 64915,
    "scheduler_time": 97.16996963465834
}
#Debug simulation 
Total elapsed time: 7.729450385086238. Arrivals time: 0.2690171077847481 Scheduler time: 7.361645844299346 Scheduler overhead time: 0.03265383606776595 Adapter cache time: 0.01755044562742114 Engine time: 0.03335410449653864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 11.300817660987377,
    "estimated_duration": 3600.2174908424263,
    "input_throughput": 4660.6064891023525,
    "output_throughput": 4107.895991733378,
    "total_throughput": 8768.50248083573,
    "itl": 208.41733869024065,
    "ttft": 2213032.072156056,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.01230384846913,
    "arrivals": 1480152,
    "finished_requests": 67984,
    "scheduler_time": 94.1254984934442
}
#Debug simulation 
Total elapsed time: 11.300950541161. Arrivals time: 0.2912144591100514 Scheduler time: 10.91631766455248 Scheduler overhead time: 0.031652385368943214 Adapter cache time: 0.016668089665472507 Engine time: 0.031492795795202255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_128_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.8632822637446225,
    "estimated_duration": 3600.134712825821,
    "input_throughput": 4451.295098182989,
    "output_throughput": 3932.446180295184,
    "total_throughput": 8383.741278478174,
    "itl": 178.34974063737002,
    "ttft": 2233930.158903197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 615,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0615946171805226,
    "arrivals": 1480152,
    "finished_requests": 64915,
    "scheduler_time": 97.17005121796178
}
#Debug simulation 
Total elapsed time: 7.863428873941302. Arrivals time: 0.3917902773246169 Scheduler time: 7.3728852197527885 Scheduler overhead time: 0.03290341794490814 Adapter cache time: 0.01708914479240775 Engine time: 0.033434723038226366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 12.297164025716484,
    "estimated_duration": 3600.1357843728,
    "input_throughput": 4610.013620053333,
    "output_throughput": 4098.171258996103,
    "total_throughput": 8708.184879049437,
    "itl": 210.479774886172,
    "ttft": 2215591.7997226077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 787,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4086038544751247,
    "arrivals": 1357053,
    "finished_requests": 67529,
    "scheduler_time": 93.60285781350132
}
#Debug simulation 
Total elapsed time: 12.297270352952182. Arrivals time: 0.2974252044223249 Scheduler time: 11.901526701636612 Scheduler overhead time: 0.032156184781342745 Adapter cache time: 0.02023639064282179 Engine time: 0.032246108166873455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 12.319530776236206,
    "estimated_duration": 3600.067173542387,
    "input_throughput": 4609.981758665259,
    "output_throughput": 4097.949923940857,
    "total_throughput": 8707.931682606115,
    "itl": 210.4882528393324,
    "ttft": 2215638.6269744663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 787,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5676821242692003,
    "arrivals": 1357053,
    "finished_requests": 67525,
    "scheduler_time": 93.59729642592958
}
#Debug simulation 
Total elapsed time: 12.319693270139396. Arrivals time: 0.39738856768235564 Scheduler time: 11.823367462027818 Scheduler overhead time: 0.032820858992636204 Adapter cache time: 0.020189253613352776 Engine time: 0.032207680866122246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.961933249142021,
    "estimated_duration": 3600.0099915438177,
    "input_throughput": 4418.109126741408,
    "output_throughput": 3933.674637921525,
    "total_throughput": 8351.783764662932,
    "itl": 179.12554747387372,
    "ttft": 2237559.7397477175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7076667617447154,
    "arrivals": 1357053,
    "finished_requests": 64719,
    "scheduler_time": 97.02090878453498
}
#Debug simulation 
Total elapsed time: 8.962034071329981. Arrivals time: 0.38351043639704585 Scheduler time: 8.466886282898486 Scheduler overhead time: 0.03406527452170849 Adapter cache time: 0.028460954315960407 Engine time: 0.03380903182551265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 12.456585055217147,
    "estimated_duration": 3600.1493695436234,
    "input_throughput": 4610.4628714625,
    "output_throughput": 4098.758547307324,
    "total_throughput": 8709.221418769823,
    "itl": 210.46612592139692,
    "ttft": 2215438.249799304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 773,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.421629613938257,
    "arrivals": 1357053,
    "finished_requests": 67520,
    "scheduler_time": 93.60552563149784
}
#Debug simulation 
Total elapsed time: 12.4566933340393. Arrivals time: 0.4006607113406062 Scheduler time: 11.957958540413529 Scheduler overhead time: 0.03243342647328973 Adapter cache time: 0.020130078308284283 Engine time: 0.03197922231629491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 8.905987706966698,
    "estimated_duration": 3600.054349728829,
    "input_throughput": 4418.054688868252,
    "output_throughput": 3933.626169023444,
    "total_throughput": 8351.680857891697,
    "itl": 179.12754284415888,
    "ttft": 2237577.7010563416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.751932094562796,
    "arrivals": 1357053,
    "finished_requests": 64719,
    "scheduler_time": 97.02100163676504
}
#Debug simulation 
Total elapsed time: 8.906119698192924. Arrivals time: 0.3689831350930035 Scheduler time: 8.42686643730849 Scheduler overhead time: 0.03299241466447711 Adapter cache time: 0.02860720967873931 Engine time: 0.033403178211301565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 12.345547352917492,
    "estimated_duration": 3600.080200344859,
    "input_throughput": 4610.084797113734,
    "output_throughput": 4098.234533382531,
    "total_throughput": 8708.319330496264,
    "itl": 210.47695833939105,
    "ttft": 2215568.654922711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 787,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3531695820879657,
    "arrivals": 1357053,
    "finished_requests": 67529,
    "scheduler_time": 93.60270805784843
}
#Debug simulation 
Total elapsed time: 12.345651695970446. Arrivals time: 0.30985175259411335 Scheduler time: 11.936860221903771 Scheduler overhead time: 0.03246922139078379 Adapter cache time: 0.02063414128497243 Engine time: 0.03197853313758969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.915947000030428,
    "estimated_duration": 3600.104621973929,
    "input_throughput": 4417.992994681136,
    "output_throughput": 3933.5712394478714,
    "total_throughput": 8351.564234129008,
    "itl": 179.1298883475655,
    "ttft": 2237597.788872454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1135,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.802107855342384,
    "arrivals": 1357053,
    "finished_requests": 64719,
    "scheduler_time": 97.02109812112846
}
#Debug simulation 
Total elapsed time: 8.916113049723208. Arrivals time: 0.3725100988522172 Scheduler time: 8.432159850839525 Scheduler overhead time: 0.03370488854125142 Adapter cache time: 0.028730093501508236 Engine time: 0.03361399890854955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 9.287078827619553,
    "estimated_duration": 3600.0747103326003,
    "input_throughput": 4650.920146724706,
    "output_throughput": 4088.51820706803,
    "total_throughput": 8739.438353792735,
    "itl": 209.43278397605138,
    "ttft": 2199997.733391393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 835,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.555507266183902,
    "arrivals": 1265213,
    "finished_requests": 67734,
    "scheduler_time": 93.65071608072188
}
#Debug simulation 
Total elapsed time: 9.287234591785818. Arrivals time: 0.28099291305989027 Scheduler time: 8.912096939515322 Scheduler overhead time: 0.029994338285177946 Adapter cache time: 0.020543700084090233 Engine time: 0.03021059511229396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 9.298579582944512,
    "estimated_duration": 3600.0019157390225,
    "input_throughput": 4650.321691999241,
    "output_throughput": 4088.136991165677,
    "total_throughput": 8738.458683164918,
    "itl": 209.4404864851701,
    "ttft": 2200008.627696477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 835,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.720193635430191,
    "arrivals": 1265213,
    "finished_requests": 67727,
    "scheduler_time": 93.64493801734622
}
#Debug simulation 
Total elapsed time: 9.298681464977562. Arrivals time: 0.29290804360061884 Scheduler time: 8.911807470489293 Scheduler overhead time: 0.029660433530807495 Adapter cache time: 0.02070105727761984 Engine time: 0.030202706344425678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.191334045026451,
    "estimated_duration": 3600.046940221142,
    "input_throughput": 4466.322597174888,
    "output_throughput": 3931.9198430036267,
    "total_throughput": 8398.242440178516,
    "itl": 178.71603165925393,
    "ttft": 2219795.8646042123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9707169157266002,
    "arrivals": 1265213,
    "finished_requests": 65010,
    "scheduler_time": 97.02488615792429
}
#Debug simulation 
Total elapsed time: 7.191441536415368. Arrivals time: 0.26339321210980415 Scheduler time: 6.817519678734243 Scheduler overhead time: 0.03274643188342452 Adapter cache time: 0.029327742289751768 Engine time: 0.0332562648691237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 9.33617206569761,
    "estimated_duration": 3600.130018139225,
    "input_throughput": 4650.848695918539,
    "output_throughput": 4088.4553962880755,
    "total_throughput": 8739.304092206614,
    "itl": 209.43571459685077,
    "ttft": 2200019.3960038214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 835,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6106900900881502,
    "arrivals": 1265213,
    "finished_requests": 67734,
    "scheduler_time": 93.65084106336252
}
#Debug simulation 
Total elapsed time: 9.3362978650257. Arrivals time: 0.30000153882429004 Scheduler time: 8.941612013150007 Scheduler overhead time: 0.02993533667176962 Adapter cache time: 0.0210971487686038 Engine time: 0.030182235408574343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 7.203638503327966,
    "estimated_duration": 3600.0948169044837,
    "input_throughput": 4466.263200763527,
    "output_throughput": 3931.8675534693725,
    "total_throughput": 8398.130754232898,
    "itl": 178.7182589394399,
    "ttft": 2219814.392522719,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.018503354564303,
    "arrivals": 1265213,
    "finished_requests": 65010,
    "scheduler_time": 97.02497640246884
}
#Debug simulation 
Total elapsed time: 7.2037454172968864. Arrivals time: 0.26239865366369486 Scheduler time: 6.831621082033962 Scheduler overhead time: 0.03257654141634703 Adapter cache time: 0.02871982241049409 Engine time: 0.03326695039868355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 9.382232404313982,
    "estimated_duration": 3600.0157508950133,
    "input_throughput": 4650.996317401471,
    "output_throughput": 4088.585166978967,
    "total_throughput": 8739.581484380438,
    "itl": 209.4297029379481,
    "ttft": 2199974.032749076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 835,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.496691996243265,
    "arrivals": 1265213,
    "finished_requests": 67734,
    "scheduler_time": 93.65057191296648
}
#Debug simulation 
Total elapsed time: 9.382362351287156. Arrivals time: 0.37670363672077656 Scheduler time: 8.910853545181453 Scheduler overhead time: 0.030162676237523556 Adapter cache time: 0.02087742555886507 Engine time: 0.03029892174527049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.2148256418295205,
    "estimated_duration": 3600.1477186170346,
    "input_throughput": 4466.197572075347,
    "output_throughput": 3931.809777360346,
    "total_throughput": 8398.007349435693,
    "itl": 178.72072602170562,
    "ttft": 2219834.0745032085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.071319944858605,
    "arrivals": 1265213,
    "finished_requests": 65010,
    "scheduler_time": 97.02506152477402
}
#Debug simulation 
Total elapsed time: 7.214982838835567. Arrivals time: 0.269604979082942 Scheduler time: 6.834426479414105 Scheduler overhead time: 0.03291926486417651 Adapter cache time: 0.02937936596572399 Engine time: 0.033348037861287594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.266591526102275,
    "estimated_duration": 3600.0992285309544,
    "input_throughput": 4648.977691325903,
    "output_throughput": 4093.2874525275865,
    "total_throughput": 8742.265143853489,
    "itl": 209.3142814712348,
    "ttft": 2203505.051911927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 823,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5187814132567077,
    "arrivals": 1250156,
    "finished_requests": 67690,
    "scheduler_time": 93.6555064958783
}
#Debug simulation 
Total elapsed time: 8.26670062635094. Arrivals time: 0.2841732054948807 Scheduler time: 7.890418506227434 Scheduler overhead time: 0.02974699018523097 Adapter cache time: 0.01994953863322735 Engine time: 0.029278909787535667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 8.273317612241954,
    "estimated_duration": 3600.04049585364,
    "input_throughput": 4648.661596800098,
    "output_throughput": 4092.9142372066913,
    "total_throughput": 8741.57583400679,
    "itl": 209.32323994909777,
    "ttft": 2203544.843646843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 823,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6839044365729254,
    "arrivals": 1250156,
    "finished_requests": 67685,
    "scheduler_time": 93.6499656647251
}
#Debug simulation 
Total elapsed time: 8.273420407436788. Arrivals time: 0.285119139123708 Scheduler time: 7.895954857580364 Scheduler overhead time: 0.02904186863452196 Adapter cache time: 0.020370481070131063 Engine time: 0.02980995550751686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.577385682146996,
    "estimated_duration": 3600.1338360563846,
    "input_throughput": 4447.86269877696,
    "output_throughput": 3928.903936386456,
    "total_throughput": 8376.766635163416,
    "itl": 178.20200341387925,
    "ttft": 2223942.073574901,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.893625493422093,
    "arrivals": 1250156,
    "finished_requests": 64829,
    "scheduler_time": 97.08923495469692
}
#Debug simulation 
Total elapsed time: 6.57751983916387. Arrivals time: 0.34964010026305914 Scheduler time: 6.1188920475542545 Scheduler overhead time: 0.03244376927614212 Adapter cache time: 0.02855804283171892 Engine time: 0.032987695187330246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 8.248166953679174,
    "estimated_duration": 3600.1590619250437,
    "input_throughput": 4648.90042693021,
    "output_throughput": 4093.2194235107972,
    "total_throughput": 8742.119850441008,
    "itl": 209.31740840939213,
    "ttft": 2203529.2930573276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 823,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.578486844415289,
    "arrivals": 1250156,
    "finished_requests": 67690,
    "scheduler_time": 93.65563445873094
}
#Debug simulation 
Total elapsed time: 8.248271898832172. Arrivals time: 0.2745299795642495 Scheduler time: 7.882534380536526 Scheduler overhead time: 0.0289309392683208 Adapter cache time: 0.019825048744678497 Engine time: 0.029251914005726576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 6.8017248823307455,
    "estimated_duration": 3600.1803235807843,
    "input_throughput": 4447.805265507748,
    "output_throughput": 3928.8532042005118,
    "total_throughput": 8376.65846970826,
    "itl": 178.20413112976217,
    "ttft": 2223960.7555913725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9400286406092295,
    "arrivals": 1250156,
    "finished_requests": 64829,
    "scheduler_time": 97.0893193319491
}
#Debug simulation 
Total elapsed time: 6.801808505319059. Arrivals time: 0.6301628067158163 Scheduler time: 6.064383110497147 Scheduler overhead time: 0.03208466339856386 Adapter cache time: 0.02802461665123701 Engine time: 0.03231694037094712 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 8.085505416616797,
    "estimated_duration": 3600.041127939972,
    "input_throughput": 4649.052720566328,
    "output_throughput": 4093.353513556225,
    "total_throughput": 8742.406234122554,
    "itl": 209.31126872939066,
    "ttft": 2203481.2566171656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 823,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.46081139270444,
    "arrivals": 1250156,
    "finished_requests": 67690,
    "scheduler_time": 93.65537592534015
}
#Debug simulation 
Total elapsed time: 8.085640263743699. Arrivals time: 0.2571562649682164 Scheduler time: 7.739019974600524 Scheduler overhead time: 0.02857859106734395 Adapter cache time: 0.019400853663682938 Engine time: 0.02839941531419754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.502436994574964,
    "estimated_duration": 3600.0382267764808,
    "input_throughput": 4447.920547315398,
    "output_throughput": 3929.0077240833166,
    "total_throughput": 8376.928271398714,
    "itl": 178.20784220652627,
    "ttft": 2223922.6084474893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.992719477117122,
    "arrivals": 1250156,
    "finished_requests": 64828,
    "scheduler_time": 97.08410376868089
}
#Debug simulation 
Total elapsed time: 6.502575066871941. Arrivals time: 0.3352446095086634 Scheduler time: 6.06120031606406 Scheduler overhead time: 0.03211443778127432 Adapter cache time: 0.026963094715029 Engine time: 0.032034226693212986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.430096345022321,
    "estimated_duration": 3600.197156238531,
    "input_throughput": 4661.62970295065,
    "output_throughput": 4088.812740294463,
    "total_throughput": 8750.442443245112,
    "itl": 209.03774619032237,
    "ttft": 2205233.766425619,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 781,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3902409280115275,
    "arrivals": 1242527,
    "finished_requests": 67567,
    "scheduler_time": 93.68304566534084
}
#Debug simulation 
Total elapsed time: 7.430228413082659. Arrivals time: 0.25014655012637377 Scheduler time: 7.091992164961994 Scheduler overhead time: 0.02830288466066122 Adapter cache time: 0.018579792231321335 Engine time: 0.028148701414465904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.439760289620608,
    "estimated_duration": 3600.122630948021,
    "input_throughput": 4661.51676493886,
    "output_throughput": 4088.7396094404116,
    "total_throughput": 8750.256374379273,
    "itl": 209.0456939673429,
    "ttft": 2205259.824433023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 781,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.552601989728871,
    "arrivals": 1242527,
    "finished_requests": 67566,
    "scheduler_time": 93.67728770136226
}
#Debug simulation 
Total elapsed time: 7.439865931868553. Arrivals time: 0.2516892687417567 Scheduler time: 7.099900403060019 Scheduler overhead time: 0.028336307499557734 Adapter cache time: 0.018647699616849422 Engine time: 0.02828090637922287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.1176825650036335,
    "estimated_duration": 3600.003865496964,
    "input_throughput": 4470.270755606101,
    "output_throughput": 3932.32744433339,
    "total_throughput": 8402.598199939492,
    "itl": 178.27361365826712,
    "ttft": 2225991.750949153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5081683818995497,
    "arrivals": 1242527,
    "finished_requests": 64833,
    "scheduler_time": 97.08193579668966
}
#Debug simulation 
Total elapsed time: 6.117787545081228. Arrivals time: 0.24212554702535272 Scheduler time: 5.771810548845679 Scheduler overhead time: 0.03182823210954666 Adapter cache time: 0.025181604083627462 Engine time: 0.03201068378984928 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.42517253709957,
    "estimated_duration": 3600.0206056378493,
    "input_throughput": 4661.648873264316,
    "output_throughput": 4088.8554851457375,
    "total_throughput": 8750.504358410053,
    "itl": 209.04057459440338,
    "ttft": 2205216.8699824396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 781,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4508617554372,
    "arrivals": 1242527,
    "finished_requests": 67566,
    "scheduler_time": 93.67700262545836
}
#Debug simulation 
Total elapsed time: 7.42530719935894. Arrivals time: 0.2505919197574258 Scheduler time: 7.0865352670662105 Scheduler overhead time: 0.028116744477301836 Adapter cache time: 0.018686010502278805 Engine time: 0.02833828516304493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 6.0963144749403,
    "estimated_duration": 3600.0448302855093,
    "input_throughput": 4470.219888546141,
    "output_throughput": 3932.2826985122006,
    "total_throughput": 8402.502587058341,
    "itl": 178.27545245079529,
    "ttft": 2226008.5649306383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5490383624844215,
    "arrivals": 1242527,
    "finished_requests": 64833,
    "scheduler_time": 97.08203060468384
}
#Debug simulation 
Total elapsed time: 6.0964190368540585. Arrivals time: 0.2364171901717782 Scheduler time: 5.756577718537301 Scheduler overhead time: 0.03181778732687235 Adapter cache time: 0.024929864332079887 Engine time: 0.031826578080654144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.515094839036465,
    "estimated_duration": 3600.142006942394,
    "input_throughput": 4661.701112799616,
    "output_throughput": 4088.8753753639207,
    "total_throughput": 8750.576488163537,
    "itl": 209.03494168118672,
    "ttft": 2205211.1205662983,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 781,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3352292803185533,
    "arrivals": 1242527,
    "finished_requests": 67567,
    "scheduler_time": 93.6829080167992
}
#Debug simulation 
Total elapsed time: 7.5151938972994685. Arrivals time: 0.2564814565703273 Scheduler time: 7.17108225915581 Scheduler overhead time: 0.028128820471465588 Adapter cache time: 0.018588262610137463 Engine time: 0.02795430924743414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.144613679964095,
    "estimated_duration": 3600.09560824657,
    "input_throughput": 4470.156837817456,
    "output_throughput": 3932.227235180258,
    "total_throughput": 8402.384072997715,
    "itl": 178.27780435535362,
    "ttft": 2226028.6602627505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.599717138409664,
    "arrivals": 1242527,
    "finished_requests": 64833,
    "scheduler_time": 97.08212978986438
}
#Debug simulation 
Total elapsed time: 6.14475459093228. Arrivals time: 0.23778424691408873 Scheduler time: 5.803014472126961 Scheduler overhead time: 0.032000858802348375 Adapter cache time: 0.025190372485667467 Engine time: 0.03184966370463371 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.2310306443832815,
    "estimated_duration": 3600.123790920201,
    "input_throughput": 4651.547272411886,
    "output_throughput": 4091.9865136733397,
    "total_throughput": 8743.533786085225,
    "itl": 209.60145500286126,
    "ttft": 2208817.1116412114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.09337361685004,
    "arrivals": 1238735,
    "finished_requests": 67390,
    "scheduler_time": 93.66652480239762
}
#Debug simulation 
Total elapsed time: 7.231128503102809. Arrivals time: 0.2507247952744365 Scheduler time: 6.894153922330588 Scheduler overhead time: 0.028202158864587545 Adapter cache time: 0.016946044750511646 Engine time: 0.028115972876548767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.224244768731296,
    "estimated_duration": 3600.0340131803923,
    "input_throughput": 4651.228554701145,
    "output_throughput": 4091.8477286791854,
    "total_throughput": 8743.076283380331,
    "itl": 209.6072640385916,
    "ttft": 2208783.466163851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2315138669218917,
    "arrivals": 1238735,
    "finished_requests": 67386,
    "scheduler_time": 93.66092602889611
}
#Debug simulation 
Total elapsed time: 7.224372495897114. Arrivals time: 0.24977341666817665 Scheduler time: 6.888446882832795 Scheduler overhead time: 0.028179479762911797 Adapter cache time: 0.016906870529055595 Engine time: 0.028078037314116955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.954240075312555,
    "estimated_duration": 3600.0115649468753,
    "input_throughput": 4452.755140034265,
    "output_throughput": 3931.3304262145753,
    "total_throughput": 8384.085566248841,
    "itl": 178.28273291801995,
    "ttft": 2231076.323449406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3269476979411756,
    "arrivals": 1238735,
    "finished_requests": 64567,
    "scheduler_time": 97.10589433136869
}
#Debug simulation 
Total elapsed time: 5.954332794994116. Arrivals time: 0.23625844297930598 Scheduler time: 5.615626425482333 Scheduler overhead time: 0.03191419644281268 Adapter cache time: 0.023911614902317524 Engine time: 0.0318184201605618 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.213410589843988,
    "estimated_duration": 3600.1733771853915,
    "input_throughput": 4651.483205259437,
    "output_throughput": 4091.930153518657,
    "total_throughput": 8743.413358778094,
    "itl": 209.60400605360363,
    "ttft": 2208838.01939664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.142848682820317,
    "arrivals": 1238735,
    "finished_requests": 67390,
    "scheduler_time": 93.66663600155778
}
#Debug simulation 
Total elapsed time: 7.213530988898128. Arrivals time: 0.24817663244903088 Scheduler time: 6.8791946158744395 Scheduler overhead time: 0.028002174571156502 Adapter cache time: 0.017034004908055067 Engine time: 0.028216874226927757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.971480753738433,
    "estimated_duration": 3600.0522802767455,
    "input_throughput": 4452.70478093383,
    "output_throughput": 3931.285964244951,
    "total_throughput": 8383.99074517878,
    "itl": 178.28453550900414,
    "ttft": 2231092.9135755207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3675661709532094,
    "arrivals": 1238735,
    "finished_requests": 64567,
    "scheduler_time": 97.10599118825861
}
#Debug simulation 
Total elapsed time: 5.971576117910445. Arrivals time: 0.23681389912962914 Scheduler time: 5.631584057118744 Scheduler overhead time: 0.03202325478196144 Adapter cache time: 0.024429759941995144 Engine time: 0.03188211843371391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.185564589686692,
    "estimated_duration": 3600.0754890507974,
    "input_throughput": 4651.609681777901,
    "output_throughput": 4092.0414154660343,
    "total_throughput": 8743.651097243936,
    "itl": 209.59902549546652,
    "ttft": 2208796.4984780443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 684,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0451944017130526,
    "arrivals": 1238735,
    "finished_requests": 67390,
    "scheduler_time": 93.66640214804758
}
#Debug simulation 
Total elapsed time: 7.1856622328050435. Arrivals time: 0.2498056902550161 Scheduler time: 6.850450940895826 Scheduler overhead time: 0.02805121662095189 Adapter cache time: 0.016576428432017565 Engine time: 0.027917563449591398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.980372968129814,
    "estimated_duration": 3600.0981327923946,
    "input_throughput": 4452.648069225394,
    "output_throughput": 3931.235893567834,
    "total_throughput": 8383.883962793227,
    "itl": 178.28665867811966,
    "ttft": 2231110.760379821,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4133405492082667,
    "arrivals": 1238735,
    "finished_requests": 64567,
    "scheduler_time": 97.10606932569193
}
#Debug simulation 
Total elapsed time: 5.980513815768063. Arrivals time: 0.23609715327620506 Scheduler time: 5.641790575347841 Scheduler overhead time: 0.03195388615131378 Adapter cache time: 0.023974456824362278 Engine time: 0.031787932850420475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.099725889973342,
    "estimated_duration": 3600.0134961036465,
    "input_throughput": 4637.720946899282,
    "output_throughput": 4093.618820026699,
    "total_throughput": 8731.339766925981,
    "itl": 209.74107044444125,
    "ttft": 2204306.5729656275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.026042886483517,
    "arrivals": 1236839,
    "finished_requests": 67555,
    "scheduler_time": 93.63686899025511
}
#Debug simulation 
Total elapsed time: 7.099821957293898. Arrivals time: 0.3902911734767258 Scheduler time: 6.624486967921257 Scheduler overhead time: 0.02818097360432148 Adapter cache time: 0.01617466052994132 Engine time: 0.027811189647763968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 7.105302540119737,
    "estimated_duration": 3600.145793423582,
    "input_throughput": 4637.5505210090305,
    "output_throughput": 4093.468388674803,
    "total_throughput": 8731.018909683833,
    "itl": 209.7477416075059,
    "ttft": 2204363.3407231863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1579694493836774,
    "arrivals": 1236839,
    "finished_requests": 67555,
    "scheduler_time": 93.63723974725393
}
#Debug simulation 
Total elapsed time: 7.105422812979668. Arrivals time: 0.3919206098653376 Scheduler time: 6.627593660261482 Scheduler overhead time: 0.0282107493840158 Adapter cache time: 0.016660713125020266 Engine time: 0.028100373223423958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.952796082012355,
    "estimated_duration": 3600.072301096956,
    "input_throughput": 4448.318994904738,
    "output_throughput": 3932.4293558455197,
    "total_throughput": 8380.748350750258,
    "itl": 178.6419047969844,
    "ttft": 2225821.858298618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.257983381096237,
    "arrivals": 1236839,
    "finished_requests": 64841,
    "scheduler_time": 97.05212290394314
}
#Debug simulation 
Total elapsed time: 5.952892003115267. Arrivals time: 0.3788444851525128 Scheduler time: 5.471794854849577 Scheduler overhead time: 0.03178737172856927 Adapter cache time: 0.023752517998218536 Engine time: 0.03186539653688669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 7.19698308268562,
    "estimated_duration": 3600.067085147857,
    "input_throughput": 4637.651911787719,
    "output_throughput": 4093.557884184466,
    "total_throughput": 8731.209795972185,
    "itl": 209.74391458818317,
    "ttft": 2204328.5056703123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0795191482431137,
    "arrivals": 1236839,
    "finished_requests": 67555,
    "scheduler_time": 93.6369817726529
}
#Debug simulation 
Total elapsed time: 7.197080242913216. Arrivals time: 0.48232671013101935 Scheduler time: 6.628945454955101 Scheduler overhead time: 0.028112199623137712 Adapter cache time: 0.01680060103535652 Engine time: 0.027992463670670986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.954897694289684,
    "estimated_duration": 3600.1089770320773,
    "input_throughput": 4448.273677871311,
    "output_throughput": 3932.3892944127006,
    "total_throughput": 8380.662972284012,
    "itl": 178.64351207002787,
    "ttft": 2225836.912147526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2945777329429955,
    "arrivals": 1236839,
    "finished_requests": 64841,
    "scheduler_time": 97.05220448724657
}
#Debug simulation 
Total elapsed time: 5.954990308266133. Arrivals time: 0.3852486554533243 Scheduler time: 5.467309601139277 Scheduler overhead time: 0.03211963456124067 Adapter cache time: 0.023627439979463816 Engine time: 0.031923646572977304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 7.103356401901692,
    "estimated_duration": 3600.19856598292,
    "input_throughput": 4638.160005333222,
    "output_throughput": 4093.8394729836473,
    "total_throughput": 8731.99947831687,
    "itl": 209.737648834804,
    "ttft": 2204342.2016587956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9794132952252068,
    "arrivals": 1236839,
    "finished_requests": 67562,
    "scheduler_time": 93.64280217703578
}
#Debug simulation 
Total elapsed time: 7.103456766810268. Arrivals time: 0.39338788762688637 Scheduler time: 6.6248375819996 Scheduler overhead time: 0.028057250194251537 Adapter cache time: 0.016383450478315353 Engine time: 0.027889084070920944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.955508411861956,
    "estimated_duration": 3600.1558384650943,
    "input_throughput": 4448.215776911366,
    "output_throughput": 3932.338108462485,
    "total_throughput": 8380.55388537385,
    "itl": 178.64565742861606,
    "ttft": 2225855.3524603243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 997,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.341358141489368,
    "arrivals": 1236839,
    "finished_requests": 64841,
    "scheduler_time": 97.05228551176026
}
#Debug simulation 
Total elapsed time: 5.955623500980437. Arrivals time: 0.23689154721796513 Scheduler time: 5.616360363550484 Scheduler overhead time: 0.03184153838083148 Adapter cache time: 0.02391184214502573 Engine time: 0.031802651938050985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 6.643956158310175,
    "estimated_duration": 3600.0105874611004,
    "input_throughput": 4643.939397908965,
    "output_throughput": 4092.099909736496,
    "total_throughput": 8736.03930764546,
    "itl": 209.33146467693177,
    "ttft": 2201555.182649938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9801355703245245,
    "arrivals": 1235881,
    "finished_requests": 67841,
    "scheduler_time": 93.64268309726452
}
#Debug simulation 
Total elapsed time: 6.644049891270697. Arrivals time: 0.24884949019178748 Scheduler time: 6.310759308282286 Scheduler overhead time: 0.027932437602430582 Adapter cache time: 0.015692210756242275 Engine time: 0.027914172504097223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.688913218677044,
    "estimated_duration": 3600.146023527775,
    "input_throughput": 4643.7646947491985,
    "output_throughput": 4091.9459665595828,
    "total_throughput": 8735.710661308782,
    "itl": 209.33820327202753,
    "ttft": 2201613.5034763464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.115161671552348,
    "arrivals": 1235881,
    "finished_requests": 67841,
    "scheduler_time": 93.64309306267745
}
#Debug simulation 
Total elapsed time: 6.689034192822874. Arrivals time: 0.2525303694419563 Scheduler time: 6.351620000787079 Scheduler overhead time: 0.027960103005170822 Adapter cache time: 0.016087648924440145 Engine time: 0.027950987219810486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.885842038784176,
    "estimated_duration": 3600.124837971896,
    "input_throughput": 4462.262761157453,
    "output_throughput": 3937.167914428486,
    "total_throughput": 8399.430675585938,
    "itl": 179.07322273031724,
    "ttft": 2222758.802541407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 904,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9587482484243557,
    "arrivals": 1235881,
    "finished_requests": 65190,
    "scheduler_time": 96.96517476529966
}
#Debug simulation 
Total elapsed time: 5.88593488978222. Arrivals time: 0.3253178116865456 Scheduler time: 5.460572550538927 Scheduler overhead time: 0.03173060296103358 Adapter cache time: 0.021758740302175283 Engine time: 0.03183002443984151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 6.654307445045561,
    "estimated_duration": 3600.0672843323514,
    "input_throughput": 4643.866261266411,
    "output_throughput": 4092.0354639238476,
    "total_throughput": 8735.901725190259,
    "itl": 209.33439676525595,
    "ttft": 2201579.524531279,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0367113704117843,
    "arrivals": 1235881,
    "finished_requests": 67841,
    "scheduler_time": 93.64280416837693
}
#Debug simulation 
Total elapsed time: 6.654406534042209. Arrivals time: 0.2503632977604866 Scheduler time: 6.31962651014328 Scheduler overhead time: 0.027827509213238955 Adapter cache time: 0.015696500428020954 Engine time: 0.027952279429882765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.747629451099783,
    "estimated_duration": 3600.157754611247,
    "input_throughput": 4462.22196219696,
    "output_throughput": 3937.1319164680804,
    "total_throughput": 8399.35387866504,
    "itl": 179.07468379268354,
    "ttft": 2222772.3896045205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 904,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.991569986678658,
    "arrivals": 1235881,
    "finished_requests": 65190,
    "scheduler_time": 96.96526966642546
}
#Debug simulation 
Total elapsed time: 5.747726138215512. Arrivals time: 0.2383635425940156 Scheduler time: 5.409586271736771 Scheduler overhead time: 0.03173587145283818 Adapter cache time: 0.021626466885209084 Engine time: 0.03173763584345579 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 6.672793795354664,
    "estimated_duration": 3600.1990594601916,
    "input_throughput": 4644.308474017277,
    "output_throughput": 4092.4289898040047,
    "total_throughput": 8736.737463821282,
    "itl": 209.3294526251484,
    "ttft": 2201607.072768015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9345625408016758,
    "arrivals": 1235881,
    "finished_requests": 67847,
    "scheduler_time": 93.6486637972938
}
#Debug simulation 
Total elapsed time: 6.672892625443637. Arrivals time: 0.2517315549775958 Scheduler time: 6.336746768094599 Scheduler overhead time: 0.0280309421941638 Adapter cache time: 0.015659918077290058 Engine time: 0.02780019026249647 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.870259023737162,
    "estimated_duration": 3600.00024627953,
    "input_throughput": 4462.417194721664,
    "output_throughput": 3937.304175089605,
    "total_throughput": 8399.721369811268,
    "itl": 179.07674135514117,
    "ttft": 2222760.0887539093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 904,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0360868270695516,
    "arrivals": 1235881,
    "finished_requests": 65190,
    "scheduler_time": 96.95990221010074
}
#Debug simulation 
Total elapsed time: 5.870378298684955. Arrivals time: 0.3270385144278407 Scheduler time: 5.442938547115773 Scheduler overhead time: 0.031906860414892435 Adapter cache time: 0.02186263771727681 Engine time: 0.03185858577489853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 6.856650830712169,
    "estimated_duration": 3600.1669520245678,
    "input_throughput": 4643.166614981451,
    "output_throughput": 4090.904170907321,
    "total_throughput": 8734.07078588877,
    "itl": 209.50764237730758,
    "ttft": 2199698.518800476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.779702363757046,
    "arrivals": 1142437,
    "finished_requests": 67349,
    "scheduler_time": 93.56706332211523
}
#Debug simulation 
Total elapsed time: 6.856747079640627. Arrivals time: 0.453635158482939 Scheduler time: 6.30785122141242 Scheduler overhead time: 0.028119804803282022 Adapter cache time: 0.02633650880306959 Engine time: 0.027870602440088987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.85488406335935,
    "estimated_duration": 3600.1963541117957,
    "input_throughput": 4642.98203649615,
    "output_throughput": 4090.7674336094474,
    "total_throughput": 8733.749470105598,
    "itl": 209.5227925235356,
    "ttft": 2199745.112090861,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.027760275325274,
    "arrivals": 1142437,
    "finished_requests": 67347,
    "scheduler_time": 93.56180064154601
}
#Debug simulation 
Total elapsed time: 6.855002783238888. Arrivals time: 0.4614661205559969 Scheduler time: 6.298188351094723 Scheduler overhead time: 0.028033544775098562 Adapter cache time: 0.02627364033833146 Engine time: 0.02809453895315528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.0497853653505445,
    "estimated_duration": 3600.185530192397,
    "input_throughput": 4446.515843627788,
    "output_throughput": 3926.587638733312,
    "total_throughput": 8373.103482361099,
    "itl": 178.40660679989884,
    "ttft": 2219481.1946265777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1638,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.35254639582703,
    "arrivals": 1142437,
    "finished_requests": 64519,
    "scheduler_time": 96.99407957207956
}
#Debug simulation 
Total elapsed time: 6.049883923027664. Arrivals time: 0.45071629574522376 Scheduler time: 5.485577585641295 Scheduler overhead time: 0.032266135793179274 Adapter cache time: 0.03456004522740841 Engine time: 0.03187204385176301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 6.8159084669314325,
    "estimated_duration": 3600.007836365309,
    "input_throughput": 4643.22489277598,
    "output_throughput": 4090.9810948826857,
    "total_throughput": 8734.205987658666,
    "itl": 209.5156138342962,
    "ttft": 2199626.5447810246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8708596730441363,
    "arrivals": 1142437,
    "finished_requests": 67346,
    "scheduler_time": 93.56064894466799
}
#Debug simulation 
Total elapsed time: 6.816026019863784. Arrivals time: 0.3737994944676757 Scheduler time: 6.34698492847383 Scheduler overhead time: 0.028063474223017693 Adapter cache time: 0.026562218088656664 Engine time: 0.027780934236943722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 6.03966013295576,
    "estimated_duration": 3600.005989549794,
    "input_throughput": 4447.322878482825,
    "output_throughput": 3927.004021948297,
    "total_throughput": 8374.326900431122,
    "itl": 178.40427439686212,
    "ttft": 2219653.860175348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.378737874403533,
    "arrivals": 1142437,
    "finished_requests": 64524,
    "scheduler_time": 96.98887432835578
}
#Debug simulation 
Total elapsed time: 6.039753512945026. Arrivals time: 0.46312674414366484 Scheduler time: 5.463124554138631 Scheduler overhead time: 0.03185464022681117 Adapter cache time: 0.03481230093166232 Engine time: 0.032003828790038824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 6.89300651429221,
    "estimated_duration": 3600.067470184145,
    "input_throughput": 4643.294921121287,
    "output_throughput": 4091.0172161986343,
    "total_throughput": 8734.312137319921,
    "itl": 209.50362268805952,
    "ttft": 2199666.863479717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6927121142040926,
    "arrivals": 1142437,
    "finished_requests": 67349,
    "scheduler_time": 93.56653591906135
}
#Debug simulation 
Total elapsed time: 6.893118332140148. Arrivals time: 0.45897974679246545 Scheduler time: 6.338652733247727 Scheduler overhead time: 0.028092724736779928 Adapter cache time: 0.026529007591307163 Engine time: 0.027906101662665606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.017136080190539,
    "estimated_duration": 3600.0782728052895,
    "input_throughput": 4447.233584042111,
    "output_throughput": 3926.9251745973393,
    "total_throughput": 8374.15875863945,
    "itl": 178.40765261453626,
    "ttft": 2219681.5100274435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1627,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.450920547805644,
    "arrivals": 1142437,
    "finished_requests": 64524,
    "scheduler_time": 96.98897491051069
}
#Debug simulation 
Total elapsed time: 6.017261208035052. Arrivals time: 0.4474239246919751 Scheduler time: 5.457180207129568 Scheduler overhead time: 0.03201299859210849 Adapter cache time: 0.0341468988917768 Engine time: 0.03164901351556182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 6.1625966019928455,
    "estimated_duration": 3600.140308947981,
    "input_throughput": 4631.081171631332,
    "output_throughput": 4093.1807472542937,
    "total_throughput": 8724.261918885626,
    "itl": 210.00082812014617,
    "ttft": 2196442.364259185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7766418760131133,
    "arrivals": 1127186,
    "finished_requests": 67479,
    "scheduler_time": 93.53787234013117
}
#Debug simulation 
Total elapsed time: 6.162714910693467. Arrivals time: 0.246160838752985 Scheduler time: 5.821713896933943 Scheduler overhead time: 0.028347160667181015 Adapter cache time: 0.025595600251108408 Engine time: 0.027960967738181353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 6.170889742672443,
    "estimated_duration": 3600.1582570598057,
    "input_throughput": 4630.646990950615,
    "output_throughput": 4092.876742600158,
    "total_throughput": 8723.523733550774,
    "itl": 210.01456706670703,
    "ttft": 2196480.641518439,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.026404606304133,
    "arrivals": 1127186,
    "finished_requests": 67474,
    "scheduler_time": 93.5322070127391
}
#Debug simulation 
Total elapsed time: 6.170988503843546. Arrivals time: 0.24407482892274857 Scheduler time: 5.832318173721433 Scheduler overhead time: 0.027894351165741682 Adapter cache time: 0.025825703982263803 Engine time: 0.027944574132561684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.427102790679783,
    "estimated_duration": 3600.193555621064,
    "input_throughput": 4435.532910464657,
    "output_throughput": 3927.4266179171623,
    "total_throughput": 8362.95952838182,
    "itl": 178.43961455273563,
    "ttft": 2217340.117140692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.3853340253792155,
    "arrivals": 1127186,
    "finished_requests": 64602,
    "scheduler_time": 96.9955519074671
}
#Debug simulation 
Total elapsed time: 5.4272029818966985. Arrivals time: 0.23576446622610092 Scheduler time: 5.077590328641236 Scheduler overhead time: 0.03214346384629607 Adapter cache time: 0.03482163418084383 Engine time: 0.032081186305731535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 6.1277403379790485,
    "estimated_duration": 3600.0014787662662,
    "input_throughput": 4630.84865334923,
    "output_throughput": 4093.0549853689895,
    "total_throughput": 8723.903638718219,
    "itl": 210.00633582530634,
    "ttft": 2196417.4143013256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.869912599341437,
    "arrivals": 1127186,
    "finished_requests": 67474,
    "scheduler_time": 93.53192072612407
}
#Debug simulation 
Total elapsed time: 6.12786155520007. Arrivals time: 0.24542766902595758 Scheduler time: 5.787875415291637 Scheduler overhead time: 0.02843604050576687 Adapter cache time: 0.025488813873380423 Engine time: 0.027737214230000973 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.551955945789814,
    "estimated_duration": 3600.059966895736,
    "input_throughput": 4435.327785322539,
    "output_throughput": 3927.4348566454005,
    "total_throughput": 8362.76264196794,
    "itl": 178.44435083383792,
    "ttft": 2217255.7424877626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.449845717810027,
    "arrivals": 1127186,
    "finished_requests": 64597,
    "scheduler_time": 96.99027913078369
}
#Debug simulation 
Total elapsed time: 5.552052803803235. Arrivals time: 0.23610163340345025 Scheduler time: 5.202353949658573 Scheduler overhead time: 0.03223342355340719 Adapter cache time: 0.034650343004614115 Engine time: 0.03189399791881442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 6.126033498439938,
    "estimated_duration": 3600.053246737778,
    "input_throughput": 4631.193167797721,
    "output_throughput": 4093.279735057582,
    "total_throughput": 8724.472902855303,
    "itl": 209.99613636446043,
    "ttft": 2196408.103341726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1234,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6897220639091906,
    "arrivals": 1127186,
    "finished_requests": 67479,
    "scheduler_time": 93.53772994187666
}
#Debug simulation 
Total elapsed time: 6.126154431141913. Arrivals time: 0.24293593363836408 Scheduler time: 5.789274932816625 Scheduler overhead time: 0.027774362359195948 Adapter cache time: 0.02546135289594531 Engine time: 0.027837486006319523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.486379012931138,
    "estimated_duration": 3600.1345017054005,
    "input_throughput": 4435.23595922212,
    "output_throughput": 3927.353545625112,
    "total_throughput": 8362.589504847232,
    "itl": 178.44794692531883,
    "ttft": 2217283.4200261827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.524291959367604,
    "arrivals": 1127186,
    "finished_requests": 64597,
    "scheduler_time": 96.990367698959
}
#Debug simulation 
Total elapsed time: 5.486499757040292. Arrivals time: 0.24029802158474922 Scheduler time: 5.132575472816825 Scheduler overhead time: 0.03196620149537921 Adapter cache time: 0.03491391707211733 Engine time: 0.031877665780484676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.796859484165907,
    "estimated_duration": 3600.0754186710915,
    "input_throughput": 4610.562854854584,
    "output_throughput": 4093.2186930238013,
    "total_throughput": 8703.781547878385,
    "itl": 210.04902253815027,
    "ttft": 2197400.9448329955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6848272436951275,
    "arrivals": 1119422,
    "finished_requests": 67516,
    "scheduler_time": 93.49208400351219
}
#Debug simulation 
Total elapsed time: 5.79696369310841. Arrivals time: 0.24389242846518755 Scheduler time: 5.459060744848102 Scheduler overhead time: 0.0279960329644382 Adapter cache time: 0.025446895509958267 Engine time: 0.027704736217856407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.888705421239138,
    "estimated_duration": 3600.0790572428455,
    "input_throughput": 4610.337088739216,
    "output_throughput": 4093.177890178202,
    "total_throughput": 8703.514978917417,
    "itl": 210.06295287340052,
    "ttft": 2197459.327429406,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.928531191088265,
    "arrivals": 1119422,
    "finished_requests": 67513,
    "scheduler_time": 93.48621107587428
}
#Debug simulation 
Total elapsed time: 5.888801683206111. Arrivals time: 0.32672547083348036 Scheduler time: 5.4677600977011025 Scheduler overhead time: 0.028168683871626854 Adapter cache time: 0.025438404642045498 Engine time: 0.027843627147376537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.223643396049738,
    "estimated_duration": 3600.1391598180912,
    "input_throughput": 4418.108382455886,
    "output_throughput": 3930.5041754872036,
    "total_throughput": 8348.61255794309,
    "itl": 178.47609191370677,
    "ttft": 2219304.9183415724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.488788792043868,
    "arrivals": 1119422,
    "finished_requests": 64734,
    "scheduler_time": 96.93719609678203
}
#Debug simulation 
Total elapsed time: 5.22373517928645. Arrivals time: 0.23341718269512057 Scheduler time: 4.877299185842276 Scheduler overhead time: 0.03174874745309353 Adapter cache time: 0.034747089724987745 Engine time: 0.03175366297364235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.77607238991186,
    "estimated_duration": 3600.168473227916,
    "input_throughput": 4610.443684352881,
    "output_throughput": 4093.1128944606794,
    "total_throughput": 8703.556578813561,
    "itl": 210.05416951878874,
    "ttft": 2197437.223117975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7777595185837316,
    "arrivals": 1119422,
    "finished_requests": 67516,
    "scheduler_time": 93.49220628533573
}
#Debug simulation 
Total elapsed time: 5.776167861651629. Arrivals time: 0.25019619101658463 Scheduler time: 5.432315661106259 Scheduler overhead time: 0.027918816078454256 Adapter cache time: 0.02506694244220853 Engine time: 0.027796092443168163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.268912214785814,
    "estimated_duration": 3600.004310870525,
    "input_throughput": 4418.084709502458,
    "output_throughput": 3930.458626750488,
    "total_throughput": 8348.543336252946,
    "itl": 178.4786569046337,
    "ttft": 2219259.4608397437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.553426238261092,
    "arrivals": 1119422,
    "finished_requests": 64732,
    "scheduler_time": 96.93187787022335
}
#Debug simulation 
Total elapsed time: 5.2690040287561715. Arrivals time: 0.2334349500015378 Scheduler time: 4.921557680238038 Scheduler overhead time: 0.03191187744960189 Adapter cache time: 0.03494268422946334 Engine time: 0.03234633430838585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.767422864213586,
    "estimated_duration": 3600.2315055873983,
    "input_throughput": 4611.058198406458,
    "output_throughput": 4093.678969568203,
    "total_throughput": 8704.737167974661,
    "itl": 210.0450535012686,
    "ttft": 2197435.1002300214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6000205550621285,
    "arrivals": 1119422,
    "finished_requests": 67523,
    "scheduler_time": 93.4982106490093
}
#Debug simulation 
Total elapsed time: 5.767542703077197. Arrivals time: 0.24946630699560046 Scheduler time: 5.424311000853777 Scheduler overhead time: 0.027971560135483742 Adapter cache time: 0.025186368729919195 Engine time: 0.027709002140909433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.301139271818101,
    "estimated_duration": 3600.082112391557,
    "input_throughput": 4417.989230094012,
    "output_throughput": 3930.3736854491594,
    "total_throughput": 8348.362915543172,
    "itl": 178.48240081658344,
    "ttft": 2219288.9488227535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.631142078265451,
    "arrivals": 1119422,
    "finished_requests": 64732,
    "scheduler_time": 96.93196355131828
}
#Debug simulation 
Total elapsed time: 5.301264222245663. Arrivals time: 0.32290913071483374 Scheduler time: 4.865018803626299 Scheduler overhead time: 0.03186165355145931 Adapter cache time: 0.03496316587552428 Engine time: 0.031692635267972946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.547723413910717,
    "estimated_duration": 3600.1176628308913,
    "input_throughput": 4642.703812868441,
    "output_throughput": 4093.4442649332477,
    "total_throughput": 8736.148077801688,
    "itl": 209.20744933985762,
    "ttft": 2194725.4197048508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6603433417436646,
    "arrivals": 1115519,
    "finished_requests": 67809,
    "scheduler_time": 93.55741189202253
}
#Debug simulation 
Total elapsed time: 5.54782427335158. Arrivals time: 0.24595703231170774 Scheduler time: 5.207512460183352 Scheduler overhead time: 0.028134589549154043 Adapter cache time: 0.02547643007710576 Engine time: 0.02783342031762004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.594631395768374,
    "estimated_duration": 3600.1234136378357,
    "input_throughput": 4642.230301519672,
    "output_throughput": 4093.064683332648,
    "total_throughput": 8735.29498485232,
    "itl": 209.22146180578324,
    "ttft": 2194764.4993531248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.89671759461286,
    "arrivals": 1115519,
    "finished_requests": 67802,
    "scheduler_time": 93.55178871475067
}
#Debug simulation 
Total elapsed time: 5.59472740907222. Arrivals time: 0.24989617336541414 Scheduler time: 5.250465937424451 Scheduler overhead time: 0.028024702798575163 Adapter cache time: 0.025553309358656406 Engine time: 0.02788998605683446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.20131717203185,
    "estimated_duration": 3600.1717945414975,
    "input_throughput": 4455.897639196698,
    "output_throughput": 3934.0511531905304,
    "total_throughput": 8389.94879238723,
    "itl": 178.81045978494205,
    "ttft": 2215823.9086018265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.491174398697862,
    "arrivals": 1115519,
    "finished_requests": 65100,
    "scheduler_time": 96.88071953543825
}
#Debug simulation 
Total elapsed time: 5.201416871976107. Arrivals time: 0.31697577657178044 Scheduler time: 4.771823636256158 Scheduler overhead time: 0.031709165312349796 Adapter cache time: 0.03434791415929794 Engine time: 0.031787178944796324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.556936220265925,
    "estimated_duration": 3600.21209891613,
    "input_throughput": 4642.582031495299,
    "output_throughput": 4093.336891022794,
    "total_throughput": 8735.918922518093,
    "itl": 209.2126696766072,
    "ttft": 2194761.4595540776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7546563068613965,
    "arrivals": 1115519,
    "finished_requests": 67809,
    "scheduler_time": 93.55753501203068
}
#Debug simulation 
Total elapsed time: 5.557040330953896. Arrivals time: 0.25451687118038535 Scheduler time: 5.208424656186253 Scheduler overhead time: 0.027844070456922054 Adapter cache time: 0.025506984908133745 Engine time: 0.027848755475133657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.138831187039614,
    "estimated_duration": 3600.035331878271,
    "input_throughput": 4455.551549165288,
    "output_throughput": 3934.1719439766052,
    "total_throughput": 8389.723493141893,
    "itl": 178.81568959776672,
    "ttft": 2215796.5883282237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.552416492681883,
    "arrivals": 1115519,
    "finished_requests": 65096,
    "scheduler_time": 96.87545941182051
}
#Debug simulation 
Total elapsed time: 5.138925336301327. Arrivals time: 0.23430890496820211 Scheduler time: 4.7913881205022335 Scheduler overhead time: 0.03176036477088928 Adapter cache time: 0.03503707889467478 Engine time: 0.03167771128937602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.576987063977867,
    "estimated_duration": 3600.033268396252,
    "input_throughput": 4642.812650296952,
    "output_throughput": 4093.540226244911,
    "total_throughput": 8736.352876541863,
    "itl": 209.2029218181673,
    "ttft": 2194692.0125673357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1196,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.576100152702912,
    "arrivals": 1115519,
    "finished_requests": 67809,
    "scheduler_time": 93.55726064626364
}
#Debug simulation 
Total elapsed time: 5.577082913834602. Arrivals time: 0.3253030311316252 Scheduler time: 5.15806658891961 Scheduler overhead time: 0.027841956354677677 Adapter cache time: 0.025317239109426737 Engine time: 0.027755656745284796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.0915729138068855,
    "estimated_duration": 3600.114017587331,
    "input_throughput": 4455.4541666292935,
    "output_throughput": 3934.085956947454,
    "total_throughput": 8389.540123576748,
    "itl": 178.8194123964027,
    "ttft": 2215826.100890243,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1681,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.631012609191139,
    "arrivals": 1115519,
    "finished_requests": 65096,
    "scheduler_time": 96.8755490044437
}
#Debug simulation 
Total elapsed time: 5.091723653953522. Arrivals time: 0.23650746000930667 Scheduler time: 4.7424001125618815 Scheduler overhead time: 0.03178907185792923 Adapter cache time: 0.03451755456626415 Engine time: 0.03174095693975687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.475969917140901,
    "estimated_duration": 3600.0455784339047,
    "input_throughput": 4648.543090746107,
    "output_throughput": 4091.050148983662,
    "total_throughput": 8739.59323972977,
    "itl": 209.40786581082938,
    "ttft": 2192380.6132738376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4246857854608344,
    "arrivals": 1113643,
    "finished_requests": 67749,
    "scheduler_time": 93.57047852662647
}
#Debug simulation 
Total elapsed time: 5.476063601206988. Arrivals time: 0.2505138204433024 Scheduler time: 5.133215361274779 Scheduler overhead time: 0.02776509989053011 Adapter cache time: 0.023675653152167797 Engine time: 0.02797914808616042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.5385989849455655,
    "estimated_duration": 3600.0334677896885,
    "input_throughput": 4648.3617860014865,
    "output_throughput": 4090.508361035126,
    "total_throughput": 8738.870147036612,
    "itl": 209.41814765489661,
    "ttft": 2192367.8620893867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6494526015967184,
    "arrivals": 1113643,
    "finished_requests": 67744,
    "scheduler_time": 93.56472945449906
}
#Debug simulation 
Total elapsed time: 5.5386991747654974. Arrivals time: 0.32616101624444127 Scheduler time: 5.120642858557403 Scheduler overhead time: 0.027806217782199383 Adapter cache time: 0.023587622679769993 Engine time: 0.027691415045410395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.457541824318469,
    "estimated_duration": 3600.0773025367434,
    "input_throughput": 4460.901989155576,
    "output_throughput": 3931.2172519260935,
    "total_throughput": 8392.11924108167,
    "itl": 178.99145939176904,
    "ttft": 2211646.691164519,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.2227117020077465,
    "arrivals": 1113643,
    "finished_requests": 65019,
    "scheduler_time": 96.90265050510382
}
#Debug simulation 
Total elapsed time: 5.45761487307027. Arrivals time: 0.5753639303147793 Scheduler time: 4.771302923094481 Scheduler overhead time: 0.03162402659654617 Adapter cache time: 0.03305721562355757 Engine time: 0.03159468760713935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.531654422171414,
    "estimated_duration": 3600.1364626097875,
    "input_throughput": 4648.42573991448,
    "output_throughput": 4090.946871864823,
    "total_throughput": 8739.372611779303,
    "itl": 209.41281381481835,
    "ttft": 2192416.271783248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5154333371482456,
    "arrivals": 1113643,
    "finished_requests": 67749,
    "scheduler_time": 93.57061515072026
}
#Debug simulation 
Total elapsed time: 5.531754525378346. Arrivals time: 0.3211132241412997 Scheduler time: 5.11839673249051 Scheduler overhead time: 0.027814954053610563 Adapter cache time: 0.02379844104871154 Engine time: 0.027815813664346933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.0903857769444585,
    "estimated_duration": 3600.134872412975,
    "input_throughput": 4460.830654723813,
    "output_throughput": 3931.154387700543,
    "total_throughput": 8391.985042424356,
    "itl": 178.9941313009285,
    "ttft": 2211668.754068254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.280181182399335,
    "arrivals": 1113643,
    "finished_requests": 65019,
    "scheduler_time": 96.9027509009955
}
#Debug simulation 
Total elapsed time: 5.090478333644569. Arrivals time: 0.23860612511634827 Scheduler time: 4.7406788328662515 Scheduler overhead time: 0.0316333738155663 Adapter cache time: 0.03305043140426278 Engine time: 0.03178134094923735 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.467683928087354,
    "estimated_duration": 3600.2037275927323,
    "input_throughput": 4648.6547057687085,
    "output_throughput": 4091.2359728733186,
    "total_throughput": 8739.890678642027,
    "itl": 209.40266045786035,
    "ttft": 2192388.9034676496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3458662799954526,
    "arrivals": 1113643,
    "finished_requests": 67756,
    "scheduler_time": 93.576510306219
}
#Debug simulation 
Total elapsed time: 5.467783580999821. Arrivals time: 0.2488150494173169 Scheduler time: 5.126220296137035 Scheduler overhead time: 0.027946852147579193 Adapter cache time: 0.023960440419614315 Engine time: 0.0279330313205719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.089726249221712,
    "estimated_duration": 3600.020450133641,
    "input_throughput": 4460.615216617416,
    "output_throughput": 3931.1082245309585,
    "total_throughput": 8391.723441148375,
    "itl": 178.999668788054,
    "ttft": 2211591.1655404735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1598,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.356639484539556,
    "arrivals": 1113643,
    "finished_requests": 65013,
    "scheduler_time": 96.89761232235986
}
#Debug simulation 
Total elapsed time: 5.089845436159521. Arrivals time: 0.2390515422448516 Scheduler time: 4.739890059456229 Scheduler overhead time: 0.031536445021629333 Adapter cache time: 0.033138807862997055 Engine time: 0.031481615267693996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.538060040213168,
    "estimated_duration": 3600.0123634747993,
    "input_throughput": 4666.41980745453,
    "output_throughput": 4091.2251161781605,
    "total_throughput": 8757.64492363269,
    "itl": 209.02276213301369,
    "ttft": 2203283.338168304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1069,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2716613982641913,
    "arrivals": 1112647,
    "finished_requests": 67621,
    "scheduler_time": 93.60986772607109
}
#Debug simulation 
Total elapsed time: 5.538155978079885. Arrivals time: 0.3681538002565503 Scheduler time: 5.078922236803919 Scheduler overhead time: 0.027748493943363428 Adapter cache time: 0.022793500684201717 Engine time: 0.0276973401196301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.553287428338081,
    "estimated_duration": 3600.0051914370774,
    "input_throughput": 4666.401881852293,
    "output_throughput": 4091.193544673928,
    "total_throughput": 8757.59542652622,
    "itl": 209.03410162779784,
    "ttft": 2203294.8040819233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1069,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.488509417935281,
    "arrivals": 1112647,
    "finished_requests": 67620,
    "scheduler_time": 93.60433681030293
}
#Debug simulation 
Total elapsed time: 5.55338716506958. Arrivals time: 0.24751113401725888 Scheduler time: 5.2145811105147 Scheduler overhead time: 0.02793862298130989 Adapter cache time: 0.02255745278671384 Engine time: 0.02793997712433338 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.13928348897025,
    "estimated_duration": 3600.034496661332,
    "input_throughput": 4464.626662579458,
    "output_throughput": 3922.4382469378334,
    "total_throughput": 8387.06490951729,
    "itl": 177.2606425017645,
    "ttft": 2228064.333148458,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.195320032369295,
    "arrivals": 1112647,
    "finished_requests": 64731,
    "scheduler_time": 97.10209946541751
}
#Debug simulation 
Total elapsed time: 5.139401810243726. Arrivals time: 0.2345218351110816 Scheduler time: 4.794051305390894 Scheduler overhead time: 0.03169248113408685 Adapter cache time: 0.03250687615945935 Engine time: 0.031800697557628155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.579560458660126,
    "estimated_duration": 3600.101029165375,
    "input_throughput": 4666.304879753504,
    "output_throughput": 4091.1243547558315,
    "total_throughput": 8757.429234509334,
    "itl": 209.02764344290912,
    "ttft": 2203316.9113410804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1069,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.360210487944975,
    "arrivals": 1112647,
    "finished_requests": 67621,
    "scheduler_time": 93.6099843268655
}
#Debug simulation 
Total elapsed time: 5.579654959030449. Arrivals time: 0.38128931215032935 Scheduler time: 5.106627006083727 Scheduler overhead time: 0.027812262997031212 Adapter cache time: 0.022893966641277075 Engine time: 0.028147170320153236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.132945513818413,
    "estimated_duration": 3600.092810257013,
    "input_throughput": 4464.554345434376,
    "output_throughput": 3922.3747120541316,
    "total_throughput": 8386.929057488507,
    "itl": 177.26332586111846,
    "ttft": 2228086.218718919,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.25354403547937,
    "arrivals": 1112647,
    "finished_requests": 64731,
    "scheduler_time": 97.1021890580407
}
#Debug simulation 
Total elapsed time: 5.133037352003157. Arrivals time: 0.3521417314186692 Scheduler time: 4.668899917509407 Scheduler overhead time: 0.032002723310142756 Adapter cache time: 0.03297908836975694 Engine time: 0.032009833957999945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.53285859990865,
    "estimated_duration": 3600.161919809486,
    "input_throughput": 4666.467890668936,
    "output_throughput": 4091.3201483947296,
    "total_throughput": 8757.788039063666,
    "itl": 209.01947447584723,
    "ttft": 2203289.444353592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1069,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.196363765250349,
    "arrivals": 1112647,
    "finished_requests": 67624,
    "scheduler_time": 93.61565804072755
}
#Debug simulation 
Total elapsed time: 5.532946684863418. Arrivals time: 0.3643517503514886 Scheduler time: 5.07697761291638 Scheduler overhead time: 0.027919838670641184 Adapter cache time: 0.022984151262789965 Engine time: 0.027898586820811033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.131800173316151,
    "estimated_duration": 3600.168601486956,
    "input_throughput": 4464.460357040374,
    "output_throughput": 3922.2921376981412,
    "total_throughput": 8386.752494738515,
    "itl": 177.26688574674412,
    "ttft": 2228114.257440972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1589,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.329247814901105,
    "arrivals": 1112647,
    "finished_requests": 64731,
    "scheduler_time": 97.10227650863649
}
#Debug simulation 
Total elapsed time: 5.131891988217831. Arrivals time: 0.35484746750444174 Scheduler time: 4.665634706616402 Scheduler overhead time: 0.03188686724752188 Adapter cache time: 0.03275848785415292 Engine time: 0.03190119145438075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693311361 . Total output tokens: 622101039
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.353726604022086,
    "estimated_duration": 3600.115977303642,
    "input_throughput": 4758.130323576304,
    "output_throughput": 4177.882627899412,
    "total_throughput": 8936.012951475715,
    "itl": 204.6985643847598,
    "ttft": 2175833.7325390247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.118694492387797,
    "arrivals": 1035175,
    "finished_requests": 68965,
    "scheduler_time": 95.59183637709782
}
#Debug simulation 
Total elapsed time: 5.35380385722965. Arrivals time: 0.24046130944043398 Scheduler time: 5.00348142767325 Scheduler overhead time: 0.02729451283812523 Adapter cache time: 0.04223965108394623 Engine time: 0.027657055761665106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693311361 . Total output tokens: 622101039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.999557695817202,
    "estimated_duration": 3600.1353315051806,
    "input_throughput": 4757.834476416363,
    "output_throughput": 4177.602955195563,
    "total_throughput": 8935.437431611927,
    "itl": 204.72626629243226,
    "ttft": 2175888.868903727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.601254779714765,
    "arrivals": 1035175,
    "finished_requests": 68960,
    "scheduler_time": 95.5799062498607
}
#Debug simulation 
Total elapsed time: 4.999681131914258. Arrivals time: 0.2385620903223753 Scheduler time: 4.651013150345534 Scheduler overhead time: 0.027215105947107077 Adapter cache time: 0.04227165970951319 Engine time: 0.027849592734128237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693311361 . Total output tokens: 622101039
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.024718238040805,
    "estimated_duration": 3600.028200056091,
    "input_throughput": 4688.969103002301,
    "output_throughput": 4120.602999656745,
    "total_throughput": 8809.572102659047,
    "itl": 170.29716473966522,
    "ttft": 2187045.890133271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.54919031305236,
    "arrivals": 1035175,
    "finished_requests": 67944,
    "scheduler_time": 101.59126929911466
}
#Debug simulation 
Total elapsed time: 5.024805213790387. Arrivals time: 0.302133159711957 Scheduler time: 4.597228750120848 Scheduler overhead time: 0.03183467034250498 Adapter cache time: 0.04616487305611372 Engine time: 0.032409900799393654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693311361 . Total output tokens: 622101039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.006295362021774,
    "estimated_duration": 3600.0693927325924,
    "input_throughput": 4758.133838914133,
    "output_throughput": 4177.936133776411,
    "total_throughput": 8936.069972690544,
    "itl": 204.71010112084215,
    "ttft": 2175828.110731275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.303797387890252,
    "arrivals": 1035175,
    "finished_requests": 68964,
    "scheduler_time": 95.58578688966126
}
#Debug simulation 
Total elapsed time: 5.0063878279179335. Arrivals time: 0.23247712338343263 Scheduler time: 4.663804905023426 Scheduler overhead time: 0.027137203607708216 Adapter cache time: 0.042222531512379646 Engine time: 0.02786218374967575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_256_slots_128_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693311361 . Total output tokens: 622101039
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.9757589269429445,
    "estimated_duration": 3600.1184602357275,
    "input_throughput": 4688.851543761343,
    "output_throughput": 4120.499690176496,
    "total_throughput": 8809.35123393784,
    "itl": 170.3013575270074,
    "ttft": 2187078.88002076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.639355777911697,
    "arrivals": 1035175,
    "finished_requests": 67944,
    "scheduler_time": 101.5913640139772
}
#Debug simulation 
Total elapsed time: 4.975860524922609. Arrivals time: 0.23641353007405996 Scheduler time: 4.614009732380509 Scheduler overhead time: 0.03174330899491906 Adapter cache time: 0.04605680238455534 Engine time: 0.032524894922971725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_256_slots_128_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_256_slots_128_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693311361 . Total output tokens: 622101039
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.035239360760897,
    "estimated_duration": 3600.1847643811484,
    "input_throughput": 4758.155517316787,
    "output_throughput": 4178.013903290758,
    "total_throughput": 8936.169420607546,
    "itl": 204.69142740839996,
    "ttft": 2175821.6607704745,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.95485698594225,
    "arrivals": 1035175,
    "finished_requests": 68969,
    "scheduler_time": 95.59788587901197
}
#Debug simulation 
Total elapsed time: 5.035354661755264. Arrivals time: 0.23684046510607004 Scheduler time: 4.6883812779560685 Scheduler overhead time: 0.027342993766069412 Adapter cache time: 0.04226600006222725 Engine time: 0.027698236983269453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_256_slots_128_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_256_slots_128_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 540, 34560, 34560, 540, 34560, 1080, 34560, 540, 1080, 540, 34560, 1080, 1080, 1080, 1080, 34560, 540, 540, 540, 1080, 34560, 540, 34560, 34560, 1080, 1080, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 540, 34560, 34560, 1080, 540, 540, 34560, 1080, 540, 1080, 1080, 34560, 1080, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 540, 1080, 1080, 1080, 540, 540, 34560, 34560, 540, 34560, 540, 1080, 34560, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 540, 540, 540, 34560, 1080, 540, 34560, 540, 34560, 540, 540, 1080, 1080, 34560, 1080, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 1080, 540, 34560, 540, 540, 34560, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 34560, 540, 34560, 1080, 540, 1080, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 1080, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 1080, 34560, 1080, 540, 1080, 34560, 1080, 34560, 540, 34560, 540, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 540, 34560, 540, 34560, 1080, 1080, 540, 1080, 34560, 1080, 1080, 34560, 540, 34560, 34560, 1080, 540, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 540, 34560, 1080, 34560, 34560, 540, 34560, 1080, 1080, 34560, 540, 1080, 540, 540, 540, 540, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 540, 540, 540, 540]
Prompts retrieved: 3109860 . Total input tokens: 693311361 . Total output tokens: 622101039
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.964536817744374,
    "estimated_duration": 3600.033638973966,
    "input_throughput": 4688.7114657102975,
    "output_throughput": 4120.488997493857,
    "total_throughput": 8809.200463204155,
    "itl": 170.3072116866858,
    "ttft": 2187093.4942675782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2307,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.74599498879134,
    "arrivals": 1035175,
    "finished_requests": 67942,
    "scheduler_time": 101.58611795487717
}
#Debug simulation 
Total elapsed time: 4.964659045916051. Arrivals time: 0.2328805774450302 Scheduler time: 4.606020215433091 Scheduler overhead time: 0.0319100865162909 Adapter cache time: 0.046110114082694054 Engine time: 0.03259103186428547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688216455 . Total output tokens: 617448558
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.220299527980387,
    "estimated_duration": 3600.1079097974607,
    "input_throughput": 4839.692708261133,
    "output_throughput": 4272.404990456336,
    "total_throughput": 9112.097698717469,
    "itl": 200.940974327944,
    "ttft": 2165646.6587445545,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1964,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.010797929084101,
    "arrivals": 1027687,
    "finished_requests": 70534,
    "scheduler_time": 97.53461342287495
}
#Debug simulation 
Total elapsed time: 5.220395104959607. Arrivals time: 0.35650499630719423 Scheduler time: 4.756647000089288 Scheduler overhead time: 0.027525561396032572 Adapter cache time: 0.038380828220397234 Engine time: 0.02815651847049594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688216455 . Total output tokens: 617448558
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.085924014914781,
    "estimated_duration": 3600.055049520121,
    "input_throughput": 4838.740730456889,
    "output_throughput": 4271.534959458417,
    "total_throughput": 9110.275689915306,
    "itl": 200.95885879156776,
    "ttft": 2165714.172786914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1964,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.405675669752314,
    "arrivals": 1027687,
    "finished_requests": 70517,
    "scheduler_time": 97.52295689426913
}
#Debug simulation 
Total elapsed time: 5.086038365028799. Arrivals time: 0.24125556088984013 Scheduler time: 4.737999459728599 Scheduler overhead time: 0.027523044496774673 Adapter cache time: 0.038119015749543905 Engine time: 0.02819227846339345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688216455 . Total output tokens: 617448558
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.029581060167402,
    "estimated_duration": 3600.1552045372687,
    "input_throughput": 4749.235249205769,
    "output_throughput": 4198.291779463069,
    "total_throughput": 8947.527028668837,
    "itl": 167.35967062138837,
    "ttft": 2177407.394842574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1931,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.3110724224335994,
    "arrivals": 1027687,
    "finished_requests": 69246,
    "scheduler_time": 103.43107274469122
}
#Debug simulation 
Total elapsed time: 5.0296766250394285. Arrivals time: 0.23667417652904987 Scheduler time: 4.6701188632287085 Scheduler overhead time: 0.032356598880141973 Adapter cache time: 0.04196224547922611 Engine time: 0.033116073813289404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688216455 . Total output tokens: 617448558
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.201791057828814,
    "estimated_duration": 3600.040680934802,
    "input_throughput": 4839.111150120779,
    "output_throughput": 4271.927003893355,
    "total_throughput": 9111.038154014133,
    "itl": 200.94680609918657,
    "ttft": 2165686.965073906,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1964,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.166647408464736,
    "arrivals": 1027687,
    "finished_requests": 70525,
    "scheduler_time": 97.52872572776006
}
#Debug simulation 
Total elapsed time: 5.2018785970285535. Arrivals time: 0.34724200470373034 Scheduler time: 4.747785302810371 Scheduler overhead time: 0.027481575962156057 Adapter cache time: 0.03831087937578559 Engine time: 0.028116144239902496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_256_slots_128_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688216455 . Total output tokens: 617448558
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.162281743250787,
    "estimated_duration": 3600.046051104876,
    "input_throughput": 4748.950918212004,
    "output_throughput": 4198.124908808206,
    "total_throughput": 8947.07582702021,
    "itl": 167.36204849792895,
    "ttft": 2177387.8142852345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1931,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.3835066034085015,
    "arrivals": 1027687,
    "finished_requests": 69242,
    "scheduler_time": 103.42584700276302
}
#Debug simulation 
Total elapsed time: 5.1623899838887155. Arrivals time: 0.3446700405329466 Scheduler time: 4.694677052088082 Scheduler overhead time: 0.03224355261772871 Adapter cache time: 0.04224046040326357 Engine time: 0.0331906289793551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_256_slots_128_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_256_slots_128_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688216455 . Total output tokens: 617448558
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.084968795999885,
    "estimated_duration": 3600.1913048571264,
    "input_throughput": 4840.056409361146,
    "output_throughput": 4272.677393350477,
    "total_throughput": 9112.733802711622,
    "itl": 200.9324758323819,
    "ttft": 2165632.419024614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1964,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.872458779187701,
    "arrivals": 1027687,
    "finished_requests": 70541,
    "scheduler_time": 97.54046321177621
}
#Debug simulation 
Total elapsed time: 5.085062351077795. Arrivals time: 0.24161155242472887 Scheduler time: 4.736784271430224 Scheduler overhead time: 0.027385557536035776 Adapter cache time: 0.03819074295461178 Engine time: 0.028159413021057844 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_256_slots_128_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_256_slots_128_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 270, 34560, 34560, 270, 34560, 1080, 34560, 270, 1080, 270, 34560, 1080, 1080, 1080, 1080, 34560, 270, 270, 270, 1080, 34560, 270, 34560, 34560, 1080, 1080, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 270, 34560, 34560, 1080, 270, 270, 34560, 1080, 270, 1080, 1080, 34560, 1080, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 270, 1080, 1080, 1080, 270, 270, 34560, 34560, 270, 34560, 270, 1080, 34560, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 270, 270, 270, 34560, 1080, 270, 34560, 270, 34560, 270, 270, 1080, 1080, 34560, 1080, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 1080, 270, 34560, 270, 270, 34560, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 34560, 270, 34560, 1080, 270, 1080, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 1080, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 1080, 34560, 1080, 270, 1080, 34560, 1080, 34560, 270, 34560, 270, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 270, 34560, 270, 34560, 1080, 1080, 270, 1080, 34560, 1080, 1080, 34560, 270, 34560, 34560, 1080, 270, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 270, 34560, 1080, 34560, 34560, 270, 34560, 1080, 1080, 34560, 270, 1080, 270, 270, 270, 270, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 270, 270, 270, 270]
Prompts retrieved: 3086910 . Total input tokens: 688216455 . Total output tokens: 617448558
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.046843732241541,
    "estimated_duration": 3600.135544095893,
    "input_throughput": 4748.832867706223,
    "output_throughput": 4198.020550861082,
    "total_throughput": 8946.853418567303,
    "itl": 167.36617870699345,
    "ttft": 2177420.678351282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1931,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.4729175455493895,
    "arrivals": 1027687,
    "finished_requests": 69242,
    "scheduler_time": 103.42592905172458
}
#Debug simulation 
Total elapsed time: 5.046981115359813. Arrivals time: 0.2383238421753049 Scheduler time: 4.686067115049809 Scheduler overhead time: 0.03225636016577482 Adapter cache time: 0.041755590587854385 Engine time: 0.03320521581918001 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685649461 . Total output tokens: 615185283
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.292617213912308,
    "estimated_duration": 3600.1053059843816,
    "input_throughput": 4979.361567619036,
    "output_throughput": 4356.6983926630855,
    "total_throughput": 9336.059960282122,
    "itl": 196.107854136263,
    "ttft": 2158613.882654681,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.872296488341077,
    "arrivals": 1023854,
    "finished_requests": 72011,
    "scheduler_time": 99.65096374160888
}
#Debug simulation 
Total elapsed time: 5.292710619047284. Arrivals time: 0.35621819365769625 Scheduler time: 4.832404148764908 Scheduler overhead time: 0.028066460508853197 Adapter cache time: 0.03408318338915706 Engine time: 0.028699043206870556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685649461 . Total output tokens: 615185283
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.3048015660606325,
    "estimated_duration": 3600.2039064831843,
    "input_throughput": 4978.912713171827,
    "output_throughput": 4356.479634877385,
    "total_throughput": 9335.392348049212,
    "itl": 196.12353845126614,
    "ttft": 2158715.7697275034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.190410939801844,
    "arrivals": 1023854,
    "finished_requests": 72008,
    "scheduler_time": 99.64529950612571
}
#Debug simulation 
Total elapsed time: 5.304911521263421. Arrivals time: 0.36976508470252156 Scheduler time: 4.830515384674072 Scheduler overhead time: 0.028099813498556614 Adapter cache time: 0.034484990406781435 Engine time: 0.028799119871109724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685649461 . Total output tokens: 615185283
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.2693075546994805,
    "estimated_duration": 3600.0623345715558,
    "input_throughput": 4871.358151660201,
    "output_throughput": 4272.596574867521,
    "total_throughput": 9143.954726527722,
    "itl": 163.8684501706597,
    "ttft": 2171359.561964316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.098635089788531,
    "arrivals": 1023854,
    "finished_requests": 70488,
    "scheduler_time": 105.3763764635955
}
#Debug simulation 
Total elapsed time: 5.269391717854887. Arrivals time: 0.3462231741286814 Scheduler time: 4.804023777600378 Scheduler overhead time: 0.033054237719625235 Adapter cache time: 0.03677477315068245 Engine time: 0.03363973926752806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685649461 . Total output tokens: 615185283
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.370127084199339,
    "estimated_duration": 3600.011585612325,
    "input_throughput": 4979.178698101641,
    "output_throughput": 4356.712368005415,
    "total_throughput": 9335.891066107057,
    "itl": 196.11381334717947,
    "ttft": 2158640.369755638,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.998371140134876,
    "arrivals": 1023854,
    "finished_requests": 72008,
    "scheduler_time": 99.64501843488168
}
#Debug simulation 
Total elapsed time: 5.370215977076441. Arrivals time: 0.4267162727192044 Scheduler time: 4.839293258730322 Scheduler overhead time: 0.02810821682214737 Adapter cache time: 0.034174848813563585 Engine time: 0.028720229864120483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685649461 . Total output tokens: 615185283
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 5.254052923992276,
    "estimated_duration": 3600.1195201084292,
    "input_throughput": 4871.280773331606,
    "output_throughput": 4272.528707473783,
    "total_throughput": 9143.809480805388,
    "itl": 163.87094048172412,
    "ttft": 2171382.1876328806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.155727308820885,
    "arrivals": 1023854,
    "finished_requests": 70488,
    "scheduler_time": 105.37646978148368
}
#Debug simulation 
Total elapsed time: 5.254158734809607. Arrivals time: 0.36181093705818057 Scheduler time: 4.7734269285574555 Scheduler overhead time: 0.03284046100452542 Adapter cache time: 0.0365966004319489 Engine time: 0.03385589458048344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685649461 . Total output tokens: 615185283
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.303566099144518,
    "estimated_duration": 3600.213143539844,
    "input_throughput": 4979.4276853213205,
    "output_throughput": 4356.58650603624,
    "total_throughput": 9336.01419135756,
    "itl": 196.10113795998078,
    "ttft": 2158638.184053342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.760160069484131,
    "arrivals": 1023854,
    "finished_requests": 72013,
    "scheduler_time": 99.65690629453547
}
#Debug simulation 
Total elapsed time: 5.303660155273974. Arrivals time: 0.3571704742498696 Scheduler time: 4.842113762162626 Scheduler overhead time: 0.028195477090775967 Adapter cache time: 0.03405824163928628 Engine time: 0.028857663739472628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_256_slots_128_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [85 85 86]
Adapter prompts. [1080, 1080, 34560, 135, 34560, 34560, 135, 34560, 1080, 34560, 135, 1080, 135, 34560, 1080, 1080, 1080, 1080, 34560, 135, 135, 135, 1080, 34560, 135, 34560, 34560, 1080, 1080, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 135, 1080, 1080, 34560, 1080, 135, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 135, 34560, 34560, 1080, 135, 135, 34560, 1080, 135, 1080, 1080, 34560, 1080, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 1080, 135, 1080, 135, 135, 135, 1080, 1080, 1080, 135, 135, 34560, 34560, 135, 34560, 135, 1080, 34560, 34560, 34560, 1080, 135, 34560, 1080, 135, 135, 135, 135, 135, 34560, 1080, 135, 34560, 135, 34560, 135, 135, 1080, 1080, 34560, 1080, 34560, 135, 1080, 135, 34560, 34560, 135, 34560, 1080, 135, 34560, 135, 135, 34560, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 34560, 135, 34560, 1080, 135, 1080, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 1080, 34560, 34560, 1080, 135, 34560, 135, 1080, 1080, 135, 1080, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 1080, 34560, 1080, 135, 1080, 34560, 1080, 34560, 135, 34560, 135, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 135, 34560, 135, 34560, 1080, 1080, 135, 1080, 34560, 1080, 1080, 34560, 135, 34560, 34560, 1080, 135, 34560, 34560, 1080, 1080, 34560, 34560, 1080, 1080, 1080, 1080, 34560, 34560, 1080, 1080, 34560, 135, 34560, 1080, 34560, 34560, 135, 34560, 1080, 1080, 34560, 135, 1080, 135, 135, 135, 135, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 135, 135, 135, 135]
Prompts retrieved: 3075435 . Total input tokens: 685649461 . Total output tokens: 615185283
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.217104568146169,
    "estimated_duration": 3600.0087904269512,
    "input_throughput": 4870.90393963187,
    "output_throughput": 4271.903457817975,
    "total_throughput": 9142.807397449846,
    "itl": 163.8718408286621,
    "ttft": 2171319.8984546633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.228287243582249,
    "arrivals": 1023854,
    "finished_requests": 70479,
    "scheduler_time": 105.37125435149527
}
#Debug simulation 
Total elapsed time: 5.217225831001997. Arrivals time: 0.3531775432638824 Scheduler time: 4.744972169864923 Scheduler overhead time: 0.0328436610288918 Adapter cache time: 0.03705748310312629 Engine time: 0.03359149117022753 
