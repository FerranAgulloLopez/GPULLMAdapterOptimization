INFO 06-01 00:47:13 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:14 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9977911529131234,
    "estimated_duration": 3599.9961856474256,
    "input_throughput": 1394.9745335901955,
    "output_throughput": 1245.5327085835816,
    "total_throughput": 2640.507242173777,
    "itl": 26.244670170839438,
    "ttft": 6031.5809352228725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.652332706414131,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.10840769107959941
}
#Debug simulation 
Total elapsed time: 1.997903733048588. Arrivals time: 0.06188103649765253 Scheduler time: 1.4467365336604416 Scheduler overhead time: 0.1269736010581255 Adapter cache time: 0.17720041470602155 Engine time: 0.12394322222098708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.9685702892020345,
    "estimated_duration": 3600.0163348571573,
    "input_throughput": 1394.9667259493867,
    "output_throughput": 1245.5257373652762,
    "total_throughput": 2640.492463314663,
    "itl": 26.242391550782664,
    "ttft": 6031.458683671262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.348648142313791,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.10827775673426392
}
#Debug simulation 
Total elapsed time: 1.968674274161458. Arrivals time: 0.06086521642282605 Scheduler time: 1.424236782360822 Scheduler overhead time: 0.12515191407874227 Adapter cache time: 0.17911096569150686 Engine time: 0.11987598380073905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.0062932479195297,
    "estimated_duration": 3600.0209713251993,
    "input_throughput": 1394.96492937134,
    "output_throughput": 1245.5241332523217,
    "total_throughput": 2640.489062623662,
    "itl": 26.246840776077327,
    "ttft": 6031.642484995421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.741617894768544,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.10846431839217688
}
#Debug simulation 
Total elapsed time: 2.0063889320008457. Arrivals time: 0.06302996957674623 Scheduler time: 1.4558780384249985 Scheduler overhead time: 0.12474170187488198 Adapter cache time: 0.1820831000804901 Engine time: 0.12079091835767031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.9742205538786948,
    "estimated_duration": 3600.020474421597,
    "input_throughput": 1394.96512191555,
    "output_throughput": 1245.5243051695186,
    "total_throughput": 2640.4894270850687,
    "itl": 26.447798565960106,
    "ttft": 6031.903628031684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.002697790660683,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.11773869075705244
}
#Debug simulation 
Total elapsed time: 1.9743100591003895. Arrivals time: 0.0614026696421206 Scheduler time: 1.4301146562211215 Scheduler overhead time: 0.12386274104937911 Adapter cache time: 0.18010994140058756 Engine time: 0.119838394690305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9679019609466195,
    "estimated_duration": 3600.017944067877,
    "input_throughput": 1394.9661023982146,
    "output_throughput": 1245.5251806143378,
    "total_throughput": 2640.4912830125522,
    "itl": 26.246823372832512,
    "ttft": 6031.605150325635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.849011628366689,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.1086036349596012
}
#Debug simulation 
Total elapsed time: 1.9679914130829275. Arrivals time: 0.06094239465892315 Scheduler time: 1.4212232884019613 Scheduler overhead time: 0.1248063356615603 Adapter cache time: 0.1808204362168908 Engine time: 0.12072943290695548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.8846472301520407,
    "estimated_duration": 3599.851055531321,
    "input_throughput": 1286.3301643785776,
    "output_throughput": 1128.395018943488,
    "total_throughput": 2414.7251833220653,
    "itl": 24.733557436384075,
    "ttft": 4502.628520215165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.930445755475802,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0007002113752346574
}
#Debug simulation 
Total elapsed time: 1.8847321728244424. Arrivals time: 0.05724861240014434 Scheduler time: 1.3296213340945542 Scheduler overhead time: 0.14055663160979748 Adapter cache time: 0.16405876679345965 Engine time: 0.12902260990813375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9066027128137648,
    "estimated_duration": 3599.8592060796736,
    "input_throughput": 1286.327251960174,
    "output_throughput": 1128.392464110747,
    "total_throughput": 2414.7197160709206,
    "itl": 24.73759508353297,
    "ttft": 4502.675958199513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.259888350276632,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006944985951753551
}
#Debug simulation 
Total elapsed time: 1.9066946120001376. Arrivals time: 0.058012249413877726 Scheduler time: 1.360756197012961 Scheduler overhead time: 0.1288788476958871 Adapter cache time: 0.16655710246413946 Engine time: 0.13008631253615022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.8914271858520806,
    "estimated_duration": 3599.8642584030977,
    "input_throughput": 1286.3254466306282,
    "output_throughput": 1128.3908804389002,
    "total_throughput": 2414.7163270695287,
    "itl": 24.73767139081119,
    "ttft": 4502.693932327593,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.268685149438629,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006985434032166046
}
#Debug simulation 
Total elapsed time: 1.8915197416208684. Arrivals time: 0.05779038881883025 Scheduler time: 1.3518547620624304 Scheduler overhead time: 0.12847926979884505 Adapter cache time: 0.1661780346184969 Engine time: 0.12524732248857617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.8871713853441179,
    "estimated_duration": 3599.8561402438922,
    "input_throughput": 1286.328347467317,
    "output_throughput": 1128.3934251119251,
    "total_throughput": 2414.721772579242,
    "itl": 24.735363255875413,
    "ttft": 4502.669440822614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.050278951916731,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006904537871341059
}
#Debug simulation 
Total elapsed time: 1.887292044237256. Arrivals time: 0.058038017712533474 Scheduler time: 1.344057414215058 Scheduler overhead time: 0.12879443541169167 Adapter cache time: 0.16636867867782712 Engine time: 0.12796933250501752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.8830368830822408,
    "estimated_duration": 3599.850819236206,
    "input_throughput": 1286.3302488136137,
    "output_throughput": 1128.3950930116214,
    "total_throughput": 2414.725341825235,
    "itl": 24.737867607182796,
    "ttft": 4502.59151824112,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.3331968418694435,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006921217591521587
}
#Debug simulation 
Total elapsed time: 1.8831808371469378. Arrivals time: 0.05715287942439318 Scheduler time: 1.3436401886865497 Scheduler overhead time: 0.12830585660412908 Adapter cache time: 0.1664505386725068 Engine time: 0.12586181238293648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.909431901294738,
    "estimated_duration": 3599.8654782467283,
    "input_throughput": 1286.325010748812,
    "output_throughput": 1128.3904980744933,
    "total_throughput": 2414.7155088233053,
    "itl": 24.733043368368406,
    "ttft": 4502.568806249231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.816971025087271,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006889109371199357
}
#Debug simulation 
Total elapsed time: 1.9095259052701294. Arrivals time: 0.058058302849531174 Scheduler time: 1.3653227644972503 Scheduler overhead time: 0.12874017702415586 Adapter cache time: 0.16676202323287725 Engine time: 0.12820678669959307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.8859108281321824,
    "estimated_duration": 3599.8496112040557,
    "input_throughput": 1286.3306804783954,
    "output_throughput": 1128.3954716767596,
    "total_throughput": 2414.726152155155,
    "itl": 24.73840561278798,
    "ttft": 4502.7212563208295,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1611,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.405002253912314,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006889109371199355
}
#Debug simulation 
Total elapsed time: 1.8859996502287686. Arrivals time: 0.05759205715730786 Scheduler time: 1.3457827176898718 Scheduler overhead time: 0.12958077201619744 Adapter cache time: 0.1661891657859087 Engine time: 0.1247149109840393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.52759481780231,
    "estimated_duration": 3599.62810424163,
    "input_throughput": 924.041568649983,
    "output_throughput": 829.5743097686266,
    "total_throughput": 1753.6158784186096,
    "itl": 22.55972803102513,
    "ttft": 5833.31591376081,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.706365620542078,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 7.901981281732058e-05
}
#Debug simulation 
Total elapsed time: 1.5276945820078254. Arrivals time: 0.04703883361071348 Scheduler time: 0.9926696573384106 Scheduler overhead time: 0.13887986214831471 Adapter cache time: 0.14864667830988765 Engine time: 0.13366509089246392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.5420093024149537,
    "estimated_duration": 3599.6379646601863,
    "input_throughput": 924.0390374408114,
    "output_throughput": 829.5720373317876,
    "total_throughput": 1753.611074772599,
    "itl": 22.56613486488099,
    "ttft": 5833.454167559784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3824,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.479956342913663,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 7.901981281732058e-05
}
#Debug simulation 
Total elapsed time: 1.5421061092056334. Arrivals time: 0.04712023539468646 Scheduler time: 1.0017454563640058 Scheduler overhead time: 0.13854005932807922 Adapter cache time: 0.150478380266577 Engine time: 0.13699713349342346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.543599448632449,
    "estimated_duration": 3599.6156311812906,
    "input_throughput": 924.0447705546924,
    "output_throughput": 829.5771843339919,
    "total_throughput": 1753.6219548886843,
    "itl": 22.566926832111562,
    "ttft": 5833.474597631909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3824,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.501769663690705,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 7.901981281732058e-05
}
#Debug simulation 
Total elapsed time: 1.5436944928951561. Arrivals time: 0.04689834266901016 Scheduler time: 1.0057902210392058 Scheduler overhead time: 0.13793518114835024 Adapter cache time: 0.14979195781052113 Engine time: 0.13655378902330995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.5401364457793534,
    "estimated_duration": 3599.634679306966,
    "input_throughput": 924.0398808026794,
    "output_throughput": 829.5727944744999,
    "total_throughput": 1753.6126752771793,
    "itl": 22.561806543631413,
    "ttft": 5833.455810034221,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3823,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.952738499059905,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 7.901981281732058e-05
}
#Debug simulation 
Total elapsed time: 1.5402591098099947. Arrivals time: 0.046858762856572866 Scheduler time: 1.001314751803875 Scheduler overhead time: 0.13813685346394777 Adapter cache time: 0.15119301341474056 Engine time: 0.13536446075886488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.5267189275473356,
    "estimated_duration": 3599.6146532689127,
    "input_throughput": 924.0450215912355,
    "output_throughput": 829.5774097063928,
    "total_throughput": 1753.6224312976283,
    "itl": 22.5680058949903,
    "ttft": 5833.432758032244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.666492538004306,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 8.30646208585699e-05
}
#Debug simulation 
Total elapsed time: 1.526823798660189. Arrivals time: 0.04670108901336789 Scheduler time: 0.9938704753294587 Scheduler overhead time: 0.13712138682603836 Adapter cache time: 0.14808830292895436 Engine time: 0.13452615914866328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.5499949059449136,
    "estimated_duration": 3599.619018111925,
    "input_throughput": 924.0439011083635,
    "output_throughput": 829.5764037735033,
    "total_throughput": 1753.6203048818668,
    "itl": 22.557136067355824,
    "ttft": 5833.299793475307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3825,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 11.436942378001472,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 7.901981281732058e-05
}
#Debug simulation 
Total elapsed time: 1.5500922170467675. Arrivals time: 0.04674241179600358 Scheduler time: 1.0097771678119898 Scheduler overhead time: 0.1383487000130117 Adapter cache time: 0.15227024722844362 Engine time: 0.1355031612329185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_256_slots_160_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.5456579639576375,
    "estimated_duration": 3599.6372745523695,
    "input_throughput": 924.0392145938172,
    "output_throughput": 829.5721963739643,
    "total_throughput": 1753.6114109677815,
    "itl": 22.56986744611073,
    "ttft": 5833.451051135336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3827,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.834218917301978,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 8.068778483537339e-05
}
#Debug simulation 
Total elapsed time: 1.5457563600502908. Arrivals time: 0.04708205861970782 Scheduler time: 0.9933869321830571 Scheduler overhead time: 0.1525506847538054 Adapter cache time: 0.1511739008128643 Engine time: 0.13360015070065856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.4667110522277653,
    "estimated_duration": 3599.9578558279622,
    "input_throughput": 865.3951309336885,
    "output_throughput": 760.485013892016,
    "total_throughput": 1625.8801448257045,
    "itl": 21.810216625039025,
    "ttft": 5145.116002209311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.505095440389235,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4667982561513782. Arrivals time: 0.04470786219462752 Scheduler time: 0.9339384767226875 Scheduler overhead time: 0.14162962045520544 Adapter cache time: 0.1384083330631256 Engine time: 0.13955602748319507 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.4757386539131403,
    "estimated_duration": 3599.9562027905586,
    "input_throughput": 865.3955283081119,
    "output_throughput": 760.4853630935346,
    "total_throughput": 1625.8808914016465,
    "itl": 21.81525243111993,
    "ttft": 5145.161422873382,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.071788633742669,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4758239299990237. Arrivals time: 0.04405311308801174 Scheduler time: 0.9416838199831545 Scheduler overhead time: 0.14525486528873444 Adapter cache time: 0.13921931199729443 Engine time: 0.1359576703980565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.460757956840098,
    "estimated_duration": 3599.957553208845,
    "input_throughput": 865.3952036804103,
    "output_throughput": 760.4850778197984,
    "total_throughput": 1625.8802815002089,
    "itl": 21.81530174610565,
    "ttft": 5145.100905344356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.087242705523748,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4608500921167433. Arrivals time: 0.04433016711845994 Scheduler time: 0.9322962490841746 Scheduler overhead time: 0.14051814004778862 Adapter cache time: 0.13871421618387103 Engine time: 0.13666100520640612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.4693626179359853,
    "estimated_duration": 3599.9580116821794,
    "input_throughput": 865.3950934678403,
    "output_throughput": 760.4849809680774,
    "total_throughput": 1625.8800744359178,
    "itl": 21.812330486599024,
    "ttft": 5145.170157543049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.697515322051323,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4694454688578844. Arrivals time: 0.04510926501825452 Scheduler time: 0.9367591207846999 Scheduler overhead time: 0.14055199827998877 Adapter cache time: 0.138418382499367 Engine time: 0.14032584056258202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.4649895941838622,
    "estimated_duration": 3599.96011884634,
    "input_throughput": 865.3945869262494,
    "output_throughput": 760.4845358335083,
    "total_throughput": 1625.8791227597576,
    "itl": 21.8156044635618,
    "ttft": 5145.199132604313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.202433173879639,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.465080485213548. Arrivals time: 0.04828125750645995 Scheduler time: 0.928537919651717 Scheduler overhead time: 0.14162480039522052 Adapter cache time: 0.13994577899575233 Engine time: 0.13818761613219976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.4667630828917027,
    "estimated_duration": 3599.9682338446232,
    "input_throughput": 865.3926361658173,
    "output_throughput": 760.482821559853,
    "total_throughput": 1625.8754577256705,
    "itl": 21.80870820300588,
    "ttft": 5145.106249810905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.309349769532979,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4668495897203684. Arrivals time: 0.044603841844946146 Scheduler time: 0.9354753289371729 Scheduler overhead time: 0.14102418767288327 Adapter cache time: 0.13793946616351604 Engine time: 0.13945583067834377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.4547154852189124,
    "estimated_duration": 3599.9657347464845,
    "input_throughput": 865.3932369218483,
    "output_throughput": 760.4833494874346,
    "total_throughput": 1625.876586409283,
    "itl": 21.816870673702763,
    "ttft": 5145.180162972885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2779,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.32189927097364,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.454802648164332. Arrivals time: 0.04446320282295346 Scheduler time: 0.9259706558659673 Scheduler overhead time: 0.14117378136143088 Adapter cache time: 0.13817169796675444 Engine time: 0.13654786068946123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.3348945127800107,
    "estimated_duration": 3599.7814068640646,
    "input_throughput": 725.3443209138171,
    "output_throughput": 655.0747763471205,
    "total_throughput": 1380.4190972609376,
    "itl": 21.141477715725976,
    "ttft": 2723.2951216838123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.462970622920119,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3349804799072444. Arrivals time: 0.040109490510076284 Scheduler time: 0.818915450014174 Scheduler overhead time: 0.14470811747014523 Adapter cache time: 0.12052185414358974 Engine time: 0.14062845520675182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.329394523985684,
    "estimated_duration": 3599.782630405635,
    "input_throughput": 725.3440743742283,
    "output_throughput": 655.0745536916708,
    "total_throughput": 1380.4186280658992,
    "itl": 21.14470192087655,
    "ttft": 2723.351848544313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.819790847478277,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3294797269627452. Arrivals time: 0.03981555812060833 Scheduler time: 0.8118148543871939 Scheduler overhead time: 0.14383662678301334 Adapter cache time: 0.11898116255179048 Engine time: 0.14498357381671667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.3338552406057715,
    "estimated_duration": 3599.791411924012,
    "input_throughput": 725.3423049321717,
    "output_throughput": 655.0729556687373,
    "total_throughput": 1380.415260600909,
    "itl": 21.145104872411483,
    "ttft": 2723.42971118931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.827589669749087,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3339391616173089. Arrivals time: 0.03990264609456062 Scheduler time: 0.8174050925299525 Scheduler overhead time: 0.14364579413086176 Adapter cache time: 0.12047115946188569 Engine time: 0.142118526622653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.340711610391736,
    "estimated_duration": 3599.791613422682,
    "input_throughput": 725.3422643310689,
    "output_throughput": 655.0729190009679,
    "total_throughput": 1380.4151833320368,
    "itl": 21.14271846973567,
    "ttft": 2723.258882524186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.581171181509123,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3408327670767903. Arrivals time: 0.03980968194082379 Scheduler time: 0.8249087953008711 Scheduler overhead time: 0.1442296332679689 Adapter cache time: 0.12050191732123494 Engine time: 0.14077207259833813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.3268802501261234,
    "estimated_duration": 3599.7730483549767,
    "input_throughput": 725.3460051302987,
    "output_throughput": 655.0762974009197,
    "total_throughput": 1380.4223025312185,
    "itl": 21.14465201848428,
    "ttft": 2723.2381160493237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.904410647358652,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.326955535914749. Arrivals time: 0.039552152156829834 Scheduler time: 0.8130082995630801 Scheduler overhead time: 0.14433179097250104 Adapter cache time: 0.11861924966797233 Engine time: 0.14101435849443078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.347779261879623,
    "estimated_duration": 3599.789421614102,
    "input_throughput": 725.3427059711795,
    "output_throughput": 655.0733178560886,
    "total_throughput": 1380.4160238272682,
    "itl": 21.140474659089683,
    "ttft": 2723.389543826062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1785,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.337239776400231,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3478592638857663. Arrivals time: 0.04035991709679365 Scheduler time: 0.8245547320693731 Scheduler overhead time: 0.14461651258170605 Adapter cache time: 0.12110999785363674 Engine time: 0.14532563462853432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.331234335899353,
    "estimated_duration": 3599.7925048168336,
    "input_throughput": 725.3420847190908,
    "output_throughput": 655.0727567893492,
    "total_throughput": 1380.41484150844,
    "itl": 21.145423124011323,
    "ttft": 2723.4184944599538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.975979137718542,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3312985617667437. Arrivals time: 0.0399517766200006 Scheduler time: 0.8151199063286185 Scheduler overhead time: 0.14548263931646943 Adapter cache time: 0.11937220115214586 Engine time: 0.14101749379187822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.0506363352760673,
    "estimated_duration": 3599.5079484045154,
    "input_throughput": 468.45736255351693,
    "output_throughput": 410.2538516839652,
    "total_throughput": 878.7112142374822,
    "itl": 19.58443360475815,
    "ttft": 3214.3271465349576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1869,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.72005159341048,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0507061979733407. Arrivals time: 0.03068662714213133 Scheduler time: 0.5611482267268002 Scheduler overhead time: 0.1487506367266178 Adapter cache time: 0.08768206369131804 Engine time: 0.14822541968896985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.0704040918499231,
    "estimated_duration": 3599.495512278821,
    "input_throughput": 468.45898105661644,
    "output_throughput": 410.25526909605776,
    "total_throughput": 878.7142501526741,
    "itl": 19.587528334236058,
    "ttft": 3214.196249735747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1868,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.08921197851403,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0705145602114499. Arrivals time: 0.03119437862187624 Scheduler time: 0.5691106072627008 Scheduler overhead time: 0.15730129927396774 Adapter cache time: 0.08900181856006384 Engine time: 0.14750699559226632 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.064265000168234,
    "estimated_duration": 3599.4995274680123,
    "input_throughput": 468.4584584974598,
    "output_throughput": 410.2548114622924,
    "total_throughput": 878.7132699597522,
    "itl": 19.58784421800237,
    "ttft": 3214.0425126067885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1868,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.101119370255567,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0643244883976877. Arrivals time: 0.031010389793664217 Scheduler time: 0.5655194101855159 Scheduler overhead time: 0.15577461058273911 Adapter cache time: 0.08873074082657695 Engine time: 0.14821177208796144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.0508262617513537,
    "estimated_duration": 3599.4979092869753,
    "input_throughput": 468.45866909644144,
    "output_throughput": 410.25499589539197,
    "total_throughput": 878.7136649918334,
    "itl": 19.5859320083619,
    "ttft": 3214.200376885877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1868,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.8424204061760765,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0508932108059525. Arrivals time: 0.03066476434469223 Scheduler time: 0.560701250564307 Scheduler overhead time: 0.1487797638401389 Adapter cache time: 0.08871237188577652 Engine time: 0.1480398024432361 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.056908099912107,
    "estimated_duration": 3599.5097085777643,
    "input_throughput": 468.4571334761746,
    "output_throughput": 410.25365106834994,
    "total_throughput": 878.7107845445245,
    "itl": 19.5880337757627,
    "ttft": 3214.198801844695,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1869,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.180455423593419,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0569956321269274. Arrivals time: 0.030988784972578287 Scheduler time: 0.5644999030046165 Scheduler overhead time: 0.14824024681001902 Adapter cache time: 0.08887764345854521 Engine time: 0.15007411874830723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.0522365639917552,
    "estimated_duration": 3599.4934340991226,
    "input_throughput": 468.45925152299225,
    "output_throughput": 410.2555059583238,
    "total_throughput": 878.714757481316,
    "itl": 19.583901850498748,
    "ttft": 3214.2466501401436,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1869,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.588404001172004,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0522944540716708. Arrivals time: 0.03079285891726613 Scheduler time: 0.5604038760066032 Scheduler overhead time: 0.1498908414505422 Adapter cache time: 0.08828653395175934 Engine time: 0.14835469843819737 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_256_slots_160_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.0514543731696904,
    "estimated_duration": 3599.5014704180785,
    "input_throughput": 468.45820563150033,
    "output_throughput": 410.254590013678,
    "total_throughput": 878.7127956451784,
    "itl": 19.588054342217546,
    "ttft": 3214.299755882114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1869,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.25968030903472,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.051573008298874. Arrivals time: 0.03054210916161537 Scheduler time: 0.5639205547049642 Scheduler overhead time: 0.14924153499305248 Adapter cache time: 0.08734246203675866 Engine time: 0.14620354445651174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.232166809029877,
    "estimated_duration": 3600.2430059778235,
    "input_throughput": 3670.6522248796787,
    "output_throughput": 3239.5204936541,
    "total_throughput": 6910.172718533779,
    "itl": 263.48242618825395,
    "ttft": 2314912.4005817994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1137,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.479774564851626,
    "arrivals": 1724092,
    "finished_requests": 53713,
    "scheduler_time": 53.63656091362948
}
#Debug simulation 
Total elapsed time: 5.232261123135686. Arrivals time: 0.2312454986386001 Scheduler time: 4.911164278630167 Scheduler overhead time: 0.022911795414984226 Adapter cache time: 0.032557680271565914 Engine time: 0.023849363904446363 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 5.271190858911723,
    "estimated_duration": 3600.139009103329,
    "input_throughput": 3661.5086158251343,
    "output_throughput": 3233.3693145085717,
    "total_throughput": 6894.877930333706,
    "itl": 260.8227746342223,
    "ttft": 2315929.52496485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7673092951742055,
    "arrivals": 1724092,
    "finished_requests": 53597,
    "scheduler_time": 53.58018282223165
}
#Debug simulation 
Total elapsed time: 5.271282854955643. Arrivals time: 0.3381865103729069 Scheduler time: 4.843337254598737 Scheduler overhead time: 0.02308200066909194 Adapter cache time: 0.03260110318660736 Engine time: 0.023477903567254543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.700344183947891,
    "estimated_duration": 3600.1065287746083,
    "input_throughput": 2973.5564529652324,
    "output_throughput": 2632.4598797985554,
    "total_throughput": 5606.016332763788,
    "itl": 128.7228976166084,
    "ttft": 2423198.6171248085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6938,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.676715177446432,
    "arrivals": 1724092,
    "finished_requests": 43469,
    "scheduler_time": 50.45553628876067
}
#Debug simulation 
Total elapsed time: 3.700445255730301. Arrivals time: 0.1958754607476294 Scheduler time: 3.190003940835595 Scheduler overhead time: 0.039991763420403004 Adapter cache time: 0.21408729394897819 Engine time: 0.041131142526865005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.213303206022829,
    "estimated_duration": 3600.2800986448933,
    "input_throughput": 3661.469007636845,
    "output_throughput": 3233.452309552713,
    "total_throughput": 6894.921317189558,
    "itl": 260.8147059911116,
    "ttft": 2315928.8939639307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6202149805356374,
    "arrivals": 1724092,
    "finished_requests": 53600,
    "scheduler_time": 53.5843963224157
}
#Debug simulation 
Total elapsed time: 5.213397664017975. Arrivals time: 0.23557994980365038 Scheduler time: 4.888015741482377 Scheduler overhead time: 0.02322787092998624 Adapter cache time: 0.03218744741752744 Engine time: 0.023646738845854998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.705952288582921,
    "estimated_duration": 3600.092643675419,
    "input_throughput": 2973.2695959379503,
    "output_throughput": 2632.346424931172,
    "total_throughput": 5605.616020869122,
    "itl": 128.7343128582015,
    "ttft": 2423258.422128885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6937,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.95439412373957,
    "arrivals": 1724092,
    "finished_requests": 43466,
    "scheduler_time": 50.45140947930126
}
#Debug simulation 
Total elapsed time: 3.70608014985919. Arrivals time: 0.1978421276435256 Scheduler time: 3.193637410644442 Scheduler overhead time: 0.0401684301905334 Adapter cache time: 0.21363362483680248 Engine time: 0.04146431107074022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.152824161108583,
    "estimated_duration": 3600.0384259797825,
    "input_throughput": 3661.113143927875,
    "output_throughput": 3233.294377082121,
    "total_throughput": 6894.407521009996,
    "itl": 260.8762517454764,
    "ttft": 2315734.4373661266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1166,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.48639864385585,
    "arrivals": 1724092,
    "finished_requests": 53599,
    "scheduler_time": 53.58402889244272
}
#Debug simulation 
Total elapsed time: 5.152917186263949. Arrivals time: 0.23294034786522388 Scheduler time: 4.830497321672738 Scheduler overhead time: 0.023028774186968803 Adapter cache time: 0.03248830419033766 Engine time: 0.02340972051024437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.7082381448708475,
    "estimated_duration": 3600.090322258825,
    "input_throughput": 2972.9329105511624,
    "output_throughput": 2632.189792964512,
    "total_throughput": 5605.122703515674,
    "itl": 128.74878819192386,
    "ttft": 2423124.697886148,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 6934,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.246066624296855,
    "arrivals": 1724092,
    "finished_requests": 43460,
    "scheduler_time": 50.44726340803747
}
#Debug simulation 
Total elapsed time: 3.70833283290267. Arrivals time: 0.19671724224463105 Scheduler time: 3.19689518539235 Scheduler overhead time: 0.03985772607848048 Adapter cache time: 0.2137702014297247 Engine time: 0.041838180273771286 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.811368334107101,
    "estimated_duration": 3600.0567117977857,
    "input_throughput": 3701.815017615356,
    "output_throughput": 3238.678980192384,
    "total_throughput": 6940.49399780774,
    "itl": 262.64217050980204,
    "ttft": 2313836.5091027073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.107174552357862,
    "arrivals": 1601458,
    "finished_requests": 53822,
    "scheduler_time": 53.598873488508
}
#Debug simulation 
Total elapsed time: 4.8114619492553174. Arrivals time: 0.4192449916154146 Scheduler time: 4.300409539137036 Scheduler overhead time: 0.02279576752334833 Adapter cache time: 0.035361353773623705 Engine time: 0.02313428418710828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.78883614577353,
    "estimated_duration": 3600.2546621372653,
    "input_throughput": 3692.377969759619,
    "output_throughput": 3232.5832731763235,
    "total_throughput": 6924.961242935943,
    "itl": 260.0367054596965,
    "ttft": 2314804.0046392577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.444938602887525,
    "arrivals": 1601458,
    "finished_requests": 53707,
    "scheduler_time": 53.546452627605895
}
#Debug simulation 
Total elapsed time: 4.788990949746221. Arrivals time: 0.40909005468711257 Scheduler time: 4.285648412071168 Scheduler overhead time: 0.02289152378216386 Adapter cache time: 0.037409527227282524 Engine time: 0.02330052712932229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.012106312904507,
    "estimated_duration": 3600.0814356784463,
    "input_throughput": 3130.0769722383825,
    "output_throughput": 2753.6279878990604,
    "total_throughput": 5883.704960137443,
    "itl": 121.61145537052298,
    "ttft": 2402424.941622091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.555409652012164,
    "arrivals": 1601458,
    "finished_requests": 45528,
    "scheduler_time": 52.90504340036117
}
#Debug simulation 
Total elapsed time: 4.01219637086615. Arrivals time: 0.38893462298437953 Scheduler time: 3.326986209489405 Scheduler overhead time: 0.041878349147737026 Adapter cache time: 0.19071368826553226 Engine time: 0.043485891073942184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.774218645878136,
    "estimated_duration": 3600.1466362257192,
    "input_throughput": 3693.2081783033163,
    "output_throughput": 3233.2571909357616,
    "total_throughput": 6926.4653692390775,
    "itl": 260.1617167898656,
    "ttft": 2314593.9928187183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1379,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.31858791888452,
    "arrivals": 1601458,
    "finished_requests": 53714,
    "scheduler_time": 53.54907370156068
}
#Debug simulation 
Total elapsed time: 4.774313015863299. Arrivals time: 0.4138749376870692 Scheduler time: 4.265823082067072 Scheduler overhead time: 0.022959962021559477 Adapter cache time: 0.03780508041381836 Engine time: 0.023189355619251728 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.00338721415028,
    "estimated_duration": 3600.1267575773463,
    "input_throughput": 3129.865351625169,
    "output_throughput": 2753.5494352068026,
    "total_throughput": 5883.4147868319715,
    "itl": 121.61876575145092,
    "ttft": 2402385.983178657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.736369350663207,
    "arrivals": 1601458,
    "finished_requests": 45526,
    "scheduler_time": 52.903118508473604
}
#Debug simulation 
Total elapsed time: 4.003475970122963. Arrivals time: 0.38744247425347567 Scheduler time: 3.3187229107134044 Scheduler overhead time: 0.04213410243391991 Adapter cache time: 0.19105215882882476 Engine time: 0.043883060570806265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.787843571975827,
    "estimated_duration": 3600.236288707517,
    "input_throughput": 3693.377582384635,
    "output_throughput": 3233.435548803704,
    "total_throughput": 6926.813131188339,
    "itl": 260.1502146084584,
    "ttft": 2314545.5372592267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1379,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.123279356669991,
    "arrivals": 1601458,
    "finished_requests": 53717,
    "scheduler_time": 53.55323339355758
}
#Debug simulation 
Total elapsed time: 4.787963258102536. Arrivals time: 0.41158232232555747 Scheduler time: 4.281594870612025 Scheduler overhead time: 0.023028281517326832 Adapter cache time: 0.037704141810536385 Engine time: 0.02339464519172907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.00112788984552,
    "estimated_duration": 3600.0503750229077,
    "input_throughput": 3129.6434289279373,
    "output_throughput": 2753.283695352993,
    "total_throughput": 5882.927124280931,
    "itl": 121.62603359733596,
    "ttft": 2402456.0918572824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.931539227179165,
    "arrivals": 1601458,
    "finished_requests": 45522,
    "scheduler_time": 52.89921932963371
}
#Debug simulation 
Total elapsed time: 4.00122034503147. Arrivals time: 0.38664181204512715 Scheduler time: 3.318004610016942 Scheduler overhead time: 0.04214683221653104 Adapter cache time: 0.19035262241959572 Engine time: 0.04375944472849369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.78092107269913,
    "estimated_duration": 3600.227547576605,
    "input_throughput": 3725.0114951837345,
    "output_throughput": 3304.3522507369707,
    "total_throughput": 7029.363745920706,
    "itl": 259.1712147927778,
    "ttft": 2305624.594387756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1170,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5807706604014102,
    "arrivals": 1509498,
    "finished_requests": 54654,
    "scheduler_time": 54.69087909098112
}
#Debug simulation 
Total elapsed time: 4.78098608693108. Arrivals time: 0.8567587742581964 Scheduler time: 3.8306459062732756 Scheduler overhead time: 0.02191287651658058 Adapter cache time: 0.03867920534685254 Engine time: 0.02267470955848694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.444631633814424,
    "estimated_duration": 3600.078503224249,
    "input_throughput": 3720.563312162215,
    "output_throughput": 3301.8171657518355,
    "total_throughput": 7022.38047791405,
    "itl": 256.5599867460451,
    "ttft": 2305973.8516399674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.833647826167295,
    "arrivals": 1509498,
    "finished_requests": 54583,
    "scheduler_time": 54.6885915670076
}
#Debug simulation 
Total elapsed time: 4.444710524752736. Arrivals time: 0.5069420281797647 Scheduler time: 3.841307940892875 Scheduler overhead time: 0.022006487473845482 Adapter cache time: 0.041070954874157906 Engine time: 0.022941235918551683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.11978823877871,
    "estimated_duration": 3600.094620700992,
    "input_throughput": 3253.830311193068,
    "output_throughput": 2907.654409917081,
    "total_throughput": 6161.48472111015,
    "itl": 116.54497942448029,
    "ttft": 2378241.998467023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1007,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2907793020456655,
    "arrivals": 1509498,
    "finished_requests": 47638,
    "scheduler_time": 55.672268613961585
}
#Debug simulation 
Total elapsed time: 4.119879410602152. Arrivals time: 0.37968585919588804 Scheduler time: 3.4780238433741033 Scheduler overhead time: 0.04345478815957904 Adapter cache time: 0.15238310769200325 Engine time: 0.045410231687128544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.3291671350598335,
    "estimated_duration": 3600.210629181712,
    "input_throughput": 3720.6623110949963,
    "output_throughput": 3302.0290267578066,
    "total_throughput": 7022.691337852803,
    "itl": 256.5513708208504,
    "ttft": 2305968.009149868,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6767472238861556,
    "arrivals": 1509498,
    "finished_requests": 54586,
    "scheduler_time": 54.692883535402345
}
#Debug simulation 
Total elapsed time: 4.329334262292832. Arrivals time: 0.406683340203017 Scheduler time: 3.8267212552018464 Scheduler overhead time: 0.022166665643453598 Adapter cache time: 0.04010880691930652 Engine time: 0.02304300433024764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.122003955766559,
    "estimated_duration": 3600.0089140965038,
    "input_throughput": 3253.597221422329,
    "output_throughput": 2907.2975233525867,
    "total_throughput": 6160.894744774915,
    "itl": 116.54541620892564,
    "ttft": 2378267.143441723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1007,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3330325742810962,
    "arrivals": 1509498,
    "finished_requests": 47634,
    "scheduler_time": 55.670339706007574
}
#Debug simulation 
Total elapsed time: 4.122098452877253. Arrivals time: 0.383496533613652 Scheduler time: 3.475045028142631 Scheduler overhead time: 0.043613508343696594 Adapter cache time: 0.1534675625152886 Engine time: 0.045431571546941996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.34861188614741,
    "estimated_duration": 3600.047108698937,
    "input_throughput": 3720.831310132782,
    "output_throughput": 3302.1790107342076,
    "total_throughput": 7023.01032086699,
    "itl": 256.5408031513544,
    "ttft": 2305908.0416903687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5133090965099685,
    "arrivals": 1509498,
    "finished_requests": 54586,
    "scheduler_time": 54.692801179962004
}
#Debug simulation 
Total elapsed time: 4.348709071986377. Arrivals time: 0.40737409330904484 Scheduler time: 3.8456150419078767 Scheduler overhead time: 0.022072883788496256 Adapter cache time: 0.040225162636488676 Engine time: 0.02290330594405532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.116346993017942,
    "estimated_duration": 3600.0509563608175,
    "input_throughput": 3253.5592251283842,
    "output_throughput": 2907.263571230131,
    "total_throughput": 6160.822796358516,
    "itl": 116.54687918017063,
    "ttft": 2378282.7333835294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1007,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3750343389437027,
    "arrivals": 1509498,
    "finished_requests": 47634,
    "scheduler_time": 55.670380205690556
}
#Debug simulation 
Total elapsed time: 4.116443912032992. Arrivals time: 0.38105145283043385 Scheduler time: 3.4718500496819615 Scheduler overhead time: 0.04345463262870908 Adapter cache time: 0.15357539616525173 Engine time: 0.04554079193621874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.225090778898448,
    "estimated_duration": 3600.2208228215304,
    "input_throughput": 3846.8190373795137,
    "output_throughput": 3394.639829459661,
    "total_throughput": 7241.458866839174,
    "itl": 252.4970166158678,
    "ttft": 2297516.958385935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9617726438609273,
    "arrivals": 1494384,
    "finished_requests": 55803,
    "scheduler_time": 56.129963903288534
}
#Debug simulation 
Total elapsed time: 4.225182300899178. Arrivals time: 0.2242478015832603 Scheduler time: 3.914239162579179 Scheduler overhead time: 0.022441447712481022 Adapter cache time: 0.0305016259662807 Engine time: 0.023067413829267025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.338708871975541,
    "estimated_duration": 3600.100305786377,
    "input_throughput": 3837.6514059327465,
    "output_throughput": 3387.9431026952457,
    "total_throughput": 7225.594508627993,
    "itl": 249.3841199986703,
    "ttft": 2298625.683225405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 638,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0845738610322653,
    "arrivals": 1494384,
    "finished_requests": 55687,
    "scheduler_time": 56.11606317682345
}
#Debug simulation 
Total elapsed time: 4.338802505284548. Arrivals time: 0.33250477723777294 Scheduler time: 3.917568410746753 Scheduler overhead time: 0.022773878648877144 Adapter cache time: 0.03156912326812744 Engine time: 0.02358888415619731 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.993827977217734,
    "estimated_duration": 3600.013019455748,
    "input_throughput": 3314.918011547678,
    "output_throughput": 2947.10517508183,
    "total_throughput": 6262.023186629508,
    "itl": 114.72684743076614,
    "ttft": 2378292.0255482267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8517974588088806,
    "arrivals": 1494384,
    "finished_requests": 48125,
    "scheduler_time": 56.47570030099961
}
#Debug simulation 
Total elapsed time: 3.9939206740818918. Arrivals time: 0.2095710914582014 Scheduler time: 3.5263028969056904 Scheduler overhead time: 0.04423545952886343 Adapter cache time: 0.14629042008891702 Engine time: 0.04615983972325921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.664890568237752,
    "estimated_duration": 3600.0148267225386,
    "input_throughput": 3837.742527460103,
    "output_throughput": 3388.0235463097015,
    "total_throughput": 7225.766073769804,
    "itl": 249.3792937139267,
    "ttft": 2298589.6201294106,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 638,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9991774394782142,
    "arrivals": 1494384,
    "finished_requests": 55687,
    "scheduler_time": 56.11598053452123
}
#Debug simulation 
Total elapsed time: 4.66495748097077. Arrivals time: 0.6542653595097363 Scheduler time: 3.9216849338263273 Scheduler overhead time: 0.02281372481957078 Adapter cache time: 0.03160712728276849 Engine time: 0.02383063966408372 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.989744701422751,
    "estimated_duration": 3600.0369523664017,
    "input_throughput": 3314.8959740970504,
    "output_throughput": 2947.085582837146,
    "total_throughput": 6261.981556934196,
    "itl": 114.72762432484656,
    "ttft": 2378301.9407956335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8756906782276965,
    "arrivals": 1494384,
    "finished_requests": 48125,
    "scheduler_time": 56.47573999225363
}
#Debug simulation 
Total elapsed time: 3.9898080970160663. Arrivals time: 0.20659838849678636 Scheduler time: 3.525639781728387 Scheduler overhead time: 0.04417732125148177 Adapter cache time: 0.14588480442762375 Engine time: 0.046229017432779074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.344429108779877,
    "estimated_duration": 3600.199344067435,
    "input_throughput": 3838.064140189783,
    "output_throughput": 3388.283768258892,
    "total_throughput": 7226.347908448674,
    "itl": 249.37629963487856,
    "ttft": 2298537.808186179,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 638,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9076520881475572,
    "arrivals": 1494384,
    "finished_requests": 55692,
    "scheduler_time": 56.12023914814771
}
#Debug simulation 
Total elapsed time: 4.344591309782118. Arrivals time: 0.3346797707490623 Scheduler time: 3.9213694324716926 Scheduler overhead time: 0.02271476062014699 Adapter cache time: 0.03135116724297404 Engine time: 0.023620777763426304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.9678834029473364,
    "estimated_duration": 3600.114878616508,
    "input_throughput": 3314.3759025227027,
    "output_throughput": 2946.7826326911127,
    "total_throughput": 6261.158535213815,
    "itl": 114.7298377802853,
    "ttft": 2378366.906407491,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 566,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8999611590057568,
    "arrivals": 1494384,
    "finished_requests": 48120,
    "scheduler_time": 56.47669419752928
}
#Debug simulation 
Total elapsed time: 3.9679757901467383. Arrivals time: 0.2033027382567525 Scheduler time: 3.507820009253919 Scheduler overhead time: 0.044104216154664755 Adapter cache time: 0.1455710008740425 Engine time: 0.04600135236978531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.291309253778309,
    "estimated_duration": 3600.1690018332642,
    "input_throughput": 3931.7657012190357,
    "output_throughput": 3458.380146504199,
    "total_throughput": 7390.1458477232345,
    "itl": 246.88576455705694,
    "ttft": 2288974.478126448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2180741220852422,
    "arrivals": 1486681,
    "finished_requests": 57327,
    "scheduler_time": 57.158710556095976
}
#Debug simulation 
Total elapsed time: 4.291426877025515. Arrivals time: 0.22942347498610616 Scheduler time: 3.979489079210907 Scheduler overhead time: 0.022902016062289476 Adapter cache time: 0.02485305769369006 Engine time: 0.023884094785898924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.391939157154411,
    "estimated_duration": 3600.052460673961,
    "input_throughput": 3923.1500524733883,
    "output_throughput": 3451.804143341183,
    "total_throughput": 7374.954195814571,
    "itl": 244.0216803613914,
    "ttft": 2290281.6225954425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2983177767577643,
    "arrivals": 1486681,
    "finished_requests": 57201,
    "scheduler_time": 57.13171818774853
}
#Debug simulation 
Total elapsed time: 4.3920318288728595. Arrivals time: 0.3373891217634082 Scheduler time: 3.9708565324544907 Scheduler overhead time: 0.02310784999281168 Adapter cache time: 0.025589614175260067 Engine time: 0.024174074176698923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.175607367418706,
    "estimated_duration": 3600.124670123804,
    "input_throughput": 3364.615981363625,
    "output_throughput": 2971.8340280788316,
    "total_throughput": 6336.450009442457,
    "itl": 113.36379428490486,
    "ttft": 2372784.753937865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.192835520971573,
    "arrivals": 1486681,
    "finished_requests": 48936,
    "scheduler_time": 56.97247888864172
}
#Debug simulation 
Total elapsed time: 4.175701586063951. Arrivals time: 0.38154689921066165 Scheduler time: 3.5395764857530594 Scheduler overhead time: 0.04462787555530667 Adapter cache time: 0.1413071658462286 Engine time: 0.04701410327106714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.300874246284366,
    "estimated_duration": 3600.096542191749,
    "input_throughput": 3921.7459405698373,
    "output_throughput": 3450.1860865203516,
    "total_throughput": 7371.932027090189,
    "itl": 243.77013030995715,
    "ttft": 2290291.648551024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2468347666342747,
    "arrivals": 1486681,
    "finished_requests": 57177,
    "scheduler_time": 57.11827828845304
}
#Debug simulation 
Total elapsed time: 4.300967526156455. Arrivals time: 0.23068462312221527 Scheduler time: 3.9867587219923735 Scheduler overhead time: 0.023076821118593216 Adapter cache time: 0.025496228132396936 Engine time: 0.023921662010252476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.005810554139316,
    "estimated_duration": 3600.0080529708484,
    "input_throughput": 3364.665529012939,
    "output_throughput": 2971.9041853728418,
    "total_throughput": 6336.569714385781,
    "itl": 113.36397513922006,
    "ttft": 2372744.3649522197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2074229601956952,
    "arrivals": 1486681,
    "finished_requests": 48935,
    "scheduler_time": 56.97045126235845
}
#Debug simulation 
Total elapsed time: 4.00593307428062. Arrivals time: 0.20561536448076367 Scheduler time: 3.546679260674864 Scheduler overhead time: 0.04446218814700842 Adapter cache time: 0.14108827849850059 Engine time: 0.04655486810952425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.398194107227027,
    "estimated_duration": 3600.056801919534,
    "input_throughput": 3923.9039207570095,
    "output_throughput": 3452.6224678934436,
    "total_throughput": 7376.526388650454,
    "itl": 244.20734154610417,
    "ttft": 2290164.8541569486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1900400173710606,
    "arrivals": 1486681,
    "finished_requests": 57213,
    "scheduler_time": 57.13496153130719
}
#Debug simulation 
Total elapsed time: 4.3983363378793. Arrivals time: 0.3386183907277882 Scheduler time: 3.976743216626346 Scheduler overhead time: 0.023049687035381794 Adapter cache time: 0.02513121720403433 Engine time: 0.02383347786962986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.203832650091499,
    "estimated_duration": 3600.024063001793,
    "input_throughput": 3364.6505656687236,
    "output_throughput": 2971.890968717303,
    "total_throughput": 6336.541534386027,
    "itl": 113.36449670497032,
    "ttft": 2372751.8026239453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 365,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2233936910703807,
    "arrivals": 1486681,
    "finished_requests": 48935,
    "scheduler_time": 56.97049056243717
}
#Debug simulation 
Total elapsed time: 4.203923002816737. Arrivals time: 0.38231233786791563 Scheduler time: 3.56679127458483 Scheduler overhead time: 0.04521934641525149 Adapter cache time: 0.14114973926916718 Engine time: 0.046844519674777985 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.490580426063389,
    "estimated_duration": 3600.071400551004,
    "input_throughput": 3925.9524124540376,
    "output_throughput": 3482.7259254027585,
    "total_throughput": 7408.678337856796,
    "itl": 246.575445956737,
    "ttft": 2285915.1469725175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8967229089722957,
    "arrivals": 1482883,
    "finished_requests": 57622,
    "scheduler_time": 57.53572872178545
}
#Debug simulation 
Total elapsed time: 4.49067481001839. Arrivals time: 0.40546984411776066 Scheduler time: 4.007003796286881 Scheduler overhead time: 0.022862054407596588 Adapter cache time: 0.020634619519114494 Engine time: 0.023853995371609926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.439347168896347,
    "estimated_duration": 3600.1959002472986,
    "input_throughput": 3919.8841926992413,
    "output_throughput": 3476.8663558391854,
    "total_throughput": 7396.750548538426,
    "itl": 244.21776000538088,
    "ttft": 2287010.421658254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9569866094575314,
    "arrivals": 1482883,
    "finished_requests": 57531,
    "scheduler_time": 57.511711956612416
}
#Debug simulation 
Total elapsed time: 4.43943991092965. Arrivals time: 0.23377186292782426 Scheduler time: 4.12644328456372 Scheduler overhead time: 0.023047647904604673 Adapter cache time: 0.021181372459977865 Engine time: 0.024106547702103853 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.222353093791753,
    "estimated_duration": 3600.046579791675,
    "input_throughput": 3326.510292178774,
    "output_throughput": 2976.729540155041,
    "total_throughput": 6303.239832333815,
    "itl": 114.04655570803081,
    "ttft": 2369363.6537076347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9122070402838336,
    "arrivals": 1482883,
    "finished_requests": 48902,
    "scheduler_time": 57.003467231267784
}
#Debug simulation 
Total elapsed time: 4.222448194865137. Arrivals time: 0.21883667539805174 Scheduler time: 3.7554755434393883 Scheduler overhead time: 0.04468264477327466 Adapter cache time: 0.13540845038369298 Engine time: 0.04635337460786104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.488757831975818,
    "estimated_duration": 3600.156186180086,
    "input_throughput": 3919.9274337521965,
    "output_throughput": 3476.9047098707897,
    "total_throughput": 7396.832143622986,
    "itl": 244.21607297618493,
    "ttft": 2286988.781506277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9173528635688157,
    "arrivals": 1482883,
    "finished_requests": 57531,
    "scheduler_time": 57.51163163528365
}
#Debug simulation 
Total elapsed time: 4.488877196330577. Arrivals time: 0.4040989060886204 Scheduler time: 4.005384216085076 Scheduler overhead time: 0.023033826611936092 Adapter cache time: 0.02136476617306471 Engine time: 0.024057166650891304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.179971647914499,
    "estimated_duration": 3600.058060965142,
    "input_throughput": 3326.499683393844,
    "output_throughput": 2976.7200468780893,
    "total_throughput": 6303.219730271933,
    "itl": 114.04690224508226,
    "ttft": 2369369.7914515687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9236506348475859,
    "arrivals": 1482883,
    "finished_requests": 48902,
    "scheduler_time": 57.00350481017517
}
#Debug simulation 
Total elapsed time: 4.180063224863261. Arrivals time: 0.2106612054631114 Scheduler time: 3.721042572055012 Scheduler overhead time: 0.04424449475482106 Adapter cache time: 0.13641090132296085 Engine time: 0.04633020702749491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.461582641117275,
    "estimated_duration": 3600.1148369492494,
    "input_throughput": 3919.972456200552,
    "output_throughput": 3476.9446439678827,
    "total_throughput": 7396.917100168435,
    "itl": 244.21422083908877,
    "ttft": 2286966.143632193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8760847364063387,
    "arrivals": 1482883,
    "finished_requests": 57531,
    "scheduler_time": 57.51155053160428
}
#Debug simulation 
Total elapsed time: 4.461678732186556. Arrivals time: 0.40675205970183015 Scheduler time: 3.9757537124678493 Scheduler overhead time: 0.023243464063853025 Adapter cache time: 0.020959397777915 Engine time: 0.024054755922406912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.174450526945293,
    "estimated_duration": 3600.0701726026355,
    "input_throughput": 3326.4884921235753,
    "output_throughput": 2976.710032363816,
    "total_throughput": 6303.198524487391,
    "itl": 114.04728665426283,
    "ttft": 2369376.2165114973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9357229983434127,
    "arrivals": 1482883,
    "finished_requests": 48902,
    "scheduler_time": 57.00354408417554
}
#Debug simulation 
Total elapsed time: 4.1745428941212595. Arrivals time: 0.3835070012137294 Scheduler time: 3.5426892996765673 Scheduler overhead time: 0.04435689467936754 Adapter cache time: 0.1359118209220469 Engine time: 0.046586715150624514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.341593028046191,
    "estimated_duration": 3600.150381192384,
    "input_throughput": 3996.791377151943,
    "output_throughput": 3499.9440761768187,
    "total_throughput": 7496.735453328762,
    "itl": 243.57994261930148,
    "ttft": 2282151.729935576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7008516933605997,
    "arrivals": 1481051,
    "finished_requests": 57715,
    "scheduler_time": 57.842995771994424
}
#Debug simulation 
Total elapsed time: 4.341747051104903. Arrivals time: 0.23031493369489908 Scheduler time: 4.0337492460384965 Scheduler overhead time: 0.0231272648088634 Adapter cache time: 0.019559426698833704 Engine time: 0.024005169980227947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.465528590139002,
    "estimated_duration": 3600.173272226956,
    "input_throughput": 3989.736580404929,
    "output_throughput": 3492.7405014097612,
    "total_throughput": 7482.47708181469,
    "itl": 240.68896634910695,
    "ttft": 2283709.560292103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7484623872092955,
    "arrivals": 1481051,
    "finished_requests": 57608,
    "scheduler_time": 57.808678226326286
}
#Debug simulation 
Total elapsed time: 4.465621878392994. Arrivals time: 0.342864528298378 Scheduler time: 4.043324249796569 Scheduler overhead time: 0.023318964056670666 Adapter cache time: 0.020802208688110113 Engine time: 0.0242159441113472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.97705580573529,
    "estimated_duration": 3600.0908175555733,
    "input_throughput": 3373.0846290831237,
    "output_throughput": 2972.640286687657,
    "total_throughput": 6345.72491577078,
    "itl": 112.6835786424416,
    "ttft": 2368746.869273511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7167805045656892,
    "arrivals": 1481051,
    "finished_requests": 48678,
    "scheduler_time": 57.06256620619311
}
#Debug simulation 
Total elapsed time: 3.977178564760834. Arrivals time: 0.20573620265349746 Scheduler time: 3.52482932806015 Scheduler overhead time: 0.044497364200651646 Adapter cache time: 0.13371954765170813 Engine time: 0.046942598186433315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.345429894048721,
    "estimated_duration": 3600.140100509332,
    "input_throughput": 3989.7733418674125,
    "output_throughput": 3492.7726835466815,
    "total_throughput": 7482.546025414094,
    "itl": 240.6875317203551,
    "ttft": 2283690.4245740687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7153661664156258,
    "arrivals": 1481051,
    "finished_requests": 57608,
    "scheduler_time": 57.80860272949292
}
#Debug simulation 
Total elapsed time: 4.345525998622179. Arrivals time: 0.23406314104795456 Scheduler time: 4.031991974916309 Scheduler overhead time: 0.023396750446408987 Adapter cache time: 0.02066956413909793 Engine time: 0.02429724857211113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.989911323878914,
    "estimated_duration": 3600.1005394538706,
    "input_throughput": 3373.07552023037,
    "output_throughput": 2972.6322592155834,
    "total_throughput": 6345.707779445954,
    "itl": 112.68393516030947,
    "ttft": 2368752.3245564383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7264635461196335,
    "arrivals": 1481051,
    "finished_requests": 48678,
    "scheduler_time": 57.062605062939824
}
#Debug simulation 
Total elapsed time: 3.990007232874632. Arrivals time: 0.2061433820053935 Scheduler time: 3.5371903087943792 Scheduler overhead time: 0.04478276381269097 Adapter cache time: 0.1333592259325087 Engine time: 0.04689907934516668 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.321872844360769,
    "estimated_duration": 3600.142070952703,
    "input_throughput": 3989.4900581518446,
    "output_throughput": 3492.6763311517084,
    "total_throughput": 7482.166389303553,
    "itl": 240.6007300227457,
    "ttft": 2283722.332406811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6847215175325991,
    "arrivals": 1481051,
    "finished_requests": 57605,
    "scheduler_time": 57.80816753038989
}
#Debug simulation 
Total elapsed time: 4.321967263240367. Arrivals time: 0.2308662123978138 Scheduler time: 4.011633833870292 Scheduler overhead time: 0.023339698556810617 Adapter cache time: 0.020897651091217995 Engine time: 0.024188540410250425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.410255870781839,
    "estimated_duration": 3600.1096321659843,
    "input_throughput": 3373.067000932966,
    "output_throughput": 2972.624751308293,
    "total_throughput": 6345.69175224126,
    "itl": 112.6842320676078,
    "ttft": 2368757.8041613516,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7355178187415029,
    "arrivals": 1481051,
    "finished_requests": 48678,
    "scheduler_time": 57.06264350243288
}
#Debug simulation 
Total elapsed time: 4.410348112694919. Arrivals time: 0.20615798002108932 Scheduler time: 3.9575741020962596 Scheduler overhead time: 0.044496326707303524 Adapter cache time: 0.13376451516523957 Engine time: 0.046815247274935246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.453573172912002,
    "estimated_duration": 3600.08456852625,
    "input_throughput": 3988.3707526009202,
    "output_throughput": 3510.2242070866664,
    "total_throughput": 7498.594959687586,
    "itl": 243.40043340782765,
    "ttft": 2279128.636086664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5937346223229535,
    "arrivals": 1480152,
    "finished_requests": 58134,
    "scheduler_time": 57.997818496899114
}
#Debug simulation 
Total elapsed time: 4.453685638029128. Arrivals time: 0.338640415109694 Scheduler time: 4.03837737813592 Scheduler overhead time: 0.023272244725376368 Adapter cache time: 0.01820329762995243 Engine time: 0.024036098271608353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.3539786958135664,
    "estimated_duration": 3600.1231794569794,
    "input_throughput": 3982.823999417363,
    "output_throughput": 3505.5942174466404,
    "total_throughput": 7488.418216864004,
    "itl": 241.10309408235537,
    "ttft": 2280112.1386826704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6323699579713882,
    "arrivals": 1480152,
    "finished_requests": 58049,
    "scheduler_time": 57.96535661853147
}
#Debug simulation 
Total elapsed time: 4.354075886774808. Arrivals time: 0.2313095317222178 Scheduler time: 4.046589785255492 Scheduler overhead time: 0.023257232271134853 Adapter cache time: 0.017650474328547716 Engine time: 0.024237629026174545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.004830223042518,
    "estimated_duration": 3600.097956870803,
    "input_throughput": 3367.9816897368755,
    "output_throughput": 2987.418711614231,
    "total_throughput": 6355.400401351107,
    "itl": 113.41112561477227,
    "ttft": 2364119.398308617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6271860507689442,
    "arrivals": 1480152,
    "finished_requests": 49129,
    "scheduler_time": 57.20859312318724
}
#Debug simulation 
Total elapsed time: 4.0049280929379165. Arrivals time: 0.20927578071132302 Scheduler time: 3.549830924719572 Scheduler overhead time: 0.044757614843547344 Adapter cache time: 0.1320082563906908 Engine time: 0.04760493990033865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.3343757460825145,
    "estimated_duration": 3600.09695427702,
    "input_throughput": 3982.85301260158,
    "output_throughput": 3505.6197542142286,
    "total_throughput": 7488.472766815808,
    "itl": 241.10225285046857,
    "ttft": 2280094.7526123123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6062198575912052,
    "arrivals": 1480152,
    "finished_requests": 58049,
    "scheduler_time": 57.96528153895176
}
#Debug simulation 
Total elapsed time: 4.334474226925522. Arrivals time: 0.23115571169182658 Scheduler time: 4.0267765377648175 Scheduler overhead time: 0.02340323757380247 Adapter cache time: 0.017762442585080862 Engine time: 0.02430959139019251 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.115868309047073,
    "estimated_duration": 3600.1060419618466,
    "input_throughput": 3367.974125948954,
    "output_throughput": 2987.412002491781,
    "total_throughput": 6355.386128440736,
    "itl": 113.41135941816617,
    "ttft": 2364124.473520526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6352342930994949,
    "arrivals": 1480152,
    "finished_requests": 49129,
    "scheduler_time": 57.208629971900734
}
#Debug simulation 
Total elapsed time: 4.115971804130822. Arrivals time: 0.31376067409291863 Scheduler time: 3.5578447207808495 Scheduler overhead time: 0.04447229113429785 Adapter cache time: 0.13186462642624974 Engine time: 0.04657089104875922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.404873158782721,
    "estimated_duration": 3600.070724559427,
    "input_throughput": 3982.8820312286366,
    "output_throughput": 3505.6452957724855,
    "total_throughput": 7488.527327001122,
    "itl": 241.10144664202147,
    "ttft": 2280077.6182325752,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5800697572110228,
    "arrivals": 1480152,
    "finished_requests": 58049,
    "scheduler_time": 57.96520192173853
}
#Debug simulation 
Total elapsed time: 4.4049736387096345. Arrivals time: 0.24557755887508392 Scheduler time: 4.08235943922773 Scheduler overhead time: 0.02346197236329317 Adapter cache time: 0.01789687480777502 Engine time: 0.024375093635171652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_256_slots_192_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.01260075205937,
    "estimated_duration": 3600.114002707334,
    "input_throughput": 3367.966678522344,
    "output_throughput": 2987.4053965824683,
    "total_throughput": 6355.372075104812,
    "itl": 113.41161600221488,
    "ttft": 2364129.4694648646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6431567816436304,
    "arrivals": 1480152,
    "finished_requests": 49129,
    "scheduler_time": 57.20866822884531
}
#Debug simulation 
Total elapsed time: 4.012695414014161. Arrivals time: 0.20792282978072762 Scheduler time: 3.5601026681251824 Scheduler overhead time: 0.04450121195986867 Adapter cache time: 0.13198062079027295 Engine time: 0.046739584766328335 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.422597303986549,
    "estimated_duration": 3600.176942403607,
    "input_throughput": 3647.2329582871794,
    "output_throughput": 3241.3323530173802,
    "total_throughput": 6888.565311304559,
    "itl": 265.3037628873788,
    "ttft": 2314994.7600797615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2003,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.130156951097483,
    "arrivals": 1357053,
    "finished_requests": 53441,
    "scheduler_time": 53.60100203488686
}
#Debug simulation 
Total elapsed time: 4.4227417800575495. Arrivals time: 0.22160631325095892 Scheduler time: 4.094181778375059 Scheduler overhead time: 0.02268333686515689 Adapter cache time: 0.05111323855817318 Engine time: 0.02267295913770795 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.4047896158881485,
    "estimated_duration": 3600.0151129199417,
    "input_throughput": 3637.8700058782897,
    "output_throughput": 3232.467263328065,
    "total_throughput": 6870.337269206355,
    "itl": 262.07609532049787,
    "ttft": 2317383.933567178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2077,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.768067072182648,
    "arrivals": 1357053,
    "finished_requests": 53294,
    "scheduler_time": 53.52789039409359
}
#Debug simulation 
Total elapsed time: 4.404883380047977. Arrivals time: 0.22158632054924965 Scheduler time: 4.074129985179752 Scheduler overhead time: 0.022826438769698143 Adapter cache time: 0.05260148039087653 Engine time: 0.023186121601611376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.9316012263298035,
    "estimated_duration": 3600.068043501658,
    "input_throughput": 3194.7510049874154,
    "output_throughput": 2871.3690616651365,
    "total_throughput": 6066.120066652552,
    "itl": 118.36096950588896,
    "ttft": 2384711.603482216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.77759481936571,
    "arrivals": 1357053,
    "finished_requests": 46922,
    "scheduler_time": 54.95058651502477
}
#Debug simulation 
Total elapsed time: 3.9316965653561056. Arrivals time: 0.2023951937444508 Scheduler time: 3.4311234643682837 Scheduler overhead time: 0.043141831643879414 Adapter cache time: 0.18903257697820663 Engine time: 0.045274275820702314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.379451752640307,
    "estimated_duration": 3600.0346162566016,
    "input_throughput": 3637.7205765919657,
    "output_throughput": 3232.423918217834,
    "total_throughput": 6870.1444948098,
    "itl": 261.9526382145948,
    "ttft": 2317354.450268122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2077,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.509834830928384,
    "arrivals": 1357053,
    "finished_requests": 53293,
    "scheduler_time": 53.52993190671582
}
#Debug simulation 
Total elapsed time: 4.379543834831566. Arrivals time: 0.2192005212418735 Scheduler time: 4.0515218446962535 Scheduler overhead time: 0.02271405467763543 Adapter cache time: 0.052605592645704746 Engine time: 0.02297793049365282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.922571850940585,
    "estimated_duration": 3600.1159450769896,
    "input_throughput": 3194.243511997244,
    "output_throughput": 2871.0161443922498,
    "total_throughput": 6065.259656389494,
    "itl": 118.36774313828849,
    "ttft": 2384709.737711128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.951638059763962,
    "arrivals": 1357053,
    "finished_requests": 46918,
    "scheduler_time": 54.94864244223114
}
#Debug simulation 
Total elapsed time: 3.9226637668907642. Arrivals time: 0.2007179856300354 Scheduler time: 3.4223783402703702 Scheduler overhead time: 0.043030702974647284 Adapter cache time: 0.19092546310275793 Engine time: 0.04467479232698679 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.389636302832514,
    "estimated_duration": 3600.0451404676733,
    "input_throughput": 3638.2182692016195,
    "output_throughput": 3232.833074556418,
    "total_throughput": 6871.051343758038,
    "itl": 262.03700825973186,
    "ttft": 2317356.164735581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2077,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.210334462511635,
    "arrivals": 1357053,
    "finished_requests": 53302,
    "scheduler_time": 53.5364964405545
}
#Debug simulation 
Total elapsed time: 4.389734141062945. Arrivals time: 0.21753797447308898 Scheduler time: 4.063245337922126 Scheduler overhead time: 0.022774660028517246 Adapter cache time: 0.05264310399070382 Engine time: 0.02300247922539711 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.929430848918855,
    "estimated_duration": 3600.0657492182886,
    "input_throughput": 3193.8575017683147,
    "output_throughput": 2870.4392419065834,
    "total_throughput": 6064.296743674898,
    "itl": 118.374009569473,
    "ttft": 2384787.015086636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4525,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.152592610455002,
    "arrivals": 1357053,
    "finished_requests": 46914,
    "scheduler_time": 54.94475275857516
}
#Debug simulation 
Total elapsed time: 3.929522297345102. Arrivals time: 0.20451310696080327 Scheduler time: 3.426200316287577 Scheduler overhead time: 0.043196819722652435 Adapter cache time: 0.1900627505965531 Engine time: 0.044826562982052565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.184777449816465,
    "estimated_duration": 3600.17852397344,
    "input_throughput": 3823.9645362935244,
    "output_throughput": 3348.9683691277273,
    "total_throughput": 7172.932905421251,
    "itl": 254.2928248413669,
    "ttft": 2285809.93531664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1775,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.432365745480791,
    "arrivals": 1265213,
    "finished_requests": 55712,
    "scheduler_time": 55.38730080849726
}
#Debug simulation 
Total elapsed time: 4.184908533934504. Arrivals time: 0.22140416130423546 Scheduler time: 3.8498361711390316 Scheduler overhead time: 0.02232365682721138 Adapter cache time: 0.05773533368483186 Engine time: 0.022968426812440157 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.2323099141940475,
    "estimated_duration": 3600.256315365645,
    "input_throughput": 3823.00530694358,
    "output_throughput": 3348.784626401116,
    "total_throughput": 7171.789933344696,
    "itl": 252.03264756177506,
    "ttft": 2286040.684424356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1780,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.792991331768998,
    "arrivals": 1265213,
    "finished_requests": 55696,
    "scheduler_time": 55.41628650887585
}
#Debug simulation 
Total elapsed time: 4.232401752844453. Arrivals time: 0.22873607277870178 Scheduler time: 3.886759656481445 Scheduler overhead time: 0.022685861214995384 Adapter cache time: 0.05982217052951455 Engine time: 0.02371351094916463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.188533551990986,
    "estimated_duration": 3600.0550657251642,
    "input_throughput": 3462.0031561904675,
    "output_throughput": 3052.8352481703478,
    "total_throughput": 6514.838404360816,
    "itl": 111.32458995691448,
    "ttft": 2342922.6649533473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1628,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.318738732393775,
    "arrivals": 1265213,
    "finished_requests": 50486,
    "scheduler_time": 58.37387516706229
}
#Debug simulation 
Total elapsed time: 4.188657441642135. Arrivals time: 0.299559754319489 Scheduler time: 3.6189640830270946 Scheduler overhead time: 0.04547472624108195 Adapter cache time: 0.1551972613669932 Engine time: 0.04755788017064333 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.246283377055079,
    "estimated_duration": 3600.0245555529564,
    "input_throughput": 3823.251421652014,
    "output_throughput": 3349.0002120688728,
    "total_throughput": 7172.251633720886,
    "itl": 252.01737699598365,
    "ttft": 2285956.0638684756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1780,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.561317786213332,
    "arrivals": 1265213,
    "finished_requests": 55696,
    "scheduler_time": 55.41620024168248
}
#Debug simulation 
Total elapsed time: 4.246373755391687. Arrivals time: 0.23500918690115213 Scheduler time: 3.894829274620861 Scheduler overhead time: 0.022569599095731974 Adapter cache time: 0.06000129505991936 Engine time: 0.02332943817600608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.102052643895149,
    "estimated_duration": 3600.002600874523,
    "input_throughput": 3462.0227765869895,
    "output_throughput": 3052.7986277927293,
    "total_throughput": 6514.821404379719,
    "itl": 111.326561161892,
    "ttft": 2342915.7204236127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1628,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.385513992980052,
    "arrivals": 1265213,
    "finished_requests": 50485,
    "scheduler_time": 58.37194142928639
}
#Debug simulation 
Total elapsed time: 4.10214885789901. Arrivals time: 0.2124837557785213 Scheduler time: 3.6194559102877975 Scheduler overhead time: 0.04571542190387845 Adapter cache time: 0.15505834249779582 Engine time: 0.04748758487403393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.233407753985375,
    "estimated_duration": 3600.0752830005667,
    "input_throughput": 3823.4861545804606,
    "output_throughput": 3349.306348380077,
    "total_throughput": 7172.792502960538,
    "itl": 252.00153851021977,
    "ttft": 2285936.248687959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1780,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.32228952492572,
    "arrivals": 1265213,
    "finished_requests": 55701,
    "scheduler_time": 55.42049583230512
}
#Debug simulation 
Total elapsed time: 4.233498590998352. Arrivals time: 0.236764722969383 Scheduler time: 3.8802157836034894 Scheduler overhead time: 0.022695680614560843 Adapter cache time: 0.05951026128605008 Engine time: 0.02347768098115921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.095373338088393,
    "estimated_duration": 3600.0715506416627,
    "input_throughput": 3461.956470775863,
    "output_throughput": 3052.7401595785423,
    "total_throughput": 6514.696630354406,
    "itl": 111.32880310964116,
    "ttft": 2342939.9383451743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1628,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.454427067935382,
    "arrivals": 1265213,
    "finished_requests": 50485,
    "scheduler_time": 58.37197812152976
}
#Debug simulation 
Total elapsed time: 4.095491072162986. Arrivals time: 0.20637974236160517 Scheduler time: 3.6186712537892163 Scheduler overhead time: 0.04562973789870739 Adapter cache time: 0.15483748679980636 Engine time: 0.04798629553988576 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.288365689106286,
    "estimated_duration": 3600.21412082009,
    "input_throughput": 3925.9770462708884,
    "output_throughput": 3456.371366369027,
    "total_throughput": 7382.348412639915,
    "itl": 247.44205283411054,
    "ttft": 2277244.138558299,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 822,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.515720925512775,
    "arrivals": 1250156,
    "finished_requests": 57283,
    "scheduler_time": 57.10303442999302
}
#Debug simulation 
Total elapsed time: 4.288534665014595. Arrivals time: 0.22770212357863784 Scheduler time: 3.9555908562615514 Scheduler overhead time: 0.022842027246952057 Adapter cache time: 0.047581128776073456 Engine time: 0.023934972006827593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.312891013920307,
    "estimated_duration": 3600.152445650529,
    "input_throughput": 3921.937532689304,
    "output_throughput": 3452.8896172211385,
    "total_throughput": 7374.827149910442,
    "itl": 244.39072218711357,
    "ttft": 2278178.8084943313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 819,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.667449686890471,
    "arrivals": 1250156,
    "finished_requests": 57218,
    "scheduler_time": 57.12440779156409
}
#Debug simulation 
Total elapsed time: 4.312994404695928. Arrivals time: 0.2300731702707708 Scheduler time: 3.9759032069705427 Scheduler overhead time: 0.02308319741860032 Adapter cache time: 0.048862948548048735 Engine time: 0.024141314439475536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.141858614049852,
    "estimated_duration": 3600.015727452672,
    "input_throughput": 3501.6891464860214,
    "output_throughput": 3104.698936387338,
    "total_throughput": 6606.38808287336,
    "itl": 109.093187964774,
    "ttft": 2342006.41444311,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 762,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.488459605015809,
    "arrivals": 1250156,
    "finished_requests": 51131,
    "scheduler_time": 59.430062924211114
}
#Debug simulation 
Total elapsed time: 4.141955457162112. Arrivals time: 0.21052810130640864 Scheduler time: 3.6689395629800856 Scheduler overhead time: 0.046317439526319504 Adapter cache time: 0.14517427515238523 Engine time: 0.04859787318855524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.3945453660562634,
    "estimated_duration": 3600.1714414019934,
    "input_throughput": 3922.217935960593,
    "output_throughput": 3453.1136092671068,
    "total_throughput": 7375.3315452277,
    "itl": 244.53986804070695,
    "ttft": 2278122.875887781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 822,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.568142078388458,
    "arrivals": 1250156,
    "finished_requests": 57221,
    "scheduler_time": 57.124533311195464
}
#Debug simulation 
Total elapsed time: 4.394663800019771. Arrivals time: 0.22764275828376412 Scheduler time: 4.059124922379851 Scheduler overhead time: 0.023466036189347506 Adapter cache time: 0.04918291233479977 Engine time: 0.024247874971479177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.160848404746503,
    "estimated_duration": 3600.0469542394053,
    "input_throughput": 3501.658772854351,
    "output_throughput": 3104.672006246484,
    "total_throughput": 6606.330779100836,
    "itl": 109.09417284962154,
    "ttft": 2342019.78216118,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 762,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5196465440467013,
    "arrivals": 1250156,
    "finished_requests": 51131,
    "scheduler_time": 59.43010277193526
}
#Debug simulation 
Total elapsed time: 4.160940557718277. Arrivals time: 0.21076228376477957 Scheduler time: 3.686911804135889 Scheduler overhead time: 0.04636435164138675 Adapter cache time: 0.14598370343446732 Engine time: 0.048628696706146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.299052101094276,
    "estimated_duration": 3600.0610382062855,
    "input_throughput": 3922.3382187529674,
    "output_throughput": 3453.219506020956,
    "total_throughput": 7375.557724773924,
    "itl": 244.53326672102418,
    "ttft": 2278076.597291133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 822,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.457821342409538,
    "arrivals": 1250156,
    "finished_requests": 57221,
    "scheduler_time": 57.12445085144171
}
#Debug simulation 
Total elapsed time: 4.299148787278682. Arrivals time: 0.22610759595409036 Scheduler time: 3.966137690935284 Scheduler overhead time: 0.02310934942215681 Adapter cache time: 0.04879460483789444 Engine time: 0.02406477928161621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 540, 34560, 34560, 540, 34560, 8640, 34560, 540, 8640, 540, 34560, 8640, 8640, 8640, 8640, 34560, 540, 540, 540, 8640, 34560, 540, 34560, 34560, 8640, 8640, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 540, 34560, 34560, 8640, 540, 540, 34560, 8640, 540, 8640, 8640, 34560, 8640, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 540, 8640, 8640, 8640, 540, 540, 34560, 34560, 540, 34560, 540, 8640, 34560, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 540, 540, 540, 34560, 8640, 540, 34560, 540, 34560, 540, 540, 8640, 8640, 34560, 8640, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 8640, 540, 34560, 540, 540, 34560, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 34560, 540, 34560, 8640, 540, 8640, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 8640, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 8640, 34560, 8640, 540, 8640, 34560, 8640, 34560, 540, 34560, 540, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 540, 34560, 540, 34560, 8640, 8640, 540, 8640, 34560, 8640, 8640, 34560, 540, 34560, 34560, 8640, 540, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 540, 34560, 8640, 34560, 34560, 540, 34560, 8640, 8640, 34560, 540, 8640, 540, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 540, 540, 540, 540]
Prompts retrieved: 3752460 . Total input tokens: 836272061 . Total output tokens: 750656964
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.18598127970472,
    "estimated_duration": 3600.0789349490524,
    "input_throughput": 3501.6276664440416,
    "output_throughput": 3104.6444264028823,
    "total_throughput": 6606.272092846923,
    "itl": 109.09521128272323,
    "ttft": 2342033.0774770034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 762,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5515880057960882,
    "arrivals": 1250156,
    "finished_requests": 51131,
    "scheduler_time": 59.43014201985727
}
#Debug simulation 
Total elapsed time: 4.186081301886588. Arrivals time: 0.23226078366860747 Scheduler time: 3.6911442410200834 Scheduler overhead time: 0.04587126709520817 Adapter cache time: 0.14594110241159797 Engine time: 0.04850590880960226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.398096844088286,
    "estimated_duration": 3600.026062400307,
    "input_throughput": 4022.588656023382,
    "output_throughput": 3530.0655550051097,
    "total_throughput": 7552.654211028492,
    "itl": 242.03119513963378,
    "ttft": 2266366.8027165546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.331312168610758,
    "arrivals": 1242527,
    "finished_requests": 58331,
    "scheduler_time": 58.29007787465235
}
#Debug simulation 
Total elapsed time: 4.398234334774315. Arrivals time: 0.22976046707481146 Scheduler time: 4.068167713936418 Scheduler overhead time: 0.023515438195317984 Adapter cache time: 0.04122969601303339 Engine time: 0.02438873564824462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.387274427805096,
    "estimated_duration": 3600.0922933644624,
    "input_throughput": 4017.800051032098,
    "output_throughput": 3527.6437283032474,
    "total_throughput": 7545.443779335345,
    "itl": 239.57968915922652,
    "ttft": 2267465.882060253,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.419981711267033,
    "arrivals": 1242527,
    "finished_requests": 58273,
    "scheduler_time": 58.29953821129956
}
#Debug simulation 
Total elapsed time: 4.387399097904563. Arrivals time: 0.22704407153651118 Scheduler time: 4.059641653671861 Scheduler overhead time: 0.023703552316874266 Adapter cache time: 0.04126710118725896 Engine time: 0.02449859119951725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.203250765800476,
    "estimated_duration": 3600.0845882984186,
    "input_throughput": 3535.6977559286397,
    "output_throughput": 3123.3907771357963,
    "total_throughput": 6659.088533064436,
    "itl": 108.30194435911666,
    "ttft": 2335863.1958559505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3280744661018329,
    "arrivals": 1242527,
    "finished_requests": 51300,
    "scheduler_time": 59.78281722884059
}
#Debug simulation 
Total elapsed time: 4.203346146736294. Arrivals time: 0.21860837377607822 Scheduler time: 3.7286004004999995 Scheduler overhead time: 0.04683617455884814 Adapter cache time: 0.13754596700891852 Engine time: 0.04921074723824859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.77843314781785,
    "estimated_duration": 3600.035828871153,
    "input_throughput": 4017.8630679171747,
    "output_throughput": 3527.6990573680573,
    "total_throughput": 7545.562125285232,
    "itl": 239.57679349359162,
    "ttft": 2267437.914556155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.363595557322258,
    "arrivals": 1242527,
    "finished_requests": 58273,
    "scheduler_time": 58.29945987192566
}
#Debug simulation 
Total elapsed time: 4.778500793967396. Arrivals time: 0.6176475523971021 Scheduler time: 4.059998615644872 Scheduler overhead time: 0.02373110642656684 Adapter cache time: 0.0413924902677536 Engine time: 0.024547372944653034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.151370196603239,
    "estimated_duration": 3600.1011038741076,
    "input_throughput": 3535.681535805311,
    "output_throughput": 3123.376448483545,
    "total_throughput": 6659.057984288856,
    "itl": 108.30244807262471,
    "ttft": 2335871.06665836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3445482121221768,
    "arrivals": 1242527,
    "finished_requests": 51300,
    "scheduler_time": 59.7828590585196
}
#Debug simulation 
Total elapsed time: 4.151491701602936. Arrivals time: 0.2090577594935894 Scheduler time: 3.6882797428406775 Scheduler overhead time: 0.04650373803451657 Adapter cache time: 0.1365751875564456 Engine time: 0.04871067497879267 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.410559845156968,
    "estimated_duration": 3600.233928323661,
    "input_throughput": 4018.1033477276583,
    "output_throughput": 3527.6535505329075,
    "total_throughput": 7545.756898260565,
    "itl": 239.57030922536202,
    "ttft": 2267430.287343979,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3006718782824371,
    "arrivals": 1242527,
    "finished_requests": 58277,
    "scheduler_time": 58.30360379661756
}
#Debug simulation 
Total elapsed time: 4.410655016079545. Arrivals time: 0.23580847634002566 Scheduler time: 4.074171171057969 Scheduler overhead time: 0.023609888274222612 Adapter cache time: 0.04117590328678489 Engine time: 0.024605279322713614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 270, 34560, 34560, 270, 34560, 8640, 34560, 270, 8640, 270, 34560, 8640, 8640, 8640, 8640, 34560, 270, 270, 270, 8640, 34560, 270, 34560, 34560, 8640, 8640, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 270, 34560, 34560, 8640, 270, 270, 34560, 8640, 270, 8640, 8640, 34560, 8640, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 270, 8640, 8640, 8640, 270, 270, 34560, 34560, 270, 34560, 270, 8640, 34560, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 270, 270, 270, 34560, 8640, 270, 34560, 270, 34560, 270, 270, 8640, 8640, 34560, 8640, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 8640, 270, 34560, 270, 270, 34560, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 34560, 270, 34560, 8640, 270, 8640, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 8640, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 8640, 34560, 8640, 270, 8640, 34560, 8640, 34560, 270, 34560, 270, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 270, 34560, 270, 34560, 8640, 8640, 270, 8640, 34560, 8640, 8640, 34560, 270, 34560, 34560, 8640, 270, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 270, 34560, 8640, 34560, 34560, 270, 34560, 8640, 8640, 34560, 270, 8640, 270, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 270, 270, 270, 270]
Prompts retrieved: 3729510 . Total input tokens: 831156773 . Total output tokens: 746030073
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.167286588810384,
    "estimated_duration": 3600.1191262003395,
    "input_throughput": 3535.6638360559814,
    "output_throughput": 3123.3608127483576,
    "total_throughput": 6659.024648804339,
    "itl": 108.30302223120681,
    "ttft": 2335879.286165871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 406,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3625310035794989,
    "arrivals": 1242527,
    "finished_requests": 51300,
    "scheduler_time": 59.7828985933035
}
#Debug simulation 
Total elapsed time: 4.167381044942886. Arrivals time: 0.20850070286542177 Scheduler time: 3.7036784207448363 Scheduler overhead time: 0.046738611068576574 Adapter cache time: 0.13703926373273134 Engine time: 0.04891895689070225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.456377341877669,
    "estimated_duration": 3600.1259914257466,
    "input_throughput": 4057.6163264260836,
    "output_throughput": 3565.55382521944,
    "total_throughput": 7623.170151645523,
    "itl": 239.96263262992647,
    "ttft": 2267108.2687631897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9671141270827489,
    "arrivals": 1238735,
    "finished_requests": 58726,
    "scheduler_time": 58.88942959160812
}
#Debug simulation 
Total elapsed time: 4.456511376891285. Arrivals time: 0.23647647397592664 Scheduler time: 4.122089848387986 Scheduler overhead time: 0.023669349029660225 Adapter cache time: 0.038299905601888895 Engine time: 0.024679642636328936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.3953596809878945,
    "estimated_duration": 3600.1815183478034,
    "input_throughput": 4050.819361656173,
    "output_throughput": 3560.567136593367,
    "total_throughput": 7611.386498249541,
    "itl": 237.01216610487973,
    "ttft": 2268293.8168392624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0318866960168873,
    "arrivals": 1238735,
    "finished_requests": 58638,
    "scheduler_time": 58.895770393081975
}
#Debug simulation 
Total elapsed time: 4.395456655882299. Arrivals time: 0.22845454001799226 Scheduler time: 4.0677605336532 Scheduler overhead time: 0.023869854863733053 Adapter cache time: 0.03924329066649079 Engine time: 0.024900480173528194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.581200554966927,
    "estimated_duration": 3600.118202620053,
    "input_throughput": 3556.392118092696,
    "output_throughput": 3143.1048546586326,
    "total_throughput": 6699.496972751329,
    "itl": 107.24672512107679,
    "ttft": 2340108.4692307473,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.97137459699065,
    "arrivals": 1238735,
    "finished_requests": 51384,
    "scheduler_time": 60.204789583696744
}
#Debug simulation 
Total elapsed time: 4.581267853733152. Arrivals time: 0.5988523312844336 Scheduler time: 3.726718397345394 Scheduler overhead time: 0.04719881573691964 Adapter cache time: 0.13627867167815566 Engine time: 0.04954670835286379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.4364675506949425,
    "estimated_duration": 3600.051112080857,
    "input_throughput": 4049.4125072501406,
    "output_throughput": 3559.9266790943702,
    "total_throughput": 7609.339186344511,
    "itl": 236.17232655293145,
    "ttft": 2268496.449559563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9893927828990892,
    "arrivals": 1238735,
    "finished_requests": 58617,
    "scheduler_time": 58.89633844264523
}
#Debug simulation 
Total elapsed time: 4.4365578489378095. Arrivals time: 0.23355523264035583 Scheduler time: 4.101988623384386 Scheduler overhead time: 0.02391683403402567 Adapter cache time: 0.04091645963490009 Engine time: 0.024880230892449617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.169483604840934,
    "estimated_duration": 3600.0111368774974,
    "input_throughput": 3556.240942938963,
    "output_throughput": 3143.064721131454,
    "total_throughput": 6699.305664070417,
    "itl": 107.24661005788325,
    "ttft": 2340119.7346425685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9836984680593064,
    "arrivals": 1238735,
    "finished_requests": 51382,
    "scheduler_time": 60.20286790477315
}
#Debug simulation 
Total elapsed time: 4.1695801028981805. Arrivals time: 0.21509733144193888 Scheduler time: 3.7001090440899134 Scheduler overhead time: 0.046912898775190115 Adapter cache time: 0.1357799074612558 Engine time: 0.049057875759899616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.422961047850549,
    "estimated_duration": 3600.006501311177,
    "input_throughput": 4049.46268699527,
    "output_throughput": 3559.9707932005813,
    "total_throughput": 7609.433480195851,
    "itl": 236.17017798408168,
    "ttft": 2268473.1497048237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 316,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9448558931890888,
    "arrivals": 1238735,
    "finished_requests": 58617,
    "scheduler_time": 58.89626456266978
}
#Debug simulation 
Total elapsed time: 4.423056292813271. Arrivals time: 0.23331003729254007 Scheduler time: 4.089790040161461 Scheduler overhead time: 0.023958218283951283 Adapter cache time: 0.03965866286307573 Engine time: 0.02506206277757883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 135, 34560, 34560, 135, 34560, 8640, 34560, 135, 8640, 135, 34560, 8640, 8640, 8640, 8640, 34560, 135, 135, 135, 8640, 34560, 135, 34560, 34560, 8640, 8640, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 135, 34560, 34560, 8640, 135, 135, 34560, 8640, 135, 8640, 8640, 34560, 8640, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 135, 8640, 8640, 8640, 135, 135, 34560, 34560, 135, 34560, 135, 8640, 34560, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 135, 135, 135, 34560, 8640, 135, 34560, 135, 34560, 135, 135, 8640, 8640, 34560, 8640, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 8640, 135, 34560, 135, 135, 34560, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 34560, 135, 34560, 8640, 135, 8640, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 8640, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 8640, 34560, 8640, 135, 8640, 34560, 8640, 34560, 135, 34560, 135, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 135, 34560, 135, 34560, 8640, 8640, 135, 8640, 34560, 8640, 8640, 34560, 135, 34560, 34560, 8640, 135, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 135, 34560, 8640, 34560, 34560, 135, 34560, 8640, 8640, 34560, 135, 8640, 135, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 135, 135, 135, 135]
Prompts retrieved: 3718035 . Total input tokens: 828563720 . Total output tokens: 743735295
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.189114595763385,
    "estimated_duration": 3600.02400282908,
    "input_throughput": 3556.2282334615397,
    "output_throughput": 3143.0534882845363,
    "total_throughput": 6699.281721746076,
    "itl": 107.24699532391693,
    "ttft": 2340126.408688433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9965253542736225,
    "arrivals": 1238735,
    "finished_requests": 51382,
    "scheduler_time": 60.20290697014669
}
#Debug simulation 
Total elapsed time: 4.189206141978502. Arrivals time: 0.21223942982032895 Scheduler time: 3.721618900541216 Scheduler overhead time: 0.04704708093777299 Adapter cache time: 0.1365035534836352 Engine time: 0.049053681548684835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.669790903106332,
    "estimated_duration": 3600.100839980184,
    "input_throughput": 4075.8497198319496,
    "output_throughput": 3590.3858182125664,
    "total_throughput": 7666.235538044516,
    "itl": 238.36866312992498,
    "ttft": 2259985.528336305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 1236839,
    "finished_requests": 59365,
    "scheduler_time": 59.31321342774561
}
#Debug simulation 
Total elapsed time: 4.669938439968973. Arrivals time: 0.45676463656127453 Scheduler time: 4.117251523770392 Scheduler overhead time: 0.02359762554988265 Adapter cache time: 0.036478189285844564 Engine time: 0.024603948928415775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.5913803381845355,
    "estimated_duration": 3600.0544938588023,
    "input_throughput": 4070.1428339475283,
    "output_throughput": 3586.0762724627652,
    "total_throughput": 7656.2191064102935,
    "itl": 235.66267002429726,
    "ttft": 2260839.4310452715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8357312702667008,
    "arrivals": 1236839,
    "finished_requests": 59290,
    "scheduler_time": 59.31603383900146
}
#Debug simulation 
Total elapsed time: 4.591476713307202. Arrivals time: 0.3703864165581763 Scheduler time: 4.12327787373215 Scheduler overhead time: 0.023874318692833185 Adapter cache time: 0.0377767370082438 Engine time: 0.024828793946653605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.339423432946205,
    "estimated_duration": 3600.039693648195,
    "input_throughput": 3561.2771221996627,
    "output_throughput": 3155.6640944943365,
    "total_throughput": 6716.941216693999,
    "itl": 107.42579026839171,
    "ttft": 2332470.343214959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8101012635976115,
    "arrivals": 1236839,
    "finished_requests": 51860,
    "scheduler_time": 60.37270887031054
}
#Debug simulation 
Total elapsed time: 4.339514730032533. Arrivals time: 0.36812269082292914 Scheduler time: 3.7179021439515054 Scheduler overhead time: 0.046796222217381 Adapter cache time: 0.13520246325060725 Engine time: 0.048940292093902826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.5774792223237455,
    "estimated_duration": 3600.0205048201497,
    "input_throughput": 4070.181261573682,
    "output_throughput": 3586.1101298490976,
    "total_throughput": 7656.291391422779,
    "itl": 235.66126770699506,
    "ttft": 2260820.0381396837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8018178588361505,
    "arrivals": 1236839,
    "finished_requests": 59290,
    "scheduler_time": 59.315958211776326
}
#Debug simulation 
Total elapsed time: 4.5775757152587175. Arrivals time: 0.3700945843011141 Scheduler time: 4.109043491072953 Scheduler overhead time: 0.02400418557226658 Adapter cache time: 0.03819915512576699 Engine time: 0.024882496800273657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.346000160090625,
    "estimated_duration": 3600.0495417696884,
    "input_throughput": 3561.267380142126,
    "output_throughput": 3155.65546201219,
    "total_throughput": 6716.922842154317,
    "itl": 107.42608112872468,
    "ttft": 2332475.9433812257,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8199100589379705,
    "arrivals": 1236839,
    "finished_requests": 51860,
    "scheduler_time": 60.37274819646761
}
#Debug simulation 
Total elapsed time: 4.34609739901498. Arrivals time: 0.3612785576842725 Scheduler time: 3.7300967150367796 Scheduler overhead time: 0.04712430341169238 Adapter cache time: 0.13577175792306662 Engine time: 0.04911405919119716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.004112942144275,
    "estimated_duration": 3600.2482319816218,
    "input_throughput": 4070.1210182761647,
    "output_throughput": 3586.1638331792415,
    "total_throughput": 7656.284851455406,
    "itl": 235.6609820125134,
    "ttft": 2260884.4129984714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 1236839,
    "finished_requests": 59293,
    "scheduler_time": 59.32021387322237
}
#Debug simulation 
Total elapsed time: 5.004180052317679. Arrivals time: 0.7817015275359154 Scheduler time: 4.124042601324618 Scheduler overhead time: 0.023955033626407385 Adapter cache time: 0.038079644087702036 Engine time: 0.025067188777029514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 66, 34560, 34560, 66, 34560, 8640, 34560, 66, 8640, 66, 34560, 8640, 8640, 8640, 8640, 34560, 66, 66, 66, 8640, 34560, 66, 34560, 34560, 8640, 8640, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 66, 34560, 34560, 8640, 66, 66, 34560, 8640, 66, 8640, 8640, 34560, 8640, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 66, 8640, 8640, 8640, 66, 66, 34560, 34560, 66, 34560, 66, 8640, 34560, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 66, 66, 66, 34560, 8640, 66, 34560, 66, 34560, 66, 66, 8640, 8640, 34560, 8640, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 8640, 66, 34560, 66, 66, 34560, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 34560, 66, 34560, 8640, 66, 8640, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 8640, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 8640, 34560, 8640, 66, 8640, 34560, 8640, 34560, 66, 34560, 66, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 66, 34560, 66, 34560, 8640, 8640, 66, 8640, 34560, 8640, 8640, 34560, 66, 34560, 34560, 8640, 66, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 66, 34560, 8640, 34560, 34560, 66, 34560, 8640, 8640, 34560, 66, 8640, 66, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 66, 66, 66, 66]
Prompts retrieved: 3712170 . Total input tokens: 827267384 . Total output tokens: 742547406
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.322448269929737,
    "estimated_duration": 3600.0603936787343,
    "input_throughput": 3561.2566451695225,
    "output_throughput": 3155.6459497034207,
    "total_throughput": 6716.902594872943,
    "itl": 107.42640338642529,
    "ttft": 2332481.78709048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8307248845696484,
    "arrivals": 1236839,
    "finished_requests": 51860,
    "scheduler_time": 60.37278527988629
}
#Debug simulation 
Total elapsed time: 4.322543000802398. Arrivals time: 0.2100224499590695 Scheduler time: 3.8575717131607234 Scheduler overhead time: 0.04691311391070485 Adapter cache time: 0.13617838267236948 Engine time: 0.04936407785862684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.4237211449071765,
    "estimated_duration": 3600.125798734074,
    "input_throughput": 4094.554697278468,
    "output_throughput": 3605.190408780689,
    "total_throughput": 7699.745106059157,
    "itl": 237.2220742101742,
    "ttft": 2254318.84717938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6457629139698102,
    "arrivals": 1235881,
    "finished_requests": 59831,
    "scheduler_time": 59.521500487511005
}
#Debug simulation 
Total elapsed time: 4.423815698828548. Arrivals time: 0.23052184469997883 Scheduler time: 4.098805000074208 Scheduler overhead time: 0.023649103473871946 Adapter cache time: 0.034931682981550694 Engine time: 0.024691422935575247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.502512764185667,
    "estimated_duration": 3600.107611608313,
    "input_throughput": 4090.6293891090068,
    "output_throughput": 3602.217322111243,
    "total_throughput": 7692.84671122025,
    "itl": 235.19503010402653,
    "ttft": 2254971.341632611,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6893297427613311,
    "arrivals": 1235881,
    "finished_requests": 59778,
    "scheduler_time": 59.52372480776906
}
#Debug simulation 
Total elapsed time: 4.502616656944156. Arrivals time: 0.25058543123304844 Scheduler time: 4.1527637722902 Scheduler overhead time: 0.024154702201485634 Adapter cache time: 0.036102962680161 Engine time: 0.027621543500572443 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.284218469168991,
    "estimated_duration": 3600.0579316459684,
    "input_throughput": 3582.4492952265914,
    "output_throughput": 3160.8441353046087,
    "total_throughput": 6743.2934305312,
    "itl": 107.05804142985683,
    "ttft": 2331570.4502725275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6873653499595851,
    "arrivals": 1235881,
    "finished_requests": 52288,
    "scheduler_time": 60.47318771589147
}
#Debug simulation 
Total elapsed time: 4.28431292809546. Arrivals time: 0.2983771087601781 Scheduler time: 3.738527066539973 Scheduler overhead time: 0.04689087672159076 Adapter cache time: 0.12846235185861588 Engine time: 0.04941622959449887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.445598163176328,
    "estimated_duration": 3600.0797507431635,
    "input_throughput": 4090.6610463170905,
    "output_throughput": 3602.245199518967,
    "total_throughput": 7692.906245836058,
    "itl": 235.19404108371614,
    "ttft": 2254953.803739427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6615452611073868,
    "arrivals": 1235881,
    "finished_requests": 59778,
    "scheduler_time": 59.52364842427166
}
#Debug simulation 
Total elapsed time: 4.445722758304328. Arrivals time: 0.2352411807514727 Scheduler time: 4.113950212020427 Scheduler overhead time: 0.024020000360906124 Adapter cache time: 0.03596273949369788 Engine time: 0.02513547893613577 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.218181530945003,
    "estimated_duration": 3600.0665226729316,
    "input_throughput": 3582.4407462405393,
    "output_throughput": 3160.8365924169925,
    "total_throughput": 6743.277338657532,
    "itl": 107.05831152190497,
    "ttft": 2331575.591723579,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6959166074357952,
    "arrivals": 1235881,
    "finished_requests": 52288,
    "scheduler_time": 60.47322748538055
}
#Debug simulation 
Total elapsed time: 4.218278515152633. Arrivals time: 0.21960503840819 Scheduler time: 3.7508053444325924 Scheduler overhead time: 0.04697644151747227 Adapter cache time: 0.1291044531390071 Engine time: 0.04916385980322957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.449647180736065,
    "estimated_duration": 3600.1944986346884,
    "input_throughput": 4087.4263892077524,
    "output_throughput": 3600.1835470043,
    "total_throughput": 7687.609936212053,
    "itl": 234.73853293048165,
    "ttft": 2255323.078811101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6309006122243599,
    "arrivals": 1235881,
    "finished_requests": 59732,
    "scheduler_time": 59.50026535474329
}
#Debug simulation 
Total elapsed time: 4.4497439078986645. Arrivals time: 0.23086714325472713 Scheduler time: 4.122925178147852 Scheduler overhead time: 0.0238386495038867 Adapter cache time: 0.0358024244196713 Engine time: 0.024919332936406136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_256_slots_192_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 33, 34560, 34560, 33, 34560, 8640, 34560, 33, 8640, 33, 34560, 8640, 8640, 8640, 8640, 34560, 33, 33, 33, 8640, 34560, 33, 34560, 34560, 8640, 8640, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 33, 34560, 34560, 8640, 33, 33, 34560, 8640, 33, 8640, 8640, 34560, 8640, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 33, 8640, 8640, 8640, 33, 33, 34560, 34560, 33, 34560, 33, 8640, 34560, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 33, 33, 33, 34560, 8640, 33, 34560, 33, 34560, 33, 33, 8640, 8640, 34560, 8640, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 8640, 33, 34560, 33, 33, 34560, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 34560, 33, 34560, 8640, 33, 8640, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 8640, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 8640, 34560, 8640, 33, 8640, 34560, 8640, 34560, 33, 34560, 33, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 33, 34560, 33, 34560, 8640, 8640, 33, 8640, 34560, 8640, 8640, 34560, 33, 34560, 34560, 8640, 33, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 33, 34560, 8640, 34560, 34560, 33, 34560, 8640, 8640, 34560, 33, 8640, 33, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 33, 33, 33, 33]
Prompts retrieved: 3709365 . Total input tokens: 826642015 . Total output tokens: 741985016
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.27502509765327,
    "estimated_duration": 3600.075992724638,
    "input_throughput": 3582.4313225786022,
    "output_throughput": 3160.8282777908494,
    "total_throughput": 6743.259600369452,
    "itl": 107.05859621543316,
    "ttft": 2331581.2983014337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7053481414169096,
    "arrivals": 1235881,
    "finished_requests": 52288,
    "scheduler_time": 60.47326600310866
}
#Debug simulation 
Total elapsed time: 4.275144506711513. Arrivals time: 0.29690411081537604 Scheduler time: 3.7300358372740448 Scheduler overhead time: 0.047025103121995926 Adapter cache time: 0.1292674895375967 Engine time: 0.049230688251554966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.91138114919886,
    "estimated_duration": 3600.217682586818,
    "input_throughput": 4002.5396435601524,
    "output_throughput": 3534.9048646569186,
    "total_throughput": 7537.444508217071,
    "itl": 242.69231013665367,
    "ttft": 2263422.7978073116,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1973,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.038342318779497,
    "arrivals": 1142437,
    "finished_requests": 58235,
    "scheduler_time": 58.292365010531746
}
#Debug simulation 
Total elapsed time: 4.911482667084783. Arrivals time: 0.6010546549223363 Scheduler time: 4.180443083867431 Scheduler overhead time: 0.023340072948485613 Adapter cache time: 0.07116378610953689 Engine time: 0.024372987914830446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.52940512727946,
    "estimated_duration": 3600.0179936448876,
    "input_throughput": 4002.5355499435177,
    "output_throughput": 3535.788160634289,
    "total_throughput": 7538.323710577807,
    "itl": 239.531427491788,
    "ttft": 2263825.5535293506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1982,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.460722361015868,
    "arrivals": 1142437,
    "finished_requests": 58233,
    "scheduler_time": 58.36541016137275
}
#Debug simulation 
Total elapsed time: 4.529495670925826. Arrivals time: 0.3551988210529089 Scheduler time: 4.042010371573269 Scheduler overhead time: 0.023552685044705868 Adapter cache time: 0.0730546279810369 Engine time: 0.024478826206177473 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.446401607245207,
    "estimated_duration": 3600.007906664443,
    "input_throughput": 3664.9563951165514,
    "output_throughput": 3252.5450786715214,
    "total_throughput": 6917.501473788073,
    "itl": 103.98065325474609,
    "ttft": 2315514.5148350717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1791,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.851929529141585,
    "arrivals": 1142437,
    "finished_requests": 53298,
    "scheduler_time": 62.23831204628271
}
#Debug simulation 
Total elapsed time: 4.446497540920973. Arrivals time: 0.34358667768538 Scheduler time: 3.8455538409762084 Scheduler overhead time: 0.048344274051487446 Adapter cache time: 0.13531022379174829 Engine time: 0.050403437577188015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.580907638184726,
    "estimated_duration": 3600.0723573678847,
    "input_throughput": 4002.846212384451,
    "output_throughput": 3536.140037281786,
    "total_throughput": 7538.986249666237,
    "itl": 239.5186661023105,
    "ttft": 2263804.0350122377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1983,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.210791909196222,
    "arrivals": 1142437,
    "finished_requests": 58239,
    "scheduler_time": 58.37004369415758
}
#Debug simulation 
Total elapsed time: 4.581009637098759. Arrivals time: 0.2279708767309785 Scheduler time: 4.220234660431743 Scheduler overhead time: 0.023786230478435755 Adapter cache time: 0.07295891782268882 Engine time: 0.024676984641700983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.480354632250965,
    "estimated_duration": 3600.007071530161,
    "input_throughput": 3665.8836323870355,
    "output_throughput": 3254.194718851082,
    "total_throughput": 6920.078351238118,
    "itl": 104.12273705508137,
    "ttft": 2315024.0711368057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.918422141987736,
    "arrivals": 1142437,
    "finished_requests": 53317,
    "scheduler_time": 62.2433319008438
}
#Debug simulation 
Total elapsed time: 4.480451617855579. Arrivals time: 0.35091038700193167 Scheduler time: 3.8674858566373587 Scheduler overhead time: 0.048319505993276834 Adapter cache time: 0.1396604822948575 Engine time: 0.05066036619246006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.5647525168024,
    "estimated_duration": 3600.0397123589655,
    "input_throughput": 4002.883065575223,
    "output_throughput": 3536.3640451782235,
    "total_throughput": 7539.247110753447,
    "itl": 239.50133411742024,
    "ttft": 2263756.520340326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1983,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.92926973479084,
    "arrivals": 1142437,
    "finished_requests": 58241,
    "scheduler_time": 58.3740985314224
}
#Debug simulation 
Total elapsed time: 4.5648473226465285. Arrivals time: 0.3718960238620639 Scheduler time: 4.0608541471883655 Scheduler overhead time: 0.02376473555341363 Adapter cache time: 0.07210714323446155 Engine time: 0.024994181469082832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 1080, 34560, 34560, 1080, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 4320, 4320, 4320, 4320, 34560, 1080, 1080, 1080, 4320, 34560, 1080, 34560, 34560, 4320, 4320, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 1080, 34560, 4320, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 34560, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 1080, 34560, 1080, 1080, 4320, 4320, 34560, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 4320, 1080, 34560, 1080, 1080, 34560, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 34560, 1080, 34560, 4320, 1080, 4320, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 4320, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 4320, 34560, 4320, 1080, 4320, 34560, 4320, 34560, 1080, 34560, 1080, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 1080, 34560, 1080, 34560, 4320, 4320, 1080, 4320, 34560, 4320, 4320, 34560, 1080, 34560, 34560, 4320, 1080, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 1080, 34560, 4320, 34560, 34560, 1080, 34560, 4320, 4320, 34560, 1080, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 1080, 1080]
Prompts retrieved: 3431160 . Total input tokens: 764899018 . Total output tokens: 686112732
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.445368282031268,
    "estimated_duration": 3600.085713883525,
    "input_throughput": 3665.803552705905,
    "output_throughput": 3254.123632340556,
    "total_throughput": 6919.927185046461,
    "itl": 104.12510804571492,
    "ttft": 2315053.0772862127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1790,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.997018258496981,
    "arrivals": 1142437,
    "finished_requests": 53317,
    "scheduler_time": 62.243378137764545
}
#Debug simulation 
Total elapsed time: 4.44549220893532. Arrivals time: 0.34305983409285545 Scheduler time: 3.8420533845201135 Scheduler overhead time: 0.04843391943722963 Adapter cache time: 0.13801316358149052 Engine time: 0.05038046417757869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.5459101581946015,
    "estimated_duration": 3600.233796957156,
    "input_throughput": 4125.401248261419,
    "output_throughput": 3644.260550825593,
    "total_throughput": 7769.661799087012,
    "itl": 235.46736245524676,
    "ttft": 2246180.445435204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1073,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2839033492399228,
    "arrivals": 1127186,
    "finished_requests": 60121,
    "scheduler_time": 60.14161747666878
}
#Debug simulation 
Total elapsed time: 4.546056012157351. Arrivals time: 0.23038127087056637 Scheduler time: 4.194421815220267 Scheduler overhead time: 0.02414165297523141 Adapter cache time: 0.06083693681284785 Engine time: 0.024851877242326736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.500872798264027,
    "estimated_duration": 3600.1849219198484,
    "input_throughput": 4123.620125624901,
    "output_throughput": 3643.2809104165276,
    "total_throughput": 7766.901036041429,
    "itl": 232.23239496511997,
    "ttft": 2246806.405855692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4914994682301823,
    "arrivals": 1127186,
    "finished_requests": 60092,
    "scheduler_time": 60.207962090673874
}
#Debug simulation 
Total elapsed time: 4.500970378983766. Arrivals time: 0.23427322879433632 Scheduler time: 4.14414436975494 Scheduler overhead time: 0.0242841225117445 Adapter cache time: 0.06162370229139924 Engine time: 0.025123270694166422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.698913086205721,
    "estimated_duration": 3600.0801391235127,
    "input_throughput": 3695.334127544981,
    "output_throughput": 3284.40938619737,
    "total_throughput": 6979.743513742352,
    "itl": 101.9579721490121,
    "ttft": 2308083.6149313007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 983,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.212676137257334,
    "arrivals": 1127186,
    "finished_requests": 53891,
    "scheduler_time": 62.97554725159655
}
#Debug simulation 
Total elapsed time: 4.6989772859960794. Arrivals time: 0.21441781427711248 Scheduler time: 4.23270893516019 Scheduler overhead time: 0.04908486781641841 Adapter cache time: 0.12788784550502896 Engine time: 0.05118913808837533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.56042794091627,
    "estimated_duration": 3600.045099443969,
    "input_throughput": 4123.780283278382,
    "output_throughput": 3643.4224121319635,
    "total_throughput": 7767.202695410346,
    "itl": 232.22426445777708,
    "ttft": 2246749.539802947,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.351759869323544,
    "arrivals": 1127186,
    "finished_requests": 60092,
    "scheduler_time": 60.20787921366646
}
#Debug simulation 
Total elapsed time: 4.560536665841937. Arrivals time: 0.2406932609155774 Scheduler time: 4.195469641126692 Scheduler overhead time: 0.024332210421562195 Adapter cache time: 0.06302368827164173 Engine time: 0.025386961176991463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.342948371078819,
    "estimated_duration": 3600.0088031295886,
    "input_throughput": 3695.4073524583873,
    "output_throughput": 3284.47446842934,
    "total_throughput": 6979.881820887727,
    "itl": 101.95913298700418,
    "ttft": 2308099.620780636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 983,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.252288579978046,
    "arrivals": 1127186,
    "finished_requests": 53891,
    "scheduler_time": 62.97366643534378
}
#Debug simulation 
Total elapsed time: 4.343042104039341. Arrivals time: 0.21378637989982963 Scheduler time: 3.8766529695130885 Scheduler overhead time: 0.049310784321278334 Adapter cache time: 0.1282315794378519 Engine time: 0.05132802715525031 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.915771241765469,
    "estimated_duration": 3600.1484294499583,
    "input_throughput": 4123.973855785817,
    "output_throughput": 3643.447001434896,
    "total_throughput": 7767.4208572207135,
    "itl": 232.2128563083752,
    "ttft": 2246755.6231546956,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1993538155452512,
    "arrivals": 1127186,
    "finished_requests": 60097,
    "scheduler_time": 60.21220943157019
}
#Debug simulation 
Total elapsed time: 4.9158444670028985. Arrivals time: 0.6232778350822628 Scheduler time: 4.169424151536077 Scheduler overhead time: 0.024481624830514193 Adapter cache time: 0.06155607523396611 Engine time: 0.02560580801218748 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 540, 34560, 34560, 540, 34560, 4320, 34560, 540, 4320, 540, 34560, 4320, 4320, 4320, 4320, 34560, 540, 540, 540, 4320, 34560, 540, 34560, 34560, 4320, 4320, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 540, 34560, 34560, 4320, 540, 540, 34560, 4320, 540, 4320, 4320, 34560, 4320, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 540, 4320, 4320, 4320, 540, 540, 34560, 34560, 540, 34560, 540, 4320, 34560, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 540, 540, 540, 34560, 4320, 540, 34560, 540, 34560, 540, 540, 4320, 4320, 34560, 4320, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 4320, 540, 34560, 540, 540, 34560, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 34560, 540, 34560, 4320, 540, 4320, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 4320, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 4320, 34560, 4320, 540, 4320, 34560, 4320, 34560, 540, 34560, 540, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 540, 34560, 540, 34560, 4320, 4320, 540, 4320, 34560, 4320, 4320, 34560, 540, 34560, 34560, 4320, 540, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 540, 34560, 4320, 34560, 34560, 540, 34560, 4320, 4320, 34560, 540, 4320, 540, 540, 540, 540, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 540, 540, 540, 540]
Prompts retrieved: 3385260 . Total input tokens: 754628110 . Total output tokens: 677025174
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.344518144149333,
    "estimated_duration": 3600.051598169366,
    "input_throughput": 3695.3634238922737,
    "output_throughput": 3284.4354247624115,
    "total_throughput": 6979.798848654685,
    "itl": 101.96041152485888,
    "ttft": 2308116.279047083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 983,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.29504486735914,
    "arrivals": 1127186,
    "finished_requests": 53891,
    "scheduler_time": 62.97370518777708
}
#Debug simulation 
Total elapsed time: 4.344637677073479. Arrivals time: 0.2136873542331159 Scheduler time: 3.878173847682774 Scheduler overhead time: 0.0494712651707232 Adapter cache time: 0.12794983852654696 Engine time: 0.05158139578998089 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.659978904295713,
    "estimated_duration": 3600.08076037877,
    "input_throughput": 4215.066830446176,
    "output_throughput": 3738.4603001472624,
    "total_throughput": 7953.527130593438,
    "itl": 229.62814862104008,
    "ttft": 2236396.510270392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 550,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.683268259163037,
    "arrivals": 1119422,
    "finished_requests": 61779,
    "scheduler_time": 61.64905989261704
}
#Debug simulation 
Total elapsed time: 4.660121194086969. Arrivals time: 0.23902315320447087 Scheduler time: 4.30255597922951 Scheduler overhead time: 0.024685312062501907 Adapter cache time: 0.05643792310729623 Engine time: 0.025699167512357235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.609364964067936,
    "estimated_duration": 3600.0729398583553,
    "input_throughput": 4213.055472314058,
    "output_throughput": 3735.952638928804,
    "total_throughput": 7949.008111242862,
    "itl": 226.41630905589483,
    "ttft": 2237214.048228159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7872762575186847,
    "arrivals": 1119422,
    "finished_requests": 61731,
    "scheduler_time": 61.70385094889845
}
#Debug simulation 
Total elapsed time: 4.6094898181036115. Arrivals time: 0.23343728249892592 Scheduler time: 4.2579314154572785 Scheduler overhead time: 0.0249029453843832 Adapter cache time: 0.055536582600325346 Engine time: 0.025853903964161873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.50351883796975,
    "estimated_duration": 3600.0723868064833,
    "input_throughput": 3759.0538594710315,
    "output_throughput": 3356.5438973629025,
    "total_throughput": 7115.597756833934,
    "itl": 101.02451180842276,
    "ttft": 2301411.51983622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6479231529310445,
    "arrivals": 1119422,
    "finished_requests": 55037,
    "scheduler_time": 64.16959795976378
}
#Debug simulation 
Total elapsed time: 4.503613448701799. Arrivals time: 0.29664968326687813 Scheduler time: 3.9614532748237252 Scheduler overhead time: 0.049715991131961346 Adapter cache time: 0.11975641641765833 Engine time: 0.05206567468121648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.647953542880714,
    "estimated_duration": 3600.260528006741,
    "input_throughput": 4212.892062119012,
    "output_throughput": 3735.891304356965,
    "total_throughput": 7948.783366475976,
    "itl": 226.41243352060886,
    "ttft": 2237232.2761411085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7137291001994084,
    "arrivals": 1119422,
    "finished_requests": 61735,
    "scheduler_time": 61.708177041095816
}
#Debug simulation 
Total elapsed time: 4.648076261859387. Arrivals time: 0.23472188785672188 Scheduler time: 4.29532727971673 Scheduler overhead time: 0.02472777059301734 Adapter cache time: 0.055339820217341185 Engine time: 0.026110084261745214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.4410615297965705,
    "estimated_duration": 3600.093801955342,
    "input_throughput": 3759.031498748674,
    "output_throughput": 3356.523930970034,
    "total_throughput": 7115.555429718708,
    "itl": 101.02512757572045,
    "ttft": 2301420.9996347837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6693012966215646,
    "arrivals": 1119422,
    "finished_requests": 55037,
    "scheduler_time": 64.16963496494739
}
#Debug simulation 
Total elapsed time: 4.441152283921838. Arrivals time: 0.22392157139256597 Scheduler time: 3.9705209201201797 Scheduler overhead time: 0.04974891524761915 Adapter cache time: 0.1204950949177146 Engine time: 0.052467326167970896 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.64275272982195,
    "estimated_duration": 3600.1852730836767,
    "input_throughput": 4212.980124494685,
    "output_throughput": 3735.969395952636,
    "total_throughput": 7948.949520447321,
    "itl": 226.40841682813794,
    "ttft": 2237198.770017787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.638547561606371,
    "arrivals": 1119422,
    "finished_requests": 61735,
    "scheduler_time": 61.70810365660908
}
#Debug simulation 
Total elapsed time: 4.642875013872981. Arrivals time: 0.23330641677603126 Scheduler time: 4.290937213692814 Scheduler overhead time: 0.024977802764624357 Adapter cache time: 0.05575683945789933 Engine time: 0.026072331238538027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 270, 34560, 34560, 270, 34560, 4320, 34560, 270, 4320, 270, 34560, 4320, 4320, 4320, 4320, 34560, 270, 270, 270, 4320, 34560, 270, 34560, 34560, 4320, 4320, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 270, 34560, 34560, 4320, 270, 270, 34560, 4320, 270, 4320, 4320, 34560, 4320, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 270, 4320, 4320, 4320, 270, 270, 34560, 34560, 270, 34560, 270, 4320, 34560, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 270, 270, 270, 34560, 4320, 270, 34560, 270, 34560, 270, 270, 4320, 4320, 34560, 4320, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 4320, 270, 34560, 270, 270, 34560, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 34560, 270, 34560, 4320, 270, 4320, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 4320, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 4320, 34560, 4320, 270, 4320, 34560, 4320, 34560, 270, 34560, 270, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 270, 34560, 270, 34560, 4320, 4320, 270, 4320, 34560, 4320, 4320, 34560, 270, 34560, 34560, 4320, 270, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 270, 34560, 4320, 34560, 34560, 270, 34560, 4320, 4320, 34560, 270, 4320, 270, 270, 270, 270, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 270, 270, 270, 270]
Prompts retrieved: 3362310 . Total input tokens: 749481355 . Total output tokens: 672460316
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.416922963224351,
    "estimated_duration": 3600.002851212051,
    "input_throughput": 3759.1261894261406,
    "output_throughput": 3356.4673416666296,
    "total_throughput": 7115.593531092771,
    "itl": 101.02551833151449,
    "ttft": 2301433.4481149674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6904279327392557,
    "arrivals": 1119422,
    "finished_requests": 55036,
    "scheduler_time": 64.16766560891297
}
#Debug simulation 
Total elapsed time: 4.417015693150461. Arrivals time: 0.21630712691694498 Scheduler time: 3.9548587361350656 Scheduler overhead time: 0.0498576108366251 Adapter cache time: 0.11998182628303766 Engine time: 0.05203861836344004 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.712027730885893,
    "estimated_duration": 3600.1839706397927,
    "input_throughput": 4293.807240426342,
    "output_throughput": 3786.7017661259392,
    "total_throughput": 8080.509006552282,
    "itl": 225.9882204272521,
    "ttft": 2228443.332755683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0069004677538749,
    "arrivals": 1115519,
    "finished_requests": 62790,
    "scheduler_time": 62.452030499544485
}
#Debug simulation 
Total elapsed time: 4.712171506602317. Arrivals time: 0.31486548390239477 Scheduler time: 4.2835833337157965 Scheduler overhead time: 0.02493716822937131 Adapter cache time: 0.05103262886404991 Engine time: 0.02592533966526389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.677175574935973,
    "estimated_duration": 3600.202958514293,
    "input_throughput": 4290.789763245696,
    "output_throughput": 3784.843009412771,
    "total_throughput": 8075.632772658468,
    "itl": 223.7600073675284,
    "ttft": 2228782.4355558013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.072391731124377,
    "arrivals": 1115519,
    "finished_requests": 62744,
    "scheduler_time": 62.484782288014195
}
#Debug simulation 
Total elapsed time: 4.677268490195274. Arrivals time: 0.24442039523273706 Scheduler time: 4.317598827648908 Scheduler overhead time: 0.025243650656193495 Adapter cache time: 0.05210181372240186 Engine time: 0.02597072208300233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.4165026899427176,
    "estimated_duration": 3600.0581956474066,
    "input_throughput": 3801.155497026377,
    "output_throughput": 3370.6813447269283,
    "total_throughput": 7171.836841753306,
    "itl": 100.40056395069372,
    "ttft": 2296227.396273802,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.015332850851124,
    "arrivals": 1115519,
    "finished_requests": 55576,
    "scheduler_time": 64.43191859130789
}
#Debug simulation 
Total elapsed time: 4.416593051049858. Arrivals time: 0.2215508404187858 Scheduler time: 3.9543630410917103 Scheduler overhead time: 0.04975332599133253 Adapter cache time: 0.11508457083255053 Engine time: 0.05181974917650223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.770252143964171,
    "estimated_duration": 3600.1591598499713,
    "input_throughput": 4290.841963954657,
    "output_throughput": 3784.8890548960735,
    "total_throughput": 8075.731018850731,
    "itl": 223.75813064208205,
    "ttft": 2228759.205765068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0286720320512572,
    "arrivals": 1115519,
    "finished_requests": 62744,
    "scheduler_time": 62.48470332275981
}
#Debug simulation 
Total elapsed time: 4.770347124896944. Arrivals time: 0.31578156212344766 Scheduler time: 4.339239197783172 Scheduler overhead time: 0.02530192956328392 Adapter cache time: 0.05189006309956312 Engine time: 0.02621829090639949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.412754151970148,
    "estimated_duration": 3600.0705625738315,
    "input_throughput": 3801.1424393349944,
    "output_throughput": 3370.6697657960526,
    "total_throughput": 7171.812205131047,
    "itl": 100.40090122138346,
    "ttft": 2296234.033648542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.02765672191978,
    "arrivals": 1115519,
    "finished_requests": 55576,
    "scheduler_time": 64.4319616466695
}
#Debug simulation 
Total elapsed time: 4.412848325911909. Arrivals time: 0.21767716156318784 Scheduler time: 3.9548294339329004 Scheduler overhead time: 0.04985402803868055 Adapter cache time: 0.11457901634275913 Engine time: 0.05198660725727677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.659998686984181,
    "estimated_duration": 3600.1141401459554,
    "input_throughput": 4290.895621263197,
    "output_throughput": 3784.9363852245997,
    "total_throughput": 8075.832006487797,
    "itl": 223.7560424918655,
    "ttft": 2228735.412450608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9837265470228171,
    "arrivals": 1115519,
    "finished_requests": 62744,
    "scheduler_time": 62.48462910376576
}
#Debug simulation 
Total elapsed time: 4.660091335885227. Arrivals time: 0.22730183228850365 Scheduler time: 4.318350539542735 Scheduler overhead time: 0.025041754823178053 Adapter cache time: 0.051060100086033344 Engine time: 0.026460357941687107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 135, 34560, 34560, 135, 34560, 4320, 34560, 135, 4320, 135, 34560, 4320, 4320, 4320, 4320, 34560, 135, 135, 135, 4320, 34560, 135, 34560, 34560, 4320, 4320, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 135, 34560, 34560, 4320, 135, 135, 34560, 4320, 135, 4320, 4320, 34560, 4320, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 135, 4320, 4320, 4320, 135, 135, 34560, 34560, 135, 34560, 135, 4320, 34560, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 135, 135, 135, 34560, 4320, 135, 34560, 135, 34560, 135, 135, 4320, 4320, 34560, 4320, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 4320, 135, 34560, 135, 135, 34560, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 34560, 135, 34560, 4320, 135, 4320, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 4320, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 4320, 34560, 4320, 135, 4320, 34560, 4320, 34560, 135, 34560, 135, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 135, 34560, 135, 34560, 4320, 4320, 135, 4320, 34560, 4320, 4320, 34560, 135, 34560, 34560, 4320, 135, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 135, 34560, 4320, 34560, 34560, 135, 34560, 4320, 4320, 34560, 135, 4320, 135, 135, 135, 135, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 135, 135, 135, 135]
Prompts retrieved: 3350835 . Total input tokens: 746922044 . Total output tokens: 670198486
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.435360920615494,
    "estimated_duration": 3600.0839304974234,
    "input_throughput": 3801.128324835813,
    "output_throughput": 3370.657249738996,
    "total_throughput": 7171.785574574809,
    "itl": 100.40128711140154,
    "ttft": 2296240.8863038323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.040986623279755,
    "arrivals": 1115519,
    "finished_requests": 55576,
    "scheduler_time": 64.4319996689089
}
#Debug simulation 
Total elapsed time: 4.435455414000899. Arrivals time: 0.2044783504679799 Scheduler time: 3.9932777546346188 Scheduler overhead time: 0.049487886019051075 Adapter cache time: 0.1127907712943852 Engine time: 0.05154342856258154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.639018862973899,
    "estimated_duration": 3600.159387107107,
    "input_throughput": 4331.536835798449,
    "output_throughput": 3806.2364819383833,
    "total_throughput": 8137.773317736832,
    "itl": 224.8648023115947,
    "ttft": 2223430.697817078,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.795726813422515,
    "arrivals": 1113643,
    "finished_requests": 63103,
    "scheduler_time": 62.772674487285116
}
#Debug simulation 
Total elapsed time: 4.639148297719657. Arrivals time: 0.21932045789435506 Scheduler time: 4.310049194376916 Scheduler overhead time: 0.024728836491703987 Adapter cache time: 0.046887519769370556 Engine time: 0.026424993760883808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.651939697097987,
    "estimated_duration": 3600.228727135825,
    "input_throughput": 4329.133280484486,
    "output_throughput": 3803.99247880434,
    "total_throughput": 8133.125759288826,
    "itl": 222.86607876106476,
    "ttft": 2223919.748897582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.845239899535667,
    "arrivals": 1113643,
    "finished_requests": 63067,
    "scheduler_time": 62.80239202070178
}
#Debug simulation 
Total elapsed time: 4.652029649820179. Arrivals time: 0.22075793938711286 Scheduler time: 4.3200181275606155 Scheduler overhead time: 0.02501650620251894 Adapter cache time: 0.047685716301202774 Engine time: 0.026725125033408403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.382277152966708,
    "estimated_duration": 3600.0411575839585,
    "input_throughput": 3829.3426093083476,
    "output_throughput": 3380.719960481774,
    "total_throughput": 7210.062569790121,
    "itl": 100.49538213848551,
    "ttft": 2291753.3929342926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8053715072758534,
    "arrivals": 1113643,
    "finished_requests": 55790,
    "scheduler_time": 64.55923502746985
}
#Debug simulation 
Total elapsed time: 4.382375644054264. Arrivals time: 0.2054660809226334 Scheduler time: 3.941333420574665 Scheduler overhead time: 0.04949119873344898 Adapter cache time: 0.11054408317431808 Engine time: 0.05180989997461438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.627811353188008,
    "estimated_duration": 3600.1943308318505,
    "input_throughput": 4329.17464108077,
    "output_throughput": 3804.0288221984997,
    "total_throughput": 8133.20346327927,
    "itl": 222.86474407546532,
    "ttft": 2223899.7643639934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8109178927866764,
    "arrivals": 1113643,
    "finished_requests": 63067,
    "scheduler_time": 62.802317723472676
}
#Debug simulation 
Total elapsed time: 4.627908767201006. Arrivals time: 0.22004255931824446 Scheduler time: 4.295826315879822 Scheduler overhead time: 0.025019865483045578 Adapter cache time: 0.04845529282465577 Engine time: 0.026723523158580065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.366542869247496,
    "estimated_duration": 3600.0512585951788,
    "input_throughput": 3829.331864952258,
    "output_throughput": 3380.7104748695424,
    "total_throughput": 7210.0423398218,
    "itl": 100.49565270037331,
    "ttft": 2291759.2318602437,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8154318101890422,
    "arrivals": 1113643,
    "finished_requests": 55790,
    "scheduler_time": 64.55927573577966
}
#Debug simulation 
Total elapsed time: 4.366635049227625. Arrivals time: 0.209296980407089 Scheduler time: 3.9220594805665314 Scheduler overhead time: 0.04938994022086263 Adapter cache time: 0.11053849058225751 Engine time: 0.05145243462175131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.714216646272689,
    "estimated_duration": 3600.160752579098,
    "input_throughput": 4329.21501875563,
    "output_throughput": 3804.0643019034487,
    "total_throughput": 8133.279320659078,
    "itl": 222.86333368732966,
    "ttft": 2223879.673162336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7774130766745667,
    "arrivals": 1113643,
    "finished_requests": 63067,
    "scheduler_time": 62.80224428682923
}
#Debug simulation 
Total elapsed time: 4.714311033952981. Arrivals time: 0.22176604298874736 Scheduler time: 4.380968939978629 Scheduler overhead time: 0.02501285308972001 Adapter cache time: 0.047683628275990486 Engine time: 0.027036824263632298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_256_slots_192_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 66, 34560, 34560, 66, 34560, 4320, 34560, 66, 4320, 66, 34560, 4320, 4320, 4320, 4320, 34560, 66, 66, 66, 4320, 34560, 66, 34560, 34560, 4320, 4320, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 66, 34560, 34560, 4320, 66, 66, 34560, 4320, 66, 4320, 4320, 34560, 4320, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 66, 4320, 4320, 4320, 66, 66, 34560, 34560, 66, 34560, 66, 4320, 34560, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 66, 66, 66, 34560, 4320, 66, 34560, 66, 34560, 66, 66, 4320, 4320, 34560, 4320, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 4320, 66, 34560, 66, 66, 34560, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 34560, 66, 34560, 4320, 66, 4320, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 4320, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 4320, 34560, 4320, 66, 4320, 34560, 4320, 34560, 66, 34560, 66, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 66, 34560, 66, 34560, 4320, 4320, 66, 4320, 34560, 4320, 4320, 34560, 66, 34560, 34560, 4320, 66, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 66, 34560, 4320, 34560, 34560, 66, 34560, 4320, 4320, 34560, 66, 4320, 66, 66, 66, 66, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 66, 66, 66, 66]
Prompts retrieved: 3344970 . Total input tokens: 745610682 . Total output tokens: 669045759
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.368056983686984,
    "estimated_duration": 3600.0612314534046,
    "input_throughput": 3829.321256970523,
    "output_throughput": 3380.701109654869,
    "total_throughput": 7210.022366625392,
    "itl": 100.49593473218712,
    "ttft": 2291765.1065711486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8253663593158159,
    "arrivals": 1113643,
    "finished_requests": 55790,
    "scheduler_time": 64.55931404488095
}
#Debug simulation 
Total elapsed time: 4.36814944492653. Arrivals time: 0.20600097486749291 Scheduler time: 3.9273447701707482 Scheduler overhead time: 0.04920789273455739 Adapter cache time: 0.11015548091381788 Engine time: 0.05161916045472026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_256_slots_192_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.775782914832234,
    "estimated_duration": 3600.1112202396034,
    "input_throughput": 4342.277236357041,
    "output_throughput": 3817.4506728393035,
    "total_throughput": 8159.727909196345,
    "itl": 224.10792043646376,
    "ttft": 2234676.4769596197,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6427024262258775,
    "arrivals": 1112647,
    "finished_requests": 63036,
    "scheduler_time": 62.99650616742973
}
#Debug simulation 
Total elapsed time: 4.775879442226142. Arrivals time: 0.34219009475782514 Scheduler time: 4.323759506922215 Scheduler overhead time: 0.024745982605963945 Adapter cache time: 0.046875753439962864 Engine time: 0.026593494694679976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_256_slots_192_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.7820214428938925,
    "estimated_duration": 3600.22239847388,
    "input_throughput": 4339.126106937735,
    "output_throughput": 3814.027435033087,
    "total_throughput": 8153.153541970822,
    "itl": 220.75689895407484,
    "ttft": 2235957.698683894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6863396924664289,
    "arrivals": 1112647,
    "finished_requests": 62980,
    "scheduler_time": 63.041318498790325
}
#Debug simulation 
Total elapsed time: 4.782119616866112. Arrivals time: 0.2234500921331346 Scheduler time: 4.445747643709183 Scheduler overhead time: 0.02582767140120268 Adapter cache time: 0.04746293881908059 Engine time: 0.027649967465549707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_256_slots_192_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.5163208660669625,
    "estimated_duration": 3600.0129287483874,
    "input_throughput": 3816.004628843419,
    "output_throughput": 3376.0148200982885,
    "total_throughput": 7192.019448941708,
    "itl": 99.61788571337414,
    "ttft": 2306918.093612671,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6674179993942408,
    "arrivals": 1112647,
    "finished_requests": 55332,
    "scheduler_time": 64.63570939283677
}
#Debug simulation 
Total elapsed time: 4.516412401106209. Arrivals time: 0.204981982242316 Scheduler time: 4.075709850061685 Scheduler overhead time: 0.0497506083920598 Adapter cache time: 0.10997597873210907 Engine time: 0.05199625110253692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_256_slots_192_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [4320, 4320, 34560, 33, 34560, 34560, 33, 34560, 4320, 34560, 33, 4320, 33, 34560, 4320, 4320, 4320, 4320, 34560, 33, 33, 33, 4320, 34560, 33, 34560, 34560, 4320, 4320, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 33, 34560, 34560, 4320, 33, 33, 34560, 4320, 33, 4320, 4320, 34560, 4320, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 33, 4320, 4320, 4320, 33, 33, 34560, 34560, 33, 34560, 33, 4320, 34560, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 33, 33, 33, 34560, 4320, 33, 34560, 33, 34560, 33, 33, 4320, 4320, 34560, 4320, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 4320, 33, 34560, 33, 33, 34560, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 34560, 33, 34560, 4320, 33, 4320, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 4320, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 4320, 34560, 4320, 33, 4320, 34560, 4320, 34560, 33, 34560, 33, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 33, 34560, 33, 34560, 4320, 4320, 33, 4320, 34560, 4320, 4320, 34560, 33, 34560, 34560, 4320, 33, 34560, 34560, 4320, 4320, 34560, 34560, 4320, 4320, 4320, 4320, 34560, 34560, 4320, 4320, 34560, 33, 34560, 4320, 34560, 34560, 33, 34560, 4320, 4320, 34560, 33, 4320, 33, 33, 33, 33, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 33, 33, 33, 33]
Prompts retrieved: 3342165 . Total input tokens: 745000425 . Total output tokens: 668483509
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.764461257029325,
    "estimated_duration": 3600.192499978201,
    "input_throughput": 4339.1621420506235,
    "output_throughput": 3814.059109362386,
    "total_throughput": 8153.22125141301,
    "itl": 220.75588168703013,
    "ttft": 2235939.505317161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6565122342202825,
    "arrivals": 1112647,
    "finished_requests": 62980,
    "scheduler_time": 63.041247461355404
}
#Debug simulation 
Total elapsed time: 4.764553795102984. Arrivals time: 0.3402515910565853 Scheduler time: 4.312871056143194 Scheduler overhead time: 0.02519419277086854 Adapter cache time: 0.04757953388616443 Engine time: 0.026817006524652243 
