INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.0153661062941,
    "estimated_duration": 3600.001578699814,
    "input_throughput": 7304.02179698393,
    "output_throughput": 6416.020241952788,
    "total_throughput": 13720.042038936717,
    "itl": 117.9381063661158,
    "ttft": 1615314.048288173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.139044126830996,
    "arrivals": 279137,
    "finished_requests": 105916,
    "scheduler_time": 213.3164807930256
}
#Debug simulation 
Total elapsed time: 111.01558803906664. Arrivals time: 0.4883055668324232 Scheduler time: 110.32667022570968 Scheduler overhead time: 0.07559134438633919 Adapter cache time: 0.01782464375719428 Engine time: 0.07815982820466161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 116.2574566481635,
    "estimated_duration": 3600.037816400887,
    "input_throughput": 7254.861568679594,
    "output_throughput": 6365.705908864839,
    "total_throughput": 13620.567477544433,
    "itl": 116.20328679283412,
    "ttft": 1625667.5867866206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0721497133933058,
    "arrivals": 279137,
    "finished_requests": 105278,
    "scheduler_time": 215.93466379048536
}
#Debug simulation 
Total elapsed time: 116.25766280386597. Arrivals time: 0.49094629287719727 Scheduler time: 115.5597766879946 Scheduler overhead time: 0.07862007850781083 Adapter cache time: 0.01812357595190406 Engine time: 0.08134687319397926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 111.83060819283128,
    "estimated_duration": 3600.0260925285766,
    "input_throughput": 7303.412343195758,
    "output_throughput": 6413.157129031762,
    "total_throughput": 13716.56947222752,
    "itl": 117.9155238554766,
    "ttft": 1616170.232919418,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 366,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0943584079341944,
    "arrivals": 279137,
    "finished_requests": 105932,
    "scheduler_time": 213.42611991637858
}
#Debug simulation 
Total elapsed time: 111.83077277103439. Arrivals time: 0.4936041468754411 Scheduler time: 111.1333883414045 Scheduler overhead time: 0.07756966911256313 Adapter cache time: 0.01737073389813304 Engine time: 0.0801073657348752 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 114.99975519813597,
    "estimated_duration": 3600.1355771655913,
    "input_throughput": 7268.900695290769,
    "output_throughput": 6384.75399254186,
    "total_throughput": 13653.654687832628,
    "itl": 116.93724688959055,
    "ttft": 1622381.4910842688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.056864708922809,
    "arrivals": 279137,
    "finished_requests": 105437,
    "scheduler_time": 214.82602493513053
}
#Debug simulation 
Total elapsed time: 114.99992700479925. Arrivals time: 0.49669046187773347 Scheduler time: 114.30212729424238 Scheduler overhead time: 0.07652732916176319 Adapter cache time: 0.01768952701240778 Engine time: 0.07837834907695651 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 108.7854700111784,
    "estimated_duration": 3600.0724298081614,
    "input_throughput": 7198.123233698629,
    "output_throughput": 6352.556079328288,
    "total_throughput": 13550.679313026916,
    "itl": 117.23859994318354,
    "ttft": 1634448.5518392785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.089533636840062,
    "arrivals": 277607,
    "finished_requests": 104790,
    "scheduler_time": 214.99850557814378
}
#Debug simulation 
Total elapsed time: 108.78564208885655. Arrivals time: 0.48220086423680186 Scheduler time: 108.1017467500642 Scheduler overhead time: 0.07734114490449429 Adapter cache time: 0.017182109877467155 Engine time: 0.07854089280590415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 113.70814995607361,
    "estimated_duration": 3600.1329519104984,
    "input_throughput": 7233.896177689675,
    "output_throughput": 6381.008509091058,
    "total_throughput": 13614.904686780734,
    "itl": 117.70719266002864,
    "ttft": 1629294.2782605067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 330,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.07579037673772,
    "arrivals": 277607,
    "finished_requests": 105266,
    "scheduler_time": 213.85525054614328
}
#Debug simulation 
Total elapsed time: 113.70833267085254. Arrivals time: 0.49354088585823774 Scheduler time: 113.00980285229161 Scheduler overhead time: 0.07893431046977639 Adapter cache time: 0.017865632195025682 Engine time: 0.07955487631261349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 113.82947583077475,
    "estimated_duration": 3600.1359664369857,
    "input_throughput": 7233.890120481881,
    "output_throughput": 6381.003166037533,
    "total_throughput": 13614.893286519415,
    "itl": 117.70726274129,
    "ttft": 1629295.98483399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 330,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0778811739012657,
    "arrivals": 277607,
    "finished_requests": 105266,
    "scheduler_time": 213.85538254024004
}
#Debug simulation 
Total elapsed time: 113.8296535727568. Arrivals time: 0.5013588690198958 Scheduler time: 113.1237827478908 Scheduler overhead time: 0.07815431663766503 Adapter cache time: 0.01741076074540615 Engine time: 0.07933803834021091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.37444980302826,
    "estimated_duration": 3600.109116116158,
    "input_throughput": 7194.497212335134,
    "output_throughput": 6355.000713056669,
    "total_throughput": 13549.497925391803,
    "itl": 117.11359576406872,
    "ttft": 1632033.8212662647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 332,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0372335876175234,
    "arrivals": 277607,
    "finished_requests": 104792,
    "scheduler_time": 214.98485658738332
}
#Debug simulation 
Total elapsed time: 111.37472144281492. Arrivals time: 0.48955140681937337 Scheduler time: 110.681509133894 Scheduler overhead time: 0.0783719839528203 Adapter cache time: 0.01756691886112094 Engine time: 0.07889712415635586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 116.79126705788076,
    "estimated_duration": 3600.1197544821175,
    "input_throughput": 7194.927326445597,
    "output_throughput": 6353.927246870317,
    "total_throughput": 13548.854573315914,
    "itl": 117.28862658698625,
    "ttft": 1631850.0508834687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1331235491670721,
    "arrivals": 277607,
    "finished_requests": 104730,
    "scheduler_time": 215.05978737286037
}
#Debug simulation 
Total elapsed time: 116.79143740609288. Arrivals time: 0.495640863198787 Scheduler time: 116.09479973558336 Scheduler overhead time: 0.07813325570896268 Adapter cache time: 0.0168416746892035 Engine time: 0.07748866267502308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 111.50873566279188,
    "estimated_duration": 3600.007362690758,
    "input_throughput": 7236.67853293718,
    "output_throughput": 6386.103328082236,
    "total_throughput": 13622.781861019415,
    "itl": 117.6908920684288,
    "ttft": 1627429.8422373706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 359,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0734280558698799,
    "arrivals": 277607,
    "finished_requests": 105334,
    "scheduler_time": 213.6375984703471
}
#Debug simulation 
Total elapsed time: 111.50890697771683. Arrivals time: 0.4978698040358722 Scheduler time: 110.81130775948986 Scheduler overhead time: 0.07772903749719262 Adapter cache time: 0.01670969557017088 Engine time: 0.077070705126971 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 116.32532583223656,
    "estimated_duration": 3600.1349239478045,
    "input_throughput": 7194.897010025378,
    "output_throughput": 6353.900474073356,
    "total_throughput": 13548.797484098734,
    "itl": 117.2889432450974,
    "ttft": 1631856.6322412207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1478367421776097,
    "arrivals": 277607,
    "finished_requests": 104730,
    "scheduler_time": 215.0601305405156
}
#Debug simulation 
Total elapsed time: 116.3254877650179. Arrivals time: 0.46791881788522005 Scheduler time: 115.659516972024 Scheduler overhead time: 0.07689346699044108 Adapter cache time: 0.016264335718005896 Engine time: 0.07627218076959252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 110.94074058067054,
    "estimated_duration": 3600.033719709851,
    "input_throughput": 7218.605719641557,
    "output_throughput": 6363.002900385672,
    "total_throughput": 13581.608620027228,
    "itl": 116.94735419395697,
    "ttft": 1633785.0246924115,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9915980290342109,
    "arrivals": 276925,
    "finished_requests": 104886,
    "scheduler_time": 214.7735409452007
}
#Debug simulation 
Total elapsed time: 110.94090652093291. Arrivals time: 0.4284483031369746 Scheduler time: 110.32562957936898 Scheduler overhead time: 0.07188525702804327 Adapter cache time: 0.015059125144034624 Engine time: 0.0723387822508812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 112.24451505532488,
    "estimated_duration": 3600.1095807892357,
    "input_throughput": 7250.023482418005,
    "output_throughput": 6369.984714457659,
    "total_throughput": 13620.008196875664,
    "itl": 117.59937775305248,
    "ttft": 1627164.6767642032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0220993544953,
    "arrivals": 276925,
    "finished_requests": 105205,
    "scheduler_time": 214.30300146103676
}
#Debug simulation 
Total elapsed time: 112.24467784212902. Arrivals time: 0.4396212478168309 Scheduler time: 111.6135087814182 Scheduler overhead time: 0.07429502392187715 Adapter cache time: 0.015149575658142567 Engine time: 0.07411618391051888 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 113.05509658437222,
    "estimated_duration": 3600.11157137773,
    "input_throughput": 7250.019473705209,
    "output_throughput": 6369.9811923395155,
    "total_throughput": 13620.000666044723,
    "itl": 117.59945448031473,
    "ttft": 1627165.6820518926,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0237806210108156,
    "arrivals": 276925,
    "finished_requests": 105205,
    "scheduler_time": 214.30308457294342
}
#Debug simulation 
Total elapsed time: 113.05526127107441. Arrivals time: 0.4523765444755554 Scheduler time: 112.40802903007716 Scheduler overhead time: 0.07492192555218935 Adapter cache time: 0.01598405744880438 Engine time: 0.07564723445102572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.26384081784636,
    "estimated_duration": 3600.0562865560473,
    "input_throughput": 7218.5604700254235,
    "output_throughput": 6362.9630140904665,
    "total_throughput": 13581.523484115889,
    "itl": 116.94810015319644,
    "ttft": 1633793.8184899047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 324,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0141303758951872,
    "arrivals": 276925,
    "finished_requests": 104886,
    "scheduler_time": 214.7735754445022
}
#Debug simulation 
Total elapsed time: 111.26400209311396. Arrivals time: 0.45033740159124136 Scheduler time: 110.6186255896464 Scheduler overhead time: 0.07554684532806277 Adapter cache time: 0.015610340982675552 Engine time: 0.07547686528414488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 110.06774659082294,
    "estimated_duration": 3600.062972472442,
    "input_throughput": 7253.153958600617,
    "output_throughput": 6393.207889970092,
    "total_throughput": 13646.36184857071,
    "itl": 118.13821599241045,
    "ttft": 1629257.3009800971,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1272880190424674,
    "arrivals": 276925,
    "finished_requests": 105357,
    "scheduler_time": 213.26484195160802
}
#Debug simulation 
Total elapsed time: 110.06796251190826. Arrivals time: 0.45414752420037985 Scheduler time: 109.41821840777993 Scheduler overhead time: 0.07692018710076809 Adapter cache time: 0.015709711704403162 Engine time: 0.07485025655478239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 107.35738720325753,
    "estimated_duration": 3600.13395786353,
    "input_throughput": 7240.057260388212,
    "output_throughput": 6382.583889638428,
    "total_throughput": 13622.64115002664,
    "itl": 118.29955736364315,
    "ttft": 1628192.8346567948,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 329,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9837265470228171,
    "arrivals": 276925,
    "finished_requests": 105177,
    "scheduler_time": 213.73952163194426
}
#Debug simulation 
Total elapsed time: 107.35755161009729. Arrivals time: 0.44231158262118697 Scheduler time: 106.7222327538766 Scheduler overhead time: 0.07428576238453388 Adapter cache time: 0.015912624076008797 Engine time: 0.07424551574513316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 110.00064697768539,
    "estimated_duration": 3600.0776180929383,
    "input_throughput": 7253.124451753391,
    "output_throughput": 6393.1818815040415,
    "total_throughput": 13646.306333257433,
    "itl": 118.13863952764052,
    "ttft": 1629263.3486508396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.141749704480175,
    "arrivals": 276925,
    "finished_requests": 105357,
    "scheduler_time": 213.26506270408677
}
#Debug simulation 
Total elapsed time: 110.00080934958532. Arrivals time: 0.4499106672592461 Scheduler time: 109.35566747467965 Scheduler overhead time: 0.07595288706943393 Adapter cache time: 0.016168053727596998 Engine time: 0.07508922694250941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 111.30399443488568,
    "estimated_duration": 3600.003632811796,
    "input_throughput": 7221.811045699902,
    "output_throughput": 6391.059384023356,
    "total_throughput": 13612.870429723258,
    "itl": 113.86553240128785,
    "ttft": 1477093.526887286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 340,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0405658329371363,
    "arrivals": 218234,
    "finished_requests": 105215,
    "scheduler_time": 209.9156506927169
}
#Debug simulation 
Total elapsed time: 111.30412082513794. Arrivals time: 0.439966450445354 Scheduler time: 110.67025958560407 Scheduler overhead time: 0.07452164450660348 Adapter cache time: 0.015722866635769606 Engine time: 0.07535007735714316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 111.14245540369302,
    "estimated_duration": 3600.0694571153526,
    "input_throughput": 7214.979963417893,
    "output_throughput": 6384.107105093438,
    "total_throughput": 13599.087068511331,
    "itl": 114.35117472894726,
    "ttft": 1465581.9092471483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2285806001513322,
    "arrivals": 218234,
    "finished_requests": 105071,
    "scheduler_time": 210.09225764851507
}
#Debug simulation 
Total elapsed time: 111.14258692599833. Arrivals time: 0.44071741681545973 Scheduler time: 110.50680519966409 Scheduler overhead time: 0.07536060456186533 Adapter cache time: 0.01644617412239313 Engine time: 0.07506690546870232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 110.93087156722322,
    "estimated_duration": 3600.0716449173615,
    "input_throughput": 7214.97557879747,
    "output_throughput": 6384.10322540333,
    "total_throughput": 13599.0788042008,
    "itl": 114.35125005437826,
    "ttft": 1465582.728090916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2310439846292207,
    "arrivals": 218234,
    "finished_requests": 105071,
    "scheduler_time": 210.09233036430845
}
#Debug simulation 
Total elapsed time: 110.93099965620786. Arrivals time: 0.4517016941681504 Scheduler time: 110.2806320944801 Scheduler overhead time: 0.0772145725786686 Adapter cache time: 0.01691281097009778 Engine time: 0.07531118206679821 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.43692324589938,
    "estimated_duration": 3600.0141540589593,
    "input_throughput": 7215.0907992165085,
    "output_throughput": 6384.205177106532,
    "total_throughput": 13599.295976323041,
    "itl": 114.34999751136927,
    "ttft": 1465561.3785049932,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 377,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1766889947094026,
    "arrivals": 218234,
    "finished_requests": 105071,
    "scheduler_time": 210.0905152096624
}
#Debug simulation 
Total elapsed time: 111.43705523107201. Arrivals time: 0.45040679862722754 Scheduler time: 110.79145945003256 Scheduler overhead time: 0.07564052101224661 Adapter cache time: 0.016667809337377548 Engine time: 0.07511448953300714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 110.95244324021041,
    "estimated_duration": 3600.072914880366,
    "input_throughput": 7230.7963242641845,
    "output_throughput": 6399.4451070071145,
    "total_throughput": 13630.241431271299,
    "itl": 114.51539366972044,
    "ttft": 1479910.7167060527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9744504491239837,
    "arrivals": 218234,
    "finished_requests": 105345,
    "scheduler_time": 209.0122644216748
}
#Debug simulation 
Total elapsed time: 110.95257691619918. Arrivals time: 0.45693910913541913 Scheduler time: 110.29936670092866 Scheduler overhead time: 0.0761583000421524 Adapter cache time: 0.015840242616832256 Engine time: 0.07580579863861203 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 115.18275476293638,
    "estimated_duration": 3600.0430595058224,
    "input_throughput": 7216.7200698889255,
    "output_throughput": 6418.112122017698,
    "total_throughput": 13634.832191906624,
    "itl": 114.36331125547167,
    "ttft": 1448029.5023742572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9029951890604583,
    "arrivals": 218234,
    "finished_requests": 105120,
    "scheduler_time": 210.13098165767755
}
#Debug simulation 
Total elapsed time: 115.18294638581574. Arrivals time: 0.4541629711166024 Scheduler time: 114.5333036375232 Scheduler overhead time: 0.07635476533323526 Adapter cache time: 0.015489722602069378 Engine time: 0.07489456329494715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 114.92719531292096,
    "estimated_duration": 3600.0598706830274,
    "input_throughput": 7216.663037070775,
    "output_throughput": 6418.023541263027,
    "total_throughput": 13634.686578333803,
    "itl": 114.36676835073222,
    "ttft": 1447962.5603063565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0094279421120933,
    "arrivals": 218234,
    "finished_requests": 105119,
    "scheduler_time": 210.1247724280478
}
#Debug simulation 
Total elapsed time: 114.92732466477901. Arrivals time: 0.45742818573489785 Scheduler time: 114.27061961591244 Scheduler overhead time: 0.07796824164688587 Adapter cache time: 0.01595511520281434 Engine time: 0.07649776944890618 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 117.76609781570733,
    "estimated_duration": 3600.0974015083293,
    "input_throughput": 7200.57434811046,
    "output_throughput": 6362.020924879415,
    "total_throughput": 13562.595272989875,
    "itl": 113.44659106517649,
    "ttft": 1469687.8277842444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8110292521421787,
    "arrivals": 212464,
    "finished_requests": 104355,
    "scheduler_time": 210.78887488876634
}
#Debug simulation 
Total elapsed time: 117.76623095013201. Arrivals time: 0.45718968426808715 Scheduler time: 117.10670451261103 Scheduler overhead time: 0.07970303623005748 Adapter cache time: 0.01569750625640154 Engine time: 0.07800642773509026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 119.90619580307975,
    "estimated_duration": 3600.0058241350507,
    "input_throughput": 7204.333622496675,
    "output_throughput": 6358.336102275512,
    "total_throughput": 13562.669724772188,
    "itl": 113.74050922067387,
    "ttft": 1463288.9798422717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8898308878275607,
    "arrivals": 212464,
    "finished_requests": 104405,
    "scheduler_time": 210.0995341328731
}
#Debug simulation 
Total elapsed time: 119.90632727276534. Arrivals time: 0.46050194557756186 Scheduler time: 119.24342891899869 Scheduler overhead time: 0.07868468901142478 Adapter cache time: 0.016385041642934084 Engine time: 0.07761894538998604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 120.09134966880083,
    "estimated_duration": 3600.0082242455023,
    "input_throughput": 7204.32881939753,
    "output_throughput": 6358.331863199381,
    "total_throughput": 13562.660682596912,
    "itl": 113.74057306204641,
    "ttft": 1463290.1103677293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 273,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.891585194729273,
    "arrivals": 212464,
    "finished_requests": 104405,
    "scheduler_time": 210.099614411253
}
#Debug simulation 
Total elapsed time: 120.09148273896426. Arrivals time: 0.4548139520920813 Scheduler time: 119.43629482993856 Scheduler overhead time: 0.07841318938881159 Adapter cache time: 0.01590133085846901 Engine time: 0.07647427590563893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 117.85392850032076,
    "estimated_duration": 3600.1133521398415,
    "input_throughput": 7200.542445307168,
    "output_throughput": 6361.992737363767,
    "total_throughput": 13562.535182670936,
    "itl": 113.44688521348408,
    "ttft": 1469694.0411467561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8283197161718296,
    "arrivals": 212464,
    "finished_requests": 104355,
    "scheduler_time": 210.789180728329
}
#Debug simulation 
Total elapsed time: 117.85406441893429. Arrivals time: 0.4595305738039315 Scheduler time: 117.19171645864844 Scheduler overhead time: 0.07907326612621546 Adapter cache time: 0.016273247078061104 Engine time: 0.07806343212723732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 121.93653899617493,
    "estimated_duration": 3600.0583521409067,
    "input_throughput": 7212.954752402204,
    "output_throughput": 6366.24069895158,
    "total_throughput": 13579.195451353784,
    "itl": 113.32879548877449,
    "ttft": 1467206.7270073972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8622538116574326,
    "arrivals": 212464,
    "finished_requests": 104523,
    "scheduler_time": 210.28800266523714
}
#Debug simulation 
Total elapsed time: 121.93667250592262. Arrivals time: 0.46167970495298505 Scheduler time: 121.27085922844708 Scheduler overhead time: 0.07992238644510508 Adapter cache time: 0.01590615324676037 Engine time: 0.07865326153114438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 117.9873049412854,
    "estimated_duration": 3600.079224396003,
    "input_throughput": 7200.610704435025,
    "output_throughput": 6362.053047274997,
    "total_throughput": 13562.66375171002,
    "itl": 113.44611869835713,
    "ttft": 1469680.344551161,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7923633281490776,
    "arrivals": 212464,
    "finished_requests": 104355,
    "scheduler_time": 210.78845886015296
}
#Debug simulation 
Total elapsed time: 117.98743554018438. Arrivals time: 0.4589997436851263 Scheduler time: 117.32729756692424 Scheduler overhead time: 0.07858016947284341 Adapter cache time: 0.015883487183600664 Engine time: 0.07747598038986325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 121.68172742426395,
    "estimated_duration": 3600.069436809773,
    "input_throughput": 7212.9325436041845,
    "output_throughput": 6366.2210971990835,
    "total_throughput": 13579.153640803268,
    "itl": 113.33002979432639,
    "ttft": 1467214.870476048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8730686372891107,
    "arrivals": 212464,
    "finished_requests": 104523,
    "scheduler_time": 210.2883856135142
}
#Debug simulation 
Total elapsed time: 121.68190982332453. Arrivals time: 0.45874994108453393 Scheduler time: 121.02081544324756 Scheduler overhead time: 0.07944806152954698 Adapter cache time: 0.015665709041059017 Engine time: 0.07778271939605474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 112.93189138779417,
    "estimated_duration": 3600.1083255959397,
    "input_throughput": 7186.173209306827,
    "output_throughput": 6356.182072996956,
    "total_throughput": 13542.355282303783,
    "itl": 114.34300076460963,
    "ttft": 1462363.640056202,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8202107153739769,
    "arrivals": 209539,
    "finished_requests": 104589,
    "scheduler_time": 209.67375338233094
}
#Debug simulation 
Total elapsed time: 112.93202731898054. Arrivals time: 0.4525623628869653 Scheduler time: 112.27782518602908 Scheduler overhead time: 0.07861626846715808 Adapter cache time: 0.01650360831990838 Engine time: 0.07725422969087958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 112.69559583207592,
    "estimated_duration": 3600.0262612243973,
    "input_throughput": 7186.087023487816,
    "output_throughput": 6356.225021596774,
    "total_throughput": 13542.31204508459,
    "itl": 114.34428398471027,
    "ttft": 1462271.6436879335,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8744720410346095,
    "arrivals": 209539,
    "finished_requests": 104586,
    "scheduler_time": 209.66750315098434
}
#Debug simulation 
Total elapsed time: 112.69573649996892. Arrivals time: 0.4571122112683952 Scheduler time: 112.03641484770924 Scheduler overhead time: 0.07918213726952672 Adapter cache time: 0.016311397310346365 Engine time: 0.07739357883110642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 112.92903363099322,
    "estimated_duration": 3600.027444539704,
    "input_throughput": 7186.084661448387,
    "output_throughput": 6356.222932329824,
    "total_throughput": 13542.30759377821,
    "itl": 114.34431215327186,
    "ttft": 1462272.079329467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8760303529910787,
    "arrivals": 209539,
    "finished_requests": 104586,
    "scheduler_time": 209.66746746941794
}
#Debug simulation 
Total elapsed time: 112.92916270298883. Arrivals time: 0.45491217402741313 Scheduler time: 112.27259461442009 Scheduler overhead time: 0.07878729002550244 Adapter cache time: 0.016077633947134018 Engine time: 0.07704047113656998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 112.83645697915927,
    "estimated_duration": 3600.1283621846883,
    "input_throughput": 7186.133214511424,
    "output_throughput": 6356.146697534363,
    "total_throughput": 13542.279912045786,
    "itl": 114.34344344760413,
    "ttft": 1462372.6548469467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.838107057693417,
    "arrivals": 209539,
    "finished_requests": 104589,
    "scheduler_time": 209.67455086526428
}
#Debug simulation 
Total elapsed time: 112.83659447124228. Arrivals time: 0.4519099621102214 Scheduler time: 112.18145386595279 Scheduler overhead time: 0.07890768349170685 Adapter cache time: 0.016588382422924042 Engine time: 0.07769469916820526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 113.80902960198,
    "estimated_duration": 3600.033054642838,
    "input_throughput": 7187.569560404252,
    "output_throughput": 6357.805234727425,
    "total_throughput": 13545.374795131678,
    "itl": 114.47924910909254,
    "ttft": 1460251.6667183063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8941097264550668,
    "arrivals": 209539,
    "finished_requests": 104591,
    "scheduler_time": 209.57185887275867
}
#Debug simulation 
Total elapsed time: 113.80916625494137. Arrivals time: 0.45170392375439405 Scheduler time: 113.15559132164344 Scheduler overhead time: 0.07927223527804017 Adapter cache time: 0.0157936904579401 Engine time: 0.07743260683491826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 112.77094040624797,
    "estimated_duration": 3600.086933643291,
    "input_throughput": 7186.215910019296,
    "output_throughput": 6356.219841847664,
    "total_throughput": 13542.43575186696,
    "itl": 114.34257648312798,
    "ttft": 1462353.9224728446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8013334790337842,
    "arrivals": 209539,
    "finished_requests": 104589,
    "scheduler_time": 209.67296736179333
}
#Debug simulation 
Total elapsed time: 112.77106954297051. Arrivals time: 0.44639815809205174 Scheduler time: 112.12463764846325 Scheduler overhead time: 0.07828112319111824 Adapter cache time: 0.015850079245865345 Engine time: 0.07665050216019154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 1080, 1080, 1080, 1080, 8640, 1080, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 8640, 1080, 135, 135, 1080, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 8640, 8640, 135, 1080, 8640, 135, 1080, 135, 1080, 135, 135, 1080, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 1080, 135, 135, 135, 135, 135, 8640, 135, 1080, 135, 8640, 8640, 135, 8640, 135, 1080, 1080, 8640, 135, 135, 135, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 135, 8640, 1080, 1080, 1080, 8640, 1080, 135, 1080, 8640, 8640, 1080, 8640, 1080, 135, 8640, 1080, 135, 1080, 1080, 135, 1080, 8640, 135, 1080, 8640, 135, 1080, 8640, 8640, 1080, 135, 8640, 1080, 135, 135, 1080, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 1080, 135, 8640, 8640, 1080, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 135, 8640, 1080, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 630720 . Total input tokens: 140632962 . Total output tokens: 126054297
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 114.337588456925,
    "estimated_duration": 3600.0429385221855,
    "input_throughput": 7187.54982700897,
    "output_throughput": 6357.787779441217,
    "total_throughput": 13545.337606450188,
    "itl": 114.47949397678089,
    "ttft": 1460255.4929441644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9055533210188194,
    "arrivals": 209539,
    "finished_requests": 104591,
    "scheduler_time": 209.57198157904887
}
#Debug simulation 
Total elapsed time: 114.3377222251147. Arrivals time: 0.4524567970074713 Scheduler time: 113.6832423065789 Scheduler overhead time: 0.07871101796627045 Adapter cache time: 0.01638009212911129 Engine time: 0.07769835740327835 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 111.4651660961099,
    "estimated_duration": 3600.029660408295,
    "input_throughput": 7159.468235346934,
    "output_throughput": 6306.729705506091,
    "total_throughput": 13466.197940853026,
    "itl": 112.98441043913213,
    "ttft": 1453495.498007612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9548721761070179,
    "arrivals": 208072,
    "finished_requests": 104151,
    "scheduler_time": 211.44122664734266
}
#Debug simulation 
Total elapsed time: 111.46534214401618. Arrivals time: 0.44153874926269054 Scheduler time: 110.82406467571855 Scheduler overhead time: 0.07766409497708082 Adapter cache time: 0.016094330698251724 Engine time: 0.07666881801560521 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 111.66096152970567,
    "estimated_duration": 3600.095675376939,
    "input_throughput": 7159.336952149574,
    "output_throughput": 6306.614058978527,
    "total_throughput": 13465.951011128102,
    "itl": 112.98603567604063,
    "ttft": 1453523.2812288483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0150233510159945,
    "arrivals": 208072,
    "finished_requests": 104151,
    "scheduler_time": 211.443205192358
}
#Debug simulation 
Total elapsed time: 111.66109722014517. Arrivals time: 0.4387906859628856 Scheduler time: 111.02103070309386 Scheduler overhead time: 0.07888104487210512 Adapter cache time: 0.01618779869750142 Engine time: 0.07691699545830488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 111.26278009591624,
    "estimated_duration": 3600.0149384776405,
    "input_throughput": 7163.0047210039265,
    "output_throughput": 6309.217152750176,
    "total_throughput": 13472.221873754102,
    "itl": 113.01869259516593,
    "ttft": 1454439.7713035769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0532040670141634,
    "arrivals": 208072,
    "finished_requests": 104201,
    "scheduler_time": 211.44953001403255
}
#Debug simulation 
Total elapsed time: 111.26291649090126. Arrivals time: 0.44102476770058274 Scheduler time: 110.62307768315077 Scheduler overhead time: 0.07747675850987434 Adapter cache time: 0.016928088385611773 Engine time: 0.07525311596691608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.81508595915511,
    "estimated_duration": 3600.0490432249517,
    "input_throughput": 7159.429688466462,
    "output_throughput": 6306.695749806011,
    "total_throughput": 13466.125438272473,
    "itl": 112.98489723329354,
    "ttft": 1453502.9450112665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9745724144903978,
    "arrivals": 208072,
    "finished_requests": 104151,
    "scheduler_time": 211.44147965065832
}
#Debug simulation 
Total elapsed time: 111.81522920681164. Arrivals time: 0.4439666997641325 Scheduler time: 111.17274690512568 Scheduler overhead time: 0.07688763923943043 Adapter cache time: 0.01636432856321335 Engine time: 0.07571511855348945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 111.18785968795419,
    "estimated_duration": 3600.0277532317004,
    "input_throughput": 7204.92695555356,
    "output_throughput": 6346.078854389768,
    "total_throughput": 13551.005809943328,
    "itl": 113.8256617940653,
    "ttft": 1455533.4734240691,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0294770045764798,
    "arrivals": 208072,
    "finished_requests": 104891,
    "scheduler_time": 209.56898293763092
}
#Debug simulation 
Total elapsed time: 111.18799723917618. Arrivals time: 0.4489688379690051 Scheduler time: 110.5385754564777 Scheduler overhead time: 0.07855262607336044 Adapter cache time: 0.016101634595543146 Engine time: 0.07709836028516293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 111.31555314967409,
    "estimated_duration": 3600.097527617035,
    "input_throughput": 7205.008420193905,
    "output_throughput": 6346.008635805573,
    "total_throughput": 13551.017055999479,
    "itl": 113.82244502459868,
    "ttft": 1455564.1438506912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9328956920094801,
    "arrivals": 208072,
    "finished_requests": 104894,
    "scheduler_time": 209.5783598278965
}
#Debug simulation 
Total elapsed time: 111.315694199875. Arrivals time: 0.44948132522404194 Scheduler time: 110.66688347188756 Scheduler overhead time: 0.07731008064001799 Adapter cache time: 0.017619139049202204 Engine time: 0.07517693657428026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 1080, 1080, 1080, 1080, 8640, 1080, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 8640, 1080, 66, 66, 1080, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 8640, 8640, 66, 1080, 8640, 66, 1080, 66, 1080, 66, 66, 1080, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 1080, 66, 66, 66, 66, 66, 8640, 66, 1080, 66, 8640, 8640, 66, 8640, 66, 1080, 1080, 8640, 66, 66, 66, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 66, 8640, 1080, 1080, 1080, 8640, 1080, 66, 1080, 8640, 8640, 1080, 8640, 1080, 66, 8640, 1080, 66, 1080, 1080, 66, 1080, 8640, 66, 1080, 8640, 66, 1080, 8640, 8640, 1080, 66, 8640, 1080, 66, 66, 1080, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 1080, 66, 8640, 8640, 1080, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 66, 8640, 1080, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 626304 . Total input tokens: 139671201 . Total output tokens: 125145705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 111.92513732379302,
    "estimated_duration": 3600.0406250322067,
    "input_throughput": 7204.901194626923,
    "output_throughput": 6346.056164239984,
    "total_throughput": 13550.957358866906,
    "itl": 113.82596413150877,
    "ttft": 1455538.7931147288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0421781370043806,
    "arrivals": 208072,
    "finished_requests": 104891,
    "scheduler_time": 209.56929127836693
}
#Debug simulation 
Total elapsed time: 111.92527463380247. Arrivals time: 0.45371281867846847 Scheduler time: 111.27281168149784 Scheduler overhead time: 0.07708451431244612 Adapter cache time: 0.01605361746624112 Engine time: 0.07637498341500759 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 116.97034679120407,
    "estimated_duration": 3600.064435593586,
    "input_throughput": 7181.0103575930725,
    "output_throughput": 6361.561969160662,
    "total_throughput": 13542.572326753734,
    "itl": 114.29269591252002,
    "ttft": 1435341.6077932187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8783599825086992,
    "arrivals": 207395,
    "finished_requests": 104633,
    "scheduler_time": 208.47067980294742
}
#Debug simulation 
Total elapsed time: 116.97048002807423. Arrivals time: 0.4482863429002464 Scheduler time: 116.32207044865936 Scheduler overhead time: 0.07882145047187805 Adapter cache time: 0.015955849550664425 Engine time: 0.0765407639555633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 116.88089259713888,
    "estimated_duration": 3600.0978246857812,
    "input_throughput": 7184.940037638461,
    "output_throughput": 6359.261640896716,
    "total_throughput": 13544.201678535177,
    "itl": 114.12560124992252,
    "ttft": 1441637.9642392502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9055983299389527,
    "arrivals": 207395,
    "finished_requests": 104638,
    "scheduler_time": 208.71777466351594
}
#Debug simulation 
Total elapsed time: 116.88107524393126. Arrivals time: 0.4569197203963995 Scheduler time: 116.22504406096414 Scheduler overhead time: 0.07841142872348428 Adapter cache time: 0.01604073168709874 Engine time: 0.07622264139354229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 116.96627890691161,
    "estimated_duration": 3600.1015910833517,
    "input_throughput": 7184.932520811501,
    "output_throughput": 6359.254987887909,
    "total_throughput": 13544.18750869941,
    "itl": 114.12569566669099,
    "ttft": 1441639.942371222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9074772839620755,
    "arrivals": 207395,
    "finished_requests": 104638,
    "scheduler_time": 208.71808026991442
}
#Debug simulation 
Total elapsed time: 116.96640757890418. Arrivals time: 0.4581671189516783 Scheduler time: 116.30881128693 Scheduler overhead time: 0.07777258800342679 Adapter cache time: 0.015833689365535975 Engine time: 0.076835370156914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 116.85311962803826,
    "estimated_duration": 3600.0596504864134,
    "input_throughput": 7185.016225079802,
    "output_throughput": 6359.329073035426,
    "total_throughput": 13544.345298115228,
    "itl": 114.12477552297469,
    "ttft": 1441623.5266566935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8696419419162001,
    "arrivals": 207395,
    "finished_requests": 104638,
    "scheduler_time": 208.7166855873478
}
#Debug simulation 
Total elapsed time: 116.85325411893427. Arrivals time: 0.4610603707842529 Scheduler time: 116.19273436954245 Scheduler overhead time: 0.07793371006846428 Adapter cache time: 0.015835946425795555 Engine time: 0.07674653036519885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 117.09165320592001,
    "estimated_duration": 3600.1121342077436,
    "input_throughput": 7184.911479345432,
    "output_throughput": 6359.236364463449,
    "total_throughput": 13544.147843808882,
    "itl": 114.12593164223554,
    "ttft": 1441643.6284284936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9185436171665833,
    "arrivals": 207395,
    "finished_requests": 104638,
    "scheduler_time": 208.71823569130305
}
#Debug simulation 
Total elapsed time: 117.09178436221555. Arrivals time: 0.46387889608740807 Scheduler time: 116.42486387817189 Scheduler overhead time: 0.08002544566988945 Adapter cache time: 0.016121952794492245 Engine time: 0.07752350065857172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 116.53388236789033,
    "estimated_duration": 3600.077315510295,
    "input_throughput": 7180.984666251697,
    "output_throughput": 6361.539209541597,
    "total_throughput": 13542.523875793295,
    "itl": 114.29163270732575,
    "ttft": 1435354.6678329085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8581444346369256,
    "arrivals": 207395,
    "finished_requests": 104633,
    "scheduler_time": 208.47505640648575
}
#Debug simulation 
Total elapsed time: 116.53401379706338. Arrivals time: 0.46766948606818914 Scheduler time: 115.8660776312463 Scheduler overhead time: 0.07739936513826251 Adapter cache time: 0.01584398653358221 Engine time: 0.0775814363732934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 1080, 1080, 1080, 1080, 8640, 1080, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 8640, 1080, 33, 33, 1080, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 8640, 8640, 33, 1080, 8640, 33, 1080, 33, 1080, 33, 33, 1080, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 1080, 33, 33, 33, 33, 33, 8640, 33, 1080, 33, 8640, 8640, 33, 8640, 33, 1080, 1080, 8640, 33, 33, 33, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 33, 8640, 1080, 1080, 1080, 8640, 1080, 33, 1080, 8640, 8640, 1080, 8640, 1080, 33, 8640, 1080, 33, 1080, 1080, 33, 1080, 8640, 33, 1080, 8640, 33, 1080, 8640, 8640, 1080, 33, 8640, 1080, 33, 33, 1080, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 1080, 33, 8640, 8640, 1080, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 33, 8640, 1080, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 624192 . Total input tokens: 139190293 . Total output tokens: 124726553
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 115.82372898887843,
    "estimated_duration": 3600.0081546638835,
    "input_throughput": 7180.847622944788,
    "output_throughput": 6361.460590118633,
    "total_throughput": 13542.308213063421,
    "itl": 114.29452753719686,
    "ttft": 1435340.229888859,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9600711491331508,
    "arrivals": 207395,
    "finished_requests": 104630,
    "scheduler_time": 208.4646880022688
}
#Debug simulation 
Total elapsed time: 115.82386160688475. Arrivals time: 0.4617924471385777 Scheduler time: 115.16311445645988 Scheduler overhead time: 0.07774947164580226 Adapter cache time: 0.016204102896153927 Engine time: 0.07640707679092884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 113.76782020414248,
    "estimated_duration": 3600.0011404045517,
    "input_throughput": 7224.959100173265,
    "output_throughput": 6359.351041046424,
    "total_throughput": 13584.31014121969,
    "itl": 113.327832156343,
    "ttft": 1413714.4056071732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8110292521421787,
    "arrivals": 200999,
    "finished_requests": 104673,
    "scheduler_time": 207.7050892450137
}
#Debug simulation 
Total elapsed time: 113.76796026993543. Arrivals time: 0.45368221681565046 Scheduler time: 113.11592832067981 Scheduler overhead time: 0.07750558108091354 Adapter cache time: 0.016025495249778032 Engine time: 0.07580414228141308 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 116.05678781820461,
    "estimated_duration": 3600.1088438224742,
    "input_throughput": 7082.654749106623,
    "output_throughput": 6226.278696674129,
    "total_throughput": 13308.93344578075,
    "itl": 109.58491073439826,
    "ttft": 1437366.461689173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8324625077191778,
    "arrivals": 200999,
    "finished_requests": 102499,
    "scheduler_time": 214.2566555881893
}
#Debug simulation 
Total elapsed time: 116.05692339781672. Arrivals time: 0.4582657292485237 Scheduler time: 115.39389523677528 Scheduler overhead time: 0.08013132680207491 Adapter cache time: 0.01640886440873146 Engine time: 0.07883802289143205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 115.97049727663398,
    "estimated_duration": 3600.11074791466,
    "input_throughput": 7082.651003103095,
    "output_throughput": 6226.275403606375,
    "total_throughput": 13308.92640670947,
    "itl": 109.584959037871,
    "ttft": 1437367.3686203687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8344494143873493,
    "arrivals": 200999,
    "finished_requests": 102499,
    "scheduler_time": 214.25669404524984
}
#Debug simulation 
Total elapsed time: 115.97068121889606. Arrivals time: 0.45332686603069305 Scheduler time: 115.31276453565806 Scheduler overhead time: 0.07996586244553328 Adapter cache time: 0.016113367397338152 Engine time: 0.07826388766989112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 113.06702092615888,
    "estimated_duration": 3600.0254843903563,
    "input_throughput": 7226.370511209176,
    "output_throughput": 6359.25276064452,
    "total_throughput": 13585.623271853696,
    "itl": 113.46400431094126,
    "ttft": 1414387.3239558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8241038799216064,
    "arrivals": 200999,
    "finished_requests": 104677,
    "scheduler_time": 207.62278078039282
}
#Debug simulation 
Total elapsed time: 113.06716116610914. Arrivals time: 0.4464726196601987 Scheduler time: 112.42178177135065 Scheduler overhead time: 0.07799591915681958 Adapter cache time: 0.01564142946153879 Engine time: 0.0766735034994781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_192_slots_32_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 116.12853525904939,
    "estimated_duration": 3600.119757019626,
    "input_throughput": 7082.633279152052,
    "output_throughput": 6226.2598226889495,
    "total_throughput": 13308.893101841,
    "itl": 109.58511351018511,
    "ttft": 1437370.4631035072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8445097173005381,
    "arrivals": 200999,
    "finished_requests": 102499,
    "scheduler_time": 214.25688700266406
}
#Debug simulation 
Total elapsed time: 116.12867677211761. Arrivals time: 0.4486271971836686 Scheduler time: 115.47546421829611 Scheduler overhead time: 0.08005614439025521 Adapter cache time: 0.01630583591759205 Engine time: 0.0778765557333827 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_192_slots_32_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_192_slots_32_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 114.06473768968135,
    "estimated_duration": 3600.1183650542034,
    "input_throughput": 7224.919395006721,
    "output_throughput": 6359.148971940903,
    "total_throughput": 13584.068366947624,
    "itl": 113.32677227094636,
    "ttft": 1413718.6726060011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7923633281490776,
    "arrivals": 200999,
    "finished_requests": 104676,
    "scheduler_time": 207.71253695840977
}
#Debug simulation 
Total elapsed time: 114.06487765163183. Arrivals time: 0.4466300425119698 Scheduler time: 113.41939555574208 Scheduler overhead time: 0.07833742117509246 Adapter cache time: 0.015973231755197048 Engine time: 0.07556526269763708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_192_slots_32_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_192_slots_32_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 540, 540, 540, 540, 8640, 540, 270, 8640, 270, 540, 540, 270, 8640, 8640, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 270, 540, 540, 8640, 540, 270, 270, 540, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 8640, 8640, 270, 540, 8640, 270, 540, 270, 540, 270, 270, 540, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 540, 270, 270, 270, 270, 270, 8640, 270, 540, 270, 8640, 8640, 270, 8640, 270, 540, 540, 8640, 270, 270, 270, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 270, 8640, 540, 540, 540, 8640, 540, 270, 540, 8640, 8640, 540, 8640, 540, 270, 8640, 540, 270, 540, 540, 270, 540, 8640, 270, 540, 8640, 270, 540, 8640, 8640, 540, 270, 8640, 540, 270, 270, 540, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 540, 270, 8640, 8640, 540, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 270, 8640, 540, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 604800 . Total input tokens: 134832327 . Total output tokens: 120889141
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 116.77752164797857,
    "estimated_duration": 3600.1407483274547,
    "input_throughput": 7094.781783702858,
    "output_throughput": 6237.515855576627,
    "total_throughput": 13332.297639279484,
    "itl": 109.81448019748528,
    "ttft": 1434279.1286078375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8349599171429908,
    "arrivals": 200999,
    "finished_requests": 102712,
    "scheduler_time": 213.7822537074706
}
#Debug simulation 
Total elapsed time: 116.77765697520226. Arrivals time: 0.44579112622886896 Scheduler time: 116.12697272654623 Scheduler overhead time: 0.08057185728102922 Adapter cache time: 0.016199840232729912 Engine time: 0.0786624294705689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 112.56659657787532,
    "estimated_duration": 3600.108763875635,
    "input_throughput": 7065.795693521091,
    "output_throughput": 6248.702046374375,
    "total_throughput": 13314.497739895465,
    "itl": 111.05889792096737,
    "ttft": 1424891.5992405307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7681824237271202,
    "arrivals": 198139,
    "finished_requests": 102921,
    "scheduler_time": 212.48604371669168
}
#Debug simulation 
Total elapsed time: 112.56673257285729. Arrivals time: 0.4384110779501498 Scheduler time: 111.92652063956484 Scheduler overhead time: 0.07890212954953313 Adapter cache time: 0.015970814041793346 Engine time: 0.07737979665398598 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 122.88186779431999,
    "estimated_duration": 3600.0738661605365,
    "input_throughput": 7080.006396419701,
    "output_throughput": 6257.246611449245,
    "total_throughput": 13337.253007868945,
    "itl": 111.27186320958907,
    "ttft": 1410629.9660921132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9012526106229097,
    "arrivals": 198139,
    "finished_requests": 103133,
    "scheduler_time": 211.98429515240255
}
#Debug simulation 
Total elapsed time: 122.88201084313914. Arrivals time: 0.4523362987674773 Scheduler time: 122.21982042724267 Scheduler overhead time: 0.08279470726847649 Adapter cache time: 0.016936543863266706 Engine time: 0.07996653858572245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 122.79618823993951,
    "estimated_duration": 3600.0754757771724,
    "input_throughput": 7080.003230903823,
    "output_throughput": 6257.243813794499,
    "total_throughput": 13337.247044698322,
    "itl": 111.27196409659456,
    "ttft": 1410630.5630987291,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9027392362430737,
    "arrivals": 198139,
    "finished_requests": 103133,
    "scheduler_time": 211.98430503836212
}
#Debug simulation 
Total elapsed time: 122.79631885001436. Arrivals time: 0.45517392130568624 Scheduler time: 122.13253007316962 Scheduler overhead time: 0.0815742346458137 Adapter cache time: 0.016981455963104963 Engine time: 0.08013842394575477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 117.96920766495168,
    "estimated_duration": 3600.0175997931888,
    "input_throughput": 7066.18462128112,
    "output_throughput": 6244.047807236092,
    "total_throughput": 13310.232428517213,
    "itl": 110.97823335207262,
    "ttft": 1416353.7816065394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 249,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7780273395427525,
    "arrivals": 198139,
    "finished_requests": 102904,
    "scheduler_time": 212.74937647680778
}
#Debug simulation 
Total elapsed time: 117.96941256755963. Arrivals time: 0.45296571077778935 Scheduler time: 117.31076624849811 Scheduler overhead time: 0.07995481882244349 Adapter cache time: 0.016558648087084293 Engine time: 0.07899840734899044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 123.08184736268595,
    "estimated_duration": 3600.0894485192043,
    "input_throughput": 7079.975751848054,
    "output_throughput": 6257.219528049689,
    "total_throughput": 13337.195279897744,
    "itl": 111.27213910322479,
    "ttft": 1410635.9390428588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9146858459524856,
    "arrivals": 198139,
    "finished_requests": 103133,
    "scheduler_time": 211.98474770027033
}
#Debug simulation 
Total elapsed time: 123.08198060467839. Arrivals time: 0.4621251239441335 Scheduler time: 122.41011938313022 Scheduler overhead time: 0.08233658829703927 Adapter cache time: 0.017215692903846502 Engine time: 0.07975912373512983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 110.41818252019584,
    "estimated_duration": 3600.0490953755398,
    "input_throughput": 7068.4827694955375,
    "output_throughput": 6251.359746429337,
    "total_throughput": 13319.842515924875,
    "itl": 110.8436130191101,
    "ttft": 1434211.4978137158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7534926743153493,
    "arrivals": 198139,
    "finished_requests": 102877,
    "scheduler_time": 212.33534395831845
}
#Debug simulation 
Total elapsed time: 110.41832966916263. Arrivals time: 0.44676798954606056 Scheduler time: 109.76981471665204 Scheduler overhead time: 0.07863001897931099 Adapter cache time: 0.016157651785761118 Engine time: 0.07668679999187589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 540, 540, 540, 540, 8640, 540, 135, 8640, 135, 540, 540, 135, 8640, 8640, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 135, 540, 540, 8640, 540, 135, 135, 540, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 8640, 8640, 135, 540, 8640, 135, 540, 135, 540, 135, 135, 540, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 540, 135, 135, 135, 135, 135, 8640, 135, 540, 135, 8640, 8640, 135, 8640, 135, 540, 540, 8640, 135, 135, 135, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 135, 8640, 540, 540, 540, 8640, 540, 135, 540, 8640, 8640, 540, 8640, 540, 135, 8640, 540, 135, 540, 540, 135, 540, 8640, 135, 540, 8640, 135, 540, 8640, 8640, 540, 135, 8640, 540, 135, 135, 540, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 540, 135, 8640, 8640, 540, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 135, 8640, 540, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 596160 . Total input tokens: 132926146 . Total output tokens: 119153540
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 122.84350441582501,
    "estimated_duration": 3600.1002818462343,
    "input_throughput": 7079.954446971334,
    "output_throughput": 6257.200698989346,
    "total_throughput": 13337.15514596068,
    "itl": 111.2722819276051,
    "ttft": 1410639.9118266846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.926129440516238,
    "arrivals": 198139,
    "finished_requests": 103133,
    "scheduler_time": 211.9851553780375
}
#Debug simulation 
Total elapsed time: 122.84363303985447. Arrivals time: 0.46357921371236444 Scheduler time: 122.17175353923813 Scheduler overhead time: 0.08167708665132523 Adapter cache time: 0.016709312796592712 Engine time: 0.07974091172218323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 102.9517466085963,
    "estimated_duration": 3600.1268620547103,
    "input_throughput": 7022.702245989849,
    "output_throughput": 6257.736980730219,
    "total_throughput": 13280.439226720067,
    "itl": 111.11613509892005,
    "ttft": 1439700.9385540152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 196768,
    "finished_requests": 102131,
    "scheduler_time": 212.83745310334362
}
#Debug simulation 
Total elapsed time: 102.95189798483625. Arrivals time: 0.46095767384395003 Scheduler time: 102.29555730288848 Scheduler overhead time: 0.07619189284741879 Adapter cache time: 0.01558667328208685 Engine time: 0.07463882816955447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 106.71660422999412,
    "estimated_duration": 3600.122018076535,
    "input_throughput": 6998.392519334987,
    "output_throughput": 6242.431752912392,
    "total_throughput": 13240.824272247379,
    "itl": 110.87283868396786,
    "ttft": 1435411.471173431,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8059606503834983,
    "arrivals": 196768,
    "finished_requests": 101794,
    "scheduler_time": 213.60885537216586
}
#Debug simulation 
Total elapsed time: 106.71674440382048. Arrivals time: 0.4546347316354513 Scheduler time: 106.06168497959152 Scheduler overhead time: 0.0780172785744071 Adapter cache time: 0.015923652797937393 Engine time: 0.07654162682592869 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 106.90094961272553,
    "estimated_duration": 3600.1233434826945,
    "input_throughput": 6998.389942836444,
    "output_throughput": 6242.429454725161,
    "total_throughput": 13240.819397561605,
    "itl": 110.87284532093511,
    "ttft": 1435411.9529669913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8073949922435024,
    "arrivals": 196768,
    "finished_requests": 101794,
    "scheduler_time": 213.60885954147722
}
#Debug simulation 
Total elapsed time: 106.9010929199867. Arrivals time: 0.4719455842860043 Scheduler time: 106.22953079314902 Scheduler overhead time: 0.07803953904658556 Adapter cache time: 0.01587364776059985 Engine time: 0.07596887601539493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 107.26437205215916,
    "estimated_duration": 3600.084424283027,
    "input_throughput": 7015.698251307,
    "output_throughput": 6258.786279571035,
    "total_throughput": 13274.484530878035,
    "itl": 110.69754513424505,
    "ttft": 1439355.100448958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7822431757929755,
    "arrivals": 196768,
    "finished_requests": 102075,
    "scheduler_time": 212.88489410676166
}
#Debug simulation 
Total elapsed time: 107.26451322622597. Arrivals time: 0.4458855972625315 Scheduler time: 106.61955728102475 Scheduler overhead time: 0.07811099197715521 Adapter cache time: 0.015703936107456684 Engine time: 0.07591910194605589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 106.88893770985305,
    "estimated_duration": 3600.133903177033,
    "input_throughput": 6998.369415583667,
    "output_throughput": 6242.411144809823,
    "total_throughput": 13240.78056039349,
    "itl": 110.87300574605494,
    "ttft": 1435415.9548477973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.817706802729521,
    "arrivals": 196768,
    "finished_requests": 101794,
    "scheduler_time": 213.60910742534745
}
#Debug simulation 
Total elapsed time: 106.88912368984893. Arrivals time: 0.4448474692180753 Scheduler time: 106.24658843036741 Scheduler overhead time: 0.07727251062169671 Adapter cache time: 0.015623945277184248 Engine time: 0.07562559796497226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 103.21970376884565,
    "estimated_duration": 3600.1072127358875,
    "input_throughput": 7022.7405757692895,
    "output_throughput": 6257.77113534334,
    "total_throughput": 13280.51171111263,
    "itl": 111.1157743521513,
    "ttft": 1439693.2000055045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 196768,
    "finished_requests": 102131,
    "scheduler_time": 212.83670542758654
}
#Debug simulation 
Total elapsed time: 103.2198371719569. Arrivals time: 0.438042838126421 Scheduler time: 102.58539304835722 Scheduler overhead time: 0.0764607172459364 Adapter cache time: 0.0156742874532938 Engine time: 0.07522695465013385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 540, 540, 540, 540, 8640, 540, 66, 8640, 66, 540, 540, 66, 8640, 8640, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 66, 540, 540, 8640, 540, 66, 66, 540, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 8640, 8640, 66, 540, 8640, 66, 540, 66, 540, 66, 66, 540, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 540, 66, 66, 66, 66, 66, 8640, 66, 540, 66, 8640, 8640, 66, 8640, 66, 540, 540, 8640, 66, 66, 66, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 66, 8640, 540, 540, 540, 8640, 540, 66, 540, 8640, 8640, 540, 8640, 540, 66, 8640, 540, 66, 540, 540, 66, 540, 8640, 66, 540, 8640, 66, 540, 8640, 8640, 540, 66, 8640, 540, 66, 66, 540, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 540, 66, 8640, 8640, 540, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 66, 8640, 540, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 591744 . Total input tokens: 131941895 . Total output tokens: 118285492
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 107.839674372226,
    "estimated_duration": 3600.0020408205914,
    "input_throughput": 6998.537977010997,
    "output_throughput": 6242.57646111706,
    "total_throughput": 13241.114438128056,
    "itl": 110.87230108948798,
    "ttft": 1435420.564349512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8281443670019543,
    "arrivals": 196768,
    "finished_requests": 101791,
    "scheduler_time": 213.60135246006044
}
#Debug simulation 
Total elapsed time: 107.83981347316876. Arrivals time: 0.44755861535668373 Scheduler time: 107.19106721971184 Scheduler overhead time: 0.07833789708092809 Adapter cache time: 0.01612618798390031 Engine time: 0.07711740583181381 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 108.68109265435487,
    "estimated_duration": 3600.0127920959994,
    "input_throughput": 7067.632108380994,
    "output_throughput": 6203.689067170529,
    "total_throughput": 13271.321175551522,
    "itl": 109.51987420735335,
    "ttft": 1430097.1286731937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.759000960495322,
    "arrivals": 196098,
    "finished_requests": 102489,
    "scheduler_time": 213.5600432845394
}
#Debug simulation 
Total elapsed time: 108.68122825399041. Arrivals time: 0.4436453743837774 Scheduler time: 108.03567828377709 Scheduler overhead time: 0.07801298703998327 Adapter cache time: 0.016207530163228512 Engine time: 0.07762583158910275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 107.72400881489739,
    "estimated_duration": 3600.0732832503304,
    "input_throughput": 7070.797730266638,
    "output_throughput": 6207.359751250405,
    "total_throughput": 13278.157481517042,
    "itl": 109.74871379592359,
    "ttft": 1431359.4594916066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 250,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.814930801268205,
    "arrivals": 196098,
    "finished_requests": 102510,
    "scheduler_time": 213.37444251792076
}
#Debug simulation 
Total elapsed time: 107.7241414678283. Arrivals time: 0.4450916242785752 Scheduler time: 107.07532624201849 Scheduler overhead time: 0.08022907469421625 Adapter cache time: 0.016255964525043964 Engine time: 0.07708998257294297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 109.88052514009178,
    "estimated_duration": 3600.1358411978867,
    "input_throughput": 7044.028647419607,
    "output_throughput": 6184.566633627799,
    "total_throughput": 13228.595281047406,
    "itl": 109.45722402645785,
    "ttft": 1430483.121142164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8036769784055688,
    "arrivals": 196098,
    "finished_requests": 102091,
    "scheduler_time": 214.5321267179886
}
#Debug simulation 
Total elapsed time: 109.88066059723496. Arrivals time: 0.4500462431460619 Scheduler time: 109.22517457325011 Scheduler overhead time: 0.08107251254841685 Adapter cache time: 0.016398989129811525 Engine time: 0.07849220605567098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 108.6087757521309,
    "estimated_duration": 3600.032518344703,
    "input_throughput": 7067.593381545055,
    "output_throughput": 6203.65507427941,
    "total_throughput": 13271.248455824465,
    "itl": 109.5203111580924,
    "ttft": 1430105.1192608543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7746286939294098,
    "arrivals": 196098,
    "finished_requests": 102489,
    "scheduler_time": 213.5605960735975
}
#Debug simulation 
Total elapsed time: 108.60891264211386. Arrivals time: 0.44834507815539837 Scheduler time: 107.958599855192 Scheduler overhead time: 0.0789640280418098 Adapter cache time: 0.01614115759730339 Engine time: 0.07716729631647468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 108.47905265819281,
    "estimated_duration": 3600.076465689061,
    "input_throughput": 7045.814788032566,
    "output_throughput": 6186.447763613588,
    "total_throughput": 13232.262551646154,
    "itl": 109.59205009812153,
    "ttft": 1433233.3216671736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8172838153131345,
    "arrivals": 196098,
    "finished_requests": 102211,
    "scheduler_time": 214.2203201537094
}
#Debug simulation 
Total elapsed time: 108.47918574418873. Arrivals time: 0.4469851632602513 Scheduler time: 107.8262864309363 Scheduler overhead time: 0.0805586026981473 Adapter cache time: 0.016163702588528395 Engine time: 0.0792499678209424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 108.88107820693403,
    "estimated_duration": 3600.1342240452423,
    "input_throughput": 7067.6514864519795,
    "output_throughput": 6203.597868888606,
    "total_throughput": 13271.249355340586,
    "itl": 109.52001716401256,
    "ttft": 1430090.9657564235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7415324731357406,
    "arrivals": 196098,
    "finished_requests": 102493,
    "scheduler_time": 213.56786562351502
}
#Debug simulation 
Total elapsed time: 108.88126546703279. Arrivals time: 0.44100900180637836 Scheduler time: 108.23774827364832 Scheduler overhead time: 0.0790009736083448 Adapter cache time: 0.015746747609227896 Engine time: 0.07811012677848339 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 540, 540, 540, 540, 8640, 540, 33, 8640, 33, 540, 540, 33, 8640, 8640, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 33, 540, 540, 8640, 540, 33, 33, 540, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 8640, 8640, 33, 540, 8640, 33, 540, 33, 540, 33, 33, 540, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 540, 33, 33, 33, 33, 33, 8640, 33, 540, 33, 8640, 8640, 33, 8640, 33, 540, 540, 8640, 33, 33, 33, 8640, 8640, 540, 8640, 540, 8640, 8640, 540, 540, 33, 8640, 540, 540, 540, 8640, 540, 33, 540, 8640, 8640, 540, 8640, 540, 33, 8640, 540, 33, 540, 540, 33, 540, 8640, 33, 540, 8640, 33, 540, 8640, 8640, 540, 33, 8640, 540, 33, 33, 540, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 540, 33, 8640, 8640, 540, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 540, 540, 540, 540, 540, 540, 33, 8640, 540, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 589632 . Total input tokens: 131478915 . Total output tokens: 117864886
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 108.34431594191119,
    "estimated_duration": 3600.0861578452445,
    "input_throughput": 7045.795819281716,
    "output_throughput": 6186.43110845165,
    "total_throughput": 13232.226927733367,
    "itl": 109.59218938200526,
    "ttft": 1433236.7836803356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8272183644399083,
    "arrivals": 196098,
    "finished_requests": 102211,
    "scheduler_time": 214.22056944827276
}
#Debug simulation 
Total elapsed time: 108.34444528678432. Arrivals time: 0.46248129894956946 Scheduler time: 107.67672198917717 Scheduler overhead time: 0.07974792364984751 Adapter cache time: 0.015863953158259392 Engine time: 0.0793648473918438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 114.10610871715471,
    "estimated_duration": 3600.0249974464614,
    "input_throughput": 7168.771888613478,
    "output_throughput": 6286.56495886917,
    "total_throughput": 13455.33684748265,
    "itl": 112.00054484744182,
    "ttft": 1398333.191126808,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7926663256785822,
    "arrivals": 192525,
    "finished_requests": 103478,
    "scheduler_time": 209.7726605712562
}
#Debug simulation 
Total elapsed time: 114.10625670896843. Arrivals time: 0.44579202588647604 Scheduler time: 113.45932835061103 Scheduler overhead time: 0.07899589510634542 Adapter cache time: 0.015683053992688656 Engine time: 0.07716262992471457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 117.97585538821295,
    "estimated_duration": 3600.113658290251,
    "input_throughput": 7187.292251291996,
    "output_throughput": 6309.396912426228,
    "total_throughput": 13496.689163718223,
    "itl": 112.70327193666327,
    "ttft": 1387166.7045684373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9132128118025186,
    "arrivals": 192525,
    "finished_requests": 103776,
    "scheduler_time": 208.57715020601668
}
#Debug simulation 
Total elapsed time: 117.97599653108045. Arrivals time: 0.4516155179589987 Scheduler time: 117.31960110226646 Scheduler overhead time: 0.08010682929307222 Adapter cache time: 0.016589412465691566 Engine time: 0.07840573042631149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 118.23040917236358,
    "estimated_duration": 3600.116761705865,
    "input_throughput": 7187.286055616558,
    "output_throughput": 6309.391473524606,
    "total_throughput": 13496.677529141163,
    "itl": 112.70334010888013,
    "ttft": 1387168.303955375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9149133116379427,
    "arrivals": 192525,
    "finished_requests": 103776,
    "scheduler_time": 208.57730896643295
}
#Debug simulation 
Total elapsed time: 118.23054305417463. Arrivals time: 0.4513497049920261 Scheduler time: 117.57561403419822 Scheduler overhead time: 0.07959483005106449 Adapter cache time: 0.016434086952358484 Engine time: 0.07840373367071152 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 113.99786739703268,
    "estimated_duration": 3600.120697298436,
    "input_throughput": 7168.309112348829,
    "output_throughput": 6286.756168198493,
    "total_throughput": 13455.065280547322,
    "itl": 112.0185124659423,
    "ttft": 1398307.125057108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8069807687890739,
    "arrivals": 192525,
    "finished_requests": 103481,
    "scheduler_time": 209.78615886339225
}
#Debug simulation 
Total elapsed time: 113.99801025586203. Arrivals time: 0.44237994123250246 Scheduler time: 113.35255762189627 Scheduler overhead time: 0.07970802579075098 Adapter cache time: 0.016296475660055876 Engine time: 0.07742990180850029 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 117.66340352222323,
    "estimated_duration": 3600.1307784990263,
    "input_throughput": 7187.258072549211,
    "output_throughput": 6309.366908462752,
    "total_throughput": 13496.624981011963,
    "itl": 112.70355701407291,
    "ttft": 1387174.1810211781,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9264826599881101,
    "arrivals": 192525,
    "finished_requests": 103776,
    "scheduler_time": 208.57783015626998
}
#Debug simulation 
Total elapsed time: 117.66354354610667. Arrivals time: 0.4405555110424757 Scheduler time: 117.01745408540592 Scheduler overhead time: 0.08040053537115455 Adapter cache time: 0.016802465543150902 Engine time: 0.07849867036566138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 114.54542913706973,
    "estimated_duration": 3600.1154725720808,
    "input_throughput": 7168.48839894629,
    "output_throughput": 6286.740014989018,
    "total_throughput": 13455.228413935309,
    "itl": 112.0186129256984,
    "ttft": 1398315.2949112286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 258,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7714329760847624,
    "arrivals": 192525,
    "finished_requests": 103482,
    "scheduler_time": 209.7863894581767
}
#Debug simulation 
Total elapsed time: 114.54556730901822. Arrivals time: 0.4412887142971158 Scheduler time: 113.90084542427212 Scheduler overhead time: 0.07958340970799327 Adapter cache time: 0.016499620862305164 Engine time: 0.07769504794850945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 270, 270, 270, 270, 8640, 270, 135, 8640, 135, 270, 270, 135, 8640, 8640, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 135, 270, 270, 8640, 270, 135, 135, 270, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 8640, 8640, 135, 270, 8640, 135, 270, 135, 270, 135, 135, 270, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 270, 135, 135, 135, 135, 135, 8640, 135, 270, 135, 8640, 8640, 135, 8640, 135, 270, 270, 8640, 135, 135, 135, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 135, 8640, 270, 270, 270, 8640, 270, 135, 270, 8640, 8640, 270, 8640, 270, 135, 8640, 270, 135, 270, 270, 135, 270, 8640, 135, 270, 8640, 135, 270, 8640, 8640, 270, 135, 8640, 270, 135, 135, 270, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 270, 135, 8640, 8640, 270, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 135, 8640, 270, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 578880 . Total input tokens: 129058715 . Total output tokens: 115715441
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 117.5439488668926,
    "estimated_duration": 3600.1412532553773,
    "input_throughput": 7187.237160931625,
    "output_throughput": 6309.348551104958,
    "total_throughput": 13496.585712036582,
    "itl": 112.7037658284556,
    "ttft": 1387178.2740157773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 280,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9383035159111071,
    "arrivals": 192525,
    "finished_requests": 103776,
    "scheduler_time": 208.57795442212037
}
#Debug simulation 
Total elapsed time: 117.54416171787307. Arrivals time: 0.44338279869407415 Scheduler time: 116.89583706716076 Scheduler overhead time: 0.08011424215510488 Adapter cache time: 0.016864039469510317 Engine time: 0.07841908419504762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 101.23506607115269,
    "estimated_duration": 3600.053860048171,
    "input_throughput": 7081.958212608201,
    "output_throughput": 6267.016516163487,
    "total_throughput": 13348.974728771687,
    "itl": 111.76433087443735,
    "ttft": 1411722.5880902798,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7773638869589184,
    "arrivals": 191098,
    "finished_requests": 103335,
    "scheduler_time": 210.18724231853733
}
#Debug simulation 
Total elapsed time: 101.23521258728579. Arrivals time: 0.42175937350839376 Scheduler time: 100.61934437835589 Scheduler overhead time: 0.07543765893206 Adapter cache time: 0.015806146897375584 Engine time: 0.07430735323578119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 101.78963791579008,
    "estimated_duration": 3600.1379379760665,
    "input_throughput": 7029.908696839562,
    "output_throughput": 6224.041796742113,
    "total_throughput": 13253.950493581675,
    "itl": 111.1727129192183,
    "ttft": 1415527.3000047635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8029706000885963,
    "arrivals": 191098,
    "finished_requests": 102548,
    "scheduler_time": 212.50010790524254
}
#Debug simulation 
Total elapsed time: 101.78979159193113. Arrivals time: 0.4238962111994624 Scheduler time: 101.16928915120661 Scheduler overhead time: 0.0768542904406786 Adapter cache time: 0.015429281629621983 Engine time: 0.07512836856767535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 101.94801020482555,
    "estimated_duration": 3600.1391066365677,
    "input_throughput": 7029.906414823125,
    "output_throughput": 6224.039776322459,
    "total_throughput": 13253.946191145584,
    "itl": 111.17271829381562,
    "ttft": 1415527.7078999837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8043514733947852,
    "arrivals": 191098,
    "finished_requests": 102548,
    "scheduler_time": 212.50012190248083
}
#Debug simulation 
Total elapsed time: 101.94815425667912. Arrivals time: 0.4253389840014279 Scheduler time: 101.32608523219824 Scheduler overhead time: 0.07674416434019804 Adapter cache time: 0.015601265709847212 Engine time: 0.07529493793845177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 101.20605659205467,
    "estimated_duration": 3600.077337050437,
    "input_throughput": 7081.912029392275,
    "output_throughput": 6266.9756473856305,
    "total_throughput": 13348.887676777906,
    "itl": 111.76489527135783,
    "ttft": 1411732.4641932796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7962463535647866,
    "arrivals": 191098,
    "finished_requests": 103335,
    "scheduler_time": 210.18861873906712
}
#Debug simulation 
Total elapsed time: 101.20619793981314. Arrivals time: 0.4261202742345631 Scheduler time: 100.58530236687511 Scheduler overhead time: 0.07547989673912525 Adapter cache time: 0.01528683677315712 Engine time: 0.07435812894254923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 102.41639044322073,
    "estimated_duration": 3600.1178859897886,
    "input_throughput": 7033.997163967265,
    "output_throughput": 6222.192358526433,
    "total_throughput": 13256.189522493698,
    "itl": 110.90998007889743,
    "ttft": 1417921.5872768986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.804480970986191,
    "arrivals": 191098,
    "finished_requests": 102594,
    "scheduler_time": 212.58652275028768
}
#Debug simulation 
Total elapsed time: 102.41653018817306. Arrivals time: 0.4286980745382607 Scheduler time: 101.79173591639847 Scheduler overhead time: 0.07620832370594144 Adapter cache time: 0.015474491287022829 Engine time: 0.07504141889512539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 101.20981425512582,
    "estimated_duration": 3600.035752882411,
    "input_throughput": 7081.993832863127,
    "output_throughput": 6267.048037491237,
    "total_throughput": 13349.041870354364,
    "itl": 111.76400174805528,
    "ttft": 1411715.2838888655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7594727749051536,
    "arrivals": 191098,
    "finished_requests": 103335,
    "scheduler_time": 210.18678372166772
}
#Debug simulation 
Total elapsed time: 101.20996106415987. Arrivals time: 0.4301678640767932 Scheduler time: 100.58352322736755 Scheduler overhead time: 0.07646520715206861 Adapter cache time: 0.015560892410576344 Engine time: 0.07516917772591114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 270, 270, 270, 270, 8640, 270, 66, 8640, 66, 270, 270, 66, 8640, 8640, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 66, 270, 270, 8640, 270, 66, 66, 270, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 8640, 8640, 66, 270, 8640, 66, 270, 66, 270, 66, 66, 270, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 270, 66, 66, 66, 66, 66, 8640, 66, 270, 66, 8640, 8640, 66, 8640, 66, 270, 270, 8640, 66, 66, 66, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 66, 8640, 270, 270, 270, 8640, 270, 66, 270, 8640, 8640, 270, 8640, 270, 66, 8640, 270, 66, 270, 270, 66, 270, 8640, 66, 270, 8640, 66, 270, 8640, 8640, 270, 66, 8640, 270, 66, 66, 270, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 270, 66, 8640, 8640, 270, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 66, 8640, 270, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 574464 . Total input tokens: 128090051 . Total output tokens: 114820790
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 102.92660618107766,
    "estimated_duration": 3600.127961210834,
    "input_throughput": 7033.977478812453,
    "output_throughput": 6222.174945266662,
    "total_throughput": 13256.152424079115,
    "itl": 110.91019238901005,
    "ttft": 1417925.1296835134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 243,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8150442890450393,
    "arrivals": 191098,
    "finished_requests": 102594,
    "scheduler_time": 212.58687545570658
}
#Debug simulation 
Total elapsed time: 102.92674430925399. Arrivals time: 0.42932309256866574 Scheduler time: 102.30225694226101 Scheduler overhead time: 0.07544183870777488 Adapter cache time: 0.015392991248518229 Engine time: 0.07537004724144936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 110.18905928405002,
    "estimated_duration": 3600.111287983888,
    "input_throughput": 6972.360016697445,
    "output_throughput": 6228.963830881542,
    "total_throughput": 13201.323847578986,
    "itl": 111.35075605551319,
    "ttft": 1397749.8008999731,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.685549254640936,
    "arrivals": 190328,
    "finished_requests": 101904,
    "scheduler_time": 212.20144214807297
}
#Debug simulation 
Total elapsed time: 110.18924076296389. Arrivals time: 0.4476409559138119 Scheduler time: 109.54179850546643 Scheduler overhead time: 0.07785551762208343 Adapter cache time: 0.015760939102619886 Engine time: 0.07683119364082813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 103.42847242858261,
    "estimated_duration": 3600.06762109258,
    "input_throughput": 6978.782246422116,
    "output_throughput": 6236.661186154595,
    "total_throughput": 13215.443432576712,
    "itl": 111.44980785222734,
    "ttft": 1409269.351104158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7404393100272894,
    "arrivals": 190328,
    "finished_requests": 101996,
    "scheduler_time": 211.7915704882541
}
#Debug simulation 
Total elapsed time: 103.42861239379272. Arrivals time: 0.441213998477906 Scheduler time: 102.7889867448248 Scheduler overhead time: 0.07748403493314981 Adapter cache time: 0.015519773121923208 Engine time: 0.07595109567046165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 103.3198405331932,
    "estimated_duration": 3600.0689019173055,
    "input_throughput": 6978.779763526067,
    "output_throughput": 6236.65896728877,
    "total_throughput": 13215.438730814836,
    "itl": 111.44982492943946,
    "ttft": 1409269.8233308871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7418031503446434,
    "arrivals": 190328,
    "finished_requests": 101996,
    "scheduler_time": 211.79160057767555
}
#Debug simulation 
Total elapsed time: 103.31997399311513. Arrivals time: 0.4403262119740248 Scheduler time: 102.68349045934156 Scheduler overhead time: 0.076924251858145 Adapter cache time: 0.015131024643778801 Engine time: 0.07494733715429902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 110.2076426059939,
    "estimated_duration": 3600.1244042866324,
    "input_throughput": 6972.334614357261,
    "output_throughput": 6228.941136950384,
    "total_throughput": 13201.275751307645,
    "itl": 111.35096522738002,
    "ttft": 1397754.1921522014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.699190128985794,
    "arrivals": 190328,
    "finished_requests": 101904,
    "scheduler_time": 212.20171094497042
}
#Debug simulation 
Total elapsed time: 110.20777230989188. Arrivals time: 0.4431716972030699 Scheduler time: 109.56130006955937 Scheduler overhead time: 0.0799062647856772 Adapter cache time: 0.015597926452755928 Engine time: 0.07840082189068198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 103.34540949529037,
    "estimated_duration": 3600.0797961915837,
    "input_throughput": 6978.758644899488,
    "output_throughput": 6236.640094408941,
    "total_throughput": 13215.398739308428,
    "itl": 111.4499536110539,
    "ttft": 1409273.852373413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7514861918985875,
    "arrivals": 190328,
    "finished_requests": 101996,
    "scheduler_time": 211.79190942012744
}
#Debug simulation 
Total elapsed time: 103.34554299293086. Arrivals time: 0.43336930638179183 Scheduler time: 102.71438020747155 Scheduler overhead time: 0.07735554361715913 Adapter cache time: 0.015284542925655842 Engine time: 0.07602548925206065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 109.82986334012821,
    "estimated_duration": 3600.0960218282435,
    "input_throughput": 6972.389582890285,
    "output_throughput": 6228.990244713497,
    "total_throughput": 13201.379827603783,
    "itl": 111.35047886399731,
    "ttft": 1397744.3135289617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6697712660580882,
    "arrivals": 190328,
    "finished_requests": 101904,
    "scheduler_time": 212.20093868850728
}
#Debug simulation 
Total elapsed time: 109.82999849691987. Arrivals time: 0.43933130567893386 Scheduler time: 109.18929741997272 Scheduler overhead time: 0.07831974932923913 Adapter cache time: 0.015672537963837385 Engine time: 0.07754909573122859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 270, 270, 270, 270, 8640, 270, 33, 8640, 33, 270, 270, 33, 8640, 8640, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 33, 270, 270, 8640, 270, 33, 33, 270, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 8640, 8640, 33, 270, 8640, 33, 270, 33, 270, 33, 33, 270, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 270, 33, 33, 33, 33, 33, 8640, 33, 270, 33, 8640, 8640, 33, 8640, 33, 270, 270, 8640, 33, 33, 33, 8640, 8640, 270, 8640, 270, 8640, 8640, 270, 270, 33, 8640, 270, 270, 270, 8640, 270, 33, 270, 8640, 8640, 270, 8640, 270, 33, 8640, 270, 33, 270, 270, 33, 270, 8640, 33, 270, 8640, 33, 270, 8640, 8640, 270, 33, 8640, 270, 33, 33, 270, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 270, 33, 8640, 8640, 270, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 270, 270, 270, 270, 270, 270, 33, 8640, 270, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 572352 . Total input tokens: 127609673 . Total output tokens: 114390946
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 103.29683982022107,
    "estimated_duration": 3600.087660770955,
    "input_throughput": 6978.743399437029,
    "output_throughput": 6236.6264701431855,
    "total_throughput": 13215.369869580216,
    "itl": 111.45000465906118,
    "ttft": 1409276.1755470855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7607919720932871,
    "arrivals": 190328,
    "finished_requests": 101996,
    "scheduler_time": 211.79193858472868
}
#Debug simulation 
Total elapsed time: 103.29696800606325. Arrivals time: 0.42507537128403783 Scheduler time: 102.67595019144937 Scheduler overhead time: 0.07645687647163868 Adapter cache time: 0.015435181558132172 Engine time: 0.07541059097275138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 101.56113106384873,
    "estimated_duration": 3600.075449278754,
    "input_throughput": 7023.084475928239,
    "output_throughput": 6215.168908330147,
    "total_throughput": 13238.253384258387,
    "itl": 111.26048439966384,
    "ttft": 1407739.507299226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.722275107568129,
    "arrivals": 188114,
    "finished_requests": 102102,
    "scheduler_time": 212.29018473134755
}
#Debug simulation 
Total elapsed time: 101.56126119708642. Arrivals time: 0.4133800007402897 Scheduler time: 100.95347485970706 Scheduler overhead time: 0.07611724734306335 Adapter cache time: 0.014993091113865376 Engine time: 0.07447943929582834 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 100.99556948198006,
    "estimated_duration": 3600.1240946467165,
    "input_throughput": 7022.989579052582,
    "output_throughput": 6215.08492811987,
    "total_throughput": 13238.074507172452,
    "itl": 111.2611744165965,
    "ttft": 1407756.063150529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7677583579998494,
    "arrivals": 188114,
    "finished_requests": 102102,
    "scheduler_time": 212.29147639714816
}
#Debug simulation 
Total elapsed time: 100.99575515976176. Arrivals time: 0.4147079363465309 Scheduler time: 100.38653783733025 Scheduler overhead time: 0.07580803148448467 Adapter cache time: 0.015271292068064213 Engine time: 0.07458149595186114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 99.66198660805821,
    "estimated_duration": 3600.1438620100685,
    "input_throughput": 7039.691459954531,
    "output_throughput": 6224.179049191933,
    "total_throughput": 13263.870509146464,
    "itl": 111.28201791739734,
    "ttft": 1409294.3545267568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7725755863264239,
    "arrivals": 188114,
    "finished_requests": 102270,
    "scheduler_time": 211.91885043109397
}
#Debug simulation 
Total elapsed time: 99.66211589286104. Arrivals time: 0.40864694071933627 Scheduler time: 99.05932519771159 Scheduler overhead time: 0.07609703857451677 Adapter cache time: 0.014890443533658981 Engine time: 0.07395277265459299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 101.12822011206299,
    "estimated_duration": 3600.0942759742666,
    "input_throughput": 7023.047748703103,
    "output_throughput": 6215.136406100032,
    "total_throughput": 13238.184154803135,
    "itl": 111.26077082073311,
    "ttft": 1407746.2913891666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7379308997537026,
    "arrivals": 188114,
    "finished_requests": 102102,
    "scheduler_time": 212.29076075215403
}
#Debug simulation 
Total elapsed time: 101.12835937784985. Arrivals time: 0.41818458307534456 Scheduler time: 100.51382333599031 Scheduler overhead time: 0.0767872966825962 Adapter cache time: 0.015213083941489458 Engine time: 0.07509001484140754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 100.92882587201893,
    "estimated_duration": 3600.1308735019306,
    "input_throughput": 7022.976355136229,
    "output_throughput": 6215.073225445064,
    "total_throughput": 13238.049580581293,
    "itl": 111.2613438646212,
    "ttft": 1407757.4622296481,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7756685750372743,
    "arrivals": 188114,
    "finished_requests": 102102,
    "scheduler_time": 212.2910465998076
}
#Debug simulation 
Total elapsed time: 100.92896584002301. Arrivals time: 0.4155435082502663 Scheduler time: 100.31651047896594 Scheduler overhead time: 0.07728659454733133 Adapter cache time: 0.01515952032059431 Engine time: 0.07501207338646054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 103.4297209540382,
    "estimated_duration": 3600.0775520222605,
    "input_throughput": 7025.923645948058,
    "output_throughput": 6218.4515962509395,
    "total_throughput": 13244.375242198998,
    "itl": 111.34235486513084,
    "ttft": 1404591.2670621283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 235,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7026618193020122,
    "arrivals": 188114,
    "finished_requests": 102109,
    "scheduler_time": 211.97340432010031
}
#Debug simulation 
Total elapsed time: 103.42986281076446. Arrivals time: 0.4202264337800443 Scheduler time: 102.81268495833501 Scheduler overhead time: 0.07730142958462238 Adapter cache time: 0.015355200506746769 Engine time: 0.07470054645091295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 135, 135, 135, 135, 8640, 135, 66, 8640, 66, 135, 135, 66, 8640, 8640, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 66, 135, 135, 8640, 135, 66, 66, 135, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 8640, 8640, 66, 135, 8640, 66, 135, 66, 135, 66, 66, 135, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 135, 66, 66, 66, 66, 66, 8640, 66, 135, 66, 8640, 8640, 66, 8640, 66, 135, 135, 8640, 66, 66, 66, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 66, 8640, 135, 135, 135, 8640, 135, 66, 135, 8640, 8640, 135, 8640, 135, 66, 8640, 135, 66, 135, 135, 66, 135, 8640, 66, 135, 8640, 66, 135, 8640, 8640, 135, 66, 8640, 135, 66, 66, 135, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 135, 66, 8640, 8640, 135, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 66, 8640, 135, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 565824 . Total input tokens: 126177526 . Total output tokens: 113088095
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 99.95550309540704,
    "estimated_duration": 3600.0160850316392,
    "input_throughput": 7039.734101570623,
    "output_throughput": 6224.15107898138,
    "total_throughput": 13263.885180552003,
    "itl": 111.28078914755,
    "ttft": 1409329.023680904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 236,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7886466430127651,
    "arrivals": 188114,
    "finished_requests": 102265,
    "scheduler_time": 211.91039733583023
}
#Debug simulation 
Total elapsed time: 99.95563833834603. Arrivals time: 0.42246842570602894 Scheduler time: 99.33832145622 Scheduler overhead time: 0.07585685467347503 Adapter cache time: 0.015115334186702967 Engine time: 0.0746278683654964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 106.72430435195565,
    "estimated_duration": 3600.031721833301,
    "input_throughput": 7169.999876238655,
    "output_throughput": 6348.044063445675,
    "total_throughput": 13518.04393968433,
    "itl": 113.92810651366437,
    "ttft": 1376189.2378187159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8630575437890354,
    "arrivals": 187444,
    "finished_requests": 104192,
    "scheduler_time": 204.9100052177753
}
#Debug simulation 
Total elapsed time: 106.72444670973346. Arrivals time: 0.4375391360372305 Scheduler time: 106.08933058893308 Scheduler overhead time: 0.07739856280386448 Adapter cache time: 0.016445451881736517 Engine time: 0.0748085156083107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 104.76996957790107,
    "estimated_duration": 3600.1429994499317,
    "input_throughput": 7165.864245931818,
    "output_throughput": 6351.389654103656,
    "total_throughput": 13517.253900035474,
    "itl": 113.92743974908036,
    "ttft": 1379414.273076791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8684919404448054,
    "arrivals": 187444,
    "finished_requests": 104223,
    "scheduler_time": 204.81878290809112
}
#Debug simulation 
Total elapsed time: 104.77010878594592. Arrivals time: 0.44283478101715446 Scheduler time: 104.13096966408193 Scheduler overhead time: 0.07577056903392076 Adapter cache time: 0.015906778629869223 Engine time: 0.07552779326215386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 104.63709547603503,
    "estimated_duration": 3600.000101451532,
    "input_throughput": 7165.892298058125,
    "output_throughput": 6351.345098790642,
    "total_throughput": 13517.237396848766,
    "itl": 113.92612176050687,
    "ttft": 1379401.8500973012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 266,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8699433152936442,
    "arrivals": 187444,
    "finished_requests": 104218,
    "scheduler_time": 204.81037596541574
}
#Debug simulation 
Total elapsed time: 104.63728521578014. Arrivals time: 0.44541276851668954 Scheduler time: 103.99596188496798 Scheduler overhead time: 0.07565009640529752 Adapter cache time: 0.01567466091364622 Engine time: 0.07549759233370423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.32068687165156,
    "estimated_duration": 3600.1017646271334,
    "input_throughput": 7165.89743475541,
    "output_throughput": 6347.831670910251,
    "total_throughput": 13513.729105665661,
    "itl": 113.89023708999875,
    "ttft": 1370916.6460945723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 261,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8151337290369,
    "arrivals": 187444,
    "finished_requests": 104200,
    "scheduler_time": 205.00814800863745
}
#Debug simulation 
Total elapsed time: 111.3208274608478. Arrivals time: 0.4559264648705721 Scheduler time: 110.66352023836225 Scheduler overhead time: 0.0789055316708982 Adapter cache time: 0.016235689166933298 Engine time: 0.07679040357470512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 107.2125930339098,
    "estimated_duration": 3600.09383368244,
    "input_throughput": 7163.117460641921,
    "output_throughput": 6348.314531741722,
    "total_throughput": 13511.431992383643,
    "itl": 113.8679865245877,
    "ttft": 1375324.313493122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.878131897300486,
    "arrivals": 187444,
    "finished_requests": 104199,
    "scheduler_time": 204.87544377184204
}
#Debug simulation 
Total elapsed time: 107.21273654606193. Arrivals time: 0.4540312048047781 Scheduler time: 106.56194985285401 Scheduler overhead time: 0.07660150760784745 Adapter cache time: 0.01603818079456687 Engine time: 0.07542243832722306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 104.74525184277445,
    "estimated_duration": 3600.0124012106376,
    "input_throughput": 7168.104474118484,
    "output_throughput": 6350.384513206673,
    "total_throughput": 13518.488987325158,
    "itl": 113.9655319439203,
    "ttft": 1377776.6151621777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8641245352267299,
    "arrivals": 187444,
    "finished_requests": 104254,
    "scheduler_time": 204.7576290941857
}
#Debug simulation 
Total elapsed time: 104.74539263080806. Arrivals time: 0.44655872602015734 Scheduler time: 104.10089253820479 Scheduler overhead time: 0.07746532745659351 Adapter cache time: 0.0159266977570951 Engine time: 0.0761073655448854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 135, 135, 135, 135, 8640, 135, 33, 8640, 33, 135, 135, 33, 8640, 8640, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 33, 135, 135, 8640, 135, 33, 33, 135, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 8640, 8640, 33, 135, 8640, 33, 135, 33, 135, 33, 33, 135, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 135, 33, 33, 33, 33, 33, 8640, 33, 135, 33, 8640, 8640, 33, 8640, 33, 135, 135, 8640, 33, 33, 33, 8640, 8640, 135, 8640, 135, 8640, 8640, 135, 135, 33, 8640, 135, 135, 135, 8640, 135, 33, 135, 8640, 8640, 135, 8640, 135, 33, 8640, 135, 33, 135, 135, 33, 135, 8640, 33, 135, 8640, 33, 135, 8640, 8640, 135, 33, 8640, 135, 33, 33, 135, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 135, 33, 8640, 8640, 135, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 135, 135, 135, 135, 135, 135, 33, 8640, 135, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 563712 . Total input tokens: 125719945 . Total output tokens: 112658382
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 107.54656543582678,
    "estimated_duration": 3600.1041473364057,
    "input_throughput": 7163.09693959259,
    "output_throughput": 6348.296344956933,
    "total_throughput": 13511.393284549524,
    "itl": 113.86823013608516,
    "ttft": 1375328.4267069432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8889467229321643,
    "arrivals": 187444,
    "finished_requests": 104199,
    "scheduler_time": 204.87555065931932
}
#Debug simulation 
Total elapsed time: 107.5467022662051. Arrivals time: 0.44268140383064747 Scheduler time: 106.90470686135814 Scheduler overhead time: 0.07769389310851693 Adapter cache time: 0.01571334572508931 Engine time: 0.07631005207076669 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 111.5929391249083,
    "estimated_duration": 3600.08522312627,
    "input_throughput": 7124.0077416079685,
    "output_throughput": 6322.544214726678,
    "total_throughput": 13446.551956334646,
    "itl": 113.16993041913719,
    "ttft": 1377199.8372191377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7375775462877927,
    "arrivals": 185958,
    "finished_requests": 103781,
    "scheduler_time": 206.19888303842447
}
#Debug simulation 
Total elapsed time: 111.59307317668572. Arrivals time: 0.44097716733813286 Scheduler time: 110.95061981398612 Scheduler overhead time: 0.07894921861588955 Adapter cache time: 0.01575217070057988 Engine time: 0.07742216484621167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 108.24633193714544,
    "estimated_duration": 3600.0916250480177,
    "input_throughput": 7128.117468304081,
    "output_throughput": 6332.148004620839,
    "total_throughput": 13460.26547292492,
    "itl": 112.9724844716508,
    "ttft": 1380641.8610265818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.817920851563107,
    "arrivals": 185958,
    "finished_requests": 103877,
    "scheduler_time": 205.64800909810972
}
#Debug simulation 
Total elapsed time: 108.24646497610956. Arrivals time: 0.4395018173381686 Scheduler time: 107.6090937894769 Scheduler overhead time: 0.07659520674496889 Adapter cache time: 0.0160548803396523 Engine time: 0.07664719130843878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 108.96077160630375,
    "estimated_duration": 3600.0930785747373,
    "input_throughput": 7128.114590348157,
    "output_throughput": 6332.145448035186,
    "total_throughput": 13460.260038383343,
    "itl": 112.97251598425608,
    "ttft": 1380642.3947124186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8195690676383713,
    "arrivals": 185958,
    "finished_requests": 103877,
    "scheduler_time": 205.64804061880278
}
#Debug simulation 
Total elapsed time: 108.96090875193477. Arrivals time: 0.44708595843985677 Scheduler time: 108.3148033497855 Scheduler overhead time: 0.07726777112111449 Adapter cache time: 0.016106709837913513 Engine time: 0.07633735379204154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 111.37326159700751,
    "estimated_duration": 3600.023410302797,
    "input_throughput": 7147.181856196834,
    "output_throughput": 6343.170417905507,
    "total_throughput": 13490.35227410234,
    "itl": 113.48058407047027,
    "ttft": 1376805.325352855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 254,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7929775910172634,
    "arrivals": 185958,
    "finished_requests": 104090,
    "scheduler_time": 205.08897283649802
}
#Debug simulation 
Total elapsed time: 111.3733993889764. Arrivals time: 0.4381042206659913 Scheduler time: 110.73569823522121 Scheduler overhead time: 0.0785883255302906 Adapter cache time: 0.015621202997863293 Engine time: 0.07627523085102439 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 108.32031740993261,
    "estimated_duration": 3600.1049580597773,
    "input_throughput": 7128.091069275403,
    "output_throughput": 6332.124553470167,
    "total_throughput": 13460.21562274557,
    "itl": 112.97277125664557,
    "ttft": 1380647.4909937505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8302581394836344,
    "arrivals": 185958,
    "finished_requests": 103877,
    "scheduler_time": 205.64844889377272
}
#Debug simulation 
Total elapsed time: 108.3204545378685. Arrivals time: 0.4322267542593181 Scheduler time: 107.68892815336585 Scheduler overhead time: 0.07799149164929986 Adapter cache time: 0.015627018176019192 Engine time: 0.07633601687848568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 111.54665708728135,
    "estimated_duration": 3600.067646998133,
    "input_throughput": 7124.042522196889,
    "output_throughput": 6322.575082437556,
    "total_throughput": 13446.617604634444,
    "itl": 113.1695578173351,
    "ttft": 1377193.0241644413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206021210714253,
    "arrivals": 185958,
    "finished_requests": 103781,
    "scheduler_time": 206.19848159598007
}
#Debug simulation 
Total elapsed time: 111.54679435212165. Arrivals time: 0.4358037658967078 Scheduler time: 110.91169179743156 Scheduler overhead time: 0.07851033518090844 Adapter cache time: 0.015745420940220356 Engine time: 0.07621832611039281 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 66, 66, 66, 66, 8640, 66, 33, 8640, 33, 66, 66, 33, 8640, 8640, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 33, 66, 66, 8640, 66, 33, 33, 66, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 8640, 8640, 33, 66, 8640, 33, 66, 33, 66, 33, 33, 66, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 66, 33, 33, 33, 33, 33, 8640, 33, 66, 33, 8640, 8640, 33, 8640, 33, 66, 66, 8640, 33, 33, 33, 8640, 8640, 66, 8640, 66, 8640, 8640, 66, 66, 33, 8640, 66, 66, 66, 8640, 66, 33, 66, 8640, 8640, 66, 8640, 66, 33, 8640, 66, 33, 66, 66, 33, 66, 8640, 33, 66, 8640, 33, 66, 8640, 8640, 66, 33, 8640, 66, 33, 33, 66, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 66, 33, 8640, 8640, 66, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 66, 66, 66, 66, 66, 66, 33, 8640, 66, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 559296 . Total input tokens: 124703275 . Total output tokens: 111799974
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 108.51931299408898,
    "estimated_duration": 3600.1113103498856,
    "input_throughput": 7128.07849196918,
    "output_throughput": 6332.113380623357,
    "total_throughput": 13460.191872592537,
    "itl": 112.97289501052005,
    "ttft": 1380649.3656470305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 251,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8403184423968233,
    "arrivals": 185958,
    "finished_requests": 103877,
    "scheduler_time": 205.64858645203162
}
#Debug simulation 
Total elapsed time: 108.51945551810786. Arrivals time: 0.4458245988935232 Scheduler time: 107.87409099889919 Scheduler overhead time: 0.07786563271656632 Adapter cache time: 0.01566202798858285 Engine time: 0.07679108809679747 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 102.464686491061,
    "estimated_duration": 3600.0583585682657,
    "input_throughput": 6736.494129957624,
    "output_throughput": 5934.513241751071,
    "total_throughput": 12671.007371708696,
    "itl": 103.51081090197226,
    "ttft": 1141823.9472749853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7199941120902313,
    "arrivals": 126417,
    "finished_requests": 97761,
    "scheduler_time": 183.21286073244417
}
#Debug simulation 
Total elapsed time: 102.46481501404196. Arrivals time: 0.40479871770367026 Scheduler time: 101.83565172925591 Scheduler overhead time: 0.08629168290644884 Adapter cache time: 0.019977092277258635 Engine time: 0.08481147652491927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 86.27141808485612,
    "estimated_duration": 3600.12900325762,
    "input_throughput": 6892.59356471575,
    "output_throughput": 6074.0698403343295,
    "total_throughput": 12966.66340505008,
    "itl": 107.55490451653141,
    "ttft": 1104381.5199608628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1082155511388616,
    "arrivals": 126417,
    "finished_requests": 100112,
    "scheduler_time": 173.4092271614449
}
#Debug simulation 
Total elapsed time: 86.2715533520095. Arrivals time: 0.41104462277144194 Scheduler time: 85.64571990538388 Scheduler overhead time: 0.08366191759705544 Adapter cache time: 0.019610463175922632 Engine time: 0.07988407835364342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 86.42457011761144,
    "estimated_duration": 3600.134244263454,
    "input_throughput": 6892.583530611289,
    "output_throughput": 6074.060997820881,
    "total_throughput": 12966.644528432169,
    "itl": 107.55411325415491,
    "ttft": 1104383.6995959172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.112486880328515,
    "arrivals": 126417,
    "finished_requests": 100112,
    "scheduler_time": 173.40988535719714
}
#Debug simulation 
Total elapsed time: 86.42470443155617. Arrivals time: 0.40657278429716825 Scheduler time: 85.80380546208471 Scheduler overhead time: 0.08315830864012241 Adapter cache time: 0.01954990392550826 Engine time: 0.08006620267406106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 86.07436736114323,
    "estimated_duration": 3600.1135216715384,
    "input_throughput": 6892.622927201117,
    "output_throughput": 6074.07762793184,
    "total_throughput": 12966.700555132957,
    "itl": 107.54813297093108,
    "ttft": 1104398.2670006142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.016560316742384,
    "arrivals": 126417,
    "finished_requests": 100111,
    "scheduler_time": 173.4147927624764
}
#Debug simulation 
Total elapsed time: 86.07449115207419. Arrivals time: 0.4120979364961386 Scheduler time: 85.45080455951393 Scheduler overhead time: 0.08174113277345896 Adapter cache time: 0.0195978875271976 Engine time: 0.07896879129111767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 83.25972641585395,
    "estimated_duration": 3600.1027826524232,
    "input_throughput": 6899.550790519228,
    "output_throughput": 6076.444568586094,
    "total_throughput": 12975.995359105322,
    "itl": 107.65239765414351,
    "ttft": 1103161.9374247214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.112775047291073,
    "arrivals": 126417,
    "finished_requests": 100136,
    "scheduler_time": 173.30134460356894
}
#Debug simulation 
Total elapsed time: 83.25991983897984. Arrivals time: 0.414272413123399 Scheduler time: 82.63206732692197 Scheduler overhead time: 0.0829345271922648 Adapter cache time: 0.019562880974262953 Engine time: 0.07953225309029222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 103.0272883689031,
    "estimated_duration": 3600.0270476206283,
    "input_throughput": 6725.538080610409,
    "output_throughput": 5921.913562869044,
    "total_throughput": 12647.451643479453,
    "itl": 102.93995860919404,
    "ttft": 1137163.3739160285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 559,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6714381148502937,
    "arrivals": 126417,
    "finished_requests": 97683,
    "scheduler_time": 183.7693728342631
}
#Debug simulation 
Total elapsed time: 103.02741938270628. Arrivals time: 0.42462395410984755 Scheduler time: 102.37879246519879 Scheduler overhead time: 0.08678101468831301 Adapter cache time: 0.02044133795425296 Engine time: 0.08370244223624468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [64 64 64]
Adapter prompts. [4320, 540, 4320, 540, 4320, 540, 4320, 540, 540, 540, 1080, 1080, 1080, 1080, 4320, 1080, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 4320, 1080, 540, 540, 1080, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 4320, 4320, 540, 1080, 4320, 540, 1080, 540, 1080, 540, 540, 1080, 4320, 4320, 540, 540, 540, 4320, 4320, 4320, 1080, 540, 540, 540, 540, 540, 4320, 540, 1080, 540, 4320, 4320, 540, 4320, 540, 1080, 1080, 4320, 540, 540, 540, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 540, 4320, 1080, 1080, 1080, 4320, 1080, 540, 1080, 4320, 4320, 1080, 4320, 1080, 540, 4320, 1080, 540, 1080, 1080, 540, 1080, 4320, 540, 1080, 4320, 540, 1080, 4320, 4320, 1080, 540, 4320, 1080, 540, 540, 1080, 4320, 4320, 4320, 540, 4320, 540, 4320, 540, 1080, 540, 4320, 4320, 1080, 4320, 540, 4320, 4320, 4320, 540, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 540, 4320, 1080, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 380160 . Total input tokens: 84800111 . Total output tokens: 75932978
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 86.17398656113073,
    "estimated_duration": 3600.015103694701,
    "input_throughput": 6892.466916189989,
    "output_throughput": 6073.8509062250705,
    "total_throughput": 12966.31782241506,
    "itl": 107.55578729227305,
    "ttft": 1104465.333068205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 646,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.162425719425082,
    "arrivals": 126417,
    "finished_requests": 100108,
    "scheduler_time": 173.4024468371537
}
#Debug simulation 
Total elapsed time: 86.17411408992484. Arrivals time: 0.4164777286350727 Scheduler time: 85.54384992970154 Scheduler overhead time: 0.08240459254011512 Adapter cache time: 0.020015856251120567 Engine time: 0.08032483980059624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 78.68629422178492,
    "estimated_duration": 3600.0800096315047,
    "input_throughput": 6882.037880745877,
    "output_throughput": 6008.07674888702,
    "total_throughput": 12890.114629632897,
    "itl": 103.02456068053075,
    "ttft": 961608.1151793612,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 708,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1668253227044287,
    "arrivals": 120741,
    "finished_requests": 99343,
    "scheduler_time": 162.57940605689927
}
#Debug simulation 
Total elapsed time: 78.68642502371222. Arrivals time: 0.3925207587890327 Scheduler time: 78.0785115249455 Scheduler overhead time: 0.08215387491509318 Adapter cache time: 0.020753056276589632 Engine time: 0.0808624536730349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 98.4507637200877,
    "estimated_duration": 3600.0693608468905,
    "input_throughput": 6678.149388306324,
    "output_throughput": 5833.882599156184,
    "total_throughput": 12512.031987462507,
    "itl": 98.91340064308747,
    "ttft": 1091480.019845172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.934086380342494,
    "arrivals": 120741,
    "finished_requests": 96373,
    "scheduler_time": 176.02557915600053
}
#Debug simulation 
Total elapsed time: 98.45089399907738. Arrivals time: 0.4123732070438564 Scheduler time: 97.80735351098701 Scheduler overhead time: 0.08868449926376343 Adapter cache time: 0.021146062295883894 Engine time: 0.08725836081430316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 97.44230851205066,
    "estimated_duration": 3600.087647271906,
    "input_throughput": 6683.420060129091,
    "output_throughput": 5837.7159278124345,
    "total_throughput": 12521.135987941525,
    "itl": 98.89339869416187,
    "ttft": 1088840.8938971977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 592,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9339641763270021,
    "arrivals": 120741,
    "finished_requests": 96470,
    "scheduler_time": 175.80045386055306
}
#Debug simulation 
Total elapsed time: 97.44244184577838. Arrivals time: 0.4009207799099386 Scheduler time: 96.81009378144518 Scheduler overhead time: 0.08981432626023889 Adapter cache time: 0.02054110122844577 Engine time: 0.08653422724455595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 94.78331200405955,
    "estimated_duration": 3600.0925125914614,
    "input_throughput": 6756.820808054724,
    "output_throughput": 5919.044837173094,
    "total_throughput": 12675.865645227817,
    "itl": 100.90908027445434,
    "ttft": 1022924.5214295391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 624,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.949962019617657,
    "arrivals": 120741,
    "finished_requests": 97467,
    "scheduler_time": 170.79411551390734
}
#Debug simulation 
Total elapsed time: 94.78344621090218. Arrivals time: 0.39836880285292864 Scheduler time: 94.15603485144675 Scheduler overhead time: 0.08877911232411861 Adapter cache time: 0.020586677361279726 Engine time: 0.08548165718093514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 116.40586894704029,
    "estimated_duration": 3600.0257750732585,
    "input_throughput": 6603.909939927085,
    "output_throughput": 5803.987056057899,
    "total_throughput": 12407.896995984984,
    "itl": 97.87851077202038,
    "ttft": 1082844.8023374656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 550,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8175571589916981,
    "arrivals": 120741,
    "finished_requests": 95381,
    "scheduler_time": 179.72147002942376
}
#Debug simulation 
Total elapsed time: 116.40600303793326. Arrivals time: 0.4048178191296756 Scheduler time: 115.76177181163803 Scheduler overhead time: 0.09400755073875189 Adapter cache time: 0.021252422593533993 Engine time: 0.08968907222151756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 77.03269311413169,
    "estimated_duration": 3600.0888117356326,
    "input_throughput": 6896.712636383501,
    "output_throughput": 6017.562658282233,
    "total_throughput": 12914.275294665735,
    "itl": 102.6574296054531,
    "ttft": 956668.2503680409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.146856111739723,
    "arrivals": 120741,
    "finished_requests": 99504,
    "scheduler_time": 161.98169124063205
}
#Debug simulation 
Total elapsed time: 77.03287687525153. Arrivals time: 0.37986580235883594 Scheduler time: 76.43556186882779 Scheduler overhead time: 0.08374948799610138 Adapter cache time: 0.0204107197932899 Engine time: 0.08121216110885143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 1080, 1080, 1080, 1080, 4320, 1080, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 4320, 1080, 270, 270, 1080, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 4320, 4320, 270, 1080, 4320, 270, 1080, 270, 1080, 270, 270, 1080, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 1080, 270, 270, 270, 270, 270, 4320, 270, 1080, 270, 4320, 4320, 270, 4320, 270, 1080, 1080, 4320, 270, 270, 270, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 270, 4320, 1080, 1080, 1080, 4320, 1080, 270, 1080, 4320, 4320, 1080, 4320, 1080, 270, 4320, 1080, 270, 1080, 1080, 270, 1080, 4320, 270, 1080, 4320, 270, 1080, 4320, 4320, 1080, 270, 4320, 1080, 270, 270, 1080, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 1080, 270, 4320, 4320, 1080, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 270, 4320, 1080, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 362880 . Total input tokens: 80942265 . Total output tokens: 72474384
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 95.57854150841013,
    "estimated_duration": 3600.117396576501,
    "input_throughput": 6707.776258342038,
    "output_throughput": 5870.233570743204,
    "total_throughput": 12578.009829085242,
    "itl": 100.46689801186626,
    "ttft": 1043353.9827010959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 625,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0971228197589493,
    "arrivals": 120741,
    "finished_requests": 96902,
    "scheduler_time": 172.39192410377996
}
#Debug simulation 
Total elapsed time: 95.57867750525475. Arrivals time: 0.394430682528764 Scheduler time: 94.95730586303398 Scheduler overhead time: 0.08782061375677586 Adapter cache time: 0.020428078714758158 Engine time: 0.08566326089203358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 79.4762554988265,
    "estimated_duration": 3600.0110542326393,
    "input_throughput": 6701.393311455369,
    "output_throughput": 5975.4344294882485,
    "total_throughput": 12676.827740943618,
    "itl": 102.65893761094348,
    "ttft": 935181.2558643554,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 688,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1056155678257715,
    "arrivals": 117840,
    "finished_requests": 97843,
    "scheduler_time": 160.21590701558313
}
#Debug simulation 
Total elapsed time: 79.47638430679217. Arrivals time: 0.37633082922548056 Scheduler time: 78.8810469834134 Scheduler overhead time: 0.08471491187810898 Adapter cache time: 0.020588313229382038 Engine time: 0.0815380192361772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 75.76535097369924,
    "estimated_duration": 3600.0867808599,
    "input_throughput": 6801.381325077806,
    "output_throughput": 6061.625824138497,
    "total_throughput": 12863.007149216302,
    "itl": 104.84201203163535,
    "ttft": 852039.0840662415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 727,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3694837219268132,
    "arrivals": 117840,
    "finished_requests": 99239,
    "scheduler_time": 154.30168184725426
}
#Debug simulation 
Total elapsed time: 75.76548779383302. Arrivals time: 0.39069795003160834 Scheduler time: 75.16023583384231 Scheduler overhead time: 0.08268903056159616 Adapter cache time: 0.020099114160984755 Engine time: 0.07995424931868911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 75.95687717897817,
    "estimated_duration": 3600.089400069871,
    "input_throughput": 6801.376376799082,
    "output_throughput": 6061.621414061681,
    "total_throughput": 12862.997790860763,
    "itl": 104.84218249991515,
    "ttft": 852040.4301812267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 727,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3741797529347224,
    "arrivals": 117840,
    "finished_requests": 99239,
    "scheduler_time": 154.30198023185758
}
#Debug simulation 
Total elapsed time: 75.95700635714456. Arrivals time: 0.38327728724107146 Scheduler time: 75.35689159901813 Scheduler overhead time: 0.08424899633973837 Adapter cache time: 0.020499273668974638 Engine time: 0.08088292693719268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 93.92316161002964,
    "estimated_duration": 3600.014319425572,
    "input_throughput": 6573.976629008597,
    "output_throughput": 5865.745834969192,
    "total_throughput": 12439.722463977789,
    "itl": 99.34204099238873,
    "ttft": 1005827.2263271692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 610,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9064669342152654,
    "arrivals": 117840,
    "finished_requests": 96106,
    "scheduler_time": 168.41665372925243
}
#Debug simulation 
Total elapsed time: 93.9232949600555. Arrivals time: 0.3969168779440224 Scheduler time: 93.29679152043536 Scheduler overhead time: 0.08935954980552197 Adapter cache time: 0.020621477160602808 Engine time: 0.08599162846803665 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 97.32020394597203,
    "estimated_duration": 3600.065394578325,
    "input_throughput": 6548.342159423821,
    "output_throughput": 5865.141514317737,
    "total_throughput": 12413.483673741559,
    "itl": 98.35420908707026,
    "ttft": 959461.5123340918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 653,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1607345450297037,
    "arrivals": 117840,
    "finished_requests": 95654,
    "scheduler_time": 165.95720840083692
}
#Debug simulation 
Total elapsed time: 97.32033292297274. Arrivals time: 0.39209557604044676 Scheduler time: 96.69322643894702 Scheduler overhead time: 0.09057111712172627 Adapter cache time: 0.02126695029437542 Engine time: 0.08894268097355962 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 81.19760823715478,
    "estimated_duration": 3600.0933049377327,
    "input_throughput": 6691.988779001019,
    "output_throughput": 5963.344608472952,
    "total_throughput": 12655.333387473971,
    "itl": 101.7996094958314,
    "ttft": 936235.3332990205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0750949046620732,
    "arrivals": 117840,
    "finished_requests": 97653,
    "scheduler_time": 160.46605937311364
}
#Debug simulation 
Total elapsed time: 81.19774470617995. Arrivals time: 0.39687924133613706 Scheduler time: 80.57796166883782 Scheduler overhead time: 0.0861330721527338 Adapter cache time: 0.02069714805111289 Engine time: 0.08346800692379475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 1080, 1080, 1080, 1080, 4320, 1080, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 4320, 1080, 135, 135, 1080, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 4320, 4320, 135, 1080, 4320, 135, 1080, 135, 1080, 135, 135, 1080, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 1080, 135, 135, 135, 135, 135, 4320, 135, 1080, 135, 4320, 4320, 135, 4320, 135, 1080, 1080, 4320, 135, 135, 135, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 135, 4320, 1080, 1080, 1080, 4320, 1080, 135, 1080, 4320, 4320, 1080, 4320, 1080, 135, 4320, 1080, 135, 1080, 1080, 135, 1080, 4320, 135, 1080, 4320, 135, 1080, 4320, 4320, 1080, 135, 4320, 1080, 135, 135, 1080, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 1080, 135, 4320, 4320, 1080, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 135, 4320, 1080, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 354240 . Total input tokens: 79031439 . Total output tokens: 70773623
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 78.42211273917928,
    "estimated_duration": 3600.1005048863894,
    "input_throughput": 6712.180386964662,
    "output_throughput": 5981.511896896212,
    "total_throughput": 12693.692283860875,
    "itl": 103.18346846933507,
    "ttft": 931459.9539777582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 689,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.309039073474711,
    "arrivals": 117840,
    "finished_requests": 97977,
    "scheduler_time": 159.83873186669112
}
#Debug simulation 
Total elapsed time: 78.42229828517884. Arrivals time: 0.3857926456257701 Scheduler time: 77.81603424670175 Scheduler overhead time: 0.0854134107939899 Adapter cache time: 0.02032485418021679 Engine time: 0.08228871179744601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 104.73041951097548,
    "estimated_duration": 3600.1257655169325,
    "input_throughput": 6430.337023705601,
    "output_throughput": 5749.301648918367,
    "total_throughput": 12179.638672623969,
    "itl": 95.48495252754448,
    "ttft": 1054039.8599229094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7444780140416942,
    "arrivals": 116430,
    "finished_requests": 94025,
    "scheduler_time": 172.38094025300046
}
#Debug simulation 
Total elapsed time: 104.73054562415928. Arrivals time: 0.3948907870799303 Scheduler time: 104.0948891332373 Scheduler overhead time: 0.09335562633350492 Adapter cache time: 0.021224767435342073 Engine time: 0.0904550077393651 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 104.85653421189636,
    "estimated_duration": 3600.1001401352783,
    "input_throughput": 6434.851003652778,
    "output_throughput": 5750.541150008698,
    "total_throughput": 12185.392153661476,
    "itl": 95.54500444661296,
    "ttft": 1051847.1412906435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 568,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8523890025564533,
    "arrivals": 116430,
    "finished_requests": 94076,
    "scheduler_time": 172.29526815759806
}
#Debug simulation 
Total elapsed time: 104.85666286526248. Arrivals time: 0.3912378177046776 Scheduler time: 104.22404535021633 Scheduler overhead time: 0.09387513436377048 Adapter cache time: 0.021164157427847385 Engine time: 0.09109676396474242 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 103.61048466712236,
    "estimated_duration": 3600.139542542226,
    "input_throughput": 6416.897936041333,
    "output_throughput": 5740.764699749854,
    "total_throughput": 12157.662635791188,
    "itl": 95.61974064171625,
    "ttft": 1058965.0047872865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8886569324880955,
    "arrivals": 116430,
    "finished_requests": 93847,
    "scheduler_time": 172.76282297396864
}
#Debug simulation 
Total elapsed time: 103.61061970097944. Arrivals time: 0.3869107300415635 Scheduler time: 102.98427472496405 Scheduler overhead time: 0.09227260015904903 Adapter cache time: 0.02142737014219165 Engine time: 0.09013818623498082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 89.98135937191546,
    "estimated_duration": 3600.026867679997,
    "input_throughput": 6569.06514012776,
    "output_throughput": 5890.93047899028,
    "total_throughput": 12459.995619118039,
    "itl": 100.37916236716782,
    "ttft": 965031.7211172099,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 642,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.007460282791859,
    "arrivals": 116430,
    "finished_requests": 96145,
    "scheduler_time": 163.46083030970567
}
#Debug simulation 
Total elapsed time: 89.98148885508999. Arrivals time: 0.37665033293887973 Scheduler time: 89.37522727670148 Scheduler overhead time: 0.08950658747926354 Adapter cache time: 0.020988055039197206 Engine time: 0.08551154471933842 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 90.0400573592633,
    "estimated_duration": 3600.0714248299837,
    "input_throughput": 6570.357698144022,
    "output_throughput": 5891.058120048709,
    "total_throughput": 12461.41581819273,
    "itl": 100.38090413069565,
    "ttft": 964794.3264154288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 641,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.118319055978211,
    "arrivals": 116430,
    "finished_requests": 96151,
    "scheduler_time": 163.4623520831251
}
#Debug simulation 
Total elapsed time: 90.04018930112943. Arrivals time: 0.3761641541495919 Scheduler time: 89.43556108744815 Scheduler overhead time: 0.08872212981805205 Adapter cache time: 0.020862046629190445 Engine time: 0.08523906674236059 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 85.42003749217838,
    "estimated_duration": 3600.1392331692423,
    "input_throughput": 6591.926995864709,
    "output_throughput": 5901.709523966398,
    "total_throughput": 12493.636519831107,
    "itl": 101.66247838865654,
    "ttft": 967461.0720604422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 633,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8927018366730468,
    "arrivals": 116430,
    "finished_requests": 96433,
    "scheduler_time": 163.15885292162207
}
#Debug simulation 
Total elapsed time: 85.42017011390999. Arrivals time: 0.37480354215949774 Scheduler time: 84.8177285543643 Scheduler overhead time: 0.08789580222219229 Adapter cache time: 0.020859848242253065 Engine time: 0.08534037182107568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 1080, 1080, 1080, 1080, 4320, 1080, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 4320, 1080, 66, 66, 1080, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 4320, 4320, 66, 1080, 4320, 66, 1080, 66, 1080, 66, 66, 1080, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 1080, 66, 66, 66, 66, 66, 4320, 66, 1080, 66, 4320, 4320, 66, 4320, 66, 1080, 1080, 4320, 66, 66, 66, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 66, 4320, 1080, 1080, 1080, 4320, 1080, 66, 1080, 4320, 4320, 1080, 4320, 1080, 66, 4320, 1080, 66, 1080, 1080, 66, 1080, 4320, 66, 1080, 4320, 66, 1080, 4320, 4320, 1080, 66, 4320, 1080, 66, 66, 1080, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 1080, 66, 4320, 4320, 1080, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 66, 4320, 1080, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 349824 . Total input tokens: 78031447 . Total output tokens: 69918000
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 104.85207246476784,
    "estimated_duration": 3600.128440010883,
    "input_throughput": 6417.033554483452,
    "output_throughput": 5740.737683219303,
    "total_throughput": 12157.771237702755,
    "itl": 95.6320959011339,
    "ttft": 1058940.221355027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 578,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9369463864713858,
    "arrivals": 116430,
    "finished_requests": 93848,
    "scheduler_time": 172.75891566106478
}
#Debug simulation 
Total elapsed time: 104.8522067181766. Arrivals time: 0.3877015393227339 Scheduler time: 104.218920935411 Scheduler overhead time: 0.09643419971689582 Adapter cache time: 0.02107173763215542 Engine time: 0.09171702805906534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 95.06517825601622,
    "estimated_duration": 3600.134395273739,
    "input_throughput": 6607.412220840521,
    "output_throughput": 5845.337892837166,
    "total_throughput": 12452.750113677686,
    "itl": 98.38431871863915,
    "ttft": 948418.5563036358,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 582,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7812038669688885,
    "arrivals": 115680,
    "finished_requests": 95961,
    "scheduler_time": 163.81184233698454
}
#Debug simulation 
Total elapsed time: 95.06536195799708. Arrivals time: 0.37799634924158454 Scheduler time: 94.45187206286937 Scheduler overhead time: 0.09158001141622663 Adapter cache time: 0.021064492873847485 Engine time: 0.08812195807695389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 98.06509850593284,
    "estimated_duration": 3600.040074599671,
    "input_throughput": 6522.007953650614,
    "output_throughput": 5768.134956750211,
    "total_throughput": 12290.142910400824,
    "itl": 97.8015662551503,
    "ttft": 1007669.9611148079,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 569,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8557876481697948,
    "arrivals": 115680,
    "finished_requests": 94606,
    "scheduler_time": 169.21800130894246
}
#Debug simulation 
Total elapsed time: 98.06522755604237. Arrivals time: 0.38157241232693195 Scheduler time: 97.44607938826084 Scheduler overhead time: 0.09226166550070047 Adapter cache time: 0.021043528337031603 Engine time: 0.0893045854754746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 98.34430975001305,
    "estimated_duration": 3600.0460291487484,
    "input_throughput": 6521.997166117307,
    "output_throughput": 5768.125416138117,
    "total_throughput": 12290.122582255424,
    "itl": 97.79969371030514,
    "ttft": 1007671.7084431016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 569,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8592417778819914,
    "arrivals": 115680,
    "finished_requests": 94606,
    "scheduler_time": 169.21846583771463
}
#Debug simulation 
Total elapsed time: 98.34443646715954. Arrivals time: 0.38018355099484324 Scheduler time: 97.72796444129199 Scheduler overhead time: 0.0915928934700787 Adapter cache time: 0.02060435013845563 Engine time: 0.08892544871196151 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 93.52674563415349,
    "estimated_duration": 3600.1373819027663,
    "input_throughput": 6590.982921729198,
    "output_throughput": 5826.505984311499,
    "total_throughput": 12417.488906040697,
    "itl": 97.70071773465965,
    "ttft": 956220.0802542191,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 565,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7641513598943028,
    "arrivals": 115680,
    "finished_requests": 95666,
    "scheduler_time": 163.65392155490557
}
#Debug simulation 
Total elapsed time: 93.52687305724248. Arrivals time: 0.3595626228488982 Scheduler time: 92.93858372559771 Scheduler overhead time: 0.08823144063353539 Adapter cache time: 0.020109693985432386 Engine time: 0.08634364232420921 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 110.84056787006557,
    "estimated_duration": 3600.023921606867,
    "input_throughput": 6382.76980941386,
    "output_throughput": 5660.003778781872,
    "total_throughput": 12042.773588195732,
    "itl": 94.4713801277408,
    "ttft": 1026989.9207704109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 521,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7249566210061351,
    "arrivals": 115680,
    "finished_requests": 92664,
    "scheduler_time": 174.5205677330742
}
#Debug simulation 
Total elapsed time: 110.84070001123473. Arrivals time: 0.3502778233960271 Scheduler time: 110.25555794360116 Scheduler overhead time: 0.09128463827073574 Adapter cache time: 0.01970545994117856 Engine time: 0.08938542613759637 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 95.54602272622287,
    "estimated_duration": 3600.03321431687,
    "input_throughput": 6532.764171861322,
    "output_throughput": 5776.863368174882,
    "total_throughput": 12309.627540036205,
    "itl": 98.32083439735503,
    "ttft": 1004327.0341531463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 571,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7073187183891185,
    "arrivals": 115680,
    "finished_requests": 94749,
    "scheduler_time": 168.77987079723755
}
#Debug simulation 
Total elapsed time: 95.54615216702223. Arrivals time: 0.3465742482803762 Scheduler time: 94.97466999059543 Scheduler overhead time: 0.08649277966469526 Adapter cache time: 0.01952653145417571 Engine time: 0.08522727433592081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_192_slots_32_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 1080, 1080, 1080, 1080, 4320, 1080, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 4320, 1080, 33, 33, 1080, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 4320, 4320, 33, 1080, 4320, 33, 1080, 33, 1080, 33, 33, 1080, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 1080, 33, 33, 33, 33, 33, 4320, 33, 1080, 33, 4320, 4320, 33, 4320, 33, 1080, 1080, 4320, 33, 33, 33, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 1080, 1080, 33, 4320, 1080, 1080, 1080, 4320, 1080, 33, 1080, 4320, 4320, 1080, 4320, 1080, 33, 4320, 1080, 33, 1080, 1080, 33, 1080, 4320, 33, 1080, 4320, 33, 1080, 4320, 4320, 1080, 33, 4320, 1080, 33, 33, 1080, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 1080, 33, 4320, 4320, 1080, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 1080, 33, 4320, 1080, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 347712 . Total input tokens: 77574953 . Total output tokens: 69479568
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 95.18429825268686,
    "estimated_duration": 3600.0495587615205,
    "input_throughput": 6539.66163957455,
    "output_throughput": 5786.137290611636,
    "total_throughput": 12325.798930186185,
    "itl": 98.26849614533697,
    "ttft": 1005746.0482719027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 570,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9107462305575564,
    "arrivals": 115680,
    "finished_requests": 94921,
    "scheduler_time": 168.38857779453167
}
#Debug simulation 
Total elapsed time: 95.18442753981799. Arrivals time: 0.35303623834624887 Scheduler time: 94.60276592615992 Scheduler overhead time: 0.08843993954360485 Adapter cache time: 0.01968067791312933 Engine time: 0.08660826925188303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 80.66080269403756,
    "estimated_duration": 3600.113925745458,
    "input_throughput": 6547.916117715304,
    "output_throughput": 5748.198925597878,
    "total_throughput": 12296.115043313182,
    "itl": 93.84029251672908,
    "ttft": 797685.4501854777,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.010740447763853,
    "arrivals": 109355,
    "finished_requests": 94675,
    "scheduler_time": 151.7693684585859
}
#Debug simulation 
Total elapsed time: 80.66093581076711. Arrivals time: 0.3387969513423741 Scheduler time: 80.09310037223622 Scheduler overhead time: 0.08805811917409301 Adapter cache time: 0.02057301253080368 Engine time: 0.08598603773862123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 104.86235572397709,
    "estimated_duration": 3600.1028541646897,
    "input_throughput": 6242.4330388233775,
    "output_throughput": 5480.604526944274,
    "total_throughput": 11723.037565767652,
    "itl": 85.56091788487527,
    "ttft": 937977.2265476672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 591,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9305578516633317,
    "arrivals": 109355,
    "finished_requests": 90359,
    "scheduler_time": 163.88803553625942
}
#Debug simulation 
Total elapsed time: 104.86253481311724. Arrivals time: 0.3634370407089591 Scheduler time: 104.24716529622674 Scheduler overhead time: 0.09764764318242669 Adapter cache time: 0.021527519915252924 Engine time: 0.09530353592708707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 91.24079637182876,
    "estimated_duration": 3600.0087714957813,
    "input_throughput": 6443.30096739243,
    "output_throughput": 5685.737257661009,
    "total_throughput": 12129.038225053439,
    "itl": 91.3666449709873,
    "ttft": 846849.2243244456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 645,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1090978225879455,
    "arrivals": 109355,
    "finished_requests": 93095,
    "scheduler_time": 156.69801168569722
}
#Debug simulation 
Total elapsed time: 91.24093398684636. Arrivals time: 0.35522908717393875 Scheduler time: 90.64647430926561 Scheduler overhead time: 0.09205276425927877 Adapter cache time: 0.021017789375036955 Engine time: 0.09063408384099603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 80.68357675801963,
    "estimated_duration": 3600.0623245492393,
    "input_throughput": 6548.071359556702,
    "output_throughput": 5748.495202118812,
    "total_throughput": 12296.566561675514,
    "itl": 93.84486306477108,
    "ttft": 797750.9940005863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0527196325338295,
    "arrivals": 109355,
    "finished_requests": 94671,
    "scheduler_time": 151.76674252282257
}
#Debug simulation 
Total elapsed time: 80.6837183306925. Arrivals time: 0.3481023362837732 Scheduler time: 80.10506710922346 Scheduler overhead time: 0.08845112239941955 Adapter cache time: 0.02029949938878417 Engine time: 0.0873174057342112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_192_slots_32_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 81.17604155978188,
    "estimated_duration": 3600.115892093062,
    "input_throughput": 6501.824302770231,
    "output_throughput": 5711.618074618489,
    "total_throughput": 12213.44237738872,
    "itl": 93.84532036055677,
    "ttft": 806207.1954537721,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 655,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1684220802783996,
    "arrivals": 109355,
    "finished_requests": 94030,
    "scheduler_time": 152.60085746024086
}
#Debug simulation 
Total elapsed time: 81.17617845581844. Arrivals time: 0.34566024551168084 Scheduler time: 80.5985467559658 Scheduler overhead time: 0.08923420310020447 Adapter cache time: 0.02080027060583234 Engine time: 0.086939109954983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_192_slots_32_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_192_slots_32_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 80.68566264538094,
    "estimated_duration": 3600.0482250419213,
    "input_throughput": 6548.471722132411,
    "output_throughput": 5748.376606749211,
    "total_throughput": 12296.848328881622,
    "itl": 93.85518957984945,
    "ttft": 796987.2041912143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9644630437506965,
    "arrivals": 109355,
    "finished_requests": 94690,
    "scheduler_time": 151.76393982214577
}
#Debug simulation 
Total elapsed time: 80.68579128198326. Arrivals time: 0.33028510585427284 Scheduler time: 80.12684703245759 Scheduler overhead time: 0.08807390322908759 Adapter cache time: 0.0204651509411633 Engine time: 0.08611299470067024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_192_slots_32_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_192_slots_32_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [64 64 64]
Adapter prompts. [4320, 270, 4320, 270, 4320, 270, 4320, 270, 270, 270, 540, 540, 540, 540, 4320, 540, 270, 4320, 270, 540, 540, 270, 4320, 4320, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 270, 540, 540, 4320, 540, 270, 270, 540, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 4320, 4320, 270, 540, 4320, 270, 540, 270, 540, 270, 270, 540, 4320, 4320, 270, 270, 270, 4320, 4320, 4320, 540, 270, 270, 270, 270, 270, 4320, 270, 540, 270, 4320, 4320, 270, 4320, 270, 540, 540, 4320, 270, 270, 270, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 270, 4320, 540, 540, 540, 4320, 540, 270, 540, 4320, 4320, 540, 4320, 540, 270, 4320, 540, 270, 540, 540, 270, 540, 4320, 270, 540, 4320, 270, 540, 4320, 4320, 540, 270, 4320, 540, 270, 270, 540, 4320, 4320, 4320, 270, 4320, 270, 4320, 270, 540, 270, 4320, 4320, 540, 4320, 270, 4320, 4320, 4320, 270, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 270, 4320, 540, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 328320 . Total input tokens: 73268759 . Total output tokens: 65629823
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 80.67399276094511,
    "estimated_duration": 3600.087727869882,
    "input_throughput": 6553.302803528987,
    "output_throughput": 5750.590420264407,
    "total_throughput": 12303.893223793393,
    "itl": 93.98123867598535,
    "ttft": 794312.2560708749,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 657,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2009974408522255,
    "arrivals": 109355,
    "finished_requests": 94758,
    "scheduler_time": 151.70846738936214
}
#Debug simulation 
Total elapsed time: 80.67412235587835. Arrivals time: 0.3338646562770009 Scheduler time: 80.10965894954279 Scheduler overhead time: 0.08842487586662173 Adapter cache time: 0.020634016022086143 Engine time: 0.08713994873687625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 84.47216115891933,
    "estimated_duration": 3600.0568279887784,
    "input_throughput": 6220.729580124786,
    "output_throughput": 5518.485665432925,
    "total_throughput": 11739.21524555771,
    "itl": 86.69172317285246,
    "ttft": 792389.3352374205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.231095565327019,
    "arrivals": 106484,
    "finished_requests": 90172,
    "scheduler_time": 148.51393986677837
}
#Debug simulation 
Total elapsed time: 84.47229019878432. Arrivals time: 0.3370736353099346 Scheduler time: 83.89195440337062 Scheduler overhead time: 0.09434382477775216 Adapter cache time: 0.021811355371028185 Engine time: 0.09075637580826879 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 64.18295842409134,
    "estimated_duration": 3600.043004219837,
    "input_throughput": 6551.060354655651,
    "output_throughput": 5790.671660189701,
    "total_throughput": 12341.732014845353,
    "itl": 94.94014647957579,
    "ttft": 663439.8580774795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.584266757017474,
    "arrivals": 106484,
    "finished_requests": 94985,
    "scheduler_time": 139.8994237688599
}
#Debug simulation 
Total elapsed time: 64.183089022059. Arrivals time: 0.3289802470244467 Scheduler time: 63.632288068532944 Scheduler overhead time: 0.08490760903805494 Adapter cache time: 0.020782966166734695 Engine time: 0.08255863096565008 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 68.4148605116643,
    "estimated_duration": 3600.008422647016,
    "input_throughput": 6501.652844131409,
    "output_throughput": 5781.603417665341,
    "total_throughput": 12283.25626179675,
    "itl": 94.06561992543328,
    "ttft": 682006.8988610859,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 750,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4519373788312024,
    "arrivals": 106484,
    "finished_requests": 94329,
    "scheduler_time": 141.306715726115
}
#Debug simulation 
Total elapsed time: 68.41503236396238. Arrivals time: 0.3286217604763806 Scheduler time: 67.85964474827051 Scheduler overhead time: 0.08658325392752886 Adapter cache time: 0.021115568466484547 Engine time: 0.08472582930698991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 85.33059668820351,
    "estimated_duration": 3600.0204083570625,
    "input_throughput": 6252.871219214304,
    "output_throughput": 5525.292566071233,
    "total_throughput": 11778.163785285537,
    "itl": 87.56987978601968,
    "ttft": 811660.0865797445,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 673,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.105872176391989,
    "arrivals": 106484,
    "finished_requests": 90633,
    "scheduler_time": 151.15141770470075
}
#Debug simulation 
Total elapsed time: 85.33073888812214. Arrivals time: 0.32486682618036866 Scheduler time: 84.7617659913376 Scheduler overhead time: 0.09447646653279662 Adapter cache time: 0.02176249260082841 Engine time: 0.09166994597762823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 64.40217023622245,
    "estimated_duration": 3600.128542890833,
    "input_throughput": 6551.168303857747,
    "output_throughput": 5790.7862876639965,
    "total_throughput": 12341.954591521742,
    "itl": 94.94554187523019,
    "ttft": 663313.435172301,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 792,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.621063822153957,
    "arrivals": 106484,
    "finished_requests": 94992,
    "scheduler_time": 139.9002424431866
}
#Debug simulation 
Total elapsed time: 64.40229751821607. Arrivals time: 0.3143549747765064 Scheduler time: 63.865625550970435 Scheduler overhead time: 0.08502847608178854 Adapter cache time: 0.020803153049200773 Engine time: 0.08316277619451284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 66.4899734808132,
    "estimated_duration": 3600.10727609708,
    "input_throughput": 6506.456947970223,
    "output_throughput": 5752.3946959855975,
    "total_throughput": 12258.85164395582,
    "itl": 93.77678292664491,
    "ttft": 664939.3605139821,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 767,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2933685761899243,
    "arrivals": 106484,
    "finished_requests": 94318,
    "scheduler_time": 138.46134544768492
}
#Debug simulation 
Total elapsed time: 66.4901050911285. Arrivals time: 0.3319112262688577 Scheduler time: 65.932910121046 Scheduler overhead time: 0.08603064203634858 Adapter cache time: 0.021163858473300934 Engine time: 0.08407174330204725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_192_slots_32_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [64 64 64]
Adapter prompts. [4320, 135, 4320, 135, 4320, 135, 4320, 135, 135, 135, 540, 540, 540, 540, 4320, 540, 135, 4320, 135, 540, 540, 135, 4320, 4320, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 135, 540, 540, 4320, 540, 135, 135, 540, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 4320, 4320, 135, 540, 4320, 135, 540, 135, 540, 135, 135, 540, 4320, 4320, 135, 135, 135, 4320, 4320, 4320, 540, 135, 135, 135, 135, 135, 4320, 135, 540, 135, 4320, 4320, 135, 4320, 135, 540, 540, 4320, 135, 135, 135, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 135, 4320, 540, 540, 540, 4320, 540, 135, 540, 4320, 4320, 540, 4320, 540, 135, 4320, 540, 135, 540, 540, 135, 540, 4320, 135, 540, 4320, 135, 540, 4320, 4320, 540, 135, 4320, 540, 135, 135, 540, 4320, 4320, 4320, 135, 4320, 135, 4320, 135, 540, 135, 4320, 4320, 540, 4320, 135, 4320, 4320, 4320, 135, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 135, 4320, 540, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 319680 . Total input tokens: 71334936 . Total output tokens: 63900430
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 68.61878581810743,
    "estimated_duration": 3600.0333756912933,
    "input_throughput": 6490.058997165121,
    "output_throughput": 5772.933145657076,
    "total_throughput": 12262.992142822197,
    "itl": 93.89135035575033,
    "ttft": 687384.2048716577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 754,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.528165857568396,
    "arrivals": 106484,
    "finished_requests": 94154,
    "scheduler_time": 141.65691401073158
}
#Debug simulation 
Total elapsed time: 68.61892408505082. Arrivals time: 0.3377922046929598 Scheduler time: 68.05631463881582 Scheduler overhead time: 0.08544803457334638 Adapter cache time: 0.020673250779509544 Engine time: 0.0849216915667057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 70.15315044019371,
    "estimated_duration": 3600.0566161608645,
    "input_throughput": 6466.944685116501,
    "output_throughput": 5654.116357121883,
    "total_throughput": 12121.061042238385,
    "itl": 88.94107132119403,
    "ttft": 656126.5658359411,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2770028814860117,
    "arrivals": 105023,
    "finished_requests": 93540,
    "scheduler_time": 138.25821132010546
}
#Debug simulation 
Total elapsed time: 70.15328839421272. Arrivals time: 0.37596797943115234 Scheduler time: 69.53704241570085 Scheduler overhead time: 0.0915456535294652 Adapter cache time: 0.02293128240853548 Engine time: 0.09026616672053933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 72.24785159202293,
    "estimated_duration": 3600.0138883579907,
    "input_throughput": 6478.587506404843,
    "output_throughput": 5682.064190405111,
    "total_throughput": 12160.651696809953,
    "itl": 88.90910112665638,
    "ttft": 649132.1964555561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 751,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.44614807282575,
    "arrivals": 105023,
    "finished_requests": 93643,
    "scheduler_time": 138.5215586733703
}
#Debug simulation 
Total elapsed time: 72.24799216305837. Arrivals time: 0.35053858859464526 Scheduler time: 71.66159946098924 Scheduler overhead time: 0.0908317444846034 Adapter cache time: 0.02200867375358939 Engine time: 0.08812940772622824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 69.37971863197163,
    "estimated_duration": 3600.13284587893,
    "input_throughput": 6468.874899063429,
    "output_throughput": 5656.295162360464,
    "total_throughput": 12125.170061423893,
    "itl": 88.8767437127583,
    "ttft": 654873.7769259653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 746,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.435379086006429,
    "arrivals": 105023,
    "finished_requests": 93573,
    "scheduler_time": 138.17181552653682
}
#Debug simulation 
Total elapsed time: 69.37984998291358. Arrivals time: 0.32456966349855065 Scheduler time: 68.81972799031064 Scheduler overhead time: 0.09045073622837663 Adapter cache time: 0.02145906863734126 Engine time: 0.088488913141191 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 69.35269271908328,
    "estimated_duration": 3600.047847185394,
    "input_throughput": 6466.960159488421,
    "output_throughput": 5653.966520448801,
    "total_throughput": 12120.926679937222,
    "itl": 88.94173239646798,
    "ttft": 656129.2100801974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3218431051960016,
    "arrivals": 105023,
    "finished_requests": 93539,
    "scheduler_time": 138.25587466660036
}
#Debug simulation 
Total elapsed time: 69.35285705374554. Arrivals time: 0.33461382519453764 Scheduler time: 68.78503365023062 Scheduler overhead time: 0.08914499823004007 Adapter cache time: 0.02121077012270689 Engine time: 0.08771912241354585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-32/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 67.95643299398944,
    "estimated_duration": 3600.0982170623993,
    "input_throughput": 6477.449945526253,
    "output_throughput": 5664.613232869066,
    "total_throughput": 12142.06317839532,
    "itl": 90.94665617640888,
    "ttft": 643883.476657142,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 747,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4716730427183236,
    "arrivals": 105023,
    "finished_requests": 93694,
    "scheduler_time": 137.85835266161797
}
#Debug simulation 
Total elapsed time: 67.95656522503123. Arrivals time: 0.32869056053459644 Scheduler time: 67.39410639554262 Scheduler overhead time: 0.08959972066804767 Adapter cache time: 0.021719359792768955 Engine time: 0.08739869808778167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-16/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 71.2059594579041,
    "estimated_duration": 3600.121388896579,
    "input_throughput": 6487.677629991982,
    "output_throughput": 5675.171138121568,
    "total_throughput": 12162.84876811355,
    "itl": 88.41369119207482,
    "ttft": 641348.05201385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 761,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.275428274420512,
    "arrivals": 105023,
    "finished_requests": 93815,
    "scheduler_time": 137.79664528926165
}
#Debug simulation 
Total elapsed time: 71.2060877289623. Arrivals time: 0.3171157240867615 Scheduler time: 70.6535914116539 Scheduler overhead time: 0.09113635635003448 Adapter cache time: 0.021526374854147434 Engine time: 0.08798896754160523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_16-16-32/adapters_192_slots_32_rate_0.4-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [64 64 64]
Adapter prompts. [4320, 66, 4320, 66, 4320, 66, 4320, 66, 66, 66, 540, 540, 540, 540, 4320, 540, 66, 4320, 66, 540, 540, 66, 4320, 4320, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 66, 540, 540, 4320, 540, 66, 66, 540, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 4320, 4320, 66, 540, 4320, 66, 540, 66, 540, 66, 66, 540, 4320, 4320, 66, 66, 66, 4320, 4320, 4320, 540, 66, 66, 66, 66, 66, 4320, 66, 540, 66, 4320, 4320, 66, 4320, 66, 540, 540, 4320, 66, 66, 66, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 66, 4320, 540, 540, 540, 4320, 540, 66, 540, 4320, 4320, 540, 4320, 540, 66, 4320, 540, 66, 540, 540, 66, 540, 4320, 66, 540, 4320, 66, 540, 4320, 4320, 540, 66, 4320, 540, 66, 66, 540, 4320, 4320, 4320, 66, 4320, 66, 4320, 66, 540, 66, 4320, 4320, 540, 4320, 66, 4320, 4320, 4320, 66, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 66, 4320, 540, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 315264 . Total input tokens: 70341685 . Total output tokens: 63018840
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 75.95600991789252,
    "estimated_duration": 3600.0639588446406,
    "input_throughput": 6360.0406164306405,
    "output_throughput": 5570.034929722579,
    "total_throughput": 11930.07554615322,
    "itl": 85.8063280172794,
    "ttft": 685795.5684016523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 717,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3988876140490265,
    "arrivals": 105023,
    "finished_requests": 91976,
    "scheduler_time": 140.1886661084896
}
#Debug simulation 
Total elapsed time: 75.95614487072453. Arrivals time: 0.3240116210654378 Scheduler time: 75.38851397158578 Scheduler overhead time: 0.0937768560834229 Adapter cache time: 0.021610647905617952 Engine time: 0.09178908169269562 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-8/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 60.300286589190364,
    "estimated_duration": 3600.121978755048,
    "input_throughput": 6568.762152936216,
    "output_throughput": 5751.456790128063,
    "total_throughput": 12320.21894306428,
    "itl": 94.51629317399085,
    "ttft": 571234.0957302784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 737,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2555794672784817,
    "arrivals": 104293,
    "finished_requests": 95040,
    "scheduler_time": 134.23044457011
}
#Debug simulation 
Total elapsed time: 60.30041460925713. Arrivals time: 0.32171386713162065 Scheduler time: 59.75660867942497 Scheduler overhead time: 0.08524915156885982 Adapter cache time: 0.020249024033546448 Engine time: 0.08271477883681655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-16/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 67.14166382327676,
    "estimated_duration": 3600.017606600863,
    "input_throughput": 6486.997718338993,
    "output_throughput": 5683.671369407434,
    "total_throughput": 12170.669087746426,
    "itl": 90.2954280047003,
    "ttft": 608489.3647488952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 708,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3085868131392693,
    "arrivals": 104293,
    "finished_requests": 93811,
    "scheduler_time": 135.04262089747462
}
#Debug simulation 
Total elapsed time: 67.14179506711662. Arrivals time: 0.3317998731508851 Scheduler time: 66.57728204550222 Scheduler overhead time: 0.08994515938684344 Adapter cache time: 0.02111905300989747 Engine time: 0.08683660067617893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-8-32/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 66.53111177077517,
    "estimated_duration": 3600.015517399614,
    "input_throughput": 6487.103704727632,
    "output_throughput": 5683.672167829915,
    "total_throughput": 12170.775872557548,
    "itl": 90.29748407005347,
    "ttft": 608466.0385747746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 708,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.312980419863015,
    "arrivals": 104293,
    "finished_requests": 93812,
    "scheduler_time": 135.04167041906908
}
#Debug simulation 
Total elapsed time: 66.53124284278601. Arrivals time: 0.31097786454483867 Scheduler time: 65.98718798719347 Scheduler overhead time: 0.09040024038404226 Adapter cache time: 0.021140798926353455 Engine time: 0.08692809287458658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-16/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 70.42298818612471,
    "estimated_duration": 3600.0381870861447,
    "input_throughput": 6486.960911629125,
    "output_throughput": 5683.753598336588,
    "total_throughput": 12170.714509965712,
    "itl": 90.29154762803019,
    "ttft": 608463.3451991455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 708,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2146098898979676,
    "arrivals": 104293,
    "finished_requests": 93812,
    "scheduler_time": 135.0470649602035
}
#Debug simulation 
Total elapsed time: 70.4231114750728. Arrivals time: 0.46859607892110944 Scheduler time: 69.70503682643175 Scheduler overhead time: 0.09337448421865702 Adapter cache time: 0.024392024613916874 Engine time: 0.0950950407423079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 32,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.003125_size_8-16-32/adapters_192_slots_32_rate_0.4-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.4     ]. Counts: [64 64 64]
Adapter prompts. [4320, 33, 4320, 33, 4320, 33, 4320, 33, 33, 33, 540, 540, 540, 540, 4320, 540, 33, 4320, 33, 540, 540, 33, 4320, 4320, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 33, 540, 540, 4320, 540, 33, 33, 540, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 4320, 4320, 33, 540, 4320, 33, 540, 33, 540, 33, 33, 540, 4320, 4320, 33, 33, 33, 4320, 4320, 4320, 540, 33, 33, 33, 33, 33, 4320, 33, 540, 33, 4320, 4320, 33, 4320, 33, 540, 540, 4320, 33, 33, 33, 4320, 4320, 540, 4320, 540, 4320, 4320, 540, 540, 33, 4320, 540, 540, 540, 4320, 540, 33, 540, 4320, 4320, 540, 4320, 540, 33, 4320, 540, 33, 540, 540, 33, 540, 4320, 33, 540, 4320, 33, 540, 4320, 4320, 540, 33, 4320, 540, 33, 33, 540, 4320, 4320, 4320, 33, 4320, 33, 4320, 33, 540, 33, 4320, 4320, 540, 4320, 33, 4320, 4320, 4320, 33, 4320, 4320, 4320, 540, 540, 540, 540, 540, 540, 33, 4320, 540, 4320, 33, 4320, 540, 33, 540, 33, 33, 540, 540, 540, 4320, 540, 33, 33]
Prompts retrieved: 313152 . Total input tokens: 69892023 . Total output tokens: 62596468
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 63.75884819868952,
    "estimated_duration": 3600.0517176248914,
    "input_throughput": 6518.379690245633,
    "output_throughput": 5720.182546040215,
    "total_throughput": 12238.562236285848,
    "itl": 91.26032483833268,
    "ttft": 584781.9261215965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 746,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4677492473647025,
    "arrivals": 104293,
    "finished_requests": 94285,
    "scheduler_time": 133.7385592472908
}
#Debug simulation 
Total elapsed time: 63.759001596830785. Arrivals time: 0.32452288549393415 Scheduler time: 63.20520925382152 Scheduler overhead time: 0.0884151291102171 Adapter cache time: 0.02074197167530656 Engine time: 0.08554106717929244 
