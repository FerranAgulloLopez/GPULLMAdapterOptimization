INFO 06-01 00:47:10 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:10 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_192_slots_96_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [64 64 64]
Adapter prompts. [270, 66, 270, 66, 270, 66, 270, 66, 66, 66, 135, 135, 135, 135, 270, 135, 66, 270, 66, 135, 135, 66, 270, 270, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 270, 135, 270, 66, 135, 135, 270, 135, 66, 66, 135, 270, 66, 135, 66, 270, 270, 135, 270, 270, 270, 66, 135, 270, 66, 135, 66, 135, 66, 66, 135, 270, 270, 66, 66, 66, 270, 270, 270, 135, 66, 66, 66, 66, 66, 270, 66, 135, 66, 270, 270, 66, 270, 66, 135, 135, 270, 66, 66, 66, 270, 270, 135, 270, 135, 270, 270, 135, 135, 66, 270, 135, 135, 135, 270, 135, 66, 135, 270, 270, 135, 270, 135, 66, 270, 135, 66, 135, 135, 66, 135, 270, 66, 135, 270, 66, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 270, 270, 66, 270, 66, 270, 66, 135, 66, 270, 270, 135, 270, 66, 270, 270, 270, 66, 270, 270, 270, 135, 135, 135, 135, 135, 135, 66, 270, 135, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 30144 . Total input tokens: 6607829 . Total output tokens: 6040768
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 1.2628833469934762,
    "estimated_duration": 3599.2332321865533,
    "input_throughput": 696.3599851175445,
    "output_throughput": 615.3755139270173,
    "total_throughput": 1311.735499044562,
    "itl": 20.875068188886715,
    "ttft": 4637.385362942726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4038,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.352247991449202,
    "arrivals": 10170,
    "finished_requests": 10157,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.262973977252841. Arrivals time: 0.03841297701001167 Scheduler time: 0.7771781603805721 Scheduler overhead time: 0.14449994545429945 Adapter cache time: 0.08675394486635923 Engine time: 0.14524269104003906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_192_slots_96_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [64 64 64]
Adapter prompts. [270, 66, 270, 66, 270, 66, 270, 66, 66, 66, 135, 135, 135, 135, 270, 135, 66, 270, 66, 135, 135, 66, 270, 270, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 270, 135, 270, 66, 135, 135, 270, 135, 66, 66, 135, 270, 66, 135, 66, 270, 270, 135, 270, 270, 270, 66, 135, 270, 66, 135, 66, 135, 66, 66, 135, 270, 270, 66, 66, 66, 270, 270, 270, 135, 66, 66, 66, 66, 66, 270, 66, 135, 66, 270, 270, 66, 270, 66, 135, 135, 270, 66, 66, 66, 270, 270, 135, 270, 135, 270, 270, 135, 135, 66, 270, 135, 135, 135, 270, 135, 66, 135, 270, 270, 135, 270, 135, 66, 270, 135, 66, 135, 135, 66, 135, 270, 66, 135, 270, 66, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 270, 270, 66, 270, 66, 270, 66, 135, 66, 270, 270, 135, 270, 66, 270, 270, 270, 66, 270, 270, 270, 135, 135, 135, 135, 135, 135, 66, 270, 135, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 30144 . Total input tokens: 6607829 . Total output tokens: 6040768
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 1.2328890077769756,
    "estimated_duration": 3599.2390968757018,
    "input_throughput": 696.3588504513725,
    "output_throughput": 615.3745112189445,
    "total_throughput": 1311.733361670317,
    "itl": 20.866588030529677,
    "ttft": 4637.222609462303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4039,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.076813141110705,
    "arrivals": 10170,
    "finished_requests": 10157,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2329974896274507. Arrivals time: 0.03783012367784977 Scheduler time: 0.7590376301668584 Scheduler overhead time: 0.141525661572814 Adapter cache time: 0.08524515246972442 Engine time: 0.1395732145756483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_192_slots_96_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [64 64 64]
Adapter prompts. [270, 66, 270, 66, 270, 66, 270, 66, 66, 66, 135, 135, 135, 135, 270, 135, 66, 270, 66, 135, 135, 66, 270, 270, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 270, 135, 270, 66, 135, 135, 270, 135, 66, 66, 135, 270, 66, 135, 66, 270, 270, 135, 270, 270, 270, 66, 135, 270, 66, 135, 66, 135, 66, 66, 135, 270, 270, 66, 66, 66, 270, 270, 270, 135, 66, 66, 66, 66, 66, 270, 66, 135, 66, 270, 270, 66, 270, 66, 135, 135, 270, 66, 66, 66, 270, 270, 135, 270, 135, 270, 270, 135, 135, 66, 270, 135, 135, 135, 270, 135, 66, 135, 270, 270, 135, 270, 135, 66, 270, 135, 66, 135, 135, 66, 135, 270, 66, 135, 270, 66, 135, 270, 270, 135, 66, 270, 135, 66, 66, 135, 270, 270, 270, 66, 270, 66, 270, 66, 135, 66, 270, 270, 135, 270, 66, 270, 270, 270, 66, 270, 270, 270, 135, 135, 135, 135, 135, 135, 66, 270, 135, 270, 66, 270, 135, 66, 135, 66, 66, 135, 135, 135, 270, 135, 66, 66]
Prompts retrieved: 30144 . Total input tokens: 6607829 . Total output tokens: 6040768
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 1.2387218810617924,
    "estimated_duration": 3599.224158834946,
    "input_throughput": 696.3617405844761,
    "output_throughput": 615.3770652386784,
    "total_throughput": 1311.7388058231545,
    "itl": 20.87575196192603,
    "ttft": 4637.389486954138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4037,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.515953993386638,
    "arrivals": 10170,
    "finished_requests": 10157,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2387959696352482. Arrivals time: 0.03725074091926217 Scheduler time: 0.7642367146909237 Scheduler overhead time: 0.1422115215100348 Adapter cache time: 0.08523560827597976 Engine time: 0.1396483276039362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 135, 135, 135, 135, 270, 135, 33, 270, 33, 135, 135, 33, 270, 270, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 33, 135, 135, 270, 135, 33, 33, 135, 270, 33, 135, 33, 270, 270, 135, 270, 270, 270, 33, 135, 270, 33, 135, 33, 135, 33, 33, 135, 270, 270, 33, 33, 33, 270, 270, 270, 135, 33, 33, 33, 33, 33, 270, 33, 135, 33, 270, 270, 33, 270, 33, 135, 135, 270, 33, 33, 33, 270, 270, 135, 270, 135, 270, 270, 135, 135, 33, 270, 135, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 135, 33, 270, 135, 33, 135, 135, 33, 135, 270, 33, 135, 270, 33, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 270, 270, 33, 270, 33, 270, 33, 135, 33, 270, 270, 135, 270, 33, 270, 270, 270, 33, 270, 270, 270, 135, 135, 135, 135, 135, 135, 33, 270, 135, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 28032 . Total input tokens: 6152033 . Total output tokens: 5608060
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 1.1840979852713645,
    "estimated_duration": 3599.7793302552873,
    "input_throughput": 641.2254164024836,
    "output_throughput": 572.6448237186281,
    "total_throughput": 1213.8702401211117,
    "itl": 20.587460884329207,
    "ttft": 4986.120493738305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.588508101741153,
    "arrivals": 9452,
    "finished_requests": 9439,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1841639820486307. Arrivals time: 0.036415255162864923 Scheduler time: 0.7140974872745574 Scheduler overhead time: 0.14273124048486352 Adapter cache time: 0.0795501652173698 Engine time: 0.14012680854648352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 135, 135, 135, 135, 270, 135, 33, 270, 33, 135, 135, 33, 270, 270, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 33, 135, 135, 270, 135, 33, 33, 135, 270, 33, 135, 33, 270, 270, 135, 270, 270, 270, 33, 135, 270, 33, 135, 33, 135, 33, 33, 135, 270, 270, 33, 33, 33, 270, 270, 270, 135, 33, 33, 33, 33, 33, 270, 33, 135, 33, 270, 270, 33, 270, 33, 135, 135, 270, 33, 33, 33, 270, 270, 135, 270, 135, 270, 270, 135, 135, 33, 270, 135, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 135, 33, 270, 135, 33, 135, 135, 33, 135, 270, 33, 135, 270, 33, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 270, 270, 33, 270, 33, 270, 33, 135, 33, 270, 270, 135, 270, 33, 270, 270, 270, 33, 270, 270, 270, 135, 135, 135, 135, 135, 135, 33, 270, 135, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 28032 . Total input tokens: 6152033 . Total output tokens: 5608060
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 1.190636542160064,
    "estimated_duration": 3599.763509412509,
    "input_throughput": 641.2282345671969,
    "output_throughput": 572.6473404738816,
    "total_throughput": 1213.8755750410785,
    "itl": 20.59120624345206,
    "ttft": 4986.249919169666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.19891045163623,
    "arrivals": 9452,
    "finished_requests": 9439,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1906994711607695. Arrivals time: 0.03641476761549711 Scheduler time: 0.7188385012559593 Scheduler overhead time: 0.14309808611869812 Adapter cache time: 0.08013612916693091 Engine time: 0.14138463186100125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 135, 135, 135, 135, 270, 135, 33, 270, 33, 135, 135, 33, 270, 270, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 33, 135, 135, 270, 135, 33, 33, 135, 270, 33, 135, 33, 270, 270, 135, 270, 270, 270, 33, 135, 270, 33, 135, 33, 135, 33, 33, 135, 270, 270, 33, 33, 33, 270, 270, 270, 135, 33, 33, 33, 33, 33, 270, 33, 135, 33, 270, 270, 33, 270, 33, 135, 135, 270, 33, 33, 33, 270, 270, 135, 270, 135, 270, 270, 135, 135, 33, 270, 135, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 135, 33, 270, 135, 33, 135, 135, 33, 135, 270, 33, 135, 270, 33, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 270, 270, 33, 270, 33, 270, 33, 135, 33, 270, 270, 135, 270, 33, 270, 270, 270, 33, 270, 270, 270, 135, 135, 135, 135, 135, 135, 33, 270, 135, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 28032 . Total input tokens: 6152033 . Total output tokens: 5608060
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 1.2194004636257887,
    "estimated_duration": 3599.7689861111103,
    "input_throughput": 641.2272590007677,
    "output_throughput": 572.6464692466166,
    "total_throughput": 1213.8737282473844,
    "itl": 20.591082454724386,
    "ttft": 4986.185074256381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.221305957063606,
    "arrivals": 9452,
    "finished_requests": 9439,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2194756087847054. Arrivals time: 0.03730543423444033 Scheduler time: 0.7346907332539558 Scheduler overhead time: 0.1462517878971994 Adapter cache time: 0.08295450918376446 Engine time: 0.1458516027778387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 135, 135, 135, 135, 270, 135, 33, 270, 33, 135, 135, 33, 270, 270, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 33, 135, 135, 270, 135, 33, 33, 135, 270, 33, 135, 33, 270, 270, 135, 270, 270, 270, 33, 135, 270, 33, 135, 33, 135, 33, 33, 135, 270, 270, 33, 33, 33, 270, 270, 270, 135, 33, 33, 33, 33, 33, 270, 33, 135, 33, 270, 270, 33, 270, 33, 135, 135, 270, 33, 33, 33, 270, 270, 135, 270, 135, 270, 270, 135, 135, 33, 270, 135, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 135, 33, 270, 135, 33, 135, 135, 33, 135, 270, 33, 135, 270, 33, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 270, 270, 33, 270, 33, 270, 33, 135, 33, 270, 270, 135, 270, 33, 270, 270, 270, 33, 270, 270, 270, 135, 135, 135, 135, 135, 135, 33, 270, 135, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 28032 . Total input tokens: 6152033 . Total output tokens: 5608060
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 1.239063153974712,
    "estimated_duration": 3599.7613986926417,
    "input_throughput": 641.2286105513314,
    "output_throughput": 572.6476762456133,
    "total_throughput": 1213.8762867969447,
    "itl": 20.588164108961138,
    "ttft": 4986.100354224107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.782143226827118,
    "arrivals": 9452,
    "finished_requests": 9439,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2391428272239864. Arrivals time: 0.0374550512060523 Scheduler time: 0.7528813634999096 Scheduler overhead time: 0.14856857853010297 Adapter cache time: 0.0841721654869616 Engine time: 0.14421873353421688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 135, 135, 135, 135, 270, 135, 33, 270, 33, 135, 135, 33, 270, 270, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 33, 135, 135, 270, 135, 33, 33, 135, 270, 33, 135, 33, 270, 270, 135, 270, 270, 270, 33, 135, 270, 33, 135, 33, 135, 33, 33, 135, 270, 270, 33, 33, 33, 270, 270, 270, 135, 33, 33, 33, 33, 33, 270, 33, 135, 33, 270, 270, 33, 270, 33, 135, 135, 270, 33, 33, 33, 270, 270, 135, 270, 135, 270, 270, 135, 135, 33, 270, 135, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 135, 33, 270, 135, 33, 135, 135, 33, 135, 270, 33, 135, 270, 33, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 270, 270, 33, 270, 33, 270, 33, 135, 33, 270, 270, 135, 270, 33, 270, 270, 270, 33, 270, 270, 270, 135, 135, 135, 135, 135, 135, 33, 270, 135, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 28032 . Total input tokens: 6152033 . Total output tokens: 5608060
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 1.2286807750351727,
    "estimated_duration": 3599.7654518247805,
    "input_throughput": 641.2278885642119,
    "output_throughput": 572.6470314767438,
    "total_throughput": 1213.8749200409557,
    "itl": 20.59196296995725,
    "ttft": 4986.184265790303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.34957481920668,
    "arrivals": 9452,
    "finished_requests": 9439,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2287543611600995. Arrivals time: 0.03706980682909489 Scheduler time: 0.7498816847801208 Scheduler overhead time: 0.14289680030196905 Adapter cache time: 0.08373132674023509 Engine time: 0.14419259037822485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 135, 135, 135, 135, 270, 135, 33, 270, 33, 135, 135, 33, 270, 270, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 33, 135, 135, 270, 135, 33, 33, 135, 270, 33, 135, 33, 270, 270, 135, 270, 270, 270, 33, 135, 270, 33, 135, 33, 135, 33, 33, 135, 270, 270, 33, 33, 33, 270, 270, 270, 135, 33, 33, 33, 33, 33, 270, 33, 135, 33, 270, 270, 33, 270, 33, 135, 135, 270, 33, 33, 33, 270, 270, 135, 270, 135, 270, 270, 135, 135, 33, 270, 135, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 135, 33, 270, 135, 33, 135, 135, 33, 135, 270, 33, 135, 270, 33, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 270, 270, 33, 270, 33, 270, 33, 135, 33, 270, 270, 135, 270, 33, 270, 270, 270, 33, 270, 270, 270, 135, 135, 135, 135, 135, 135, 33, 270, 135, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 28032 . Total input tokens: 6152033 . Total output tokens: 5608060
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 1.2198453689925373,
    "estimated_duration": 3599.762861536901,
    "input_throughput": 641.2283499737246,
    "output_throughput": 572.647443537405,
    "total_throughput": 1213.8757935111296,
    "itl": 20.58542076515887,
    "ttft": 4986.158650776087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.367827573928626,
    "arrivals": 9452,
    "finished_requests": 9439,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2199105471372604. Arrivals time: 0.036723941564559937 Scheduler time: 0.7427390040829778 Scheduler overhead time: 0.14325968828052282 Adapter cache time: 0.08205970330163836 Engine time: 0.1440247460268438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 135, 135, 135, 135, 270, 135, 33, 270, 33, 135, 135, 33, 270, 270, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 33, 135, 135, 270, 135, 33, 33, 135, 270, 33, 135, 33, 270, 270, 135, 270, 270, 270, 33, 135, 270, 33, 135, 33, 135, 33, 33, 135, 270, 270, 33, 33, 33, 270, 270, 270, 135, 33, 33, 33, 33, 33, 270, 33, 135, 33, 270, 270, 33, 270, 33, 135, 135, 270, 33, 33, 33, 270, 270, 135, 270, 135, 270, 270, 135, 135, 33, 270, 135, 135, 135, 270, 135, 33, 135, 270, 270, 135, 270, 135, 33, 270, 135, 33, 135, 135, 33, 135, 270, 33, 135, 270, 33, 135, 270, 270, 135, 33, 270, 135, 33, 33, 135, 270, 270, 270, 33, 270, 33, 270, 33, 135, 33, 270, 270, 135, 270, 33, 270, 270, 270, 33, 270, 270, 270, 135, 135, 135, 135, 135, 135, 33, 270, 135, 270, 33, 270, 135, 33, 135, 33, 33, 135, 135, 135, 270, 135, 33, 33]
Prompts retrieved: 28032 . Total input tokens: 6152033 . Total output tokens: 5608060
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 1.222911992110312,
    "estimated_duration": 3599.7606837792337,
    "input_throughput": 641.2287378995003,
    "output_throughput": 572.6477899735908,
    "total_throughput": 1213.876527873091,
    "itl": 20.592809541038907,
    "ttft": 4986.206001214,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3133,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.477089158631276,
    "arrivals": 9452,
    "finished_requests": 9439,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.2229747180826962. Arrivals time: 0.03798317350447178 Scheduler time: 0.7448097602464259 Scheduler overhead time: 0.14254778530448675 Adapter cache time: 0.08331472612917423 Engine time: 0.14314495166763663 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 66, 66, 66, 66, 270, 66, 33, 270, 33, 66, 66, 33, 270, 270, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 33, 66, 66, 270, 66, 33, 33, 66, 270, 33, 66, 33, 270, 270, 66, 270, 270, 270, 33, 66, 270, 33, 66, 33, 66, 33, 33, 66, 270, 270, 33, 33, 33, 270, 270, 270, 66, 33, 33, 33, 33, 33, 270, 33, 66, 33, 270, 270, 33, 270, 33, 66, 66, 270, 33, 33, 33, 270, 270, 66, 270, 66, 270, 270, 66, 66, 33, 270, 66, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 66, 33, 270, 66, 33, 66, 66, 33, 66, 270, 33, 66, 270, 33, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 270, 270, 33, 270, 33, 270, 33, 66, 33, 270, 270, 66, 270, 33, 270, 270, 270, 33, 270, 270, 270, 66, 66, 66, 66, 66, 66, 33, 270, 66, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 23616 . Total input tokens: 5190960 . Total output tokens: 4735188
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 1.1158451517112553,
    "estimated_duration": 3598.7929256435737,
    "input_throughput": 543.8165074891087,
    "output_throughput": 487.28366322616273,
    "total_throughput": 1031.1001707152714,
    "itl": 19.85304603901833,
    "ttft": 5402.1377617297085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.745314987627988,
    "arrivals": 8047,
    "finished_requests": 8035,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.115918317809701. Arrivals time: 0.0338821723125875 Scheduler time: 0.6422413308173418 Scheduler overhead time: 0.14737979089841247 Adapter cache time: 0.07263142755255103 Engine time: 0.14684415794909 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 66, 66, 66, 66, 270, 66, 33, 270, 33, 66, 66, 33, 270, 270, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 33, 66, 66, 270, 66, 33, 33, 66, 270, 33, 66, 33, 270, 270, 66, 270, 270, 270, 33, 66, 270, 33, 66, 33, 66, 33, 33, 66, 270, 270, 33, 33, 33, 270, 270, 270, 66, 33, 33, 33, 33, 33, 270, 33, 66, 33, 270, 270, 33, 270, 33, 66, 66, 270, 33, 33, 33, 270, 270, 66, 270, 66, 270, 270, 66, 66, 33, 270, 66, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 66, 33, 270, 66, 33, 66, 66, 33, 66, 270, 33, 66, 270, 33, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 270, 270, 33, 270, 33, 270, 33, 66, 33, 270, 270, 66, 270, 33, 270, 270, 270, 33, 270, 270, 270, 66, 66, 66, 66, 66, 66, 33, 270, 66, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 23616 . Total input tokens: 5190960 . Total output tokens: 4735188
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 1.1039420268498361,
    "estimated_duration": 3598.794235332334,
    "input_throughput": 543.8163095810537,
    "output_throughput": 487.28348589178484,
    "total_throughput": 1031.0997954728384,
    "itl": 19.855944362538473,
    "ttft": 5402.159415425237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.180899680428856,
    "arrivals": 8047,
    "finished_requests": 8035,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1040310859680176. Arrivals time: 0.033622605726122856 Scheduler time: 0.6355287819169462 Scheduler overhead time: 0.14473952120169997 Adapter cache time: 0.07283856114372611 Engine time: 0.14468240970745683 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 66, 66, 66, 66, 270, 66, 33, 270, 33, 66, 66, 33, 270, 270, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 33, 66, 66, 270, 66, 33, 33, 66, 270, 33, 66, 33, 270, 270, 66, 270, 270, 270, 33, 66, 270, 33, 66, 33, 66, 33, 33, 66, 270, 270, 33, 33, 33, 270, 270, 270, 66, 33, 33, 33, 33, 33, 270, 33, 66, 33, 270, 270, 33, 270, 33, 66, 66, 270, 33, 33, 33, 270, 270, 66, 270, 66, 270, 270, 66, 66, 33, 270, 66, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 66, 33, 270, 66, 33, 66, 66, 33, 66, 270, 33, 66, 270, 33, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 270, 270, 33, 270, 33, 270, 33, 66, 33, 270, 270, 66, 270, 33, 270, 270, 270, 33, 270, 270, 270, 66, 66, 66, 66, 66, 66, 33, 270, 66, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 23616 . Total input tokens: 5190960 . Total output tokens: 4735188
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 1.1095065670087934,
    "estimated_duration": 3598.792112654563,
    "input_throughput": 543.8166303405629,
    "output_throughput": 487.2837733065038,
    "total_throughput": 1031.1004036470667,
    "itl": 19.85670783573369,
    "ttft": 5402.262543347863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.195575419776096,
    "arrivals": 8047,
    "finished_requests": 8035,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1095593119971454. Arrivals time: 0.03317124582827091 Scheduler time: 0.6406304524280131 Scheduler overhead time: 0.14518354088068008 Adapter cache time: 0.07273674570024014 Engine time: 0.14517413824796677 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 66, 66, 66, 66, 270, 66, 33, 270, 33, 66, 66, 33, 270, 270, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 33, 66, 66, 270, 66, 33, 33, 66, 270, 33, 66, 33, 270, 270, 66, 270, 270, 270, 33, 66, 270, 33, 66, 33, 66, 33, 33, 66, 270, 270, 33, 33, 33, 270, 270, 270, 66, 33, 33, 33, 33, 33, 270, 33, 66, 33, 270, 270, 33, 270, 33, 66, 66, 270, 33, 33, 33, 270, 270, 66, 270, 66, 270, 270, 66, 66, 33, 270, 66, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 66, 33, 270, 66, 33, 66, 66, 33, 66, 270, 33, 66, 270, 33, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 270, 270, 33, 270, 33, 270, 33, 66, 33, 270, 270, 66, 270, 33, 270, 270, 270, 33, 270, 270, 270, 66, 66, 66, 66, 66, 66, 33, 270, 66, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 23616 . Total input tokens: 5190960 . Total output tokens: 4735188
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 1.1280349479056895,
    "estimated_duration": 3598.799842720448,
    "input_throughput": 543.815462246041,
    "output_throughput": 487.28272664210544,
    "total_throughput": 1031.0981888881463,
    "itl": 19.854834096812066,
    "ttft": 5402.078079936765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.880990716693694,
    "arrivals": 8047,
    "finished_requests": 8035,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1281078420579433. Arrivals time: 0.03401442850008607 Scheduler time: 0.6546137291006744 Scheduler overhead time: 0.14490148006007075 Adapter cache time: 0.07402358157560229 Engine time: 0.14779880596324801 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 66, 66, 66, 66, 270, 66, 33, 270, 33, 66, 66, 33, 270, 270, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 33, 66, 66, 270, 66, 33, 33, 66, 270, 33, 66, 33, 270, 270, 66, 270, 270, 270, 33, 66, 270, 33, 66, 33, 66, 33, 33, 66, 270, 270, 33, 33, 33, 270, 270, 270, 66, 33, 33, 33, 33, 33, 270, 33, 66, 33, 270, 270, 33, 270, 33, 66, 66, 270, 33, 33, 33, 270, 270, 66, 270, 66, 270, 270, 66, 66, 33, 270, 66, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 66, 33, 270, 66, 33, 66, 66, 33, 66, 270, 33, 66, 270, 33, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 270, 270, 33, 270, 33, 270, 33, 66, 33, 270, 270, 66, 270, 33, 270, 270, 270, 33, 270, 270, 270, 66, 66, 66, 66, 66, 66, 33, 270, 66, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 23616 . Total input tokens: 5190960 . Total output tokens: 4735188
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 1.1008292636834085,
    "estimated_duration": 3598.8104855334454,
    "input_throughput": 543.8138540129058,
    "output_throughput": 487.2812855940265,
    "total_throughput": 1031.0951396069324,
    "itl": 19.856289170410026,
    "ttft": 5402.349107305812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.287878699004485,
    "arrivals": 8047,
    "finished_requests": 8035,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1008982779458165. Arrivals time: 0.033395049162209034 Scheduler time: 0.6332712601870298 Scheduler overhead time: 0.14493594551458955 Adapter cache time: 0.07203115988522768 Engine time: 0.14503044774755836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 66, 66, 66, 66, 270, 66, 33, 270, 33, 66, 66, 33, 270, 270, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 33, 66, 66, 270, 66, 33, 33, 66, 270, 33, 66, 33, 270, 270, 66, 270, 270, 270, 33, 66, 270, 33, 66, 33, 66, 33, 33, 66, 270, 270, 33, 33, 33, 270, 270, 270, 66, 33, 33, 33, 33, 33, 270, 33, 66, 33, 270, 270, 33, 270, 33, 66, 66, 270, 33, 33, 33, 270, 270, 66, 270, 66, 270, 270, 66, 66, 33, 270, 66, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 66, 33, 270, 66, 33, 66, 66, 33, 66, 270, 33, 66, 270, 33, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 270, 270, 33, 270, 33, 270, 33, 66, 33, 270, 270, 66, 270, 33, 270, 270, 270, 33, 270, 270, 270, 66, 66, 66, 66, 66, 66, 33, 270, 66, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 23616 . Total input tokens: 5190960 . Total output tokens: 4735188
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 1.1131923380307853,
    "estimated_duration": 3598.791160398469,
    "input_throughput": 543.816774236854,
    "output_throughput": 487.2839022439503,
    "total_throughput": 1031.1006764808044,
    "itl": 19.85274070748998,
    "ttft": 5402.161942372131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.590070849964198,
    "arrivals": 8047,
    "finished_requests": 8035,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1132461591623724. Arrivals time: 0.03378400253131986 Scheduler time: 0.641922113019973 Scheduler overhead time: 0.14550162805244327 Adapter cache time: 0.07294848887249827 Engine time: 0.14643356669694185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [64 64 64]
Adapter prompts. [270, 33, 270, 33, 270, 33, 270, 33, 33, 33, 66, 66, 66, 66, 270, 66, 33, 270, 33, 66, 66, 33, 270, 270, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 33, 66, 66, 270, 66, 33, 33, 66, 270, 33, 66, 33, 270, 270, 66, 270, 270, 270, 33, 66, 270, 33, 66, 33, 66, 33, 33, 66, 270, 270, 33, 33, 33, 270, 270, 270, 66, 33, 33, 33, 33, 33, 270, 33, 66, 33, 270, 270, 33, 270, 33, 66, 66, 270, 33, 33, 33, 270, 270, 66, 270, 66, 270, 270, 66, 66, 33, 270, 66, 66, 66, 270, 66, 33, 66, 270, 270, 66, 270, 66, 33, 270, 66, 33, 66, 66, 33, 66, 270, 33, 66, 270, 33, 66, 270, 270, 66, 33, 270, 66, 33, 33, 66, 270, 270, 270, 33, 270, 33, 270, 33, 66, 33, 270, 270, 66, 270, 33, 270, 270, 270, 33, 270, 270, 270, 66, 66, 66, 66, 66, 66, 33, 270, 66, 270, 33, 270, 66, 33, 66, 33, 33, 66, 66, 66, 270, 66, 33, 33]
Prompts retrieved: 23616 . Total input tokens: 5190960 . Total output tokens: 4735188
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 1.1057443036697805,
    "estimated_duration": 3598.807070304818,
    "input_throughput": 543.8143700863175,
    "output_throughput": 487.2817480186477,
    "total_throughput": 1031.0961181049652,
    "itl": 19.857624559043284,
    "ttft": 5402.18946306281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2204,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.3774153949317585,
    "arrivals": 8047,
    "finished_requests": 8035,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.1058263410814106. Arrivals time: 0.03339071152731776 Scheduler time: 0.6377655435353518 Scheduler overhead time: 0.14486275520175695 Adapter cache time: 0.07214420475065708 Engine time: 0.14450343931093812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [64 64 64]
Adapter prompts. [135, 33, 135, 33, 135, 33, 135, 33, 33, 33, 66, 66, 66, 66, 135, 66, 33, 135, 33, 66, 66, 33, 135, 135, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 33, 66, 66, 135, 66, 33, 33, 66, 135, 33, 66, 33, 135, 135, 66, 135, 135, 135, 33, 66, 135, 33, 66, 33, 66, 33, 33, 66, 135, 135, 33, 33, 33, 135, 135, 135, 66, 33, 33, 33, 33, 33, 135, 33, 66, 33, 135, 135, 33, 135, 33, 66, 66, 135, 33, 33, 33, 135, 135, 66, 135, 66, 135, 135, 66, 66, 33, 135, 66, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 66, 33, 135, 66, 33, 66, 66, 33, 66, 135, 33, 66, 135, 33, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 135, 135, 33, 135, 33, 135, 33, 66, 33, 135, 135, 66, 135, 33, 135, 135, 135, 33, 135, 135, 135, 66, 66, 66, 66, 66, 66, 33, 135, 66, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 14976 . Total input tokens: 3282180 . Total output tokens: 3038664
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 0.9177972041070461,
    "estimated_duration": 3599.932303962939,
    "input_throughput": 340.32445517136944,
    "output_throughput": 312.1611477979529,
    "total_throughput": 652.4856029693224,
    "itl": 18.969705531951067,
    "ttft": 7821.565903874973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.176064267256476,
    "arrivals": 5084,
    "finished_requests": 5073,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9178578020073473. Arrivals time: 0.02722188364714384 Scheduler time: 0.4589479989372194 Scheduler overhead time: 0.14844988100230694 Adapter cache time: 0.05702950432896614 Engine time: 0.15107525140047073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [64 64 64]
Adapter prompts. [135, 33, 135, 33, 135, 33, 135, 33, 33, 33, 66, 66, 66, 66, 135, 66, 33, 135, 33, 66, 66, 33, 135, 135, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 33, 66, 66, 135, 66, 33, 33, 66, 135, 33, 66, 33, 135, 135, 66, 135, 135, 135, 33, 66, 135, 33, 66, 33, 66, 33, 33, 66, 135, 135, 33, 33, 33, 135, 135, 135, 66, 33, 33, 33, 33, 33, 135, 33, 66, 33, 135, 135, 33, 135, 33, 66, 66, 135, 33, 33, 33, 135, 135, 66, 135, 66, 135, 135, 66, 66, 33, 135, 66, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 66, 33, 135, 66, 33, 66, 66, 33, 66, 135, 33, 66, 135, 33, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 135, 135, 33, 135, 33, 135, 33, 66, 33, 135, 135, 66, 135, 33, 135, 135, 135, 33, 135, 135, 135, 66, 66, 66, 66, 66, 66, 33, 135, 66, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 14976 . Total input tokens: 3282180 . Total output tokens: 3038664
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 0.9233528790064156,
    "estimated_duration": 3599.9299194235477,
    "input_throughput": 340.32468059716587,
    "output_throughput": 312.1613545687984,
    "total_throughput": 652.4860351659643,
    "itl": 18.97197192938145,
    "ttft": 7821.553248328822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.575718887364268,
    "arrivals": 5084,
    "finished_requests": 5073,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9234125087969005. Arrivals time: 0.027227106504142284 Scheduler time: 0.4596830727532506 Scheduler overhead time: 0.15154962334781885 Adapter cache time: 0.0568249081261456 Engine time: 0.15130295744165778 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [64 64 64]
Adapter prompts. [135, 33, 135, 33, 135, 33, 135, 33, 33, 33, 66, 66, 66, 66, 135, 66, 33, 135, 33, 66, 66, 33, 135, 135, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 33, 66, 66, 135, 66, 33, 33, 66, 135, 33, 66, 33, 135, 135, 66, 135, 135, 135, 33, 66, 135, 33, 66, 33, 66, 33, 33, 66, 135, 135, 33, 33, 33, 135, 135, 135, 66, 33, 33, 33, 33, 33, 135, 33, 66, 33, 135, 135, 33, 135, 33, 66, 66, 135, 33, 33, 33, 135, 135, 66, 135, 66, 135, 135, 66, 66, 33, 135, 66, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 66, 33, 135, 66, 33, 66, 66, 33, 66, 135, 33, 66, 135, 33, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 135, 135, 33, 135, 33, 135, 33, 66, 33, 135, 135, 66, 135, 33, 135, 135, 135, 33, 135, 135, 135, 66, 66, 66, 66, 66, 66, 33, 135, 66, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 14976 . Total input tokens: 3282180 . Total output tokens: 3038664
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 0.9193628029897809,
    "estimated_duration": 3599.9305579995907,
    "input_throughput": 340.3246202284492,
    "output_throughput": 312.1612991958518,
    "total_throughput": 652.4859194243011,
    "itl": 18.972272616291114,
    "ttft": 7821.664558813887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.5890112145617055,
    "arrivals": 5084,
    "finished_requests": 5073,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9194107982330024. Arrivals time: 0.02723572263494134 Scheduler time: 0.45901865791529417 Scheduler overhead time: 0.14859328279271722 Adapter cache time: 0.05709377769380808 Engine time: 0.15234115347266197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [64 64 64]
Adapter prompts. [135, 33, 135, 33, 135, 33, 135, 33, 33, 33, 66, 66, 66, 66, 135, 66, 33, 135, 33, 66, 66, 33, 135, 135, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 33, 66, 66, 135, 66, 33, 33, 66, 135, 33, 66, 33, 135, 135, 66, 135, 135, 135, 33, 66, 135, 33, 66, 33, 66, 33, 33, 66, 135, 135, 33, 33, 33, 135, 135, 135, 66, 33, 33, 33, 33, 33, 135, 33, 66, 33, 135, 135, 33, 135, 33, 66, 66, 135, 33, 33, 33, 135, 135, 66, 135, 66, 135, 135, 66, 66, 33, 135, 66, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 66, 33, 135, 66, 33, 66, 66, 33, 66, 135, 33, 66, 135, 33, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 135, 135, 33, 135, 33, 135, 33, 66, 33, 135, 135, 66, 135, 33, 135, 135, 135, 33, 135, 135, 135, 66, 66, 66, 66, 66, 66, 33, 135, 66, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 14976 . Total input tokens: 3282180 . Total output tokens: 3038664
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 0.9247383363544941,
    "estimated_duration": 3599.936503348845,
    "input_throughput": 340.3240581772227,
    "output_throughput": 312.1607836567734,
    "total_throughput": 652.4848418339961,
    "itl": 18.969748845675458,
    "ttft": 7821.627831369055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.3105405256965135,
    "arrivals": 5084,
    "finished_requests": 5073,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9248103443533182. Arrivals time: 0.02753983996808529 Scheduler time: 0.46360216569155455 Scheduler overhead time: 0.15054224897176027 Adapter cache time: 0.056784238666296005 Engine time: 0.15014087408781052 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [64 64 64]
Adapter prompts. [135, 33, 135, 33, 135, 33, 135, 33, 33, 33, 66, 66, 66, 66, 135, 66, 33, 135, 33, 66, 66, 33, 135, 135, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 33, 66, 66, 135, 66, 33, 33, 66, 135, 33, 66, 33, 135, 135, 66, 135, 135, 135, 33, 66, 135, 33, 66, 33, 66, 33, 33, 66, 135, 135, 33, 33, 33, 135, 135, 135, 66, 33, 33, 33, 33, 33, 135, 33, 66, 33, 135, 135, 33, 135, 33, 66, 66, 135, 33, 33, 33, 135, 135, 66, 135, 66, 135, 135, 66, 66, 33, 135, 66, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 66, 33, 135, 66, 33, 66, 66, 33, 66, 135, 33, 66, 135, 33, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 135, 135, 33, 135, 33, 135, 33, 66, 33, 135, 135, 66, 135, 33, 135, 135, 135, 33, 135, 135, 135, 66, 66, 66, 66, 66, 66, 33, 135, 66, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 14976 . Total input tokens: 3282180 . Total output tokens: 3038664
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 0.9159459751099348,
    "estimated_duration": 3599.934565388725,
    "input_throughput": 340.32424138456736,
    "output_throughput": 312.160951702925,
    "total_throughput": 652.4851930874923,
    "itl": 18.973748278209637,
    "ttft": 7821.646670607664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2017,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.667244655601545,
    "arrivals": 5084,
    "finished_requests": 5073,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9159984048455954. Arrivals time: 0.02806029748171568 Scheduler time: 0.4543723906390369 Scheduler overhead time: 0.149809327442199 Adapter cache time: 0.056981758680194616 Engine time: 0.14983476419001818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [64 64 64]
Adapter prompts. [135, 33, 135, 33, 135, 33, 135, 33, 33, 33, 66, 66, 66, 66, 135, 66, 33, 135, 33, 66, 66, 33, 135, 135, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 33, 66, 66, 135, 66, 33, 33, 66, 135, 33, 66, 33, 135, 135, 66, 135, 135, 135, 33, 66, 135, 33, 66, 33, 66, 33, 33, 66, 135, 135, 33, 33, 33, 135, 135, 135, 66, 33, 33, 33, 33, 33, 135, 33, 66, 33, 135, 135, 33, 135, 33, 66, 66, 135, 33, 33, 33, 135, 135, 66, 135, 66, 135, 135, 66, 66, 33, 135, 66, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 66, 33, 135, 66, 33, 66, 66, 33, 66, 135, 33, 66, 135, 33, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 135, 135, 33, 135, 33, 135, 33, 66, 33, 135, 135, 66, 135, 33, 135, 135, 135, 33, 135, 135, 135, 66, 66, 66, 66, 66, 66, 33, 135, 66, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 14976 . Total input tokens: 3282180 . Total output tokens: 3038664
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 0.9124587597325444,
    "estimated_duration": 3599.9475851493394,
    "input_throughput": 340.3230105499373,
    "output_throughput": 312.1598227251362,
    "total_throughput": 652.4828332750735,
    "itl": 18.96979043312756,
    "ttft": 7821.320342126646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.033921495112413,
    "arrivals": 5084,
    "finished_requests": 5073,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9126382716931403. Arrivals time: 0.027101727202534676 Scheduler time: 0.45426251366734505 Scheduler overhead time: 0.1483206986449659 Adapter cache time: 0.05685390206053853 Engine time: 0.1510421670973301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_192_slots_96_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [64 64 64]
Adapter prompts. [135, 33, 135, 33, 135, 33, 135, 33, 33, 33, 66, 66, 66, 66, 135, 66, 33, 135, 33, 66, 66, 33, 135, 135, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 33, 66, 66, 135, 66, 33, 33, 66, 135, 33, 66, 33, 135, 135, 66, 135, 135, 135, 33, 66, 135, 33, 66, 33, 66, 33, 33, 66, 135, 135, 33, 33, 33, 135, 135, 135, 66, 33, 33, 33, 33, 33, 135, 33, 66, 33, 135, 135, 33, 135, 33, 66, 66, 135, 33, 33, 33, 135, 135, 66, 135, 66, 135, 135, 66, 66, 33, 135, 66, 66, 66, 135, 66, 33, 66, 135, 135, 66, 135, 66, 33, 135, 66, 33, 66, 66, 33, 66, 135, 33, 66, 135, 33, 66, 135, 135, 66, 33, 135, 66, 33, 33, 66, 135, 135, 135, 33, 135, 33, 135, 33, 66, 33, 135, 135, 66, 135, 33, 135, 135, 135, 33, 135, 135, 135, 66, 66, 66, 66, 66, 66, 33, 135, 66, 135, 33, 135, 66, 33, 66, 33, 33, 66, 66, 66, 135, 66, 33, 33]
Prompts retrieved: 14976 . Total input tokens: 3282180 . Total output tokens: 3038664
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 0.9156907466240227,
    "estimated_duration": 3599.9378202460666,
    "input_throughput": 340.3239336829039,
    "output_throughput": 312.16066946489303,
    "total_throughput": 652.484603147797,
    "itl": 18.97357974843953,
    "ttft": 7821.5254048965735,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2018,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.755760735347653,
    "arrivals": 5084,
    "finished_requests": 5073,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 0.9157397300004959. Arrivals time: 0.027123226318508387 Scheduler time: 0.45438028825446963 Scheduler overhead time: 0.1475421735085547 Adapter cache time: 0.05686219222843647 Engine time: 0.154997275210917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 17280, 34560, 8640, 8640, 8640, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 17280, 8640, 34560, 17280, 8640, 17280, 17280, 8640, 17280, 34560, 8640, 17280, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 34560, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3870720 . Total input tokens: 862992993 . Total output tokens: 774449452
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.520304739940912,
    "estimated_duration": 3600.2047308416636,
    "input_throughput": 4676.732369068291,
    "output_throughput": 4113.954374071786,
    "total_throughput": 8790.686743140077,
    "itl": 208.0288533455905,
    "ttft": 2200830.7371003353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 532,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6281794797722455,
    "arrivals": 1289329,
    "finished_requests": 67994,
    "scheduler_time": 76.60181811122692
}
#Debug simulation 
Total elapsed time: 10.520426830742508. Arrivals time: 0.360837513115257 Scheduler time: 10.070885988418013 Scheduler overhead time: 0.030248855706304312 Adapter cache time: 0.01467790687456727 Engine time: 0.030549267772585154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 17280, 34560, 8640, 8640, 8640, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 17280, 8640, 34560, 17280, 8640, 17280, 17280, 8640, 17280, 34560, 8640, 17280, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 34560, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3870720 . Total input tokens: 862992993 . Total output tokens: 774449452
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 10.471497016958892,
    "estimated_duration": 3600.0875987192644,
    "input_throughput": 4676.408153509721,
    "output_throughput": 4113.760455514649,
    "total_throughput": 8790.16860902437,
    "itl": 208.03569691167698,
    "ttft": 2200850.0865520765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 532,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.738209666844929,
    "arrivals": 1289329,
    "finished_requests": 67988,
    "scheduler_time": 76.59716652188261
}
#Debug simulation 
Total elapsed time: 10.471590348985046. Arrivals time: 0.2838208503089845 Scheduler time: 10.09810729091987 Scheduler overhead time: 0.031109918374568224 Adapter cache time: 0.014394695404917002 Engine time: 0.030907539185136557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 17280, 34560, 8640, 8640, 8640, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 17280, 8640, 34560, 17280, 8640, 17280, 17280, 8640, 17280, 34560, 8640, 17280, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 34560, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3870720 . Total input tokens: 862992993 . Total output tokens: 774449452
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.5297914911061525,
    "estimated_duration": 3600.0624658247025,
    "input_throughput": 4488.724057819408,
    "output_throughput": 3952.9052995862476,
    "total_throughput": 8441.629357405654,
    "itl": 176.84962075120922,
    "ttft": 2221454.778011867,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.644649351797985,
    "arrivals": 1289329,
    "finished_requests": 65269,
    "scheduler_time": 77.00840345919737
}
#Debug simulation 
Total elapsed time: 7.529914026148617. Arrivals time: 0.25803976645693183 Scheduler time: 7.169326877221465 Scheduler overhead time: 0.033408643677830696 Adapter cache time: 0.02075372263789177 Engine time: 0.033321038354188204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 17280, 34560, 8640, 8640, 8640, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 17280, 8640, 34560, 17280, 8640, 17280, 17280, 8640, 17280, 34560, 8640, 17280, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 34560, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3870720 . Total input tokens: 862992993 . Total output tokens: 774449452
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 10.474533637985587,
    "estimated_duration": 3600.013906418918,
    "input_throughput": 4676.503879604994,
    "output_throughput": 4113.844664209093,
    "total_throughput": 8790.348543814087,
    "itl": 208.03228531560265,
    "ttft": 2200816.786470123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 532,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.664662509525653,
    "arrivals": 1289329,
    "finished_requests": 67988,
    "scheduler_time": 76.59702137883903
}
#Debug simulation 
Total elapsed time: 10.474631442222744. Arrivals time: 0.29054457135498524 Scheduler time: 10.094534133095294 Scheduler overhead time: 0.030689675360918045 Adapter cache time: 0.014883837662637234 Engine time: 0.030703368596732616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.8_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 17280, 34560, 8640, 8640, 8640, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 17280, 8640, 34560, 17280, 8640, 17280, 17280, 8640, 17280, 34560, 8640, 17280, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 34560, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3870720 . Total input tokens: 862992993 . Total output tokens: 774449452
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 7.51459488645196,
    "estimated_duration": 3600.0977248784484,
    "input_throughput": 4488.680095634239,
    "output_throughput": 3952.866585164845,
    "total_throughput": 8441.546680799083,
    "itl": 176.85115941740608,
    "ttft": 2221469.1986210733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6798604119941607,
    "arrivals": 1289329,
    "finished_requests": 65269,
    "scheduler_time": 77.00845145277634
}
#Debug simulation 
Total elapsed time: 7.514717051293701. Arrivals time: 0.2676908252760768 Scheduler time: 7.144586160313338 Scheduler overhead time: 0.033281922806054354 Adapter cache time: 0.02095104707404971 Engine time: 0.033209499437361956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 17280, 34560, 8640, 8640, 8640, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 17280, 8640, 34560, 17280, 8640, 17280, 17280, 8640, 17280, 34560, 8640, 17280, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 34560, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3870720 . Total input tokens: 862992993 . Total output tokens: 774449452
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 10.475870942231268,
    "estimated_duration": 3600.1671865401213,
    "input_throughput": 4676.781140317291,
    "output_throughput": 4113.997276397025,
    "total_throughput": 8790.778416714316,
    "itl": 208.02709792488395,
    "ttft": 2200814.06156636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 532,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5907067568879378,
    "arrivals": 1289329,
    "finished_requests": 67994,
    "scheduler_time": 76.60174653250873
}
#Debug simulation 
Total elapsed time: 10.475965922232717. Arrivals time: 0.2858353010378778 Scheduler time: 10.100860292091966 Scheduler overhead time: 0.03069264255464077 Adapter cache time: 0.014545044861733913 Engine time: 0.03069707565009594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.8_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.8_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 34560, 34560, 8640, 17280, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 17280, 34560, 34560, 8640, 8640, 8640, 34560, 34560, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 17280, 34560, 8640, 8640, 8640, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 8640, 34560, 17280, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 34560, 17280, 34560, 17280, 8640, 34560, 17280, 8640, 17280, 17280, 8640, 17280, 34560, 8640, 17280, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 17280, 34560, 34560, 34560, 8640, 34560, 8640, 34560, 8640, 17280, 8640, 34560, 34560, 17280, 34560, 8640, 34560, 34560, 34560, 8640, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 34560, 17280, 34560, 8640, 34560, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 17280, 34560, 17280, 8640, 8640]
Prompts retrieved: 3870720 . Total input tokens: 862992993 . Total output tokens: 774449452
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 7.493402360007167,
    "estimated_duration": 3600.1317221325407,
    "input_throughput": 4488.637707519157,
    "output_throughput": 3952.829256916864,
    "total_throughput": 8441.46696443602,
    "itl": 176.8526835522567,
    "ttft": 2221483.0904056276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7138139343261902,
    "arrivals": 1289329,
    "finished_requests": 65269,
    "scheduler_time": 77.00849518456425
}
#Debug simulation 
Total elapsed time: 7.4935222449712455. Arrivals time: 0.2614569440484047 Scheduler time: 7.129594380967319 Scheduler overhead time: 0.03334759268909693 Adapter cache time: 0.020856664050370455 Engine time: 0.03326036175712943 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 17280, 34560, 4320, 4320, 4320, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 17280, 4320, 34560, 17280, 4320, 17280, 17280, 4320, 17280, 34560, 4320, 17280, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 34560, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3594240 . Total input tokens: 801509885 . Total output tokens: 718996203
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 8.51481600990519,
    "estimated_duration": 3600.016733546585,
    "input_throughput": 4689.80153416321,
    "output_throughput": 4111.5747774366455,
    "total_throughput": 8801.376311599855,
    "itl": 207.79249862890768,
    "ttft": 2198074.3966421536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.726115087578097,
    "arrivals": 1197398,
    "finished_requests": 67677,
    "scheduler_time": 76.58366394941663
}
#Debug simulation 
Total elapsed time: 8.514912351965904. Arrivals time: 0.27826972445473075 Scheduler time: 8.149481700267643 Scheduler overhead time: 0.029717470984905958 Adapter cache time: 0.014507248066365719 Engine time: 0.029747840482741594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 17280, 34560, 4320, 4320, 4320, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 17280, 4320, 34560, 17280, 4320, 17280, 17280, 4320, 17280, 34560, 4320, 17280, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 34560, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3594240 . Total input tokens: 801509885 . Total output tokens: 718996203
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 8.50646806601435,
    "estimated_duration": 3600.133710226847,
    "input_throughput": 4689.64915165225,
    "output_throughput": 4111.441182851881,
    "total_throughput": 8801.090334504132,
    "itl": 207.79806595075374,
    "ttft": 2198125.865731071,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8428803732874868,
    "arrivals": 1197398,
    "finished_requests": 67677,
    "scheduler_time": 76.58387534393957
}
#Debug simulation 
Total elapsed time: 8.506588303018361. Arrivals time: 0.2736279726959765 Scheduler time: 8.145840862300247 Scheduler overhead time: 0.029653251636773348 Adapter cache time: 0.014540291391313076 Engine time: 0.029795047361403704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 17280, 34560, 4320, 4320, 4320, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 17280, 4320, 34560, 17280, 4320, 17280, 17280, 4320, 17280, 34560, 4320, 17280, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 34560, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3594240 . Total input tokens: 801509885 . Total output tokens: 718996203
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.638104205019772,
    "estimated_duration": 3600.0923113209,
    "input_throughput": 4500.159328985578,
    "output_throughput": 3950.1856536512532,
    "total_throughput": 8450.34498263683,
    "itl": 176.40489381307893,
    "ttft": 2218867.868387838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8701572457141955,
    "arrivals": 1197398,
    "finished_requests": 64935,
    "scheduler_time": 76.9928146420628
}
#Debug simulation 
Total elapsed time: 6.638200093060732. Arrivals time: 0.25095080491155386 Scheduler time: 6.284789331257343 Scheduler overhead time: 0.03313838550820947 Adapter cache time: 0.021324402652680874 Engine time: 0.03300416516140103 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 17280, 34560, 4320, 4320, 4320, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 17280, 4320, 34560, 17280, 4320, 17280, 17280, 4320, 17280, 34560, 4320, 17280, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 34560, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3594240 . Total input tokens: 801509885 . Total output tokens: 718996203
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 8.484731615986675,
    "estimated_duration": 3600.0518467465763,
    "input_throughput": 4689.755792061095,
    "output_throughput": 4111.53467508435,
    "total_throughput": 8801.290467145445,
    "itl": 207.79414325610537,
    "ttft": 2198089.760140509,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7611613095994012,
    "arrivals": 1197398,
    "finished_requests": 67677,
    "scheduler_time": 76.58373092733764
}
#Debug simulation 
Total elapsed time: 8.484854157082736. Arrivals time: 0.2733717570081353 Scheduler time: 8.124321839306504 Scheduler overhead time: 0.029714887030422688 Adapter cache time: 0.014637915417551994 Engine time: 0.02974236523732543 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 17280, 34560, 4320, 4320, 4320, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 17280, 4320, 34560, 17280, 4320, 17280, 17280, 4320, 17280, 34560, 4320, 17280, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 34560, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3594240 . Total input tokens: 801509885 . Total output tokens: 718996203
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 6.623040997888893,
    "estimated_duration": 3600.131094870728,
    "input_throughput": 4500.110849597198,
    "output_throughput": 3950.143099028077,
    "total_throughput": 8450.253948625274,
    "itl": 176.4065941610416,
    "ttft": 2218883.5771996276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9088894119299953,
    "arrivals": 1197398,
    "finished_requests": 64935,
    "scheduler_time": 76.99286602570285
}
#Debug simulation 
Total elapsed time: 6.623133524786681. Arrivals time: 0.2496467991732061 Scheduler time: 6.271095116622746 Scheduler overhead time: 0.032988802529871464 Adapter cache time: 0.021340194158256054 Engine time: 0.03309001820161939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 17280, 34560, 4320, 4320, 4320, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 17280, 4320, 34560, 17280, 4320, 17280, 17280, 4320, 17280, 34560, 4320, 17280, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 34560, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3594240 . Total input tokens: 801509885 . Total output tokens: 718996203
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 8.601582423783839,
    "estimated_duration": 3600.2066538866334,
    "input_throughput": 4689.8860602271625,
    "output_throughput": 4111.601478215067,
    "total_throughput": 8801.487538442228,
    "itl": 207.79004733127206,
    "ttft": 2198165.514568033,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.686388366324804,
    "arrivals": 1197398,
    "finished_requests": 67681,
    "scheduler_time": 76.58850224566109
}
#Debug simulation 
Total elapsed time: 8.601680823601782. Arrivals time: 0.2731085862033069 Scheduler time: 8.24160872027278 Scheduler overhead time: 0.029704803600907326 Adapter cache time: 0.014266574755311012 Engine time: 0.02978875068947673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 34560, 34560, 4320, 17280, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 17280, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 17280, 34560, 4320, 4320, 4320, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 4320, 34560, 17280, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 34560, 17280, 34560, 17280, 4320, 34560, 17280, 4320, 17280, 17280, 4320, 17280, 34560, 4320, 17280, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 17280, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 17280, 4320, 34560, 34560, 17280, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 34560, 17280, 34560, 4320, 34560, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 17280, 34560, 17280, 4320, 4320]
Prompts retrieved: 3594240 . Total input tokens: 801509885 . Total output tokens: 718996203
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.688883685972542,
    "estimated_duration": 3600.1684905765333,
    "input_throughput": 4500.064106001207,
    "output_throughput": 3950.102068062552,
    "total_throughput": 8450.16617406376,
    "itl": 176.40825478831385,
    "ttft": 2218898.6526457756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 876,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9462382864952352,
    "arrivals": 1197398,
    "finished_requests": 64935,
    "scheduler_time": 76.99291285697517
}
#Debug simulation 
Total elapsed time: 6.6889829421415925. Arrivals time: 0.2541141863912344 Scheduler time: 6.333054872229695 Scheduler overhead time: 0.032801189459860325 Adapter cache time: 0.021138549782335758 Engine time: 0.03293689154088497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 17280, 34560, 1080, 1080, 1080, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 17280, 1080, 17280, 17280, 1080, 17280, 34560, 1080, 17280, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 3386880 . Total input tokens: 755191082 . Total output tokens: 677556911
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.377097009215504,
    "estimated_duration": 3600.000706738423,
    "input_throughput": 4660.993807193497,
    "output_throughput": 4111.094470703216,
    "total_throughput": 8772.088277896712,
    "itl": 208.34573111820671,
    "ttft": 2193308.6270619826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8424136218475458,
    "arrivals": 1127741,
    "finished_requests": 68021,
    "scheduler_time": 76.55417824980927
}
#Debug simulation 
Total elapsed time: 6.37719358690083. Arrivals time: 0.38211898785084486 Scheduler time: 5.90828957548365 Scheduler overhead time: 0.02892028633505106 Adapter cache time: 0.01518432516604662 Engine time: 0.029688518028706312 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 17280, 34560, 1080, 1080, 1080, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 17280, 1080, 17280, 17280, 1080, 17280, 34560, 1080, 17280, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 3386880 . Total input tokens: 755191082 . Total output tokens: 677556911
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.717976574320346,
    "estimated_duration": 3600.1243875376385,
    "input_throughput": 4660.833680659755,
    "output_throughput": 4110.953235736017,
    "total_throughput": 8771.786916395773,
    "itl": 208.35201720868983,
    "ttft": 2193364.416580837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9658999768178982,
    "arrivals": 1127741,
    "finished_requests": 68021,
    "scheduler_time": 76.55437269402682
}
#Debug simulation 
Total elapsed time: 6.718042724300176. Arrivals time: 0.7680910550989211 Scheduler time: 5.864815025124699 Scheduler overhead time: 0.028537001460790634 Adapter cache time: 0.015005623456090689 Engine time: 0.028730885591357946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 17280, 34560, 1080, 1080, 1080, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 17280, 1080, 17280, 17280, 1080, 17280, 34560, 1080, 17280, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 3386880 . Total input tokens: 755191082 . Total output tokens: 677556911
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.505830097943544,
    "estimated_duration": 3600.035878470711,
    "input_throughput": 4482.590603195648,
    "output_throughput": 3957.592224345359,
    "total_throughput": 8440.182827541008,
    "itl": 177.5472298418268,
    "ttft": 2214053.705350419,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 927,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.032796151880147,
    "arrivals": 1127741,
    "finished_requests": 65368,
    "scheduler_time": 76.95123977166836
}
#Debug simulation 
Total elapsed time: 5.505924545694143. Arrivals time: 0.24471353273838758 Scheduler time: 5.159470779355615 Scheduler overhead time: 0.03241392783820629 Adapter cache time: 0.021887007169425488 Engine time: 0.03268697764724493 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 17280, 34560, 1080, 1080, 1080, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 17280, 1080, 17280, 17280, 1080, 17280, 34560, 1080, 17280, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 3386880 . Total input tokens: 755191082 . Total output tokens: 677556911
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.356606349349022,
    "estimated_duration": 3600.0400873532967,
    "input_throughput": 4660.94282087179,
    "output_throughput": 4111.049499696191,
    "total_throughput": 8771.99232056798,
    "itl": 208.34775733150357,
    "ttft": 2193326.7268856405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8817293412191682,
    "arrivals": 1127741,
    "finished_requests": 68021,
    "scheduler_time": 76.5542431452642
}
#Debug simulation 
Total elapsed time: 6.356697926297784. Arrivals time: 0.37941336911171675 Scheduler time: 5.891836212016642 Scheduler overhead time: 0.02875118888914585 Adapter cache time: 0.0147993890568614 Engine time: 0.0289420448243618 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 17280, 34560, 1080, 1080, 1080, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 17280, 1080, 17280, 17280, 1080, 17280, 34560, 1080, 17280, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 3386880 . Total input tokens: 755191082 . Total output tokens: 677556911
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.507211653050035,
    "estimated_duration": 3600.0756594788136,
    "input_throughput": 4482.541070355238,
    "output_throughput": 3957.5484927621274,
    "total_throughput": 8440.089563117364,
    "itl": 177.54900730121838,
    "ttft": 2214069.777961379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 927,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0725343483872725,
    "arrivals": 1127741,
    "finished_requests": 65368,
    "scheduler_time": 76.95128258329683
}
#Debug simulation 
Total elapsed time: 5.507334433030337. Arrivals time: 0.24447756120935082 Scheduler time: 5.160700714215636 Scheduler overhead time: 0.0326724611222744 Adapter cache time: 0.021978199016302824 Engine time: 0.03262869082391262 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 17280, 34560, 1080, 1080, 1080, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 17280, 1080, 17280, 17280, 1080, 17280, 34560, 1080, 17280, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 3386880 . Total input tokens: 755191082 . Total output tokens: 677556911
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.374925220850855,
    "estimated_duration": 3600.1928390237304,
    "input_throughput": 4661.086711274743,
    "output_throughput": 4111.174779185888,
    "total_throughput": 8772.26149046063,
    "itl": 208.3424309129478,
    "ttft": 2193391.6581583982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 602,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8000102775310827,
    "arrivals": 1127741,
    "finished_requests": 68027,
    "scheduler_time": 76.55911752096642
}
#Debug simulation 
Total elapsed time: 6.375016666948795. Arrivals time: 0.3822574713267386 Scheduler time: 5.907425120938569 Scheduler overhead time: 0.028629459906369448 Adapter cache time: 0.014725534245371819 Engine time: 0.028944639023393393 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 34560, 34560, 1080, 17280, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 17280, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 17280, 34560, 1080, 1080, 1080, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 1080, 34560, 17280, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 34560, 17280, 34560, 17280, 1080, 34560, 17280, 1080, 17280, 17280, 1080, 17280, 34560, 1080, 17280, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 17280, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 17280, 1080, 34560, 34560, 17280, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 34560, 17280, 34560, 1080, 34560, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 17280, 34560, 17280, 1080, 1080]
Prompts retrieved: 3386880 . Total input tokens: 755191082 . Total output tokens: 677556911
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.4700122331269085,
    "estimated_duration": 3600.1148130255838,
    "input_throughput": 4482.492319859612,
    "output_throughput": 3957.5054518959178,
    "total_throughput": 8439.997771755528,
    "itl": 177.55079674332924,
    "ttft": 2214085.479200726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 927,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1116437759623254,
    "arrivals": 1127741,
    "finished_requests": 65368,
    "scheduler_time": 76.95132670252029
}
#Debug simulation 
Total elapsed time: 5.47010664222762. Arrivals time: 0.24243613565340638 Scheduler time: 5.1265230742283165 Scheduler overhead time: 0.03236885787919164 Adapter cache time: 0.021573067642748356 Engine time: 0.03243339853361249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 34560, 34560, 540, 17280, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 17280, 540, 540, 540, 540, 540, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 540, 17280, 17280, 34560, 540, 540, 540, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 17280, 540, 34560, 17280, 540, 17280, 17280, 540, 17280, 34560, 540, 17280, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 3352320 . Total input tokens: 747547103 . Total output tokens: 670674281
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.93294039554894,
    "estimated_duration": 3600.0818002647,
    "input_throughput": 4710.676295953354,
    "output_throughput": 4111.190750974538,
    "total_throughput": 8821.867046927891,
    "itl": 207.26838726458286,
    "ttft": 2190763.4346855716,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.704691673370567,
    "arrivals": 1116016,
    "finished_requests": 68104,
    "scheduler_time": 76.57231343516719
}
#Debug simulation 
Total elapsed time: 5.93300648778677. Arrivals time: 0.25319144828245044 Scheduler time: 5.595392934046686 Scheduler overhead time: 0.02874197205528617 Adapter cache time: 0.014097559731453657 Engine time: 0.028629016131162643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 34560, 34560, 540, 17280, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 17280, 540, 540, 540, 540, 540, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 540, 17280, 17280, 34560, 540, 540, 540, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 17280, 540, 34560, 17280, 540, 17280, 17280, 540, 17280, 34560, 540, 17280, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 3352320 . Total input tokens: 747547103 . Total output tokens: 670674281
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.5698658879846334,
    "estimated_duration": 3600.19720281579,
    "input_throughput": 4710.525297540965,
    "output_throughput": 4111.058968776521,
    "total_throughput": 8821.584266317486,
    "itl": 207.27412591611483,
    "ttft": 2190812.922902105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8199070446309704,
    "arrivals": 1116016,
    "finished_requests": 68104,
    "scheduler_time": 76.57250061496815
}
#Debug simulation 
Total elapsed time: 5.569968577940017. Arrivals time: 0.251955886837095 Scheduler time: 5.233269352000207 Scheduler overhead time: 0.028499025385826826 Adapter cache time: 0.01436970429494977 Engine time: 0.028923554345965385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 34560, 34560, 540, 17280, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 17280, 540, 540, 540, 540, 540, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 540, 17280, 17280, 34560, 540, 540, 540, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 17280, 540, 34560, 17280, 540, 17280, 17280, 540, 17280, 34560, 540, 17280, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 3352320 . Total input tokens: 747547103 . Total output tokens: 670674281
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.078164622187614,
    "estimated_duration": 3600.1752493096983,
    "input_throughput": 4515.502961450851,
    "output_throughput": 3948.8586014599996,
    "total_throughput": 8464.361562910852,
    "itl": 175.77158668989043,
    "ttft": 2211211.543011264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.667302973717436,
    "arrivals": 1116016,
    "finished_requests": 65313,
    "scheduler_time": 76.98749506438853
}
#Debug simulation 
Total elapsed time: 5.078254401218146. Arrivals time: 0.24927423102781177 Scheduler time: 4.728644616436213 Scheduler overhead time: 0.03250277088955045 Adapter cache time: 0.02024383330717683 Engine time: 0.03266381658613682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 34560, 34560, 540, 17280, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 17280, 540, 540, 540, 540, 540, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 540, 17280, 17280, 34560, 540, 540, 540, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 17280, 540, 34560, 17280, 540, 17280, 17280, 540, 17280, 34560, 540, 17280, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 3352320 . Total input tokens: 747547103 . Total output tokens: 670674281
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.932614566758275,
    "estimated_duration": 3600.120252697952,
    "input_throughput": 4710.625981810179,
    "output_throughput": 4111.1468398613415,
    "total_throughput": 8821.77282167152,
    "itl": 207.27035629584748,
    "ttft": 2190779.3677956276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7430911247641696,
    "arrivals": 1116016,
    "finished_requests": 68104,
    "scheduler_time": 76.5723664169789
}
#Debug simulation 
Total elapsed time: 5.932680511847138. Arrivals time: 0.622796778101474 Scheduler time: 5.225344645790756 Scheduler overhead time: 0.028576768934726715 Adapter cache time: 0.014325849246233702 Engine time: 0.02873190538957715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 34560, 34560, 540, 17280, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 17280, 540, 540, 540, 540, 540, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 540, 17280, 17280, 34560, 540, 540, 540, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 17280, 540, 34560, 17280, 540, 17280, 17280, 540, 17280, 34560, 540, 17280, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 3352320 . Total input tokens: 747547103 . Total output tokens: 670674281
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.0679070157930255,
    "estimated_duration": 3600.013907561255,
    "input_throughput": 4515.533111096294,
    "output_throughput": 3948.9477999351607,
    "total_throughput": 8464.480911031455,
    "itl": 175.77262178102077,
    "ttft": 2211124.106726212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7023882801272,
    "arrivals": 1116016,
    "finished_requests": 65311,
    "scheduler_time": 76.98332113937799
}
#Debug simulation 
Total elapsed time: 5.06800007680431. Arrivals time: 0.23990544909611344 Scheduler time: 4.727626712992787 Scheduler overhead time: 0.0324786608107388 Adapter cache time: 0.020235880743712187 Engine time: 0.032877196092158556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 34560, 34560, 540, 17280, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 17280, 540, 540, 540, 540, 540, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 540, 17280, 17280, 34560, 540, 540, 540, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 17280, 540, 34560, 17280, 540, 17280, 17280, 540, 17280, 34560, 540, 17280, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 3352320 . Total input tokens: 747547103 . Total output tokens: 670674281
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.64279625331983,
    "estimated_duration": 3600.042499821452,
    "input_throughput": 4710.727720809155,
    "output_throughput": 4111.235631449921,
    "total_throughput": 8821.963352259076,
    "itl": 207.26645412508591,
    "ttft": 2190746.142632192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6654580142604896,
    "arrivals": 1116016,
    "finished_requests": 68104,
    "scheduler_time": 76.57224665096396
}
#Debug simulation 
Total elapsed time: 5.64290597429499. Arrivals time: 0.2700604349374771 Scheduler time: 5.287666347343475 Scheduler overhead time: 0.02881272416561842 Adapter cache time: 0.014219412114471197 Engine time: 0.028967532329261303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 17280, 17280, 17280, 17280, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 540, 17280, 17280, 34560, 17280, 540, 540, 17280, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 34560, 34560, 540, 17280, 34560, 540, 17280, 540, 17280, 540, 540, 17280, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 17280, 540, 540, 540, 540, 540, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 540, 17280, 17280, 34560, 540, 540, 540, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 540, 34560, 17280, 17280, 17280, 34560, 17280, 540, 17280, 34560, 34560, 17280, 34560, 17280, 540, 34560, 17280, 540, 17280, 17280, 540, 17280, 34560, 540, 17280, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 17280, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 17280, 540, 34560, 34560, 17280, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 540, 34560, 17280, 34560, 540, 34560, 17280, 540, 17280, 540, 540, 17280, 17280, 17280, 34560, 17280, 540, 540]
Prompts retrieved: 3352320 . Total input tokens: 747547103 . Total output tokens: 670674281
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.5143938404507935,
    "estimated_duration": 3600.0485311143725,
    "input_throughput": 4515.489682848265,
    "output_throughput": 3948.909820834955,
    "total_throughput": 8464.39950368322,
    "itl": 175.77418128480775,
    "ttft": 2211137.564350073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 815,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7369705713913053,
    "arrivals": 1116016,
    "finished_requests": 65311,
    "scheduler_time": 76.98336240126426
}
#Debug simulation 
Total elapsed time: 5.514461326412857. Arrivals time: 0.6082714884541929 Scheduler time: 4.8059639893472195 Scheduler overhead time: 0.03246649540960789 Adapter cache time: 0.020043413620442152 Engine time: 0.032838042825460434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 34560, 34560, 270, 17280, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 17280, 270, 270, 270, 270, 270, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 270, 17280, 17280, 34560, 270, 270, 270, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 17280, 270, 34560, 17280, 270, 17280, 17280, 270, 17280, 34560, 270, 17280, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 3335040 . Total input tokens: 743711985 . Total output tokens: 667230169
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.4160369648598135,
    "estimated_duration": 3600.1478098406546,
    "input_throughput": 4682.0544295224545,
    "output_throughput": 4115.063264765153,
    "total_throughput": 8797.117694287608,
    "itl": 208.28710890067026,
    "ttft": 2190196.309831897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5363648474542597,
    "arrivals": 1110264,
    "finished_requests": 68076,
    "scheduler_time": 76.56324560898959
}
#Debug simulation 
Total elapsed time: 5.416148961056024. Arrivals time: 0.38721161102876067 Scheduler time: 4.945681751705706 Scheduler overhead time: 0.028490852564573288 Adapter cache time: 0.013089870102703571 Engine time: 0.028767801355570555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 34560, 34560, 270, 17280, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 17280, 270, 270, 270, 270, 270, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 270, 17280, 17280, 34560, 270, 270, 270, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 17280, 270, 34560, 17280, 270, 17280, 17280, 270, 17280, 34560, 270, 17280, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 3335040 . Total input tokens: 743711985 . Total output tokens: 667230169
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.365000931080431,
    "estimated_duration": 3600.0254999503436,
    "input_throughput": 4681.692949183965,
    "output_throughput": 4114.490022419094,
    "total_throughput": 8796.182971603059,
    "itl": 208.29276455593268,
    "ttft": 2190163.823713059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6436050141765806,
    "arrivals": 1110264,
    "finished_requests": 68067,
    "scheduler_time": 76.55850431692141
}
#Debug simulation 
Total elapsed time: 5.365096176974475. Arrivals time: 0.3733664145693183 Scheduler time: 4.909026606939733 Scheduler overhead time: 0.028155825566500425 Adapter cache time: 0.013085390906780958 Engine time: 0.028573560528457165 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 34560, 34560, 270, 17280, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 17280, 270, 270, 270, 270, 270, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 270, 17280, 17280, 34560, 270, 270, 270, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 17280, 270, 34560, 17280, 270, 17280, 17280, 270, 17280, 34560, 270, 17280, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 3335040 . Total input tokens: 743711985 . Total output tokens: 667230169
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.381258194800466,
    "estimated_duration": 3600.000707969438,
    "input_throughput": 4495.238004863577,
    "output_throughput": 3955.2381110589718,
    "total_throughput": 8450.476115922549,
    "itl": 177.6954969192416,
    "ttft": 2210435.0283548003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 707,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.318705335874109,
    "arrivals": 1110264,
    "finished_requests": 65356,
    "scheduler_time": 76.96121224526986
}
#Debug simulation 
Total elapsed time: 5.381324315909296. Arrivals time: 0.7484733732417226 Scheduler time: 4.53582069510594 Scheduler overhead time: 0.03198775975033641 Adapter cache time: 0.01811467856168747 Engine time: 0.0322538148611784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 34560, 34560, 270, 17280, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 17280, 270, 270, 270, 270, 270, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 270, 17280, 17280, 34560, 270, 270, 270, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 17280, 270, 34560, 17280, 270, 17280, 17280, 270, 17280, 34560, 270, 17280, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 3335040 . Total input tokens: 743711985 . Total output tokens: 667230169
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.362991967704147,
    "estimated_duration": 3600.1876835509524,
    "input_throughput": 4682.002573647614,
    "output_throughput": 4115.017688574438,
    "total_throughput": 8797.020262222051,
    "itl": 208.28906855502603,
    "ttft": 2190211.925693273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5761867866339123,
    "arrivals": 1110264,
    "finished_requests": 68076,
    "scheduler_time": 76.5632973800652
}
#Debug simulation 
Total elapsed time: 5.363087803591043. Arrivals time: 0.3725562570616603 Scheduler time: 4.907946135383099 Scheduler overhead time: 0.028223499190062284 Adapter cache time: 0.013060263358056545 Engine time: 0.028440142050385475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 34560, 34560, 270, 17280, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 17280, 270, 270, 270, 270, 270, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 270, 17280, 17280, 34560, 270, 270, 270, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 17280, 270, 34560, 17280, 270, 17280, 17280, 270, 17280, 34560, 270, 17280, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 3335040 . Total input tokens: 743711985 . Total output tokens: 667230169
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.03624430205673,
    "estimated_duration": 3600.0309301884713,
    "input_throughput": 4495.200267391254,
    "output_throughput": 3955.204906879663,
    "total_throughput": 8450.405174270916,
    "itl": 177.6968488884941,
    "ttft": 2210447.523675551,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 707,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3488862446136793,
    "arrivals": 1110264,
    "finished_requests": 65356,
    "scheduler_time": 76.96125355558557
}
#Debug simulation 
Total elapsed time: 5.036341349128634. Arrivals time: 0.36712754843756557 Scheduler time: 4.571910803206265 Scheduler overhead time: 0.03226682171225548 Adapter cache time: 0.01782847149297595 Engine time: 0.032553205732256174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 34560, 34560, 270, 17280, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 17280, 270, 270, 270, 270, 270, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 270, 17280, 17280, 34560, 270, 270, 270, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 17280, 270, 34560, 17280, 270, 17280, 17280, 270, 17280, 34560, 270, 17280, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 3335040 . Total input tokens: 743711985 . Total output tokens: 667230169
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.747632637154311,
    "estimated_duration": 3600.1123862175778,
    "input_throughput": 4682.100499009611,
    "output_throughput": 4115.103755292778,
    "total_throughput": 8797.204254302389,
    "itl": 208.28542999229833,
    "ttft": 2190181.2043356504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5010052480408758,
    "arrivals": 1110264,
    "finished_requests": 68076,
    "scheduler_time": 76.56318158526466
}
#Debug simulation 
Total elapsed time: 5.7476982129737735. Arrivals time: 0.6197919980622828 Scheduler time: 5.045197850558907 Scheduler overhead time: 0.02825802145525813 Adapter cache time: 0.013194161001592875 Engine time: 0.028487561736255884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 17280, 17280, 17280, 17280, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 270, 17280, 17280, 34560, 17280, 270, 270, 17280, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 34560, 34560, 270, 17280, 34560, 270, 17280, 270, 17280, 270, 270, 17280, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 17280, 270, 270, 270, 270, 270, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 270, 17280, 17280, 34560, 270, 270, 270, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 17280, 34560, 34560, 17280, 34560, 17280, 270, 34560, 17280, 270, 17280, 17280, 270, 17280, 34560, 270, 17280, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 17280, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 17280, 270, 34560, 34560, 17280, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 270, 34560, 17280, 34560, 270, 34560, 17280, 270, 17280, 270, 270, 17280, 17280, 17280, 34560, 17280, 270, 270]
Prompts retrieved: 3335040 . Total input tokens: 743711985 . Total output tokens: 667230169
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.059740155003965,
    "estimated_duration": 3600.0630364381095,
    "input_throughput": 4495.160178087122,
    "output_throughput": 3955.1696333872756,
    "total_throughput": 8450.329811474398,
    "itl": 177.69825266614777,
    "ttft": 2210460.2903520074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 707,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.380953460149475,
    "arrivals": 1110264,
    "finished_requests": 65356,
    "scheduler_time": 76.96129258971742
}
#Debug simulation 
Total elapsed time: 5.05983230099082. Arrivals time: 0.3640648815780878 Scheduler time: 4.598038366530091 Scheduler overhead time: 0.032249102368950844 Adapter cache time: 0.018021635711193085 Engine time: 0.032686535734683275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 34560, 34560, 135, 17280, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 17280, 135, 135, 135, 135, 135, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 135, 17280, 17280, 34560, 135, 135, 135, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 17280, 135, 34560, 17280, 135, 17280, 17280, 135, 17280, 34560, 135, 17280, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 3326400 . Total input tokens: 741778155 . Total output tokens: 665499970
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.131743752863258,
    "estimated_duration": 3600.2310616617347,
    "input_throughput": 4667.22571751945,
    "output_throughput": 4112.771304507791,
    "total_throughput": 8779.99702202724,
    "itl": 208.53289690840157,
    "ttft": 2189844.3309323704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2854048524517652,
    "arrivals": 1107343,
    "finished_requests": 68070,
    "scheduler_time": 76.577197682367
}
#Debug simulation 
Total elapsed time: 5.131837598979473. Arrivals time: 0.32378798397257924 Scheduler time: 4.727157951332629 Scheduler overhead time: 0.02806122088804841 Adapter cache time: 0.011814827099442482 Engine time: 0.028184218797832727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 34560, 34560, 135, 17280, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 17280, 135, 135, 135, 135, 135, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 135, 17280, 17280, 34560, 135, 135, 135, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 17280, 135, 34560, 17280, 135, 17280, 17280, 135, 17280, 34560, 135, 17280, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 3326400 . Total input tokens: 741778155 . Total output tokens: 665499970
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.0131870321929455,
    "estimated_duration": 3600.0842805145962,
    "input_throughput": 4667.057960542621,
    "output_throughput": 4112.53010940169,
    "total_throughput": 8779.588069944311,
    "itl": 208.5356369424364,
    "ttft": 2189835.7825052734,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3722707896144195,
    "arrivals": 1107343,
    "finished_requests": 68066,
    "scheduler_time": 76.57238094150098
}
#Debug simulation 
Total elapsed time: 5.01328598940745. Arrivals time: 0.2495594657957554 Scheduler time: 4.683003934565932 Scheduler overhead time: 0.027936697006225586 Adapter cache time: 0.011632743291556835 Engine time: 0.02836219547316432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 34560, 34560, 135, 17280, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 17280, 135, 135, 135, 135, 135, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 135, 17280, 17280, 34560, 135, 135, 135, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 17280, 135, 34560, 17280, 135, 17280, 17280, 135, 17280, 34560, 135, 17280, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 3326400 . Total input tokens: 741778155 . Total output tokens: 665499970
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.694470223039389,
    "estimated_duration": 3600.121201065987,
    "input_throughput": 4334.293244177363,
    "output_throughput": 3827.259481130855,
    "total_throughput": 8161.552725308217,
    "itl": 158.17073053260935,
    "ttft": 2224781.9933392773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 536,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7604918933473637,
    "arrivals": 1107343,
    "finished_requests": 63235,
    "scheduler_time": 77.40722527155135
}
#Debug simulation 
Total elapsed time: 4.694560853764415. Arrivals time: 0.23295517172664404 Scheduler time: 4.358972689602524 Scheduler overhead time: 0.035042226780205965 Adapter cache time: 0.016028795391321182 Engine time: 0.03540730103850365 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 34560, 34560, 135, 17280, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 17280, 135, 135, 135, 135, 135, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 135, 17280, 17280, 34560, 135, 135, 135, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 17280, 135, 34560, 17280, 135, 17280, 17280, 135, 17280, 34560, 135, 17280, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 3326400 . Total input tokens: 741778155 . Total output tokens: 665499970
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.129228199832141,
    "estimated_duration": 3600.0191934149525,
    "input_throughput": 4667.142339333455,
    "output_throughput": 4112.604462521115,
    "total_throughput": 8779.746801854571,
    "itl": 208.53246009598027,
    "ttft": 2189808.0005530454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3073041339823948,
    "arrivals": 1107343,
    "finished_requests": 68066,
    "scheduler_time": 76.57226049747383
}
#Debug simulation 
Total elapsed time: 5.129321770742536. Arrivals time: 0.2550463364459574 Scheduler time: 4.793500270694494 Scheduler overhead time: 0.02796542225405574 Adapter cache time: 0.011655132286250591 Engine time: 0.02833749307319522 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 34560, 34560, 135, 17280, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 17280, 135, 135, 135, 135, 135, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 135, 17280, 17280, 34560, 135, 135, 135, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 17280, 135, 34560, 17280, 135, 17280, 17280, 135, 17280, 34560, 135, 17280, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 3326400 . Total input tokens: 741778155 . Total output tokens: 665499970
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.696590160951018,
    "estimated_duration": 3600.147022677368,
    "input_throughput": 4334.262156992573,
    "output_throughput": 3827.232030583321,
    "total_throughput": 8161.494187575894,
    "itl": 158.17173657053084,
    "ttft": 2224792.6593676433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 536,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7862714195624019,
    "arrivals": 1107343,
    "finished_requests": 63235,
    "scheduler_time": 77.40726735673817
}
#Debug simulation 
Total elapsed time: 4.696681895758957. Arrivals time: 0.23177576251327991 Scheduler time: 4.362647470552474 Scheduler overhead time: 0.034900421276688576 Adapter cache time: 0.01595458621159196 Engine time: 0.0352540765888989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 34560, 34560, 135, 17280, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 17280, 135, 135, 135, 135, 135, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 135, 17280, 17280, 34560, 135, 135, 135, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 17280, 135, 34560, 17280, 135, 17280, 17280, 135, 17280, 34560, 135, 17280, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 3326400 . Total input tokens: 741778155 . Total output tokens: 665499970
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.025622977875173,
    "estimated_duration": 3600.2014111005565,
    "input_throughput": 4667.264155886049,
    "output_throughput": 4112.805176495285,
    "total_throughput": 8780.069332381334,
    "itl": 208.53151877396556,
    "ttft": 2189830.899135156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 420,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.255821123858906,
    "arrivals": 1107343,
    "finished_requests": 68070,
    "scheduler_time": 76.57713084973433
}
#Debug simulation 
Total elapsed time: 5.0257140211761. Arrivals time: 0.25018270034343004 Scheduler time: 4.69455046672374 Scheduler overhead time: 0.028210561256855726 Adapter cache time: 0.011836524587124586 Engine time: 0.028147902339696884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 17280, 17280, 17280, 17280, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 135, 17280, 17280, 34560, 17280, 135, 135, 17280, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 34560, 34560, 135, 17280, 34560, 135, 17280, 135, 17280, 135, 135, 17280, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 17280, 135, 135, 135, 135, 135, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 135, 17280, 17280, 34560, 135, 135, 135, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 17280, 34560, 34560, 17280, 34560, 17280, 135, 34560, 17280, 135, 17280, 17280, 135, 17280, 34560, 135, 17280, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 17280, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 17280, 135, 34560, 34560, 17280, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 135, 34560, 17280, 34560, 135, 34560, 17280, 135, 17280, 135, 135, 17280, 17280, 17280, 34560, 17280, 135, 135]
Prompts retrieved: 3326400 . Total input tokens: 741778155 . Total output tokens: 665499970
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.7736365627497435,
    "estimated_duration": 3600.0026995430585,
    "input_throughput": 4334.14313883166,
    "output_throughput": 3827.0996301610444,
    "total_throughput": 8161.2427689927035,
    "itl": 158.17317437798124,
    "ttft": 2224797.632158219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 536,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8086555935442392,
    "arrivals": 1107343,
    "finished_requests": 63231,
    "scheduler_time": 77.4037046103416
}
#Debug simulation 
Total elapsed time: 4.773754068650305. Arrivals time: 0.3057112847454846 Scheduler time: 4.365695059299469 Scheduler overhead time: 0.034976383205503225 Adapter cache time: 0.01583060808479786 Engine time: 0.035336114931851625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 34560, 34560, 66, 17280, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 17280, 66, 66, 66, 66, 66, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 66, 17280, 17280, 34560, 66, 66, 66, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 17280, 66, 34560, 17280, 66, 17280, 17280, 66, 17280, 34560, 66, 17280, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 3321984 . Total input tokens: 740786661 . Total output tokens: 664621116
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.9525655107572675,
    "estimated_duration": 3600.1501672454756,
    "input_throughput": 4648.371657453404,
    "output_throughput": 4120.203966755421,
    "total_throughput": 8768.575624208825,
    "itl": 208.64710270724177,
    "ttft": 2194749.0402429304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9885375412902782,
    "arrivals": 1105914,
    "finished_requests": 67968,
    "scheduler_time": 76.60551129894809
}
#Debug simulation 
Total elapsed time: 4.952659504022449. Arrivals time: 0.24768823897466063 Scheduler time: 4.626253124792129 Scheduler overhead time: 0.027686847373843193 Adapter cache time: 0.010003576055169106 Engine time: 0.028215231373906136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 34560, 34560, 66, 17280, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 17280, 66, 66, 66, 66, 66, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 66, 17280, 17280, 34560, 66, 66, 66, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 17280, 66, 34560, 17280, 66, 17280, 17280, 66, 17280, 34560, 66, 17280, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 3321984 . Total input tokens: 740786661 . Total output tokens: 664621116
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.981096616014838,
    "estimated_duration": 3600.212982358356,
    "input_throughput": 4648.290554476495,
    "output_throughput": 4120.132079042519,
    "total_throughput": 8768.422633519014,
    "itl": 208.64983864456389,
    "ttft": 2194779.783939314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0511826668074409,
    "arrivals": 1105914,
    "finished_requests": 67968,
    "scheduler_time": 76.60568128629643
}
#Debug simulation 
Total elapsed time: 4.981186388991773. Arrivals time: 0.24720375938341022 Scheduler time: 4.655375460162759 Scheduler overhead time: 0.02782756695523858 Adapter cache time: 0.010047022253274918 Engine time: 0.027989191934466362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 34560, 34560, 66, 17280, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 17280, 66, 66, 66, 66, 66, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 66, 17280, 17280, 34560, 66, 66, 66, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 17280, 66, 34560, 17280, 66, 17280, 17280, 66, 17280, 34560, 66, 17280, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 3321984 . Total input tokens: 740786661 . Total output tokens: 664621116
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.904205704107881,
    "estimated_duration": 3600.1778296176526,
    "input_throughput": 4462.203191142397,
    "output_throughput": 3965.6632743378295,
    "total_throughput": 8427.866465480227,
    "itl": 176.7771239828176,
    "ttft": 2214570.1700541168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.332146310228862,
    "arrivals": 1105914,
    "finished_requests": 65261,
    "scheduler_time": 77.18838381602146
}
#Debug simulation 
Total elapsed time: 4.904304776340723. Arrivals time: 0.31306016026064754 Scheduler time: 4.497964879032224 Scheduler overhead time: 0.03162520471960306 Adapter cache time: 0.014994995202869177 Engine time: 0.03212781623005867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 34560, 34560, 66, 17280, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 17280, 66, 66, 66, 66, 66, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 66, 17280, 17280, 34560, 66, 66, 66, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 17280, 66, 34560, 17280, 66, 17280, 17280, 66, 17280, 34560, 66, 17280, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 3321984 . Total input tokens: 740786661 . Total output tokens: 664621116
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.981109134852886,
    "estimated_duration": 3600.1711889693515,
    "input_throughput": 4648.344515192571,
    "output_throughput": 4120.179908513311,
    "total_throughput": 8768.524423705881,
    "itl": 208.64801715738014,
    "ttft": 2194758.9109567264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0095059443265233,
    "arrivals": 1105914,
    "finished_requests": 67968,
    "scheduler_time": 76.6055646197659
}
#Debug simulation 
Total elapsed time: 4.981212020851672. Arrivals time: 0.257104504853487 Scheduler time: 4.645931538660079 Scheduler overhead time: 0.027641561813652515 Adapter cache time: 0.009928751271218061 Engine time: 0.028020296711474657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 34560, 34560, 66, 17280, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 17280, 66, 66, 66, 66, 66, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 66, 17280, 17280, 34560, 66, 66, 66, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 17280, 66, 34560, 17280, 66, 17280, 17280, 66, 17280, 34560, 66, 17280, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 3321984 . Total input tokens: 740786661 . Total output tokens: 664621116
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.774236455094069,
    "estimated_duration": 3600.0002147552314,
    "input_throughput": 4462.205289366131,
    "output_throughput": 3965.6983745404245,
    "total_throughput": 8427.903663906554,
    "itl": 176.77867675200807,
    "ttft": 2214493.233995584,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3486200562492061,
    "arrivals": 1105914,
    "finished_requests": 65259,
    "scheduler_time": 77.18425029256082
}
#Debug simulation 
Total elapsed time: 4.774334675166756. Arrivals time: 0.23836468951776624 Scheduler time: 4.442717347294092 Scheduler overhead time: 0.03132165363058448 Adapter cache time: 0.015226212330162525 Engine time: 0.03200678201392293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 34560, 34560, 66, 17280, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 17280, 66, 66, 66, 66, 66, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 66, 17280, 17280, 34560, 66, 66, 66, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 17280, 66, 34560, 17280, 66, 17280, 17280, 66, 17280, 34560, 66, 17280, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 3321984 . Total input tokens: 740786661 . Total output tokens: 664621116
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.066227180417627,
    "estimated_duration": 3600.1273533786275,
    "input_throughput": 4648.401114003587,
    "output_throughput": 4120.230076327516,
    "total_throughput": 8768.631190331103,
    "itl": 208.64610911740445,
    "ttft": 2194737.8749679653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9657862452534041,
    "arrivals": 1105914,
    "finished_requests": 67968,
    "scheduler_time": 76.60544872810648
}
#Debug simulation 
Total elapsed time: 5.066320470068604. Arrivals time: 0.3245758879929781 Scheduler time: 4.663750750944018 Scheduler overhead time: 0.02753575425595045 Adapter cache time: 0.009881230536848307 Engine time: 0.02796659804880619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 17280, 17280, 17280, 17280, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 66, 17280, 17280, 34560, 17280, 66, 66, 17280, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 34560, 34560, 66, 17280, 34560, 66, 17280, 66, 17280, 66, 66, 17280, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 17280, 66, 66, 66, 66, 66, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 66, 17280, 17280, 34560, 66, 66, 66, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 17280, 34560, 34560, 17280, 34560, 17280, 66, 34560, 17280, 66, 17280, 17280, 66, 17280, 34560, 66, 17280, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 17280, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 17280, 66, 34560, 34560, 17280, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 66, 34560, 17280, 34560, 66, 34560, 17280, 66, 17280, 66, 66, 17280, 17280, 17280, 34560, 17280, 66, 66]
Prompts retrieved: 3321984 . Total input tokens: 740786661 . Total output tokens: 664621116
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.786245169583708,
    "estimated_duration": 3600.016350661448,
    "input_throughput": 4462.1852889775055,
    "output_throughput": 3965.6805995830846,
    "total_throughput": 8427.865888560591,
    "itl": 176.779326175935,
    "ttft": 2214500.859335537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 409,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3647165409103055,
    "arrivals": 1105914,
    "finished_requests": 65259,
    "scheduler_time": 77.18428971412821
}
#Debug simulation 
Total elapsed time: 4.78633838891983. Arrivals time: 0.23823254369199276 Scheduler time: 4.454811280127615 Scheduler overhead time: 0.031490498688071966 Adapter cache time: 0.01539720967411995 Engine time: 0.031924805138260126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 34560, 34560, 33, 17280, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 17280, 33, 33, 33, 33, 33, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 33, 17280, 17280, 34560, 33, 33, 33, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 17280, 33, 34560, 17280, 33, 17280, 17280, 33, 17280, 34560, 33, 17280, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 3319872 . Total input tokens: 740314272 . Total output tokens: 664208977
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.951161221601069,
    "estimated_duration": 3600.1022728935,
    "input_throughput": 4611.66093113687,
    "output_throughput": 4121.615408462856,
    "total_throughput": 8733.276339599726,
    "itl": 209.47028375725833,
    "ttft": 2195908.5407387842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7375775462877927,
    "arrivals": 1105156,
    "finished_requests": 67612,
    "scheduler_time": 76.59197024701221
}
#Debug simulation 
Total elapsed time: 4.95124992961064. Arrivals time: 0.2513931072317064 Scheduler time: 4.623245405498892 Scheduler overhead time: 0.027508210856467485 Adapter cache time: 0.008593682199716568 Engine time: 0.02797287004068494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 34560, 34560, 33, 17280, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 17280, 33, 33, 33, 33, 33, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 33, 17280, 17280, 34560, 33, 33, 33, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 17280, 33, 34560, 17280, 33, 17280, 17280, 33, 17280, 34560, 33, 17280, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 3319872 . Total input tokens: 740314272 . Total output tokens: 664208977
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.954478702042252,
    "estimated_duration": 3600.1492634036676,
    "input_throughput": 4611.600737993747,
    "output_throughput": 4121.561611579286,
    "total_throughput": 8733.162349573033,
    "itl": 209.47209153645278,
    "ttft": 2195935.0231472217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783525800111241,
    "arrivals": 1105156,
    "finished_requests": 67612,
    "scheduler_time": 76.59215672743464
}
#Debug simulation 
Total elapsed time: 4.954573654104024. Arrivals time: 0.24916861532256007 Scheduler time: 4.6288122702389956 Scheduler overhead time: 0.027450616005808115 Adapter cache time: 0.008599343243986368 Engine time: 0.027932886499911547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 34560, 34560, 33, 17280, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 17280, 33, 33, 33, 33, 33, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 33, 17280, 17280, 34560, 33, 33, 33, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 17280, 33, 34560, 17280, 33, 17280, 17280, 33, 17280, 34560, 33, 17280, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 3319872 . Total input tokens: 740314272 . Total output tokens: 664208977
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.836654266342521,
    "estimated_duration": 3600.0150626059385,
    "input_throughput": 4429.237578927706,
    "output_throughput": 3963.1267513842945,
    "total_throughput": 8392.364330312,
    "itl": 176.5915834383163,
    "ttft": 2218035.9354605805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0920870256610273,
    "arrivals": 1105156,
    "finished_requests": 64877,
    "scheduler_time": 77.16303623221022
}
#Debug simulation 
Total elapsed time: 4.8367517362348735. Arrivals time: 0.24103641929104924 Scheduler time: 4.503364067990333 Scheduler overhead time: 0.03149629710242152 Adapter cache time: 0.014215848408639431 Engine time: 0.0320906937122345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 34560, 34560, 33, 17280, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 17280, 33, 33, 33, 33, 33, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 33, 17280, 17280, 34560, 33, 33, 33, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 17280, 33, 34560, 17280, 33, 17280, 17280, 33, 17280, 34560, 33, 17280, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 3319872 . Total input tokens: 740314272 . Total output tokens: 664208977
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.04524191422388,
    "estimated_duration": 3600.11441482748,
    "input_throughput": 4611.645377608256,
    "output_throughput": 4121.601507687377,
    "total_throughput": 8733.246885295634,
    "itl": 209.4706915156189,
    "ttft": 2195915.8019261598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.74879519804381,
    "arrivals": 1105156,
    "finished_requests": 67612,
    "scheduler_time": 76.59203875330911
}
#Debug simulation 
Total elapsed time: 5.045329239219427. Arrivals time: 0.31924700271338224 Scheduler time: 4.64987519569695 Scheduler overhead time: 0.02739862399175763 Adapter cache time: 0.008545198943465948 Engine time: 0.027713637333363295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 34560, 34560, 33, 17280, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 17280, 33, 33, 33, 33, 33, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 33, 17280, 17280, 34560, 33, 33, 33, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 17280, 33, 34560, 17280, 33, 17280, 17280, 33, 17280, 34560, 33, 17280, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 3319872 . Total input tokens: 740314272 . Total output tokens: 664208977
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.22416798165068,
    "estimated_duration": 3600.0299401025636,
    "input_throughput": 4429.219274644623,
    "output_throughput": 3963.1103733524865,
    "total_throughput": 8392.329647997109,
    "itl": 176.59219140903548,
    "ttft": 2218043.086566285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.10692597245798,
    "arrivals": 1105156,
    "finished_requests": 64877,
    "scheduler_time": 77.16307478204762
}
#Debug simulation 
Total elapsed time: 5.224236225709319. Arrivals time: 0.6100347968749702 Scheduler time: 4.522143743000925 Scheduler overhead time: 0.0314276497811079 Adapter cache time: 0.01403795275837183 Engine time: 0.03199165780097246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 34560, 34560, 33, 17280, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 17280, 33, 33, 33, 33, 33, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 33, 17280, 17280, 34560, 33, 33, 33, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 17280, 33, 34560, 17280, 33, 17280, 17280, 33, 17280, 34560, 33, 17280, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 3319872 . Total input tokens: 740314272 . Total output tokens: 664208977
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.039661793969572,
    "estimated_duration": 3600.0852330571397,
    "input_throughput": 4611.68275893886,
    "output_throughput": 4121.63491679323,
    "total_throughput": 8733.31767573209,
    "itl": 209.46960700363053,
    "ttft": 2195899.1456708275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206021210714253,
    "arrivals": 1105156,
    "finished_requests": 67612,
    "scheduler_time": 76.59190583585172
}
#Debug simulation 
Total elapsed time: 5.039780014660209. Arrivals time: 0.32087742583826184 Scheduler time: 4.642486581578851 Scheduler overhead time: 0.027377184946089983 Adapter cache time: 0.008638117928057909 Engine time: 0.02787280874326825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_192_slots_128_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 17280, 17280, 17280, 17280, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 33, 17280, 17280, 34560, 17280, 33, 33, 17280, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 34560, 34560, 33, 17280, 34560, 33, 17280, 33, 17280, 33, 33, 17280, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 17280, 33, 33, 33, 33, 33, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 33, 17280, 17280, 34560, 33, 33, 33, 34560, 34560, 17280, 34560, 17280, 34560, 34560, 17280, 17280, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 17280, 34560, 34560, 17280, 34560, 17280, 33, 34560, 17280, 33, 17280, 17280, 33, 17280, 34560, 33, 17280, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 17280, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 17280, 33, 34560, 34560, 17280, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 17280, 17280, 17280, 17280, 17280, 17280, 33, 34560, 17280, 34560, 33, 34560, 17280, 33, 17280, 33, 33, 17280, 17280, 17280, 34560, 17280, 33, 33]
Prompts retrieved: 3319872 . Total input tokens: 740314272 . Total output tokens: 664208977
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.811657794751227,
    "estimated_duration": 3600.0421785615654,
    "input_throughput": 4429.2042173714535,
    "output_throughput": 3963.0969006314963,
    "total_throughput": 8392.30111800295,
    "itl": 176.59267305866067,
    "ttft": 2218049.3023503004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1191240897402206,
    "arrivals": 1105156,
    "finished_requests": 64877,
    "scheduler_time": 77.16311512377443
}
#Debug simulation 
Total elapsed time: 4.81175614381209. Arrivals time: 0.23664416279643774 Scheduler time: 4.483151519205421 Scheduler overhead time: 0.031514124013483524 Adapter cache time: 0.013870616909116507 Engine time: 0.031992136500775814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 8640, 34560, 4320, 4320, 4320, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 8640, 4320, 8640, 8640, 4320, 8640, 34560, 4320, 8640, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 3041280 . Total input tokens: 678338410 . Total output tokens: 608620142
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.9250988638959825,
    "estimated_duration": 3600.0049637908096,
    "input_throughput": 4675.992441486582,
    "output_throughput": 4110.485165669865,
    "total_throughput": 8786.477607156447,
    "itl": 208.1295726474496,
    "ttft": 2189301.9671553797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 880,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6932292146608807,
    "arrivals": 1012546,
    "finished_requests": 67822,
    "scheduler_time": 76.51611441217501
}
#Debug simulation 
Total elapsed time: 6.925196247640997. Arrivals time: 0.2651413241401315 Scheduler time: 6.568233469966799 Scheduler overhead time: 0.029193785972893238 Adapter cache time: 0.020329478662461042 Engine time: 0.02917923079803586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 8640, 34560, 4320, 4320, 4320, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 8640, 4320, 8640, 8640, 4320, 8640, 34560, 4320, 8640, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 3041280 . Total input tokens: 678338410 . Total output tokens: 608620142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.929929319769144,
    "estimated_duration": 3600.1883200586626,
    "input_throughput": 4675.754294910247,
    "output_throughput": 4110.2758201712295,
    "total_throughput": 8786.030115081478,
    "itl": 208.13908566780444,
    "ttft": 2189376.7526461794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 880,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8764014505781312,
    "arrivals": 1012546,
    "finished_requests": 67822,
    "scheduler_time": 76.51629844406212
}
#Debug simulation 
Total elapsed time: 6.93002517381683. Arrivals time: 0.264114695135504 Scheduler time: 6.573793385643512 Scheduler overhead time: 0.029011528939008713 Adapter cache time: 0.020658885594457388 Engine time: 0.02935147238895297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 8640, 34560, 4320, 4320, 4320, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 8640, 4320, 8640, 8640, 4320, 8640, 34560, 4320, 8640, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 3041280 . Total input tokens: 678338410 . Total output tokens: 608620142
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.862787843681872,
    "estimated_duration": 3600.09342947565,
    "input_throughput": 4488.160464866982,
    "output_throughput": 3950.9727396357302,
    "total_throughput": 8439.133204502712,
    "itl": 176.8359123161619,
    "ttft": 2212830.3324247035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.184034151453459,
    "arrivals": 1012546,
    "finished_requests": 65145,
    "scheduler_time": 76.91051895876224
}
#Debug simulation 
Total elapsed time: 5.862885552924126. Arrivals time: 0.24247195897623897 Scheduler time: 5.511683197226375 Scheduler overhead time: 0.03291233582422137 Adapter cache time: 0.028063835110515356 Engine time: 0.03283905331045389 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 8640, 34560, 4320, 4320, 4320, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 8640, 4320, 8640, 8640, 4320, 8640, 34560, 4320, 8640, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 3041280 . Total input tokens: 678338410 . Total output tokens: 608620142
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.888821206986904,
    "estimated_duration": 3600.061932716215,
    "input_throughput": 4675.918446574946,
    "output_throughput": 4110.420119588114,
    "total_throughput": 8786.33856616306,
    "itl": 208.13262282771564,
    "ttft": 2189325.4772167415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 880,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7501454971800294,
    "arrivals": 1012546,
    "finished_requests": 67822,
    "scheduler_time": 76.51616705498061
}
#Debug simulation 
Total elapsed time: 6.888944189995527. Arrivals time: 0.2611660580150783 Scheduler time: 6.536632330622524 Scheduler overhead time: 0.02883314387872815 Adapter cache time: 0.020033817272633314 Engine time: 0.029218447860330343 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 8640, 34560, 4320, 4320, 4320, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 8640, 4320, 8640, 8640, 4320, 8640, 34560, 4320, 8640, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 3041280 . Total input tokens: 678338410 . Total output tokens: 608620142
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.974767550826073,
    "estimated_duration": 3600.1499401928386,
    "input_throughput": 4488.090015254898,
    "output_throughput": 3950.9107221345653,
    "total_throughput": 8439.000737389464,
    "itl": 176.8385152438884,
    "ttft": 2212852.353977632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.240497601553789,
    "arrivals": 1012546,
    "finished_requests": 65145,
    "scheduler_time": 76.91056622589956
}
#Debug simulation 
Total elapsed time: 5.974877470172942. Arrivals time: 0.32937740394845605 Scheduler time: 5.536683171056211 Scheduler overhead time: 0.032943516969680786 Adapter cache time: 0.02812332008033991 Engine time: 0.03276656614616513 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 8640, 34560, 4320, 4320, 4320, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 8640, 4320, 8640, 8640, 4320, 8640, 34560, 4320, 8640, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 3041280 . Total input tokens: 678338410 . Total output tokens: 608620142
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.8940444169566035,
    "estimated_duration": 3600.175560626488,
    "input_throughput": 4675.893082578064,
    "output_throughput": 4110.518154127131,
    "total_throughput": 8786.411236705195,
    "itl": 208.12747757064014,
    "ttft": 2189311.878897421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 880,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.631244259513858,
    "arrivals": 1012546,
    "finished_requests": 67825,
    "scheduler_time": 76.52101488185284
}
#Debug simulation 
Total elapsed time: 6.894142751116306. Arrivals time: 0.26957593811675906 Scheduler time: 6.532380084041506 Scheduler overhead time: 0.029214856680482626 Adapter cache time: 0.020681355614215136 Engine time: 0.029185897670686245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 4320, 34560, 4320, 34560, 4320, 34560, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 34560, 34560, 4320, 8640, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 8640, 34560, 4320, 4320, 4320, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 34560, 8640, 34560, 8640, 4320, 34560, 8640, 4320, 8640, 8640, 4320, 8640, 34560, 4320, 8640, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 34560, 8640, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 34560, 8640, 34560, 4320, 34560, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 34560, 8640, 4320, 4320]
Prompts retrieved: 3041280 . Total input tokens: 678338410 . Total output tokens: 608620142
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.880793195217848,
    "estimated_duration": 3600.0078060306273,
    "input_throughput": 4487.721102419897,
    "output_throughput": 3950.6231003653006,
    "total_throughput": 8438.344202785198,
    "itl": 176.84067552225955,
    "ttft": 2212800.649959856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.293817206993734,
    "arrivals": 1012546,
    "finished_requests": 65139,
    "scheduler_time": 76.90640806942038
}
#Debug simulation 
Total elapsed time: 5.880884420126677. Arrivals time: 0.24800816923379898 Scheduler time: 5.524005171842873 Scheduler overhead time: 0.03304865537211299 Adapter cache time: 0.028103912714868784 Engine time: 0.0328392805531621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 8640, 34560, 1080, 1080, 1080, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 8640, 1080, 8640, 8640, 1080, 8640, 34560, 1080, 8640, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2833920 . Total input tokens: 632185017 . Total output tokens: 567029397
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.881581797730178,
    "estimated_duration": 3600.1968740263997,
    "input_throughput": 4640.835927762511,
    "output_throughput": 4113.656424415694,
    "total_throughput": 8754.492352178206,
    "itl": 208.82796464907258,
    "ttft": 2182190.5524622365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.384899444789707,
    "arrivals": 943308,
    "finished_requests": 67624,
    "scheduler_time": 76.47204015286057
}
#Debug simulation 
Total elapsed time: 5.881687933113426. Arrivals time: 0.5960587523877621 Scheduler time: 5.1925713596865535 Scheduler overhead time: 0.028557091020047665 Adapter cache time: 0.023139899596571922 Engine time: 0.02844652533531189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 8640, 34560, 1080, 1080, 1080, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 8640, 1080, 8640, 8640, 1080, 8640, 34560, 1080, 8640, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2833920 . Total input tokens: 632185017 . Total output tokens: 567029397
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.54511020379141,
    "estimated_duration": 3600.016700391543,
    "input_throughput": 4640.59791672161,
    "output_throughput": 4113.197029999751,
    "total_throughput": 8753.794946721362,
    "itl": 208.83859459697385,
    "ttft": 2182149.697798321,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1124,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6733919500769208,
    "arrivals": 943308,
    "finished_requests": 67619,
    "scheduler_time": 76.46221067147236
}
#Debug simulation 
Total elapsed time: 5.545202283654362. Arrivals time: 0.2501195352524519 Scheduler time: 5.201664814725518 Scheduler overhead time: 0.02848400454968214 Adapter cache time: 0.02341622719541192 Engine time: 0.028589842841029167 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 8640, 34560, 1080, 1080, 1080, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 8640, 1080, 8640, 8640, 1080, 8640, 34560, 1080, 8640, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2833920 . Total input tokens: 632185017 . Total output tokens: 567029397
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.055850064847618,
    "estimated_duration": 3600.1725100301287,
    "input_throughput": 4448.361281405922,
    "output_throughput": 3950.987615279847,
    "total_throughput": 8399.34889668577,
    "itl": 177.16996434607637,
    "ttft": 2203858.0545510766,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.48607422929253,
    "arrivals": 943308,
    "finished_requests": 64858,
    "scheduler_time": 76.85235694731787
}
#Debug simulation 
Total elapsed time: 5.055937353987247. Arrivals time: 0.23646713886409998 Scheduler time: 4.705390547402203 Scheduler overhead time: 0.0325184753164649 Adapter cache time: 0.03426823951303959 Engine time: 0.03237754572182894 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 8640, 34560, 1080, 1080, 1080, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 8640, 1080, 8640, 8640, 1080, 8640, 34560, 1080, 8640, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2833920 . Total input tokens: 632185017 . Total output tokens: 567029397
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.872621704824269,
    "estimated_duration": 3600.034818123134,
    "input_throughput": 4640.351786572253,
    "output_throughput": 4113.370216713694,
    "total_throughput": 8753.722003285948,
    "itl": 208.8318976672338,
    "ttft": 2182122.041331126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.458175893984697,
    "arrivals": 943308,
    "finished_requests": 67617,
    "scheduler_time": 76.46707017363808
}
#Debug simulation 
Total elapsed time: 5.8726961188949645. Arrivals time: 0.5989951193332672 Scheduler time: 5.180274713784456 Scheduler overhead time: 0.028520277235656977 Adapter cache time: 0.02337764948606491 Engine time: 0.028673114255070686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 8640, 34560, 1080, 1080, 1080, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 8640, 1080, 8640, 8640, 1080, 8640, 34560, 1080, 8640, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2833920 . Total input tokens: 632185017 . Total output tokens: 567029397
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.067587106022984,
    "estimated_duration": 3600.0562396461683,
    "input_throughput": 4448.18300993373,
    "output_throughput": 3950.8880009602153,
    "total_throughput": 8399.071010893946,
    "itl": 177.1750457312013,
    "ttft": 2203811.2232891098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.559388686772374,
    "arrivals": 943308,
    "finished_requests": 64852,
    "scheduler_time": 76.84832260498156
}
#Debug simulation 
Total elapsed time: 5.067678738385439. Arrivals time: 0.23735203547403216 Scheduler time: 4.716531491838396 Scheduler overhead time: 0.032503622118383646 Adapter cache time: 0.03398446552455425 Engine time: 0.03241158975288272 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 8640, 34560, 1080, 1080, 1080, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 8640, 1080, 8640, 8640, 1080, 8640, 34560, 1080, 8640, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2833920 . Total input tokens: 632185017 . Total output tokens: 567029397
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.50574335642159,
    "estimated_duration": 3600.11890124439,
    "input_throughput": 4640.93644080057,
    "output_throughput": 4113.74551959406,
    "total_throughput": 8754.68196039463,
    "itl": 208.8237852064197,
    "ttft": 2182160.086349314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1106,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3069956261617257,
    "arrivals": 943308,
    "finished_requests": 67624,
    "scheduler_time": 76.47197118933236
}
#Debug simulation 
Total elapsed time: 5.505866849329323. Arrivals time: 0.24786296766251326 Scheduler time: 5.165039866697043 Scheduler overhead time: 0.028294946532696486 Adapter cache time: 0.023114446084946394 Engine time: 0.028599624522030354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 34560, 34560, 1080, 8640, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 8640, 34560, 1080, 1080, 1080, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 34560, 8640, 34560, 8640, 1080, 34560, 8640, 1080, 8640, 8640, 1080, 8640, 34560, 1080, 8640, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 34560, 8640, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 34560, 8640, 34560, 1080, 34560, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 34560, 8640, 1080, 1080]
Prompts retrieved: 2833920 . Total input tokens: 632185017 . Total output tokens: 567029397
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.429356367792934,
    "estimated_duration": 3600.1265809068022,
    "input_throughput": 4448.096098878405,
    "output_throughput": 3950.810806329314,
    "total_throughput": 8398.90690520772,
    "itl": 177.17838867335396,
    "ttft": 2203837.266026724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1676,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.629685053378249,
    "arrivals": 943308,
    "finished_requests": 64852,
    "scheduler_time": 76.84836749907613
}
#Debug simulation 
Total elapsed time: 5.429419476073235. Arrivals time: 0.5832833494059741 Scheduler time: 4.732142569031566 Scheduler overhead time: 0.032492407131940126 Adapter cache time: 0.0339677962474525 Engine time: 0.0327052716165781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 34560, 34560, 540, 8640, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 8640, 540, 540, 540, 540, 540, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 540, 8640, 8640, 34560, 540, 540, 540, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 8640, 540, 34560, 8640, 540, 8640, 8640, 540, 8640, 34560, 540, 8640, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2799360 . Total input tokens: 624454548 . Total output tokens: 560131340
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.388568852096796,
    "estimated_duration": 3600.025023578242,
    "input_throughput": 4679.404695708465,
    "output_throughput": 4109.771988555302,
    "total_throughput": 8789.176684263766,
    "itl": 208.1236425713666,
    "ttft": 2174666.628268601,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5073189545470216,
    "arrivals": 932003,
    "finished_requests": 67986,
    "scheduler_time": 76.48188779738206
}
#Debug simulation 
Total elapsed time: 5.388663561083376. Arrivals time: 0.34574258234351873 Scheduler time: 4.948495991062373 Scheduler overhead time: 0.02827769471332431 Adapter cache time: 0.024537527468055487 Engine time: 0.02873378386721015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 34560, 34560, 540, 8640, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 8640, 540, 540, 540, 540, 540, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 540, 8640, 8640, 34560, 540, 540, 540, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 8640, 540, 34560, 8640, 540, 8640, 8640, 540, 8640, 34560, 540, 8640, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2799360 . Total input tokens: 624454548 . Total output tokens: 560131340
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.400813969783485,
    "estimated_duration": 3600.0227436941686,
    "input_throughput": 4678.333221505553,
    "output_throughput": 4108.844597137527,
    "total_throughput": 8787.17781864308,
    "itl": 208.13258534278455,
    "ttft": 2174769.534077408,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.741216033156968,
    "arrivals": 932003,
    "finished_requests": 67975,
    "scheduler_time": 76.47703072666083
}
#Debug simulation 
Total elapsed time: 5.400903707835823. Arrivals time: 0.4108989373780787 Scheduler time: 4.895231826230884 Scheduler overhead time: 0.02840178506448865 Adapter cache time: 0.024830364156514406 Engine time: 0.028604673221707344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 34560, 34560, 540, 8640, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 8640, 540, 540, 540, 540, 540, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 540, 8640, 8640, 34560, 540, 540, 540, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 8640, 540, 34560, 8640, 540, 8640, 8640, 540, 8640, 34560, 540, 8640, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2799360 . Total input tokens: 624454548 . Total output tokens: 560131340
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.412208647001535,
    "estimated_duration": 3600.0753455738786,
    "input_throughput": 4498.450572683343,
    "output_throughput": 3955.2663856067593,
    "total_throughput": 8453.716958290102,
    "itl": 177.3114807590241,
    "ttft": 2196616.1162716392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.538159588612566,
    "arrivals": 932003,
    "finished_requests": 65383,
    "scheduler_time": 76.89480277731843
}
#Debug simulation 
Total elapsed time: 5.412321452982724. Arrivals time: 0.695925994310528 Scheduler time: 4.601101242005825 Scheduler overhead time: 0.032249276991933584 Adapter cache time: 0.03560691373422742 Engine time: 0.032638905569911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 34560, 34560, 540, 8640, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 8640, 540, 540, 540, 540, 540, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 540, 8640, 8640, 34560, 540, 540, 540, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 8640, 540, 34560, 8640, 540, 8640, 8640, 540, 8640, 34560, 540, 8640, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2799360 . Total input tokens: 624454548 . Total output tokens: 560131340
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.358393318019807,
    "estimated_duration": 3600.097586720993,
    "input_throughput": 4679.310378178801,
    "output_throughput": 4109.689152475364,
    "total_throughput": 8788.999530654164,
    "itl": 208.12757403776465,
    "ttft": 2174695.988440246,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.579820882372982,
    "arrivals": 932003,
    "finished_requests": 67986,
    "scheduler_time": 76.48194901219924
}
#Debug simulation 
Total elapsed time: 5.358486659824848. Arrivals time: 0.3086649188771844 Scheduler time: 4.955740588251501 Scheduler overhead time: 0.028302017599344254 Adapter cache time: 0.024444666225463152 Engine time: 0.028462473303079605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 34560, 34560, 540, 8640, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 8640, 540, 540, 540, 540, 540, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 540, 8640, 8640, 34560, 540, 540, 540, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 8640, 540, 34560, 8640, 540, 8640, 8640, 540, 8640, 34560, 540, 8640, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2799360 . Total input tokens: 624454548 . Total output tokens: 560131340
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.059441865887493,
    "estimated_duration": 3600.148576895423,
    "input_throughput": 4498.359068826404,
    "output_throughput": 3955.185930764885,
    "total_throughput": 8453.54499959129,
    "itl": 177.31495783328185,
    "ttft": 2196644.423057077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.611348292305996,
    "arrivals": 932003,
    "finished_requests": 65383,
    "scheduler_time": 76.89484539522913
}
#Debug simulation 
Total elapsed time: 5.059535106178373. Arrivals time: 0.394190632738173 Scheduler time: 4.5501707824878395 Scheduler overhead time: 0.032310076989233494 Adapter cache time: 0.035585165489465 Engine time: 0.0325092519633472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 34560, 34560, 540, 8640, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 8640, 540, 540, 540, 540, 540, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 540, 8640, 8640, 34560, 540, 540, 540, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 8640, 540, 34560, 8640, 540, 8640, 8640, 540, 8640, 34560, 540, 8640, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2799360 . Total input tokens: 624454548 . Total output tokens: 560131340
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.640986503101885,
    "estimated_duration": 3600.179425021286,
    "input_throughput": 4679.883975477245,
    "output_throughput": 4110.073208340863,
    "total_throughput": 8789.95718381811,
    "itl": 208.1200177550761,
    "ttft": 2174715.7906029844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1146,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4265976379578085,
    "arrivals": 932003,
    "finished_requests": 67993,
    "scheduler_time": 76.48683968725777
}
#Debug simulation 
Total elapsed time: 5.641052945051342. Arrivals time: 0.7081438088789582 Scheduler time: 4.838777339551598 Scheduler overhead time: 0.028326517902314663 Adapter cache time: 0.024431690108031034 Engine time: 0.028555933386087418 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 8640, 8640, 8640, 8640, 34560, 8640, 540, 34560, 540, 8640, 8640, 540, 34560, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 540, 8640, 8640, 34560, 8640, 540, 540, 8640, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 34560, 34560, 540, 8640, 34560, 540, 8640, 540, 8640, 540, 540, 8640, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 8640, 540, 540, 540, 540, 540, 34560, 540, 8640, 540, 34560, 34560, 540, 34560, 540, 8640, 8640, 34560, 540, 540, 540, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 8640, 34560, 34560, 8640, 34560, 8640, 540, 34560, 8640, 540, 8640, 8640, 540, 8640, 34560, 540, 8640, 34560, 540, 8640, 34560, 34560, 8640, 540, 34560, 8640, 540, 540, 8640, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 8640, 540, 34560, 34560, 8640, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 540, 34560, 8640, 34560, 540, 34560, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 34560, 8640, 540, 540]
Prompts retrieved: 2799360 . Total input tokens: 624454548 . Total output tokens: 560131340
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.0909855351783335,
    "estimated_duration": 3600.0211740102527,
    "input_throughput": 4497.894378205089,
    "output_throughput": 3955.0603487532294,
    "total_throughput": 8452.954726958318,
    "itl": 177.31823156238244,
    "ttft": 2196643.568794372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.680764382406966,
    "arrivals": 932003,
    "finished_requests": 65377,
    "scheduler_time": 76.89066255730197
}
#Debug simulation 
Total elapsed time: 5.091078630182892. Arrivals time: 0.39745549159124494 Scheduler time: 4.577846975065768 Scheduler overhead time: 0.03221604926511645 Adapter cache time: 0.035583528224378824 Engine time: 0.032881833147257566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.022303840145469,
    "estimated_duration": 3600.041462015966,
    "input_throughput": 4669.8253832292785,
    "output_throughput": 4114.086228247533,
    "total_throughput": 8783.911611476811,
    "itl": 208.16507080972502,
    "ttft": 2173470.8378225267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1041,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1859677414340712,
    "arrivals": 926191,
    "finished_requests": 68112,
    "scheduler_time": 76.52878952160918
}
#Debug simulation 
Total elapsed time: 5.02243458526209. Arrivals time: 0.24781969236209989 Scheduler time: 4.683356822002679 Scheduler overhead time: 0.027901025488972664 Adapter cache time: 0.022506768349558115 Engine time: 0.0281399036757648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.982203104998916,
    "estimated_duration": 3600.038495995659,
    "input_throughput": 4669.1228492964065,
    "output_throughput": 4113.669066725963,
    "total_throughput": 8782.791916022368,
    "itl": 208.1795178187646,
    "ttft": 2173488.873989824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1042,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.393885819145949,
    "arrivals": 926191,
    "finished_requests": 68104,
    "scheduler_time": 76.52444163994801
}
#Debug simulation 
Total elapsed time: 4.98232423979789. Arrivals time: 0.2433244134299457 Scheduler time: 4.6474419394508 Scheduler overhead time: 0.027865362353622913 Adapter cache time: 0.022742673754692078 Engine time: 0.02824509283527732 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.97208192711696,
    "estimated_duration": 3600.0845718606406,
    "input_throughput": 4522.850970577228,
    "output_throughput": 3985.0030502437176,
    "total_throughput": 8507.854020820945,
    "itl": 176.40020405045314,
    "ttft": 2192012.2068795515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1356,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.425138344094096,
    "arrivals": 926191,
    "finished_requests": 65885,
    "scheduler_time": 77.42854583568011
}
#Debug simulation 
Total elapsed time: 4.972174603957683. Arrivals time: 0.3029085109010339 Scheduler time: 4.556757183745503 Scheduler overhead time: 0.03176404349505901 Adapter cache time: 0.03347112610936165 Engine time: 0.03260851325467229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.03480009175837,
    "estimated_duration": 3600.124987931573,
    "input_throughput": 4669.717039368395,
    "output_throughput": 4113.990778000597,
    "total_throughput": 8783.707817368992,
    "itl": 208.16893656539298,
    "ttft": 2173525.7503592027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1042,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2500602670549044,
    "arrivals": 926191,
    "finished_requests": 68112,
    "scheduler_time": 76.52925696496654
}
#Debug simulation 
Total elapsed time: 5.034895848017186. Arrivals time: 0.2439579893834889 Scheduler time: 4.699200983624905 Scheduler overhead time: 0.027866879478096962 Adapter cache time: 0.02278188383206725 Engine time: 0.028310120105743408 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.891405324917287,
    "estimated_duration": 3600.0324457403467,
    "input_throughput": 4522.565906111364,
    "output_throughput": 3985.022417499268,
    "total_throughput": 8507.588323610633,
    "itl": 176.3994559060548,
    "ttft": 2191915.9104896616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.485651343334441,
    "arrivals": 926191,
    "finished_requests": 65882,
    "scheduler_time": 77.42856017507184
}
#Debug simulation 
Total elapsed time: 4.891524372622371. Arrivals time: 0.2422401960939169 Scheduler time: 4.537753003649414 Scheduler overhead time: 0.03165963478386402 Adapter cache time: 0.03280440764501691 Engine time: 0.03237922675907612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.131972493138164,
    "estimated_duration": 3600.199834255895,
    "input_throughput": 4669.779949444058,
    "output_throughput": 4113.965524655723,
    "total_throughput": 8783.745474099782,
    "itl": 208.16041867821767,
    "ttft": 2173522.5914290254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1041,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.112642356993091,
    "arrivals": 926191,
    "finished_requests": 68115,
    "scheduler_time": 76.53367548382015
}
#Debug simulation 
Total elapsed time: 5.1321195382624865. Arrivals time: 0.3253429871983826 Scheduler time: 4.7148193516768515 Scheduler overhead time: 0.027905826922506094 Adapter cache time: 0.022889175917953253 Engine time: 0.0283437161706388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 8640, 8640, 8640, 8640, 34560, 8640, 270, 34560, 270, 8640, 8640, 270, 34560, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 270, 8640, 8640, 34560, 8640, 270, 270, 8640, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 34560, 34560, 270, 8640, 34560, 270, 8640, 270, 8640, 270, 270, 8640, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 8640, 270, 270, 270, 270, 270, 34560, 270, 8640, 270, 34560, 34560, 270, 34560, 270, 8640, 8640, 34560, 270, 270, 270, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 8640, 34560, 34560, 8640, 34560, 8640, 270, 34560, 8640, 270, 8640, 8640, 270, 8640, 34560, 270, 8640, 34560, 270, 8640, 34560, 34560, 8640, 270, 34560, 8640, 270, 270, 8640, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 8640, 270, 34560, 34560, 8640, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 270, 34560, 8640, 34560, 270, 34560, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 34560, 8640, 270, 270]
Prompts retrieved: 2782080 . Total input tokens: 620620174 . Total output tokens: 556713099
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.873364864382893,
    "estimated_duration": 3600.0861840142747,
    "input_throughput": 4522.498398037085,
    "output_throughput": 3984.962933304909,
    "total_throughput": 8507.461331341994,
    "itl": 176.401981672025,
    "ttft": 2191936.9410583847,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1357,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.539348210133619,
    "arrivals": 926191,
    "finished_requests": 65882,
    "scheduler_time": 77.42860158224643
}
#Debug simulation 
Total elapsed time: 4.873465210199356. Arrivals time: 0.2380370874889195 Scheduler time: 4.523900838103145 Scheduler overhead time: 0.031423797365278006 Adapter cache time: 0.033080654218792915 Engine time: 0.03240811778232455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.9827387989498675,
    "estimated_duration": 3600.0924088952816,
    "input_throughput": 4681.18178254524,
    "output_throughput": 4126.370190746986,
    "total_throughput": 8807.551973292226,
    "itl": 207.64164156070694,
    "ttft": 2177320.6660699565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5891726313671635,
    "arrivals": 923294,
    "finished_requests": 68236,
    "scheduler_time": 76.71937652591042
}
#Debug simulation 
Total elapsed time: 4.9828292252495885. Arrivals time: 0.24580305302515626 Scheduler time: 4.647501396480948 Scheduler overhead time: 0.02763958554714918 Adapter cache time: 0.02102969540283084 Engine time: 0.028169305995106697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.011561899911612,
    "estimated_duration": 3600.0316090674405,
    "input_throughput": 4681.216397531996,
    "output_throughput": 4126.294047692546,
    "total_throughput": 8807.510445224543,
    "itl": 207.64984928803224,
    "ttft": 2177349.657425361,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.756352951221639,
    "arrivals": 923294,
    "finished_requests": 68234,
    "scheduler_time": 76.71467311326317
}
#Debug simulation 
Total elapsed time: 5.011656929273158. Arrivals time: 0.24797588307410479 Scheduler time: 4.673844684846699 Scheduler overhead time: 0.02766396291553974 Adapter cache time: 0.021223718766123056 Engine time: 0.02825191104784608 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.8851274023763835,
    "estimated_duration": 3600.0876176176234,
    "input_throughput": 4535.540724090867,
    "output_throughput": 4002.543140751548,
    "total_throughput": 8538.083864842414,
    "itl": 174.97289347427622,
    "ttft": 2195404.2447688277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1923998305946255,
    "arrivals": 923294,
    "finished_requests": 66072,
    "scheduler_time": 77.8247430961574
}
#Debug simulation 
Total elapsed time: 4.885227102320641. Arrivals time: 0.23969261813908815 Scheduler time: 4.537045415956527 Scheduler overhead time: 0.03161059459671378 Adapter cache time: 0.029817415867000818 Engine time: 0.03235440328717232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.994929275009781,
    "estimated_duration": 3600.1403364321204,
    "input_throughput": 4681.1194634433805,
    "output_throughput": 4126.315257677482,
    "total_throughput": 8807.434721120862,
    "itl": 207.64412131471337,
    "ttft": 2177340.0046458305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6370431182370258,
    "arrivals": 923294,
    "finished_requests": 68236,
    "scheduler_time": 76.7194335757954
}
#Debug simulation 
Total elapsed time: 4.995072547812015. Arrivals time: 0.24677602155134082 Scheduler time: 4.6584110842086375 Scheduler overhead time: 0.027672807686030865 Adapter cache time: 0.02106094779446721 Engine time: 0.028321899939328432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.870247785001993,
    "estimated_duration": 3600.131294862204,
    "input_throughput": 4535.485698341725,
    "output_throughput": 4002.494581395962,
    "total_throughput": 8537.980279737687,
    "itl": 174.9749084767676,
    "ttft": 2195421.1469184244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.236036394480623,
    "arrivals": 923294,
    "finished_requests": 66072,
    "scheduler_time": 77.82478377689034
}
#Debug simulation 
Total elapsed time: 4.870342550333589. Arrivals time: 0.23967944039031863 Scheduler time: 4.521972521208227 Scheduler overhead time: 0.03169209184125066 Adapter cache time: 0.029824425000697374 Engine time: 0.032470638398081064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.011064227670431,
    "estimated_duration": 3600.032756920684,
    "input_throughput": 4681.259348988557,
    "output_throughput": 4126.438564049792,
    "total_throughput": 8807.697913038348,
    "itl": 207.63851472047278,
    "ttft": 2177296.4714264725,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5295825494871877,
    "arrivals": 923294,
    "finished_requests": 68236,
    "scheduler_time": 76.71931463308101
}
#Debug simulation 
Total elapsed time: 5.011155824642628. Arrivals time: 0.24841325730085373 Scheduler time: 4.673450264148414 Scheduler overhead time: 0.02749432111158967 Adapter cache time: 0.021087816450744867 Engine time: 0.028011929243803024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 8640, 8640, 8640, 8640, 34560, 8640, 135, 34560, 135, 8640, 8640, 135, 34560, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 135, 8640, 8640, 34560, 8640, 135, 135, 8640, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 34560, 34560, 135, 8640, 34560, 135, 8640, 135, 8640, 135, 135, 8640, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 8640, 135, 135, 135, 135, 135, 34560, 135, 8640, 135, 34560, 34560, 135, 34560, 135, 8640, 8640, 34560, 135, 135, 135, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 8640, 34560, 34560, 8640, 34560, 8640, 135, 34560, 8640, 135, 8640, 8640, 135, 8640, 34560, 135, 8640, 34560, 135, 8640, 34560, 34560, 8640, 135, 34560, 8640, 135, 135, 8640, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 8640, 135, 34560, 34560, 8640, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 135, 34560, 8640, 34560, 135, 34560, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 34560, 8640, 135, 135]
Prompts retrieved: 2773440 . Total input tokens: 618725733 . Total output tokens: 554986694
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.827500162180513,
    "estimated_duration": 3600.168179755726,
    "input_throughput": 4535.439230816125,
    "output_throughput": 4002.453574537647,
    "total_throughput": 8537.892805353771,
    "itl": 174.97661686730922,
    "ttft": 2195435.8413288975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 978,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.272882253900209,
    "arrivals": 923294,
    "finished_requests": 66072,
    "scheduler_time": 77.82482281102219
}
#Debug simulation 
Total elapsed time: 4.827621035743505. Arrivals time: 0.23432497819885612 Scheduler time: 4.484943526331335 Scheduler overhead time: 0.0315562323667109 Adapter cache time: 0.02984262863174081 Engine time: 0.032308656722307205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.111379874870181,
    "estimated_duration": 3600.019186539197,
    "input_throughput": 4685.157530011502,
    "output_throughput": 4154.428414137887,
    "total_throughput": 8839.585944149389,
    "itl": 207.4054836994761,
    "ttft": 2175362.495167843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7169336243462985,
    "arrivals": 921863,
    "finished_requests": 68412,
    "scheduler_time": 77.20007931617582
}
#Debug simulation 
Total elapsed time: 5.111496750731021. Arrivals time: 0.34395000291988254 Scheduler time: 4.679070818703622 Scheduler overhead time: 0.02741864463314414 Adapter cache time: 0.020404557697474957 Engine time: 0.02800394594669342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.154229762032628,
    "estimated_duration": 3600.128979038489,
    "input_throughput": 4685.0146475876245,
    "output_throughput": 4154.301717266365,
    "total_throughput": 8839.316364853988,
    "itl": 207.41078160608419,
    "ttft": 2175406.4023247417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8265555066708523,
    "arrivals": 921863,
    "finished_requests": 68412,
    "scheduler_time": 77.20024993310693
}
#Debug simulation 
Total elapsed time: 5.154365018941462. Arrivals time: 0.4098811368457973 Scheduler time: 4.656174393836409 Scheduler overhead time: 0.027320306282490492 Adapter cache time: 0.020386509597301483 Engine time: 0.027952122036367655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.999459732323885,
    "estimated_duration": 3600.14152727433,
    "input_throughput": 4553.07544878942,
    "output_throughput": 4038.4109596400713,
    "total_throughput": 8591.486408429491,
    "itl": 174.61547300899952,
    "ttft": 2192410.605284218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8727731346525367,
    "arrivals": 921863,
    "finished_requests": 66429,
    "scheduler_time": 78.46580119728699
}
#Debug simulation 
Total elapsed time: 4.999551358167082. Arrivals time: 0.3332754531875253 Scheduler time: 4.558865671977401 Scheduler overhead time: 0.0317416749894619 Adapter cache time: 0.028305333107709885 Engine time: 0.032617191318422556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.076934378128499,
    "estimated_duration": 3600.0585841224934,
    "input_throughput": 4685.106257544754,
    "output_throughput": 4154.3829497556635,
    "total_throughput": 8839.489207300418,
    "itl": 207.4074080119069,
    "ttft": 2175378.215362889,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7562771118990992,
    "arrivals": 921863,
    "finished_requests": 68412,
    "scheduler_time": 77.20013341186474
}
#Debug simulation 
Total elapsed time: 5.077034220099449. Arrivals time: 0.34345265850424767 Scheduler time: 4.645543104968965 Scheduler overhead time: 0.027400740887969732 Adapter cache time: 0.020027357153594494 Engine time: 0.028023179154843092 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.0209373771212995,
    "estimated_duration": 3600.162817100175,
    "input_throughput": 4553.048523845109,
    "output_throughput": 4038.3870782018175,
    "total_throughput": 8591.435602046926,
    "itl": 174.6163744184337,
    "ttft": 2192419.0900770733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8940255245566417,
    "arrivals": 921863,
    "finished_requests": 66429,
    "scheduler_time": 78.46583863324719
}
#Debug simulation 
Total elapsed time: 5.021057253237814. Arrivals time: 0.3909263024106622 Scheduler time: 4.522679202724248 Scheduler overhead time: 0.03168930718675256 Adapter cache time: 0.028609182219952345 Engine time: 0.03249784605577588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.104914186988026,
    "estimated_duration": 3600.210899065525,
    "input_throughput": 4684.991372138228,
    "output_throughput": 4154.395511630215,
    "total_throughput": 8839.386883768444,
    "itl": 207.40421152143244,
    "ttft": 2175429.5180142475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 561,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6774182154400978,
    "arrivals": 921863,
    "finished_requests": 68414,
    "scheduler_time": 77.2049782342125
}
#Debug simulation 
Total elapsed time: 5.105006606783718. Arrivals time: 0.3440764592960477 Scheduler time: 4.672533946111798 Scheduler overhead time: 0.027627776842564344 Adapter cache time: 0.02006383752450347 Engine time: 0.028118913527578115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 8640, 8640, 8640, 8640, 34560, 8640, 66, 34560, 66, 8640, 8640, 66, 34560, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 66, 8640, 8640, 34560, 8640, 66, 66, 8640, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 34560, 34560, 66, 8640, 34560, 66, 8640, 66, 8640, 66, 66, 8640, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 8640, 66, 66, 66, 66, 66, 34560, 66, 8640, 66, 34560, 34560, 66, 34560, 66, 8640, 8640, 34560, 66, 66, 66, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 8640, 34560, 34560, 8640, 34560, 8640, 66, 34560, 8640, 66, 8640, 8640, 66, 8640, 34560, 66, 8640, 34560, 66, 8640, 34560, 34560, 8640, 66, 34560, 8640, 66, 66, 8640, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 8640, 66, 34560, 34560, 8640, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 66, 34560, 8640, 34560, 66, 34560, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 34560, 8640, 66, 66]
Prompts retrieved: 2769024 . Total input tokens: 617742491 . Total output tokens: 554114417
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.004461848177016,
    "estimated_duration": 3600.1885102941474,
    "input_throughput": 4553.016030446901,
    "output_throughput": 4038.358257749155,
    "total_throughput": 8591.374288196057,
    "itl": 174.61748382168486,
    "ttft": 2192429.302482306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9196792969852645,
    "arrivals": 921863,
    "finished_requests": 66429,
    "scheduler_time": 78.46587805481458
}
#Debug simulation 
Total elapsed time: 5.004622185137123. Arrivals time: 0.24233164824545383 Scheduler time: 4.655022478662431 Scheduler overhead time: 0.03153235372155905 Adapter cache time: 0.028613676317036152 Engine time: 0.032336510717868805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.0864329296164215,
    "estimated_duration": 3600.178245561861,
    "input_throughput": 4722.258132901625,
    "output_throughput": 4168.165289732223,
    "total_throughput": 8890.423422633849,
    "itl": 205.82986417715227,
    "ttft": 2168625.400527041,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.068110222632532,
    "arrivals": 921076,
    "finished_requests": 68669,
    "scheduler_time": 77.44868458642759
}
#Debug simulation 
Total elapsed time: 5.08652573870495. Arrivals time: 0.3497503506951034 Scheduler time: 4.649638006929308 Scheduler overhead time: 0.027633509133011103 Adapter cache time: 0.01869111182168126 Engine time: 0.028146015945822 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.07394139515236,
    "estimated_duration": 3600.1176042194793,
    "input_throughput": 4721.810748647882,
    "output_throughput": 4167.9755079148845,
    "total_throughput": 8889.786256562766,
    "itl": 205.883629049348,
    "ttft": 2168628.1316850195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1247270843456558,
    "arrivals": 921076,
    "finished_requests": 68659,
    "scheduler_time": 77.42961286956739
}
#Debug simulation 
Total elapsed time: 5.074067321140319. Arrivals time: 0.3450214546173811 Scheduler time: 4.641947486903518 Scheduler overhead time: 0.027460922952741385 Adapter cache time: 0.0186939244158566 Engine time: 0.028219758067280054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.009753833990544,
    "estimated_duration": 3600.011283201812,
    "input_throughput": 4574.975938784221,
    "output_throughput": 4044.530379096527,
    "total_throughput": 8619.506317880749,
    "itl": 172.8546964824459,
    "ttft": 2184227.723755793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1776262181252306,
    "arrivals": 921076,
    "finished_requests": 66530,
    "scheduler_time": 78.67418740467484
}
#Debug simulation 
Total elapsed time: 5.009852332994342. Arrivals time: 0.345076615922153 Scheduler time: 4.557185746729374 Scheduler overhead time: 0.03187480429187417 Adapter cache time: 0.027904571034014225 Engine time: 0.032984794583171606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.172837607096881,
    "estimated_duration": 3600.1986630056113,
    "input_throughput": 4722.2313520351145,
    "output_throughput": 4168.141651236598,
    "total_throughput": 8890.373003271712,
    "itl": 205.83074243619689,
    "ttft": 2168636.8111920417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0884730379492993,
    "arrivals": 921076,
    "finished_requests": 68669,
    "scheduler_time": 77.44873921484039
}
#Debug simulation 
Total elapsed time: 5.172933927271515. Arrivals time: 0.2477766931988299 Scheduler time: 4.8381194127723575 Scheduler overhead time: 0.02735213004052639 Adapter cache time: 0.01867735106498003 Engine time: 0.028282047249376774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.986999464221299,
    "estimated_duration": 3600.027039425197,
    "input_throughput": 4574.95591550604,
    "output_throughput": 4044.512677417222,
    "total_throughput": 8619.468592923262,
    "itl": 172.85532878674726,
    "ttft": 2184235.0340664666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1933454414270865,
    "arrivals": 921076,
    "finished_requests": 66530,
    "scheduler_time": 78.67422440477004
}
#Debug simulation 
Total elapsed time: 4.987141450401396. Arrivals time: 0.3379242829978466 Scheduler time: 4.542129171546549 Scheduler overhead time: 0.03198257042095065 Adapter cache time: 0.027507381979376078 Engine time: 0.03278299747034907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.0797717911191285,
    "estimated_duration": 3600.112420079194,
    "input_throughput": 4722.335031867149,
    "output_throughput": 4168.240946117427,
    "total_throughput": 8890.575977984576,
    "itl": 205.83011644790582,
    "ttft": 2168567.8537479746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0435275529208592,
    "arrivals": 921076,
    "finished_requests": 68668,
    "scheduler_time": 77.44771361028297
}
#Debug simulation 
Total elapsed time: 5.079865374136716. Arrivals time: 0.3453366169705987 Scheduler time: 4.647472056094557 Scheduler overhead time: 0.027662204578518867 Adapter cache time: 0.018617380410432816 Engine time: 0.02811537915840745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_192_slots_128_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 8640, 8640, 8640, 8640, 34560, 8640, 33, 34560, 33, 8640, 8640, 33, 34560, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 33, 8640, 8640, 34560, 8640, 33, 33, 8640, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 34560, 34560, 33, 8640, 34560, 33, 8640, 33, 8640, 33, 33, 8640, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 8640, 33, 33, 33, 33, 33, 34560, 33, 8640, 33, 34560, 34560, 33, 34560, 33, 8640, 8640, 34560, 33, 33, 33, 34560, 34560, 8640, 34560, 8640, 34560, 34560, 8640, 8640, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 8640, 34560, 34560, 8640, 34560, 8640, 33, 34560, 8640, 33, 8640, 8640, 33, 8640, 34560, 33, 8640, 34560, 33, 8640, 34560, 34560, 8640, 33, 34560, 8640, 33, 33, 8640, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 8640, 33, 34560, 34560, 8640, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 33, 34560, 8640, 34560, 33, 34560, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 34560, 8640, 33, 33]
Prompts retrieved: 2766912 . Total input tokens: 617284907 . Total output tokens: 553676923
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.051851940806955,
    "estimated_duration": 3600.040786493773,
    "input_throughput": 4574.938445639326,
    "output_throughput": 4044.4972330941077,
    "total_throughput": 8619.435678733435,
    "itl": 172.85585850894458,
    "ttft": 2184242.4059485286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2070526041463048,
    "arrivals": 921076,
    "finished_requests": 66530,
    "scheduler_time": 78.67426431063187
}
#Debug simulation 
Total elapsed time: 5.051957034971565. Arrivals time: 0.35211811820045114 Scheduler time: 4.5922790062613785 Scheduler overhead time: 0.03190942294895649 Adapter cache time: 0.027871996629983187 Engine time: 0.03297095326706767 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.199667946435511,
    "estimated_duration": 3600.0448820581246,
    "input_throughput": 4667.1912574585285,
    "output_throughput": 4110.224034634993,
    "total_throughput": 8777.415292093521,
    "itl": 208.5241760694211,
    "ttft": 2169251.5419447036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.218911095671536,
    "arrivals": 851723,
    "finished_requests": 67899,
    "scheduler_time": 76.38874774876093
}
#Debug simulation 
Total elapsed time: 5.19975718902424. Arrivals time: 0.24762570532038808 Scheduler time: 4.844623465556651 Scheduler overhead time: 0.028413322288542986 Adapter cache time: 0.03743803594261408 Engine time: 0.02870465163141489 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.1640274142846465,
    "estimated_duration": 3600.0687508839455,
    "input_throughput": 4666.809486867382,
    "output_throughput": 4109.937066164928,
    "total_throughput": 8776.74655303231,
    "itl": 208.5498714944479,
    "ttft": 2169344.179921782,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2038,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.65717544513964,
    "arrivals": 851723,
    "finished_requests": 67896,
    "scheduler_time": 76.3802021328496
}
#Debug simulation 
Total elapsed time: 5.1641260222531855. Arrivals time: 0.24197417218238115 Scheduler time: 4.814734716434032 Scheduler overhead time: 0.028379329945892096 Adapter cache time: 0.037542514968663454 Engine time: 0.02855857089161873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.9283016212284565,
    "estimated_duration": 3600.0884501813503,
    "input_throughput": 4499.246122462369,
    "output_throughput": 3963.9014978315727,
    "total_throughput": 8463.147620293941,
    "itl": 177.25228928588783,
    "ttft": 2189040.9230313143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2911,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.521025705542117,
    "arrivals": 851723,
    "finished_requests": 65412,
    "scheduler_time": 77.0222055940457
}
#Debug simulation 
Total elapsed time: 4.928396278992295. Arrivals time: 0.23603904154151678 Scheduler time: 4.557764727622271 Scheduler overhead time: 0.03325650515034795 Adapter cache time: 0.05391198303550482 Engine time: 0.03249750845134258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.231807005126029,
    "estimated_duration": 3600.011297696541,
    "input_throughput": 4667.113408991384,
    "output_throughput": 4110.130712497352,
    "total_throughput": 8777.244121488735,
    "itl": 208.5327168966385,
    "ttft": 2169290.030244729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2039,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.372105795934133,
    "arrivals": 851723,
    "finished_requests": 67898,
    "scheduler_time": 76.38492486694604
}
#Debug simulation 
Total elapsed time: 5.231910842936486. Arrivals time: 0.25918097887188196 Scheduler time: 4.864167472347617 Scheduler overhead time: 0.028464642819017172 Adapter cache time: 0.03831177297979593 Engine time: 0.02876676432788372 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.889344439841807,
    "estimated_duration": 3600.018199062243,
    "input_throughput": 4499.106977908908,
    "output_throughput": 3963.953016603423,
    "total_throughput": 8463.05999451233,
    "itl": 177.26182484992063,
    "ttft": 2189037.3141794144,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2911,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.646527984384042,
    "arrivals": 851723,
    "finished_requests": 65408,
    "scheduler_time": 77.01801498034435
}
#Debug simulation 
Total elapsed time: 4.889433544129133. Arrivals time: 0.23114677565172315 Scheduler time: 4.525552156381309 Scheduler overhead time: 0.03181130159646273 Adapter cache time: 0.053918450605124235 Engine time: 0.032243747264146805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.225056572351605,
    "estimated_duration": 3600.1302212656888,
    "input_throughput": 4667.218952455768,
    "output_throughput": 4110.432426190815,
    "total_throughput": 8777.651378646582,
    "itl": 208.51588031492062,
    "ttft": 2169243.3814079347,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2032,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.075782199241042,
    "arrivals": 851723,
    "finished_requests": 67903,
    "scheduler_time": 76.3935561099122
}
#Debug simulation 
Total elapsed time: 5.225152825936675. Arrivals time: 0.26318266382440925 Scheduler time: 4.854297960177064 Scheduler overhead time: 0.02837946219369769 Adapter cache time: 0.03784066950902343 Engine time: 0.028591446112841368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [64 64 64]
Adapter prompts. [34560, 1080, 34560, 1080, 34560, 1080, 34560, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 34560, 4320, 1080, 34560, 1080, 4320, 4320, 1080, 34560, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 1080, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 34560, 34560, 1080, 4320, 34560, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 34560, 4320, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 1080, 34560, 1080, 4320, 4320, 34560, 1080, 1080, 1080, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 4320, 34560, 34560, 4320, 34560, 4320, 1080, 34560, 4320, 1080, 4320, 4320, 1080, 4320, 34560, 1080, 4320, 34560, 1080, 4320, 34560, 34560, 4320, 1080, 34560, 4320, 1080, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 34560, 4320, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 34560, 4320, 34560, 1080, 34560, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 34560, 4320, 1080, 1080]
Prompts retrieved: 2557440 . Total input tokens: 570520789 . Total output tokens: 511723372
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.915484241209924,
    "estimated_duration": 3600.1393500345052,
    "input_throughput": 4498.9555751070475,
    "output_throughput": 3963.819622666336,
    "total_throughput": 8462.775197773384,
    "itl": 177.26764907376153,
    "ttft": 2189082.616229768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2911,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.76762888070145,
    "arrivals": 851723,
    "finished_requests": 65408,
    "scheduler_time": 77.01806505638942
}
#Debug simulation 
Total elapsed time: 4.91557661537081. Arrivals time: 0.23198945308104157 Scheduler time: 4.549914276693016 Scheduler overhead time: 0.03197513474151492 Adapter cache time: 0.0545049044303596 Engine time: 0.03248526714742184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.027825165074319,
    "estimated_duration": 3600.162649654363,
    "input_throughput": 4662.469625257492,
    "output_throughput": 4142.257573121514,
    "total_throughput": 8804.727198379007,
    "itl": 207.9778205697958,
    "ttft": 2165212.964629893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.766738401835518,
    "arrivals": 839966,
    "finished_requests": 68103,
    "scheduler_time": 76.93430633894107
}
#Debug simulation 
Total elapsed time: 5.027917697094381. Arrivals time: 0.2424211665056646 Scheduler time: 4.674802713561803 Scheduler overhead time: 0.027611555065959692 Adapter cache time: 0.04208449134603143 Engine time: 0.0283064185641706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.017400417011231,
    "estimated_duration": 3600.125194256274,
    "input_throughput": 4661.977596439458,
    "output_throughput": 4141.562638929336,
    "total_throughput": 8803.540235368793,
    "itl": 208.00211045914534,
    "ttft": 2165305.832159109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.201421437174724,
    "arrivals": 839966,
    "finished_requests": 68092,
    "scheduler_time": 76.92442468195085
}
#Debug simulation 
Total elapsed time: 5.017493593040854. Arrivals time: 0.24141835048794746 Scheduler time: 4.665405508130789 Scheduler overhead time: 0.02775998692959547 Adapter cache time: 0.04186406405642629 Engine time: 0.028332121204584837 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.9690100606530905,
    "estimated_duration": 3600.050591920379,
    "input_throughput": 4584.453073254205,
    "output_throughput": 4079.3104499584765,
    "total_throughput": 8663.763523212681,
    "itl": 172.55455210164598,
    "ttft": 2176114.067588902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.5076508314348365,
    "arrivals": 839966,
    "finished_requests": 66973,
    "scheduler_time": 79.24862997098764
}
#Debug simulation 
Total elapsed time: 4.969103559851646. Arrivals time: 0.2365503767505288 Scheduler time: 4.6012415126897395 Scheduler overhead time: 0.03217354556545615 Adapter cache time: 0.05130976112559438 Engine time: 0.03292558994144201 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.093628427945077,
    "estimated_duration": 3600.046399196014,
    "input_throughput": 4662.402130080463,
    "output_throughput": 4141.99522631989,
    "total_throughput": 8804.397356400354,
    "itl": 207.98474922193398,
    "ttft": 2165246.2434588927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.886394446657274,
    "arrivals": 839966,
    "finished_requests": 68098,
    "scheduler_time": 76.92933672009099
}
#Debug simulation 
Total elapsed time: 5.093772658146918. Arrivals time: 0.24988625152036548 Scheduler time: 4.733678778167814 Scheduler overhead time: 0.027519063092768192 Adapter cache time: 0.04187071183696389 Engine time: 0.028076127637177706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.996177216991782,
    "estimated_duration": 3600.1493561519064,
    "input_throughput": 4584.327306254017,
    "output_throughput": 4079.198540723082,
    "total_throughput": 8663.525846977098,
    "itl": 172.55920285561683,
    "ttft": 2176149.496452633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.606367553770367,
    "arrivals": 839966,
    "finished_requests": 66973,
    "scheduler_time": 79.24867748027216
}
#Debug simulation 
Total elapsed time: 4.996287622023374. Arrivals time: 0.2883506962098181 Scheduler time: 4.57705201394856 Scheduler overhead time: 0.03190184663981199 Adapter cache time: 0.05152794858440757 Engine time: 0.03262996440753341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.0217740992084146,
    "estimated_duration": 3600.0068407792896,
    "input_throughput": 4662.671417692759,
    "output_throughput": 4142.436850695495,
    "total_throughput": 8805.108268388254,
    "itl": 207.96912901047514,
    "ttft": 2165155.27770141,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2211,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.611001202028512,
    "arrivals": 839966,
    "finished_requests": 68103,
    "scheduler_time": 76.934234663364
}
#Debug simulation 
Total elapsed time: 5.021863621193916. Arrivals time: 0.24631612934172153 Scheduler time: 4.666102288290858 Scheduler overhead time: 0.027573302388191223 Adapter cache time: 0.041153636295348406 Engine time: 0.02804244263097644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 4320, 4320, 4320, 4320, 34560, 4320, 540, 34560, 540, 4320, 4320, 540, 34560, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 540, 4320, 4320, 34560, 4320, 540, 540, 4320, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 34560, 34560, 540, 4320, 34560, 540, 4320, 540, 4320, 540, 540, 4320, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 4320, 540, 540, 540, 540, 540, 34560, 540, 4320, 540, 34560, 34560, 540, 34560, 540, 4320, 4320, 34560, 540, 540, 540, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 4320, 34560, 34560, 4320, 34560, 4320, 540, 34560, 4320, 540, 4320, 4320, 540, 4320, 34560, 540, 4320, 34560, 540, 4320, 34560, 34560, 4320, 540, 34560, 4320, 540, 540, 4320, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 4320, 540, 34560, 34560, 4320, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 540, 34560, 4320, 34560, 540, 34560, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 34560, 4320, 540, 540]
Prompts retrieved: 2522880 . Total input tokens: 562856970 . Total output tokens: 504718618
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.996001815889031,
    "estimated_duration": 3600.055317648196,
    "input_throughput": 4584.270391373127,
    "output_throughput": 4079.0598211121796,
    "total_throughput": 8663.330212485305,
    "itl": 172.5628070064821,
    "ttft": 2176166.0343129416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2300,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.696910279988939,
    "arrivals": 839966,
    "finished_requests": 66969,
    "scheduler_time": 79.24451135430105
}
#Debug simulation 
Total elapsed time: 4.996093669906259. Arrivals time: 0.24566756514832377 Scheduler time: 4.6188940913416445 Scheduler overhead time: 0.03208750067278743 Adapter cache time: 0.051658631302416325 Engine time: 0.03287205612286925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.110151677858084,
    "estimated_duration": 3600.0748020967285,
    "input_throughput": 4798.491128556237,
    "output_throughput": 4229.011016974679,
    "total_throughput": 9027.502145530914,
    "itl": 202.76388988660403,
    "ttft": 2152188.885437634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.373436986080021,
    "arrivals": 834363,
    "finished_requests": 70123,
    "scheduler_time": 78.55654601056843
}
#Debug simulation 
Total elapsed time: 5.11024442082271. Arrivals time: 0.32351407688111067 Scheduler time: 4.6829603761434555 Scheduler overhead time: 0.027352015022188425 Adapter cache time: 0.03569155652076006 Engine time: 0.02807106263935566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.452278142794967,
    "estimated_duration": 3600.1293910859795,
    "input_throughput": 4798.390591952876,
    "output_throughput": 4228.742732884851,
    "total_throughput": 9027.133324837727,
    "itl": 202.7778543844498,
    "ttft": 2152126.5642651026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.645829397151194,
    "arrivals": 834363,
    "finished_requests": 70121,
    "scheduler_time": 78.55184651880722
}
#Debug simulation 
Total elapsed time: 5.452354679815471. Arrivals time: 0.6353699355386198 Scheduler time: 4.713561442214996 Scheduler overhead time: 0.02759830141440034 Adapter cache time: 0.034868559800088406 Engine time: 0.028324614744633436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.094787875190377,
    "estimated_duration": 3600.177515972804,
    "input_throughput": 4713.090375326975,
    "output_throughput": 4160.196527407328,
    "total_throughput": 8873.286902734304,
    "itl": 168.67882268225435,
    "ttft": 2163509.502386698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.561405614502668,
    "arrivals": 834363,
    "finished_requests": 68855,
    "scheduler_time": 80.79416952037275
}
#Debug simulation 
Total elapsed time: 5.094884185120463. Arrivals time: 0.30869572423398495 Scheduler time: 4.6619297079741955 Scheduler overhead time: 0.03206957085058093 Adapter cache time: 0.043834835290908813 Engine time: 0.03331795195117593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.087082664947957,
    "estimated_duration": 3600.151529460488,
    "input_throughput": 4798.38886186793,
    "output_throughput": 4228.92088719431,
    "total_throughput": 9027.30974906224,
    "itl": 202.7678891914601,
    "ttft": 2152219.194339384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.450112239618241,
    "arrivals": 834363,
    "finished_requests": 70123,
    "scheduler_time": 78.55659812065014
}
#Debug simulation 
Total elapsed time: 5.087171964813024. Arrivals time: 0.3108863029628992 Scheduler time: 4.673313727602363 Scheduler overhead time: 0.027316024992614985 Adapter cache time: 0.03488240530714393 Engine time: 0.028158511966466904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.103644147049636,
    "estimated_duration": 3600.046997314219,
    "input_throughput": 4713.209858832023,
    "output_throughput": 4160.34679857618,
    "total_throughput": 8873.556657408204,
    "itl": 168.68264987679746,
    "ttft": 2163431.0959866676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.6218931857682435,
    "arrivals": 834363,
    "finished_requests": 68854,
    "scheduler_time": 80.78996745215844
}
#Debug simulation 
Total elapsed time: 5.103735039010644. Arrivals time: 0.30825711879879236 Scheduler time: 4.671030018012971 Scheduler overhead time: 0.0323505369015038 Adapter cache time: 0.04388277232646942 Engine time: 0.03314090520143509 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.092418517917395,
    "estimated_duration": 3600.193460137397,
    "input_throughput": 4798.541853731575,
    "output_throughput": 4229.076345085894,
    "total_throughput": 9027.61819881747,
    "itl": 202.7579345636067,
    "ttft": 2152225.0749443313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1429,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.272781871415094,
    "arrivals": 834363,
    "finished_requests": 70127,
    "scheduler_time": 78.5613705490529
}
#Debug simulation 
Total elapsed time: 5.092526345979422. Arrivals time: 0.31312779849395156 Scheduler time: 4.676129145547748 Scheduler overhead time: 0.027101232204586267 Adapter cache time: 0.035496669355779886 Engine time: 0.028057699091732502 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 4320, 4320, 4320, 4320, 34560, 4320, 270, 34560, 270, 4320, 4320, 270, 34560, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 270, 4320, 4320, 34560, 4320, 270, 270, 4320, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 34560, 34560, 270, 4320, 34560, 270, 4320, 270, 4320, 270, 270, 4320, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 4320, 270, 270, 270, 270, 270, 34560, 270, 4320, 270, 34560, 34560, 270, 34560, 270, 4320, 4320, 34560, 270, 270, 270, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 4320, 34560, 34560, 4320, 34560, 4320, 270, 34560, 4320, 270, 4320, 4320, 270, 4320, 34560, 270, 4320, 34560, 270, 4320, 34560, 34560, 4320, 270, 34560, 4320, 270, 270, 4320, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 4320, 270, 34560, 34560, 4320, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 270, 34560, 4320, 34560, 270, 34560, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 34560, 4320, 270, 270]
Prompts retrieved: 2505600 . Total input tokens: 559018795 . Total output tokens: 501233349
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.407795262057334,
    "estimated_duration": 3600.0997308170226,
    "input_throughput": 4713.140820726446,
    "output_throughput": 4160.285858692296,
    "total_throughput": 8873.426679418742,
    "itl": 168.68503120830485,
    "ttft": 2163451.9816669985,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1399,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.674584022276094,
    "arrivals": 834363,
    "finished_requests": 68854,
    "scheduler_time": 80.79001011849859
}
#Debug simulation 
Total elapsed time: 5.40787866897881. Arrivals time: 0.6312094004824758 Scheduler time: 4.652258009649813 Scheduler overhead time: 0.03218053001910448 Adapter cache time: 0.04432547511532903 Engine time: 0.03290480235591531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.137611171230674,
    "estimated_duration": 3600.175449111855,
    "input_throughput": 4822.719960573942,
    "output_throughput": 4262.575315262979,
    "total_throughput": 9085.295275836921,
    "itl": 201.59332235437557,
    "ttft": 2149764.6728773066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 939,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8737979915529195,
    "arrivals": 831492,
    "finished_requests": 70253,
    "scheduler_time": 79.19005697638461
}
#Debug simulation 
Total elapsed time: 5.137699127197266. Arrivals time: 0.2821215963922441 Scheduler time: 4.754940690938383 Scheduler overhead time: 0.02754246536642313 Adapter cache time: 0.03152331104502082 Engine time: 0.02889072149991989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.077472260221839,
    "estimated_duration": 3600.128827642448,
    "input_throughput": 4822.562700171327,
    "output_throughput": 4262.520519314058,
    "total_throughput": 9085.083219485387,
    "itl": 201.60469787556056,
    "ttft": 2149730.6980990195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 939,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0609863243461635,
    "arrivals": 831492,
    "finished_requests": 70249,
    "scheduler_time": 79.18517488900714
}
#Debug simulation 
Total elapsed time: 5.077563053928316. Arrivals time: 0.22945416811853647 Scheduler time: 4.748297862708569 Scheduler overhead time: 0.027532839216291904 Adapter cache time: 0.03136478504166007 Engine time: 0.028153481893241405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.010988991241902,
    "estimated_duration": 3600.032335639477,
    "input_throughput": 4729.820016179213,
    "output_throughput": 4191.783182225071,
    "total_throughput": 8921.603198404284,
    "itl": 167.64481720978247,
    "ttft": 2161251.7519681067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 913,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9810812056436884,
    "arrivals": 831492,
    "finished_requests": 68929,
    "scheduler_time": 81.3874018142229
}
#Debug simulation 
Total elapsed time: 5.011092042084783. Arrivals time: 0.22382559161633253 Scheduler time: 4.6658547590486705 Scheduler overhead time: 0.03272033156827092 Adapter cache time: 0.04010172002017498 Engine time: 0.03348028752952814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.081717936322093,
    "estimated_duration": 3600.2266317208646,
    "input_throughput": 4822.6513983929035,
    "output_throughput": 4262.514716376282,
    "total_throughput": 9085.166114769187,
    "itl": 201.59590940892838,
    "ttft": 2149784.555268351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 939,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.92492408330549,
    "arrivals": 831492,
    "finished_requests": 70253,
    "scheduler_time": 79.19011349354572
}
#Debug simulation 
Total elapsed time: 5.081802943255752. Arrivals time: 0.22833477472886443 Scheduler time: 4.754398720338941 Scheduler overhead time: 0.027261314447969198 Adapter cache time: 0.031089776195585728 Engine time: 0.027997939381748438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.0286115813069046,
    "estimated_duration": 3600.072999684555,
    "input_throughput": 4729.766591258561,
    "output_throughput": 4191.735834612871,
    "total_throughput": 8921.502425871433,
    "itl": 167.64665023543355,
    "ttft": 2161267.5332470583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 913,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0216996786557173,
    "arrivals": 831492,
    "finished_requests": 68929,
    "scheduler_time": 81.38744738632967
}
#Debug simulation 
Total elapsed time: 5.028698000125587. Arrivals time: 0.22374338563531637 Scheduler time: 4.684547002892941 Scheduler overhead time: 0.03226059256121516 Adapter cache time: 0.039771152660250664 Engine time: 0.0332398870959878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.049018214922398,
    "estimated_duration": 3600.109246115506,
    "input_throughput": 4822.80864635821,
    "output_throughput": 4262.653700455966,
    "total_throughput": 9085.462346814176,
    "itl": 201.5899252312748,
    "ttft": 2149738.8565364685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 939,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.80765722691308,
    "arrivals": 831492,
    "finished_requests": 70253,
    "scheduler_time": 79.1899947445491
}
#Debug simulation 
Total elapsed time: 5.049105059821159. Arrivals time: 0.2257397803477943 Scheduler time: 4.72389146219939 Scheduler overhead time: 0.027404408901929855 Adapter cache time: 0.031130607705563307 Engine time: 0.028229936957359314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [64 64 64]
Adapter prompts. [34560, 135, 34560, 135, 34560, 135, 34560, 135, 135, 135, 4320, 4320, 4320, 4320, 34560, 4320, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 34560, 4320, 135, 135, 4320, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 34560, 34560, 135, 4320, 34560, 135, 4320, 135, 4320, 135, 135, 4320, 34560, 34560, 135, 135, 135, 34560, 34560, 34560, 4320, 135, 135, 135, 135, 135, 34560, 135, 4320, 135, 34560, 34560, 135, 34560, 135, 4320, 4320, 34560, 135, 135, 135, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 4320, 34560, 34560, 4320, 34560, 4320, 135, 34560, 4320, 135, 4320, 4320, 135, 4320, 34560, 135, 4320, 34560, 135, 4320, 34560, 34560, 4320, 135, 34560, 4320, 135, 135, 4320, 34560, 34560, 34560, 135, 34560, 135, 34560, 135, 4320, 135, 34560, 34560, 4320, 34560, 135, 34560, 34560, 34560, 135, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 135, 34560, 4320, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2496960 . Total input tokens: 557094675 . Total output tokens: 499509474
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.08639200637117,
    "estimated_duration": 3600.107873243933,
    "input_throughput": 4729.720774910309,
    "output_throughput": 4191.695230066098,
    "total_throughput": 8921.416004976407,
    "itl": 167.64816910311595,
    "ttft": 2161281.3220619797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 913,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0565334774926622,
    "arrivals": 831492,
    "finished_requests": 68929,
    "scheduler_time": 81.38748714690317
}
#Debug simulation 
Total elapsed time: 5.086483144201338. Arrivals time: 0.27728611743077636 Scheduler time: 4.688649710267782 Scheduler overhead time: 0.032092803157866 Adapter cache time: 0.0400594724342227 Engine time: 0.03331276448443532 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.073156184051186,
    "estimated_duration": 3600.062793949363,
    "input_throughput": 4876.5252177006705,
    "output_throughput": 4301.859408127267,
    "total_throughput": 9178.384625827937,
    "itl": 199.35543923445888,
    "ttft": 2147929.879453015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 637,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9495306928851959,
    "arrivals": 830076,
    "finished_requests": 70923,
    "scheduler_time": 79.89391207608682
}
#Debug simulation 
Total elapsed time: 5.073246534913778. Arrivals time: 0.2277228832244873 Scheduler time: 4.748477223794907 Scheduler overhead time: 0.027195160277187824 Adapter cache time: 0.0287947291508317 Engine time: 0.028271828778088093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.116260835900903,
    "estimated_duration": 3600.186842194041,
    "input_throughput": 4876.357191867595,
    "output_throughput": 4301.711183012343,
    "total_throughput": 9178.068374879938,
    "itl": 199.3615072922148,
    "ttft": 2147980.695563353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 637,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0734119043685544,
    "arrivals": 830076,
    "finished_requests": 70923,
    "scheduler_time": 79.89407910923909
}
#Debug simulation 
Total elapsed time: 5.116355809848756. Arrivals time: 0.23234552750363946 Scheduler time: 4.785945747047663 Scheduler overhead time: 0.027576905209571123 Adapter cache time: 0.029058614745736122 Engine time: 0.028588014654815197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.027010959107429,
    "estimated_duration": 3600.1476497960994,
    "input_throughput": 4777.058241201315,
    "output_throughput": 4220.40662717279,
    "total_throughput": 8997.464868374103,
    "itl": 166.34832052874484,
    "ttft": 2161143.4445120995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0303284542076403,
    "arrivals": 830076,
    "finished_requests": 69488,
    "scheduler_time": 81.94635779938639
}
#Debug simulation 
Total elapsed time: 5.027122654952109. Arrivals time: 0.22282094787806273 Scheduler time: 4.685622756835073 Scheduler overhead time: 0.03237128723412752 Adapter cache time: 0.03786307107657194 Engine time: 0.03334819758310914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.074638165999204,
    "estimated_duration": 3600.100923513287,
    "input_throughput": 4876.4735692097065,
    "output_throughput": 4301.813846064763,
    "total_throughput": 9178.28741527447,
    "itl": 199.35729546117122,
    "ttft": 2147946.730498014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 637,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.987606887496062,
    "arrivals": 830076,
    "finished_requests": 70923,
    "scheduler_time": 79.89396544533408
}
#Debug simulation 
Total elapsed time: 5.074724238365889. Arrivals time: 0.23077066661790013 Scheduler time: 4.746291077230126 Scheduler overhead time: 0.027631938457489014 Adapter cache time: 0.029038917738944292 Engine time: 0.02822544239461422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.053850902244449,
    "estimated_duration": 3600.1735968222583,
    "input_throughput": 4777.023812179543,
    "output_throughput": 4220.376210028112,
    "total_throughput": 8997.400022207656,
    "itl": 166.34940129230185,
    "ttft": 2161153.48044557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.056233734209095,
    "arrivals": 830076,
    "finished_requests": 69488,
    "scheduler_time": 81.94639954556709
}
#Debug simulation 
Total elapsed time: 5.053936664946377. Arrivals time: 0.22584525030106306 Scheduler time: 4.709234826732427 Scheduler overhead time: 0.0324277663603425 Adapter cache time: 0.03801583964377642 Engine time: 0.0332953822799027 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.154574600048363,
    "estimated_duration": 3600.0178644186067,
    "input_throughput": 4876.586078506923,
    "output_throughput": 4301.913096895452,
    "total_throughput": 9178.499175402374,
    "itl": 199.35324431847795,
    "ttft": 2147911.5615783534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 637,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.904662037852655,
    "arrivals": 830076,
    "finished_requests": 70923,
    "scheduler_time": 79.89385120027575
}
#Debug simulation 
Total elapsed time: 5.1546582779847085. Arrivals time: 0.2798061235807836 Scheduler time: 4.777144835330546 Scheduler overhead time: 0.027469545137137175 Adapter cache time: 0.029102596454322338 Engine time: 0.02837875671684742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [64 64 64]
Adapter prompts. [34560, 66, 34560, 66, 34560, 66, 34560, 66, 66, 66, 4320, 4320, 4320, 4320, 34560, 4320, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 34560, 4320, 66, 66, 4320, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 34560, 34560, 66, 4320, 34560, 66, 4320, 66, 4320, 66, 66, 4320, 34560, 34560, 66, 66, 66, 34560, 34560, 34560, 4320, 66, 66, 66, 66, 66, 34560, 66, 4320, 66, 34560, 34560, 66, 34560, 66, 4320, 4320, 34560, 66, 66, 66, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 4320, 34560, 34560, 4320, 34560, 4320, 66, 34560, 4320, 66, 4320, 4320, 66, 4320, 34560, 66, 4320, 34560, 66, 4320, 34560, 34560, 4320, 66, 34560, 4320, 66, 66, 4320, 34560, 34560, 34560, 66, 34560, 66, 34560, 66, 4320, 66, 34560, 34560, 4320, 34560, 66, 34560, 34560, 34560, 66, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 66, 34560, 4320, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2492544 . Total input tokens: 556120633 . Total output tokens: 498622856
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.054013443179429,
    "estimated_duration": 3600.014281837179,
    "input_throughput": 4777.234658975676,
    "output_throughput": 4220.396312496398,
    "total_throughput": 8997.630971472074,
    "itl": 166.3500899347847,
    "ttft": 2161111.3530853987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0815102452784755,
    "arrivals": 830076,
    "finished_requests": 69486,
    "scheduler_time": 81.94219265767363
}
#Debug simulation 
Total elapsed time: 5.054097346961498. Arrivals time: 0.22350974194705486 Scheduler time: 4.712008654605597 Scheduler overhead time: 0.03239951562136412 Adapter cache time: 0.03790557011961937 Engine time: 0.033151994459331036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.110266523901373,
    "estimated_duration": 3600.0390998618363,
    "input_throughput": 4890.756047809516,
    "output_throughput": 4316.387841619933,
    "total_throughput": 9207.14388942945,
    "itl": 198.47010920159016,
    "ttft": 2141282.891700485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1446224162308536,
    "arrivals": 829350,
    "finished_requests": 71253,
    "scheduler_time": 80.19887661230678
}
#Debug simulation 
Total elapsed time: 5.110375902149826. Arrivals time: 0.23553368588909507 Scheduler time: 4.778109573293477 Scheduler overhead time: 0.027429625391960144 Adapter cache time: 0.02790224552154541 Engine time: 0.02861236408352852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.119303415995091,
    "estimated_duration": 3600.1383147029414,
    "input_throughput": 4890.703206621889,
    "output_throughput": 4316.293331436126,
    "total_throughput": 9206.996538058016,
    "itl": 198.47220684506448,
    "ttft": 2141319.786748742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.220836235221947,
    "arrivals": 829350,
    "finished_requests": 71256,
    "scheduler_time": 80.20014108939353
}
#Debug simulation 
Total elapsed time: 5.119406685698777. Arrivals time: 0.22963940259069204 Scheduler time: 4.792735842522234 Scheduler overhead time: 0.02739542443305254 Adapter cache time: 0.028068733401596546 Engine time: 0.028617938049137592 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.095703583210707,
    "estimated_duration": 3600.186481143258,
    "input_throughput": 4788.550007144478,
    "output_throughput": 4236.401108632767,
    "total_throughput": 9024.951115777245,
    "itl": 165.0778477578944,
    "ttft": 2153885.134243627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1884430121444232,
    "arrivals": 829350,
    "finished_requests": 69786,
    "scheduler_time": 82.29120930824082
}
#Debug simulation 
Total elapsed time: 5.095790263265371. Arrivals time: 0.22843936923891306 Scheduler time: 4.748206237331033 Scheduler overhead time: 0.032678159419447184 Adapter cache time: 0.037424742709845304 Engine time: 0.0337518285959959 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.14646334387362,
    "estimated_duration": 3600.0671572580322,
    "input_throughput": 4890.717931331646,
    "output_throughput": 4316.354201524203,
    "total_throughput": 9207.072132855848,
    "itl": 198.47135009792473,
    "ttft": 2141296.0370630543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1726219876459814,
    "arrivals": 829350,
    "finished_requests": 71253,
    "scheduler_time": 80.19893443706287
}
#Debug simulation 
Total elapsed time: 5.1465655961073935. Arrivals time: 0.2308412604033947 Scheduler time: 4.818403854500502 Scheduler overhead time: 0.027530248276889324 Adapter cache time: 0.0281789842993021 Engine time: 0.028699286747723818 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.067748315166682,
    "estimated_duration": 3600.0105208659343,
    "input_throughput": 4788.205173315363,
    "output_throughput": 4236.406230371668,
    "total_throughput": 9024.611403687031,
    "itl": 165.0783748624186,
    "ttft": 2153894.5653329254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.202778943795716,
    "arrivals": 829350,
    "finished_requests": 69781,
    "scheduler_time": 82.28698444922577
}
#Debug simulation 
Total elapsed time: 5.067832433152944. Arrivals time: 0.22612693905830383 Scheduler time: 4.722680316772312 Scheduler overhead time: 0.032498789485543966 Adapter cache time: 0.03760174987837672 Engine time: 0.03364842711016536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.126730345189571,
    "estimated_duration": 3600.1744655075836,
    "input_throughput": 4891.064354992967,
    "output_throughput": 4316.520810001635,
    "total_throughput": 9207.585164994603,
    "itl": 198.46831041873216,
    "ttft": 2141302.1703897677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 374,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.118278810293411,
    "arrivals": 829350,
    "finished_requests": 71260,
    "scheduler_time": 80.2028740252863
}
#Debug simulation 
Total elapsed time: 5.126861493103206. Arrivals time: 0.238766276743263 Scheduler time: 4.790937558282167 Scheduler overhead time: 0.027593212202191353 Adapter cache time: 0.028088610619306564 Engine time: 0.02860016282647848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_192_slots_128_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 4320, 4320, 4320, 4320, 34560, 4320, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 34560, 4320, 33, 33, 4320, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 34560, 34560, 33, 4320, 34560, 33, 4320, 33, 4320, 33, 33, 4320, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 4320, 33, 33, 33, 33, 33, 34560, 33, 4320, 33, 34560, 34560, 33, 34560, 33, 4320, 4320, 34560, 33, 33, 33, 34560, 34560, 4320, 34560, 4320, 34560, 34560, 4320, 4320, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 4320, 34560, 34560, 4320, 34560, 4320, 33, 34560, 4320, 33, 4320, 4320, 33, 4320, 34560, 33, 4320, 34560, 33, 4320, 34560, 34560, 4320, 33, 34560, 4320, 33, 33, 4320, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 4320, 33, 34560, 34560, 4320, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 33, 34560, 4320, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2490432 . Total input tokens: 555619750 . Total output tokens: 498201374
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.085067311301827,
    "estimated_duration": 3600.026282755568,
    "input_throughput": 4788.184209256892,
    "output_throughput": 4236.387682238349,
    "total_throughput": 9024.571891495241,
    "itl": 165.07897826788493,
    "ttft": 2153902.4341837186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2184981670975716,
    "arrivals": 829350,
    "finished_requests": 69781,
    "scheduler_time": 82.2870271155659
}
#Debug simulation 
Total elapsed time: 5.0851527210325. Arrivals time: 0.22368475096300244 Scheduler time: 4.742125945165753 Scheduler overhead time: 0.03276318917050958 Adapter cache time: 0.0373815163038671 Engine time: 0.03388690063729882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.494857218116522,
    "estimated_duration": 3600.18226382318,
    "input_throughput": 5276.948389781042,
    "output_throughput": 4646.250876819927,
    "total_throughput": 9923.19926660097,
    "itl": 184.49345452025148,
    "ttft": 2098992.2088868073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.184466238222229,
    "arrivals": 771049,
    "finished_requests": 76615,
    "scheduler_time": 86.23791806626082
}
#Debug simulation 
Total elapsed time: 5.49494514008984. Arrivals time: 0.2404419439844787 Scheduler time: 5.141222518868744 Scheduler overhead time: 0.029939045663923025 Adapter cache time: 0.038843545597046614 Engine time: 0.03064842289313674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.529655031859875,
    "estimated_duration": 3600.128966496401,
    "input_throughput": 5276.652913489173,
    "output_throughput": 4646.150222856668,
    "total_throughput": 9922.803136345841,
    "itl": 184.51538805468425,
    "ttft": 2098997.619246772,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.536664197044314,
    "arrivals": 771049,
    "finished_requests": 76607,
    "scheduler_time": 86.22819360459295
}
#Debug simulation 
Total elapsed time: 5.529762230813503. Arrivals time: 0.23972488893195987 Scheduler time: 5.175757999531925 Scheduler overhead time: 0.030222457367926836 Adapter cache time: 0.03922882303595543 Engine time: 0.030916930176317692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.566310124006122,
    "estimated_duration": 3600.1649859564814,
    "input_throughput": 5208.266585876589,
    "output_throughput": 4593.934462591299,
    "total_throughput": 9802.201048467889,
    "itl": 152.52351538407714,
    "ttft": 2107794.617140811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.532055968120642,
    "arrivals": 771049,
    "finished_requests": 75638,
    "scheduler_time": 89.15206401890485
}
#Debug simulation 
Total elapsed time: 5.566404311917722. Arrivals time: 0.24167187418788671 Scheduler time: 5.193174660671502 Scheduler overhead time: 0.035504667554050684 Adapter cache time: 0.042974441312253475 Engine time: 0.03657438699156046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.47393466392532,
    "estimated_duration": 3600.0824357626534,
    "input_throughput": 5276.983052188218,
    "output_throughput": 4646.211940548676,
    "total_throughput": 9923.194992736893,
    "itl": 184.49965775564138,
    "ttft": 2098976.128943835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.287421052795703,
    "arrivals": 771049,
    "finished_requests": 76610,
    "scheduler_time": 86.23302060300638
}
#Debug simulation 
Total elapsed time: 5.474066241178662. Arrivals time: 0.2410352728329599 Scheduler time: 5.119494702201337 Scheduler overhead time: 0.029917720705270767 Adapter cache time: 0.038909584283828735 Engine time: 0.030846585053950548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_192_slots_128_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.515260050073266,
    "estimated_duration": 3600.0712522894833,
    "input_throughput": 5208.189417938864,
    "output_throughput": 4593.878243238211,
    "total_throughput": 9802.067661177076,
    "itl": 152.5260863541085,
    "ttft": 2107835.8156796508,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.6085142702608595,
    "arrivals": 771049,
    "finished_requests": 75635,
    "scheduler_time": 89.147965298646
}
#Debug simulation 
Total elapsed time: 5.515340039040893. Arrivals time: 0.24166818102821708 Scheduler time: 5.141885802615434 Scheduler overhead time: 0.03564594313502312 Adapter cache time: 0.04310757573693991 Engine time: 0.036484125535935163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_192_slots_128_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_192_slots_128_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.460572557989508,
    "estimated_duration": 3600.062877840872,
    "input_throughput": 5277.123384965427,
    "output_throughput": 4646.404956691252,
    "total_throughput": 9923.52834165668,
    "itl": 184.48754116235813,
    "ttft": 2098945.8402413814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.065145199564142,
    "arrivals": 771049,
    "finished_requests": 76615,
    "scheduler_time": 86.23785312237645
}
#Debug simulation 
Total elapsed time: 5.460672739893198. Arrivals time: 0.239014009013772 Scheduler time: 5.107905171811581 Scheduler overhead time: 0.03004422876983881 Adapter cache time: 0.039153692312538624 Engine time: 0.03068680688738823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_192_slots_128_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_192_slots_128_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [64 64 64]
Adapter prompts. [34560, 540, 34560, 540, 34560, 540, 34560, 540, 540, 540, 1080, 1080, 1080, 1080, 34560, 1080, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 34560, 1080, 540, 540, 1080, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 34560, 34560, 540, 1080, 34560, 540, 1080, 540, 1080, 540, 540, 1080, 34560, 34560, 540, 540, 540, 34560, 34560, 34560, 1080, 540, 540, 540, 540, 540, 34560, 540, 1080, 540, 34560, 34560, 540, 34560, 540, 1080, 1080, 34560, 540, 540, 540, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 1080, 34560, 34560, 1080, 34560, 1080, 540, 34560, 1080, 540, 1080, 1080, 540, 1080, 34560, 540, 1080, 34560, 540, 1080, 34560, 34560, 1080, 540, 34560, 1080, 540, 540, 1080, 34560, 34560, 34560, 540, 34560, 540, 34560, 540, 1080, 540, 34560, 34560, 1080, 34560, 540, 34560, 34560, 34560, 540, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 540, 34560, 1080, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 2315520 . Total input tokens: 516634855 . Total output tokens: 463244655
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.507346520200372,
    "estimated_duration": 3600.1397045796994,
    "input_throughput": 5208.090390533598,
    "output_throughput": 4593.790896214894,
    "total_throughput": 9801.881286748492,
    "itl": 152.52892965632918,
    "ttft": 2107861.8828084585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1690,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.676924330070513,
    "arrivals": 771049,
    "finished_requests": 75635,
    "scheduler_time": 89.14800752912114
}
#Debug simulation 
Total elapsed time: 5.507430909201503. Arrivals time: 0.2377404528670013 Scheduler time: 5.137775867711753 Scheduler overhead time: 0.03561453428119421 Adapter cache time: 0.043128122575581074 Engine time: 0.03658639919012785 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.731954061891884,
    "estimated_duration": 3600.09638545542,
    "input_throughput": 5450.441849077815,
    "output_throughput": 4773.271368351477,
    "total_throughput": 10223.713217429293,
    "itl": 179.25672646680343,
    "ttft": 2084215.5435096114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.629738464304336,
    "arrivals": 765271,
    "finished_requests": 78903,
    "scheduler_time": 88.59583936013861
}
#Debug simulation 
Total elapsed time: 5.732048409990966. Arrivals time: 0.24761793855577707 Scheduler time: 5.373622984625399 Scheduler overhead time: 0.030967853032052517 Adapter cache time: 0.03361899917945266 Engine time: 0.031926450319588184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.713481321930885,
    "estimated_duration": 3600.131267016478,
    "input_throughput": 5449.992387710955,
    "output_throughput": 4773.035682735107,
    "total_throughput": 10223.028070446062,
    "itl": 179.2692593076435,
    "ttft": 2084227.621253452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8644954028190153,
    "arrivals": 765271,
    "finished_requests": 78898,
    "scheduler_time": 88.59108721417816
}
#Debug simulation 
Total elapsed time: 5.713592185638845. Arrivals time: 0.32079534558579326 Scheduler time: 5.2825734950602055 Scheduler overhead time: 0.03074800269678235 Adapter cache time: 0.03354798164218664 Engine time: 0.03172343363985419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.702098283916712,
    "estimated_duration": 3600.069741502654,
    "input_throughput": 5356.155126025851,
    "output_throughput": 4697.103171379641,
    "total_throughput": 10053.258297405491,
    "itl": 149.47664231837183,
    "ttft": 2094769.6801064417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.830402675382736,
    "arrivals": 765271,
    "finished_requests": 77556,
    "scheduler_time": 91.11810840750108
}
#Debug simulation 
Total elapsed time: 5.702186470851302. Arrivals time: 0.32318534748628736 Scheduler time: 5.251147266943008 Scheduler overhead time: 0.03653529193252325 Adapter cache time: 0.03720104927197099 Engine time: 0.03731202194467187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.010214722715318,
    "estimated_duration": 3600.1624446860537,
    "input_throughput": 5450.341839147515,
    "output_throughput": 4773.183783794101,
    "total_throughput": 10223.525622941617,
    "itl": 179.25986828910465,
    "ttft": 2084241.7360809427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6957455363031015,
    "arrivals": 765271,
    "finished_requests": 78903,
    "scheduler_time": 88.59589151864976
}
#Debug simulation 
Total elapsed time: 6.010281960945576. Arrivals time: 0.6117166839540005 Scheduler time: 5.288387145381421 Scheduler overhead time: 0.030781439505517483 Adapter cache time: 0.03360776277258992 Engine time: 0.03158019157126546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_192_slots_128_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.681816293857992,
    "estimated_duration": 3600.1213433666658,
    "input_throughput": 5356.078354283435,
    "output_throughput": 4697.035846071413,
    "total_throughput": 10053.114200354848,
    "itl": 149.47871690623177,
    "ttft": 2094790.3683866507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8819617278128917,
    "arrivals": 765271,
    "finished_requests": 77556,
    "scheduler_time": 91.11815121912956
}
#Debug simulation 
Total elapsed time: 5.6819074298255146. Arrivals time: 0.32347372174263 Scheduler time: 5.23132362915203 Scheduler overhead time: 0.03614372527226806 Adapter cache time: 0.03713837917894125 Engine time: 0.037031589075922966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_192_slots_128_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_192_slots_128_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [64 64 64]
Adapter prompts. [34560, 270, 34560, 270, 34560, 270, 34560, 270, 270, 270, 1080, 1080, 1080, 1080, 34560, 1080, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 34560, 1080, 270, 270, 1080, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 34560, 34560, 270, 1080, 34560, 270, 1080, 270, 1080, 270, 270, 1080, 34560, 34560, 270, 270, 270, 34560, 34560, 34560, 1080, 270, 270, 270, 270, 270, 34560, 270, 1080, 270, 34560, 34560, 270, 34560, 270, 1080, 1080, 34560, 270, 270, 270, 34560, 34560, 1080, 34560, 1080, 34560, 34560, 1080, 1080, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 1080, 34560, 34560, 1080, 34560, 1080, 270, 34560, 1080, 270, 1080, 1080, 270, 1080, 34560, 270, 1080, 34560, 270, 1080, 34560, 34560, 1080, 270, 34560, 1080, 270, 270, 1080, 34560, 34560, 34560, 270, 34560, 270, 34560, 270, 1080, 270, 34560, 34560, 1080, 34560, 270, 34560, 34560, 34560, 270, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 270, 34560, 1080, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 2298240 . Total input tokens: 512823787 . Total output tokens: 459769584
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.690767734777182,
    "estimated_duration": 3600.0127771447815,
    "input_throughput": 5450.568432582777,
    "output_throughput": 4773.382224945615,
    "total_throughput": 10223.950657528392,
    "itl": 179.25277978619647,
    "ttft": 2084182.1325966176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5461996497538912,
    "arrivals": 765271,
    "finished_requests": 78903,
    "scheduler_time": 88.5957698638865
}
#Debug simulation 
Total elapsed time: 5.690856395754963. Arrivals time: 0.32594378804787993 Scheduler time: 5.254634333308786 Scheduler overhead time: 0.03071072418242693 Adapter cache time: 0.03367093624547124 Engine time: 0.03167850477620959 
