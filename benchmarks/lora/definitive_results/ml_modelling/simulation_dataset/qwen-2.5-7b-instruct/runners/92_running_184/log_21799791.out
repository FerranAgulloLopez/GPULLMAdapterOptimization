INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:08 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.488242526887916,
    "estimated_duration": 3600.0800520124167,
    "input_throughput": 4949.508550522238,
    "output_throughput": 4376.413238698076,
    "total_throughput": 9325.921789220314,
    "itl": 116.6920323971721,
    "ttft": 2126204.7850723336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.519667731374504,
    "arrivals": 700393,
    "finished_requests": 71925,
    "scheduler_time": 30.21632619257333
}
#Debug simulation 
Total elapsed time: 5.488390520913526. Arrivals time: 0.31378937512636185 Scheduler time: 5.00194893986918 Scheduler overhead time: 0.04546301253139973 Adapter cache time: 0.05772294453345239 Engine time: 0.04779740597587079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.590699660941027,
    "estimated_duration": 3600.108380665447,
    "input_throughput": 5281.330446081058,
    "output_throughput": 4650.675543524949,
    "total_throughput": 9932.005989606007,
    "itl": 184.67882372409937,
    "ttft": 2087835.587686211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.497482144085225,
    "arrivals": 700393,
    "finished_requests": 76746,
    "scheduler_time": 47.07770513100784
}
#Debug simulation 
Total elapsed time: 5.5908096509519964. Arrivals time: 0.31909763836301863 Scheduler time: 5.165784323122352 Scheduler overhead time: 0.030270349117927253 Adapter cache time: 0.030074752401560545 Engine time: 0.03151250584051013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.461724034976214,
    "estimated_duration": 3600.0706919673166,
    "input_throughput": 4949.693360121877,
    "output_throughput": 4376.80766523766,
    "total_throughput": 9326.501025359537,
    "itl": 116.69339811264777,
    "ttft": 2126178.547400793,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5262069282680759,
    "arrivals": 700393,
    "finished_requests": 71928,
    "scheduler_time": 30.216159325114283
}
#Debug simulation 
Total elapsed time: 5.461817845003679. Arrivals time: 0.3114735893905163 Scheduler time: 4.979766660719179 Scheduler overhead time: 0.04529695713426918 Adapter cache time: 0.05629968421999365 Engine time: 0.047543773893266916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.572543079964817,
    "estimated_duration": 3600.062156959867,
    "input_throughput": 5281.3979789882715,
    "output_throughput": 4650.689702018566,
    "total_throughput": 9932.087681006838,
    "itl": 184.67743437274697,
    "ttft": 2087795.8088585811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4754179968894465,
    "arrivals": 700393,
    "finished_requests": 76745,
    "scheduler_time": 47.07721411130902
}
#Debug simulation 
Total elapsed time: 5.57263781898655. Arrivals time: 0.3203346946975216 Scheduler time: 5.146194463013671 Scheduler overhead time: 0.030096011236310005 Adapter cache time: 0.03022829012479633 Engine time: 0.031669689575210214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 4320, 135, 135, 135, 34560, 135, 4320, 34560, 4320, 135, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 135, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 135, 4320, 4320, 4320, 34560, 135, 135, 34560, 135, 34560, 135, 135, 4320, 135, 4320, 34560, 135, 135, 4320, 135, 135, 135, 135, 135, 34560, 4320, 4320, 34560, 4320, 135, 34560, 34560, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 135, 135, 34560, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 135, 4320, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 135, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 135, 4320, 4320, 34560, 4320, 34560, 135, 135, 135, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 135, 4320, 4320, 135, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 135, 34560, 135, 34560, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 34560, 4320, 135, 135]
Prompts retrieved: 2102355 . Total input tokens: 468285541 . Total output tokens: 420523490
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.430868910974823,
    "estimated_duration": 3600.0774396890943,
    "input_throughput": 4949.684082778754,
    "output_throughput": 4376.799461669572,
    "total_throughput": 9326.483544448327,
    "itl": 116.69356979575,
    "ttft": 2126183.138746188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5329976327344776,
    "arrivals": 700393,
    "finished_requests": 71928,
    "scheduler_time": 30.21611634242537
}
#Debug simulation 
Total elapsed time: 5.43096303101629. Arrivals time: 0.24350984441116452 Scheduler time: 5.015988051192835 Scheduler overhead time: 0.04534929245710373 Adapter cache time: 0.057506036129780114 Engine time: 0.04727385367732495 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.607336267014034,
    "estimated_duration": 3600.0690498898275,
    "input_throughput": 5342.773356135663,
    "output_throughput": 4691.770287160715,
    "total_throughput": 10034.543643296378,
    "itl": 182.65612693258493,
    "ttft": 2073841.8602786942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47437560030957626,
    "arrivals": 699127,
    "finished_requests": 77966,
    "scheduler_time": 47.42113852367544
}
#Debug simulation 
Total elapsed time: 5.6074511400656775. Arrivals time: 0.26422223716508597 Scheduler time: 5.2381841748720035 Scheduler overhead time: 0.03052252985071391 Adapter cache time: 0.028246184810996056 Engine time: 0.03194492985494435 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.607059984002262,
    "estimated_duration": 3600.149633188198,
    "input_throughput": 5342.67904386143,
    "output_throughput": 4691.665825301277,
    "total_throughput": 10034.344869162707,
    "itl": 182.65650732630436,
    "ttft": 2073872.5513088342,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5059517088276334,
    "arrivals": 699127,
    "finished_requests": 77967,
    "scheduler_time": 47.42173707315977
}
#Debug simulation 
Total elapsed time: 5.607157606049441. Arrivals time: 0.29993904347065836 Scheduler time: 5.203106937929988 Scheduler overhead time: 0.030158273060806096 Adapter cache time: 0.02798791998066008 Engine time: 0.0317508903099224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.439887664979324,
    "estimated_duration": 3600.0368917729616,
    "input_throughput": 4993.211053219854,
    "output_throughput": 4401.327396452489,
    "total_throughput": 9394.538449672344,
    "itl": 116.09439467602058,
    "ttft": 2112934.952892049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5068191609904187,
    "arrivals": 699127,
    "finished_requests": 72795,
    "scheduler_time": 30.422937208656034
}
#Debug simulation 
Total elapsed time: 5.439983151038177. Arrivals time: 0.2504195839865133 Scheduler time: 5.023086184402928 Scheduler overhead time: 0.04548944428097457 Adapter cache time: 0.05177260341588408 Engine time: 0.047600322286598384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.6159354279516265,
    "estimated_duration": 3600.0792840050044,
    "input_throughput": 5342.758167981853,
    "output_throughput": 4691.756949644035,
    "total_throughput": 10034.515117625888,
    "itl": 182.65641378153023,
    "ttft": 2073848.7805934902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4847047522687356,
    "arrivals": 699127,
    "finished_requests": 77966,
    "scheduler_time": 47.42108595285573
}
#Debug simulation 
Total elapsed time: 5.616031413897872. Arrivals time: 0.2630816694581881 Scheduler time: 5.247930364916101 Scheduler overhead time: 0.030415679677389562 Adapter cache time: 0.0283872839063406 Engine time: 0.03189839643891901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.420167842996307,
    "estimated_duration": 3600.063979849816,
    "input_throughput": 4993.157927362695,
    "output_throughput": 4401.343722969338,
    "total_throughput": 9394.501650332033,
    "itl": 116.09353254030135,
    "ttft": 2113008.888352337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5133583578839905,
    "arrivals": 699127,
    "finished_requests": 72795,
    "scheduler_time": 30.422567205279194
}
#Debug simulation 
Total elapsed time: 5.42026118200738. Arrivals time: 0.24601747270207852 Scheduler time: 5.0053500139620155 Scheduler overhead time: 0.04545251757372171 Adapter cache time: 0.0542691150913015 Engine time: 0.04755236010532826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.713190169073641,
    "estimated_duration": 3600.0544057838815,
    "input_throughput": 5342.795089179182,
    "output_throughput": 4691.789372089279,
    "total_throughput": 10034.584461268461,
    "itl": 182.6561375290983,
    "ttft": 2073822.8007362573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4634577957098378,
    "arrivals": 699127,
    "finished_requests": 77966,
    "scheduler_time": 47.42093514775668
}
#Debug simulation 
Total elapsed time: 5.713300693081692. Arrivals time: 0.2695821279194206 Scheduler time: 5.337453629123047 Scheduler overhead time: 0.030616052099503577 Adapter cache time: 0.02867902792058885 Engine time: 0.03252871392760426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 4320, 66, 66, 66, 34560, 66, 4320, 34560, 4320, 66, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 66, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 66, 4320, 4320, 4320, 34560, 66, 66, 34560, 66, 34560, 66, 66, 4320, 66, 4320, 34560, 66, 66, 4320, 66, 66, 66, 66, 66, 34560, 4320, 4320, 34560, 4320, 66, 34560, 34560, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 66, 66, 34560, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 66, 4320, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 66, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 66, 4320, 4320, 34560, 4320, 34560, 66, 66, 66, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 66, 4320, 4320, 66, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 66, 34560, 66, 34560, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 34560, 4320, 66, 66]
Prompts retrieved: 2098698 . Total input tokens: 467455399 . Total output tokens: 419799289
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.400292620994151,
    "estimated_duration": 3600.0411488796008,
    "input_throughput": 4993.018761908923,
    "output_throughput": 4401.285247789791,
    "total_throughput": 9394.304009698715,
    "itl": 116.09699236822368,
    "ttft": 2112897.001340505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 155,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5198975547775627,
    "arrivals": 699127,
    "finished_requests": 72793,
    "scheduler_time": 30.42367114281899
}
#Debug simulation 
Total elapsed time: 5.400388712994754. Arrivals time: 0.2496623801998794 Scheduler time: 4.981808711658232 Scheduler overhead time: 0.045465811737813056 Adapter cache time: 0.05390697380062193 Engine time: 0.04774758091662079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.6151410159654915,
    "estimated_duration": 3600.048815856243,
    "input_throughput": 5341.687289155888,
    "output_throughput": 4710.100575668725,
    "total_throughput": 10051.787864824613,
    "itl": 182.28978686001363,
    "ttft": 2079787.633393696,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.440710235126316,
    "arrivals": 698509,
    "finished_requests": 77838,
    "scheduler_time": 47.69148315289553
}
#Debug simulation 
Total elapsed time: 5.615229564951733. Arrivals time: 0.26289523812010884 Scheduler time: 5.250385316903703 Scheduler overhead time: 0.030473187216557562 Adapter cache time: 0.02515830600168556 Engine time: 0.031963015790097415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.623337480938062,
    "estimated_duration": 3600.0765181033203,
    "input_throughput": 5341.646185379246,
    "output_throughput": 4710.064331891891,
    "total_throughput": 10051.710517271138,
    "itl": 182.29054377968916,
    "ttft": 2079806.697943482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46856660708086567,
    "arrivals": 698509,
    "finished_requests": 77838,
    "scheduler_time": 47.69133556124112
}
#Debug simulation 
Total elapsed time: 5.623434165026993. Arrivals time: 0.29992255091201514 Scheduler time: 5.221113513223827 Scheduler overhead time: 0.030299273086711764 Adapter cache time: 0.025439757388085127 Engine time: 0.03226382378488779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.3868948930175975,
    "estimated_duration": 3600.072403988588,
    "input_throughput": 4993.462348169202,
    "output_throughput": 4415.119257709903,
    "total_throughput": 9408.581605879104,
    "itl": 115.69747163223431,
    "ttft": 2122892.9639498657,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46624996487051384,
    "arrivals": 698509,
    "finished_requests": 72790,
    "scheduler_time": 30.460414508047574
}
#Debug simulation 
Total elapsed time: 5.386991532053798. Arrivals time: 0.24850010056979954 Scheduler time: 4.974462075857446 Scheduler overhead time: 0.045448111719451845 Adapter cache time: 0.04918347008060664 Engine time: 0.04792425618506968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.903537121950649,
    "estimated_duration": 3600.057828738999,
    "input_throughput": 5341.673916037025,
    "output_throughput": 4710.0887837514065,
    "total_throughput": 10051.76269978843,
    "itl": 182.29003434727088,
    "ttft": 2079794.168005256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4497712224326099,
    "arrivals": 698509,
    "finished_requests": 77838,
    "scheduler_time": 47.69143504834187
}
#Debug simulation 
Total elapsed time: 5.903601071913727. Arrivals time: 0.5751463085180148 Scheduler time: 5.225944318575785 Scheduler overhead time: 0.030590910813771188 Adapter cache time: 0.025232971063815057 Engine time: 0.03241338406223804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.402582552051172,
    "estimated_duration": 3600.078021360408,
    "input_throughput": 4993.4545566339875,
    "output_throughput": 4415.112368590736,
    "total_throughput": 9408.566925224723,
    "itl": 115.69761144929623,
    "ttft": 2122896.738452381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4719088852591819,
    "arrivals": 698509,
    "finished_requests": 72790,
    "scheduler_time": 30.46037295948079
}
#Debug simulation 
Total elapsed time: 5.4026728860335425. Arrivals time: 0.24644020106643438 Scheduler time: 4.99113545846194 Scheduler overhead time: 0.045643892721273005 Adapter cache time: 0.04978567722719163 Engine time: 0.048045445582829416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.617567962035537,
    "estimated_duration": 3600.0041440928526,
    "input_throughput": 5341.753573132555,
    "output_throughput": 4710.159022406572,
    "total_throughput": 10051.912595539126,
    "itl": 182.28897533538318,
    "ttft": 2079785.011788385,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4305672424659138,
    "arrivals": 698509,
    "finished_requests": 77838,
    "scheduler_time": 47.691164619948054
}
#Debug simulation 
Total elapsed time: 5.617660401971079. Arrivals time: 0.3021742806304246 Scheduler time: 5.213648932753131 Scheduler overhead time: 0.03054126736242324 Adapter cache time: 0.025062011438421905 Engine time: 0.03190961794462055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 4320, 33, 33, 33, 34560, 33, 4320, 34560, 4320, 33, 4320, 4320, 4320, 4320, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 34560, 33, 34560, 4320, 34560, 34560, 34560, 4320, 34560, 33, 4320, 4320, 4320, 34560, 33, 33, 34560, 33, 34560, 33, 33, 4320, 33, 4320, 34560, 33, 33, 4320, 33, 33, 33, 33, 33, 34560, 4320, 4320, 34560, 4320, 33, 34560, 34560, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 33, 33, 34560, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 33, 4320, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 33, 34560, 4320, 34560, 4320, 34560, 34560, 34560, 33, 4320, 4320, 34560, 4320, 34560, 33, 33, 33, 4320, 4320, 4320, 34560, 34560, 4320, 34560, 33, 4320, 4320, 33, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 4320, 4320, 34560, 33, 34560, 33, 34560, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 34560, 4320, 33, 33]
Prompts retrieved: 2096949 . Total input tokens: 467061267 . Total output tokens: 419463998
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.518265604041517,
    "estimated_duration": 3600.0838895432285,
    "input_throughput": 4993.446417239145,
    "output_throughput": 4415.105171901062,
    "total_throughput": 9408.551589140206,
    "itl": 115.69775789653322,
    "ttft": 2122900.731032862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 143,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47781931322067966,
    "arrivals": 698509,
    "finished_requests": 72790,
    "scheduler_time": 30.460330714340397
}
#Debug simulation 
Total elapsed time: 5.518389177042991. Arrivals time: 0.24729704100172967 Scheduler time: 5.1063398527912796 Scheduler overhead time: 0.04565288650337607 Adapter cache time: 0.049637622432783246 Engine time: 0.04787619016133249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.941356871975586,
    "estimated_duration": 3600.1483800679976,
    "input_throughput": 5643.8840444726975,
    "output_throughput": 5020.012258399931,
    "total_throughput": 10663.896302872628,
    "itl": 171.9685469016674,
    "ttft": 2033075.635410634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 650482,
    "finished_requests": 82548,
    "scheduler_time": 50.9425400537482
}
#Debug simulation 
Total elapsed time: 5.941450690967031. Arrivals time: 0.2666478658793494 Scheduler time: 5.550048560486175 Scheduler overhead time: 0.032276199432089925 Adapter cache time: 0.043579904129728675 Engine time: 0.033715987810865045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.042963573941961,
    "estimated_duration": 3600.179326990567,
    "input_throughput": 5643.835529988653,
    "output_throughput": 5019.969106679822,
    "total_throughput": 10663.804636668476,
    "itl": 171.96945920621823,
    "ttft": 2033098.5333487026,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 650482,
    "finished_requests": 82548,
    "scheduler_time": 50.94230778145009
}
#Debug simulation 
Total elapsed time: 6.04307139897719. Arrivals time: 0.2714193247957155 Scheduler time: 5.645980468485504 Scheduler overhead time: 0.03245836007408798 Adapter cache time: 0.04375114862341434 Engine time: 0.034171103150583804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.121066696941853,
    "estimated_duration": 3600.102537437971,
    "input_throughput": 5369.7017790380305,
    "output_throughput": 4788.676661489961,
    "total_throughput": 10158.378440527991,
    "itl": 106.78956103950803,
    "ttft": 2066232.0928255785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 650482,
    "finished_requests": 78469,
    "scheduler_time": 33.11549136900873
}
#Debug simulation 
Total elapsed time: 6.121162132010795. Arrivals time: 0.5631381793646142 Scheduler time: 5.3799979474861175 Scheduler overhead time: 0.04891571786720306 Adapter cache time: 0.05462323303800076 Engine time: 0.05130635085515678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.003457936923951,
    "estimated_duration": 3600.1583904641525,
    "input_throughput": 5643.868351408946,
    "output_throughput": 5019.99830003867,
    "total_throughput": 10663.866651447615,
    "itl": 171.96888180941923,
    "ttft": 2033082.8959033655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 650482,
    "finished_requests": 82548,
    "scheduler_time": 50.94248033851435
}
#Debug simulation 
Total elapsed time: 6.003550802939571. Arrivals time: 0.2698508062167093 Scheduler time: 5.607641608454287 Scheduler overhead time: 0.03232797747477889 Adapter cache time: 0.044335677521303296 Engine time: 0.034143970580771565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.8255696760024875,
    "estimated_duration": 3600.109152153402,
    "input_throughput": 5369.691912934611,
    "output_throughput": 4788.6678629418975,
    "total_throughput": 10158.359775876508,
    "itl": 106.78971829960307,
    "ttft": 2066236.946284881,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978163,
    "arrivals": 650482,
    "finished_requests": 78469,
    "scheduler_time": 33.11544113375935
}
#Debug simulation 
Total elapsed time: 5.825665598968044. Arrivals time: 0.2612434033071622 Scheduler time: 5.386306366999634 Scheduler overhead time: 0.0491619233507663 Adapter cache time: 0.054221009369939566 Engine time: 0.05145120446104556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.969005745020695,
    "estimated_duration": 3600.1352050636965,
    "input_throughput": 5643.904698751586,
    "output_throughput": 5020.03062956638,
    "total_throughput": 10663.935328317966,
    "itl": 171.96864444669052,
    "ttft": 2033074.6827906943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 650482,
    "finished_requests": 82548,
    "scheduler_time": 50.942349719537795
}
#Debug simulation 
Total elapsed time: 5.969098367961124. Arrivals time: 0.2684550421545282 Scheduler time: 5.575634278357029 Scheduler overhead time: 0.032328959670849144 Adapter cache time: 0.04341615433804691 Engine time: 0.03404867427889258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [53 53 54]
Adapter prompts. [540, 34560, 540, 540, 1080, 540, 540, 540, 34560, 540, 1080, 34560, 1080, 540, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 540, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 540, 1080, 1080, 1080, 34560, 540, 540, 34560, 540, 34560, 540, 540, 1080, 540, 1080, 34560, 540, 540, 1080, 540, 540, 540, 540, 540, 34560, 1080, 1080, 34560, 1080, 540, 34560, 34560, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 540, 540, 34560, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 540, 1080, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 540, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 540, 1080, 1080, 34560, 1080, 34560, 540, 540, 540, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 540, 1080, 1080, 540, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 540, 34560, 540, 34560, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 34560, 1080, 540, 540]
Prompts retrieved: 1952100 . Total input tokens: 434804699 . Total output tokens: 390559755
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.856808960903436,
    "estimated_duration": 3600.037618975811,
    "input_throughput": 5369.803053752448,
    "output_throughput": 4788.968845526901,
    "total_throughput": 10158.771899279349,
    "itl": 106.79090168664405,
    "ttft": 2066278.206120937,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 650482,
    "finished_requests": 78469,
    "scheduler_time": 33.11521245620824
}
#Debug simulation 
Total elapsed time: 5.856919392943382. Arrivals time: 0.26992734824307263 Scheduler time: 5.4085013614967465 Scheduler overhead time: 0.04901643702760339 Adapter cache time: 0.05447145889047533 Engine time: 0.051765561453066766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.175566227990203,
    "estimated_duration": 3600.1060670813567,
    "input_throughput": 5885.729366073468,
    "output_throughput": 5145.9083856990555,
    "total_throughput": 11031.637751772523,
    "itl": 166.27892026587227,
    "ttft": 2011431.793655974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 645743,
    "finished_requests": 85373,
    "scheduler_time": 51.9615993550641
}
#Debug simulation 
Total elapsed time: 6.175659763976. Arrivals time: 0.33983334770891815 Scheduler time: 5.711649826378562 Scheduler overhead time: 0.033514673821628094 Adapter cache time: 0.03968591697048396 Engine time: 0.03524280176497996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.2013388520572335,
    "estimated_duration": 3600.138745108783,
    "input_throughput": 5885.6759420142125,
    "output_throughput": 5145.861676906071,
    "total_throughput": 11031.537618920283,
    "itl": 166.27870854987117,
    "ttft": 2011451.0799385123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 645743,
    "finished_requests": 85373,
    "scheduler_time": 51.96172134893547
}
#Debug simulation 
Total elapsed time: 6.201429117005318. Arrivals time: 0.279743080958724 Scheduler time: 5.7973422759678215 Scheduler overhead time: 0.03349054919090122 Adapter cache time: 0.040180623647756875 Engine time: 0.03502409462817013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.966469143982977,
    "estimated_duration": 3600.0728668563697,
    "input_throughput": 5549.978219592838,
    "output_throughput": 4866.9003789636445,
    "total_throughput": 10416.878598556483,
    "itl": 104.88953786465487,
    "ttft": 2049372.2589580624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 645743,
    "finished_requests": 80515,
    "scheduler_time": 33.61021251338791
}
#Debug simulation 
Total elapsed time: 5.966562620946206. Arrivals time: 0.3237346155801788 Scheduler time: 5.468319958657958 Scheduler overhead time: 0.04996248916722834 Adapter cache time: 0.0484976900042966 Engine time: 0.05230545427184552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.209119873936288,
    "estimated_duration": 3600.107095184715,
    "input_throughput": 5885.727685251768,
    "output_throughput": 5145.906916152302,
    "total_throughput": 11031.63460140407,
    "itl": 166.27861162962913,
    "ttft": 2011435.952476086,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 645743,
    "finished_requests": 85373,
    "scheduler_time": 51.96146567736076
}
#Debug simulation 
Total elapsed time: 6.209212817950174. Arrivals time: 0.3393013686873019 Scheduler time: 5.745382345048711 Scheduler overhead time: 0.03368623729329556 Adapter cache time: 0.039773163152858615 Engine time: 0.03535466408357024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.00284066493623,
    "estimated_duration": 3600.081700250836,
    "input_throughput": 5549.964601805528,
    "output_throughput": 4866.888437220525,
    "total_throughput": 10416.853039026053,
    "itl": 104.88973183918468,
    "ttft": 2049337.600443133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978163,
    "arrivals": 645743,
    "finished_requests": 80515,
    "scheduler_time": 33.610372777928646
}
#Debug simulation 
Total elapsed time: 6.002930865972303. Arrivals time: 0.3265182615723461 Scheduler time: 5.501219646888785 Scheduler overhead time: 0.04983177431859076 Adapter cache time: 0.049426742596551776 Engine time: 0.05225820990744978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.218961223959923,
    "estimated_duration": 3600.0846977865713,
    "input_throughput": 5885.641527552795,
    "output_throughput": 5145.8633768783275,
    "total_throughput": 11031.504904431124,
    "itl": 166.27895957490648,
    "ttft": 2011414.7074469149,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 645743,
    "finished_requests": 85372,
    "scheduler_time": 51.961230961513685
}
#Debug simulation 
Total elapsed time: 6.219056112924591. Arrivals time: 0.37779392511583865 Scheduler time: 5.717053229454905 Scheduler overhead time: 0.033603900810703635 Adapter cache time: 0.03971177898347378 Engine time: 0.03515827388036996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 1080, 270, 270, 270, 34560, 270, 1080, 34560, 1080, 270, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 270, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 270, 1080, 1080, 1080, 34560, 270, 270, 34560, 270, 34560, 270, 270, 1080, 270, 1080, 34560, 270, 270, 1080, 270, 270, 270, 270, 270, 34560, 1080, 1080, 34560, 1080, 270, 34560, 34560, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 270, 270, 34560, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 270, 1080, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 270, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 270, 1080, 1080, 34560, 1080, 34560, 270, 270, 270, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 270, 1080, 1080, 270, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 270, 34560, 270, 34560, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 34560, 1080, 270, 270]
Prompts retrieved: 1937790 . Total input tokens: 431587978 . Total output tokens: 387713723
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.310978213092312,
    "estimated_duration": 3600.0797516717007,
    "input_throughput": 5549.96760577932,
    "output_throughput": 4866.891071472518,
    "total_throughput": 10416.85867725184,
    "itl": 104.88929863847734,
    "ttft": 2049365.1505577099,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642178,
    "arrivals": 645743,
    "finished_requests": 80515,
    "scheduler_time": 33.61006726309209
}
#Debug simulation 
Total elapsed time: 6.311042852001265. Arrivals time: 0.6557484740624204 Scheduler time: 5.479484563344158 Scheduler overhead time: 0.0499649322591722 Adapter cache time: 0.049169671605341136 Engine time: 0.05286574992351234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.117338974960148,
    "estimated_duration": 3600.010558079534,
    "input_throughput": 5910.214888744762,
    "output_throughput": 5210.022220047507,
    "total_throughput": 11120.23710879227,
    "itl": 164.89281973910437,
    "ttft": 2010812.9609585446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48661755128530726,
    "arrivals": 643390,
    "finished_requests": 86069,
    "scheduler_time": 52.689628901101266
}
#Debug simulation 
Total elapsed time: 6.117463367991149. Arrivals time: 0.275992151000537 Scheduler time: 5.719624101300724 Scheduler overhead time: 0.03369653970003128 Adapter cache time: 0.03692142339423299 Engine time: 0.03535785560961813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.182653459021822,
    "estimated_duration": 3600.138605701287,
    "input_throughput": 5910.136894814164,
    "output_throughput": 5210.20356557804,
    "total_throughput": 11120.340460392204,
    "itl": 164.89441452703298,
    "ttft": 2010854.5390415958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5191376959625633,
    "arrivals": 643390,
    "finished_requests": 86072,
    "scheduler_time": 52.691535544854226
}
#Debug simulation 
Total elapsed time: 6.1827457409817725. Arrivals time: 0.2770426601637155 Scheduler time: 5.785774782649241 Scheduler overhead time: 0.03350112203042954 Adapter cache time: 0.03568732284475118 Engine time: 0.03495190304238349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.999431247008033,
    "estimated_duration": 3600.0319577174055,
    "input_throughput": 5544.3402265394,
    "output_throughput": 4905.579508021215,
    "total_throughput": 10449.919734560615,
    "itl": 104.01274381836319,
    "ttft": 2051036.6542870393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5200049788691121,
    "arrivals": 643390,
    "finished_requests": 80803,
    "scheduler_time": 33.899244074782466
}
#Debug simulation 
Total elapsed time: 5.999545781989582. Arrivals time: 0.27047225390560925 Scheduler time: 5.556748270755634 Scheduler overhead time: 0.05024642241187394 Adapter cache time: 0.045502538327127695 Engine time: 0.05257577449083328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.172483716974966,
    "estimated_duration": 3600.0295414307225,
    "input_throughput": 5910.3156668942165,
    "output_throughput": 5210.280578015905,
    "total_throughput": 11120.596244910123,
    "itl": 164.89222864237274,
    "ttft": 2010828.180575714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.497482144085225,
    "arrivals": 643390,
    "finished_requests": 86071,
    "scheduler_time": 52.69038460108113
}
#Debug simulation 
Total elapsed time: 6.1725759679684415. Arrivals time: 0.2799236912978813 Scheduler time: 5.771877165068872 Scheduler overhead time: 0.033728190581314266 Adapter cache time: 0.035898031084798276 Engine time: 0.0353003884665668 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.9334100189153105,
    "estimated_duration": 3600.061341857998,
    "input_throughput": 5544.294972956965,
    "output_throughput": 4905.539468081818,
    "total_throughput": 10449.834441038782,
    "itl": 104.0162045548806,
    "ttft": 2051013.683823433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5266699295490991,
    "arrivals": 643390,
    "finished_requests": 80803,
    "scheduler_time": 33.9024572480064
}
#Debug simulation 
Total elapsed time: 5.933501307968982. Arrivals time: 0.2668935084948316 Scheduler time: 5.495464903418906 Scheduler overhead time: 0.05009542719926685 Adapter cache time: 0.04488153744023293 Engine time: 0.05248700035735965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.132761387969367,
    "estimated_duration": 3600.0000139711033,
    "input_throughput": 5910.232199285427,
    "output_throughput": 5210.037479780563,
    "total_throughput": 11120.26967906599,
    "itl": 164.89257476266286,
    "ttft": 2010806.7407481682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4754179968894465,
    "arrivals": 643390,
    "finished_requests": 86069,
    "scheduler_time": 52.689634339080655
}
#Debug simulation 
Total elapsed time: 6.132886216044426. Arrivals time: 0.2780210106866434 Scheduler time: 5.7342889931751415 Scheduler overhead time: 0.03356269816868007 Adapter cache time: 0.03593587491195649 Engine time: 0.035263350582681596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 1080, 135, 135, 135, 34560, 135, 1080, 34560, 1080, 135, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 135, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 135, 1080, 1080, 1080, 34560, 135, 135, 34560, 135, 34560, 135, 135, 1080, 135, 1080, 34560, 135, 135, 1080, 135, 135, 135, 135, 135, 34560, 1080, 1080, 34560, 1080, 135, 34560, 34560, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 135, 135, 34560, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 135, 1080, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 135, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 135, 1080, 1080, 34560, 1080, 34560, 135, 135, 135, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 135, 1080, 1080, 135, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 135, 34560, 135, 34560, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 34560, 1080, 135, 135]
Prompts retrieved: 1930635 . Total input tokens: 430002328 . Total output tokens: 386277069
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.940846850979142,
    "estimated_duration": 3600.068087858829,
    "input_throughput": 5544.284583759431,
    "output_throughput": 4905.530275818638,
    "total_throughput": 10449.81485957807,
    "itl": 104.01636807866554,
    "ttft": 2051018.4365081668,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5334606340155008,
    "arrivals": 643390,
    "finished_requests": 80803,
    "scheduler_time": 33.90241254437093
}
#Debug simulation 
Total elapsed time: 5.941003157990053. Arrivals time: 0.2640188407385722 Scheduler time: 5.504235909786075 Scheduler overhead time: 0.050575517234392464 Adapter cache time: 0.045375539106316864 Engine time: 0.05278600624296814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.343442713026889,
    "estimated_duration": 3600.0510491778527,
    "input_throughput": 5990.174501810136,
    "output_throughput": 5269.138615223806,
    "total_throughput": 11259.313117033942,
    "itl": 162.90595069872114,
    "ttft": 2008871.7358515547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48661755128530726,
    "arrivals": 642182,
    "finished_requests": 86853,
    "scheduler_time": 53.27784489507201
}
#Debug simulation 
Total elapsed time: 6.3435554639436305. Arrivals time: 0.3526637095492333 Scheduler time: 5.873218764318153 Scheduler overhead time: 0.033936987980268896 Adapter cache time: 0.03216808079741895 Engine time: 0.035585967474617064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.27520266501233,
    "estimated_duration": 3600.124548508136,
    "input_throughput": 5990.1052614794735,
    "output_throughput": 5269.093817284953,
    "total_throughput": 11259.199078764426,
    "itl": 162.9078361874483,
    "ttft": 2008901.9497892268,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.518729100644123,
    "arrivals": 642182,
    "finished_requests": 86854,
    "scheduler_time": 53.277864879014864
}
#Debug simulation 
Total elapsed time: 6.2752984090475366. Arrivals time: 0.27825221698731184 Scheduler time: 5.877809671103023 Scheduler overhead time: 0.03391454927623272 Adapter cache time: 0.03258042538072914 Engine time: 0.03683882032055408 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.06986416503787,
    "estimated_duration": 3600.047821390953,
    "input_throughput": 5584.785535496533,
    "output_throughput": 4932.847528991612,
    "total_throughput": 10517.633064488145,
    "itl": 103.21626812552694,
    "ttft": 2049293.567452355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.519667731374504,
    "arrivals": 642182,
    "finished_requests": 81003,
    "scheduler_time": 33.95847899537544
}
#Debug simulation 
Total elapsed time: 6.069955701008439. Arrivals time: 0.2697403314523399 Scheduler time: 5.6313505889847875 Scheduler overhead time: 0.050727405003272 Adapter cache time: 0.04065809049643576 Engine time: 0.053453354397788644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.621268491027877,
    "estimated_duration": 3600.061454817163,
    "input_throughput": 5990.15718777368,
    "output_throughput": 5269.12338527382,
    "total_throughput": 11259.2805730475,
    "itl": 162.906308527314,
    "ttft": 2008878.991875346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49707354876678467,
    "arrivals": 642182,
    "finished_requests": 86853,
    "scheduler_time": 53.27779453689787
}
#Debug simulation 
Total elapsed time: 6.621333486051299. Arrivals time: 0.2825906053185463 Scheduler time: 6.2208189009688795 Scheduler overhead time: 0.03394345019478351 Adapter cache time: 0.032282098196446896 Engine time: 0.03568357904441655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.373989482061006,
    "estimated_duration": 3600.0484425330965,
    "input_throughput": 5584.83151572624,
    "output_throughput": 4932.968065147566,
    "total_throughput": 10517.799580873805,
    "itl": 103.2190338234857,
    "ttft": 2049286.6110090895,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5263326820544909,
    "arrivals": 642182,
    "finished_requests": 81004,
    "scheduler_time": 33.95985450865457
}
#Debug simulation 
Total elapsed time: 6.3740541690494865. Arrivals time: 0.26787163864355534 Scheduler time: 5.938489599851891 Scheduler overhead time: 0.050499568809755147 Adapter cache time: 0.040180535754188895 Engine time: 0.05296715907752514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.602989503997378,
    "estimated_duration": 3600.03974460126,
    "input_throughput": 5990.1933117098215,
    "output_throughput": 5269.15516098032,
    "total_throughput": 11259.34847269014,
    "itl": 162.90561425056305,
    "ttft": 2008864.0067706902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4754179968894465,
    "arrivals": 642182,
    "finished_requests": 86853,
    "scheduler_time": 53.27789823270633
}
#Debug simulation 
Total elapsed time: 6.603095941012725. Arrivals time: 0.6636432717787102 Scheduler time: 5.820689458865672 Scheduler overhead time: 0.034171591280028224 Adapter cache time: 0.03278972895350307 Engine time: 0.03574373049195856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 1080, 66, 66, 66, 34560, 66, 1080, 34560, 1080, 66, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 66, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 66, 1080, 1080, 1080, 34560, 66, 66, 34560, 66, 34560, 66, 66, 1080, 66, 1080, 34560, 66, 66, 1080, 66, 66, 66, 66, 66, 34560, 1080, 1080, 34560, 1080, 66, 34560, 34560, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 66, 66, 34560, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 66, 1080, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 66, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 66, 1080, 1080, 34560, 1080, 34560, 66, 66, 66, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 66, 1080, 1080, 66, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 66, 34560, 66, 34560, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 34560, 1080, 66, 66]
Prompts retrieved: 1926978 . Total input tokens: 429180325 . Total output tokens: 385543276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.371707282029092,
    "estimated_duration": 3600.0764339125694,
    "input_throughput": 5584.919478542464,
    "output_throughput": 4933.021652737303,
    "total_throughput": 10517.941131279767,
    "itl": 103.21950011870837,
    "ttft": 2049254.9030305103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5329976327344778,
    "arrivals": 642182,
    "finished_requests": 81005,
    "scheduler_time": 33.95981761629392
}
#Debug simulation 
Total elapsed time: 6.371772094047628. Arrivals time: 0.5758266991470009 Scheduler time: 5.627668579109013 Scheduler overhead time: 0.05064149806275964 Adapter cache time: 0.040309091098606586 Engine time: 0.05315669160336256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.293904165038839,
    "estimated_duration": 3600.132680104451,
    "input_throughput": 6019.230935502989,
    "output_throughput": 5303.041219982507,
    "total_throughput": 11322.272155485496,
    "itl": 162.12515167736936,
    "ttft": 2000356.6430966377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46213364933384526,
    "arrivals": 641592,
    "finished_requests": 87536,
    "scheduler_time": 53.60738528360598
}
#Debug simulation 
Total elapsed time: 6.294029747019522. Arrivals time: 0.3204019172117114 Scheduler time: 5.856163971591741 Scheduler overhead time: 0.03436083148699254 Adapter cache time: 0.030405280296690762 Engine time: 0.03646270907483995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.297711706953123,
    "estimated_duration": 3600.0277414659013,
    "input_throughput": 6019.1800053119805,
    "output_throughput": 5302.986635382522,
    "total_throughput": 11322.166640694502,
    "itl": 162.1267621264836,
    "ttft": 2000370.0122847552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.493174317011144,
    "arrivals": 641592,
    "finished_requests": 87533,
    "scheduler_time": 53.60516115768174
}
#Debug simulation 
Total elapsed time: 6.297810159972869. Arrivals time: 0.2780049170833081 Scheduler time: 5.904262793366797 Scheduler overhead time: 0.03417839913163334 Adapter cache time: 0.029484413447789848 Engine time: 0.03582567453850061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.32017340301536,
    "estimated_duration": 3600.0120235512127,
    "input_throughput": 5603.231563682872,
    "output_throughput": 4957.610108866935,
    "total_throughput": 10560.841672549806,
    "itl": 102.89334553521955,
    "ttft": 2042658.4489006281,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.490589824263008,
    "arrivals": 641592,
    "finished_requests": 81641,
    "scheduler_time": 34.177205211558196
}
#Debug simulation 
Total elapsed time: 6.32027891499456. Arrivals time: 0.27802741958294064 Scheduler time: 5.876275850925595 Scheduler overhead time: 0.05065023433417082 Adapter cache time: 0.03781536116730422 Engine time: 0.053447577403858304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.324308606912382,
    "estimated_duration": 3600.0064231730953,
    "input_throughput": 6019.215649315552,
    "output_throughput": 5303.018038277003,
    "total_throughput": 11322.233687592556,
    "itl": 162.12616872276232,
    "ttft": 2000355.6126263302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47233595577068654,
    "arrivals": 641592,
    "finished_requests": 87533,
    "scheduler_time": 53.605247977684435
}
#Debug simulation 
Total elapsed time: 6.324404851999134. Arrivals time: 0.31500215572305024 Scheduler time: 5.892827351926826 Scheduler overhead time: 0.03420505963731557 Adapter cache time: 0.030423660529777408 Engine time: 0.03579825337510556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.08248822495807,
    "estimated_duration": 3600.018394708202,
    "input_throughput": 5603.2216473257795,
    "output_throughput": 4957.601335102794,
    "total_throughput": 10560.822982428574,
    "itl": 102.89348559575278,
    "ttft": 2042662.643295001,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49700326737016504,
    "arrivals": 641592,
    "finished_requests": 81641,
    "scheduler_time": 34.17716292544289
}
#Debug simulation 
Total elapsed time: 6.082579865935259. Arrivals time: 0.2699354245560244 Scheduler time: 5.646250150515698 Scheduler overhead time: 0.05073574185371399 Adapter cache time: 0.03809952922165394 Engine time: 0.053336157114245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.537333754007705,
    "estimated_duration": 3600.1212587189107,
    "input_throughput": 6019.250031514549,
    "output_throughput": 5303.058043882025,
    "total_throughput": 11322.308075396573,
    "itl": 162.12481165832435,
    "ttft": 2000348.8739550489,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4514975945302291,
    "arrivals": 641592,
    "finished_requests": 87536,
    "scheduler_time": 53.607421429344015
}
#Debug simulation 
Total elapsed time: 6.537427822011523. Arrivals time: 0.5856758808949962 Scheduler time: 5.836048922967166 Scheduler overhead time: 0.03414459398481995 Adapter cache time: 0.029757967917248607 Engine time: 0.035678813816048205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 1080, 33, 33, 33, 34560, 33, 1080, 34560, 1080, 33, 1080, 1080, 1080, 1080, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 34560, 33, 34560, 1080, 34560, 34560, 34560, 1080, 34560, 33, 1080, 1080, 1080, 34560, 33, 33, 34560, 33, 34560, 33, 33, 1080, 33, 1080, 34560, 33, 33, 1080, 33, 33, 33, 33, 33, 34560, 1080, 1080, 34560, 1080, 33, 34560, 34560, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 33, 33, 34560, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 33, 1080, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 33, 34560, 1080, 34560, 1080, 34560, 34560, 34560, 33, 1080, 1080, 34560, 1080, 34560, 33, 33, 33, 1080, 1080, 1080, 34560, 34560, 1080, 34560, 33, 1080, 1080, 33, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 1080, 1080, 34560, 33, 34560, 33, 34560, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 34560, 1080, 33, 33]
Prompts retrieved: 1925229 . Total input tokens: 428795948 . Total output tokens: 385186046
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.052196993958205,
    "estimated_duration": 3600.024637939736,
    "input_throughput": 5603.211930111705,
    "output_throughput": 4957.592737535805,
    "total_throughput": 10560.80466764751,
    "itl": 102.89364569308262,
    "ttft": 2042666.8675487891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5032909566909072,
    "arrivals": 641592,
    "finished_requests": 81641,
    "scheduler_time": 34.17711846765693
}
#Debug simulation 
Total elapsed time: 6.052288803039119. Arrivals time: 0.30390668369363993 Scheduler time: 5.581755537423305 Scheduler overhead time: 0.050580469658598304 Adapter cache time: 0.038589772186242044 Engine time: 0.053298663813620806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.4027862349757925,
    "estimated_duration": 3600.154870749082,
    "input_throughput": 6110.321025001591,
    "output_throughput": 5413.657939649893,
    "total_throughput": 11523.978964651484,
    "itl": 159.60932252371575,
    "ttft": 1991457.791747832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 636231,
    "finished_requests": 89081,
    "scheduler_time": 54.802072226333856
}
#Debug simulation 
Total elapsed time: 6.402900032000616. Arrivals time: 0.3571091528283432 Scheduler time: 5.926210721256211 Scheduler overhead time: 0.03450684866402298 Adapter cache time: 0.03256348101422191 Engine time: 0.03619385708589107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.445988698978908,
    "estimated_duration": 3600.0964988370815,
    "input_throughput": 6110.238158089848,
    "output_throughput": 5413.410725600093,
    "total_throughput": 11523.64888368994,
    "itl": 159.610165685012,
    "ttft": 1991438.543338241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 636231,
    "finished_requests": 89076,
    "scheduler_time": 54.80036310800411
}
#Debug simulation 
Total elapsed time: 6.446082953945734. Arrivals time: 0.3486660710768774 Scheduler time: 5.976703067310154 Scheduler overhead time: 0.0346730895107612 Adapter cache time: 0.03311119507998228 Engine time: 0.03651629714295268 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.1379564120434225,
    "estimated_duration": 3600.0232911598096,
    "input_throughput": 5684.2362798734475,
    "output_throughput": 5050.53343533851,
    "total_throughput": 10734.769715211956,
    "itl": 101.35618549652479,
    "ttft": 2034992.809438683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 636231,
    "finished_requests": 82832,
    "scheduler_time": 34.90856397045481
}
#Debug simulation 
Total elapsed time: 6.138050416950136. Arrivals time: 0.33485996269155294 Scheduler time: 5.63636854768265 Scheduler overhead time: 0.05133959243539721 Adapter cache time: 0.03698976256418973 Engine time: 0.05387469264678657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.462830503005534,
    "estimated_duration": 3600.1656838519657,
    "input_throughput": 6110.302672643478,
    "output_throughput": 5413.641679720373,
    "total_throughput": 11523.944352363851,
    "itl": 159.60973912639963,
    "ttft": 1991465.7052291639,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 636231,
    "finished_requests": 89081,
    "scheduler_time": 54.80203040265148
}
#Debug simulation 
Total elapsed time: 6.462954413960688. Arrivals time: 0.28066652186680585 Scheduler time: 6.0623116380302235 Scheduler overhead time: 0.034763889969326556 Adapter cache time: 0.03222538682166487 Engine time: 0.03657276777084917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_160_slots_160_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.484079801011831,
    "estimated_duration": 3600.0299117756285,
    "input_throughput": 5684.225826309017,
    "output_throughput": 5050.524147181917,
    "total_throughput": 10734.749973490934,
    "itl": 101.35633083457465,
    "ttft": 2034997.6446231024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 636231,
    "finished_requests": 82832,
    "scheduler_time": 34.9085196355936
}
#Debug simulation 
Total elapsed time: 6.484147756011225. Arrivals time: 0.5755621833959594 Scheduler time: 5.741128236986697 Scheduler overhead time: 0.05182206956669688 Adapter cache time: 0.03720889484975487 Engine time: 0.054107742733322084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_160_slots_160_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.431557720061392,
    "estimated_duration": 3600.1143800163163,
    "input_throughput": 6110.326694648047,
    "output_throughput": 5413.592442557803,
    "total_throughput": 11523.91913720585,
    "itl": 159.60829208960791,
    "ttft": 1991436.4886462127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 636231,
    "finished_requests": 89079,
    "scheduler_time": 54.801495773966955
}
#Debug simulation 
Total elapsed time: 6.431678603054024. Arrivals time: 0.3456297726370394 Scheduler time: 5.965567671577446 Scheduler overhead time: 0.03479998873081058 Adapter cache time: 0.03293889539781958 Engine time: 0.036333695403300226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_160_slots_160_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [53 53 54]
Adapter prompts. [270, 34560, 270, 270, 540, 270, 270, 270, 34560, 270, 540, 34560, 540, 270, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 270, 34560, 540, 34560, 34560, 34560, 540, 34560, 270, 540, 540, 540, 34560, 270, 270, 34560, 270, 34560, 270, 270, 540, 270, 540, 34560, 270, 270, 540, 270, 270, 270, 270, 270, 34560, 540, 540, 34560, 540, 270, 34560, 34560, 34560, 540, 540, 270, 270, 270, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 270, 270, 34560, 540, 34560, 34560, 270, 34560, 270, 540, 270, 270, 540, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 270, 34560, 540, 34560, 540, 34560, 34560, 34560, 270, 540, 540, 34560, 540, 34560, 270, 270, 270, 540, 540, 540, 34560, 34560, 540, 34560, 270, 540, 540, 270, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 270, 34560, 270, 34560, 540, 270, 540, 270, 270, 540, 540, 540, 34560, 540, 270, 270]
Prompts retrieved: 1909170 . Total input tokens: 425251465 . Total output tokens: 381966379
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.17624212289229,
    "estimated_duration": 3600.0691432059693,
    "input_throughput": 5684.470821517545,
    "output_throughput": 5050.67716110799,
    "total_throughput": 10735.147982625534,
    "itl": 101.35356179046587,
    "ttft": 2034941.1269479673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 636231,
    "finished_requests": 82836,
    "scheduler_time": 34.90708893486437
}
#Debug simulation 
Total elapsed time: 6.176335955969989. Arrivals time: 0.3302259318297729 Scheduler time: 5.679704443318769 Scheduler overhead time: 0.051482488750480115 Adapter cache time: 0.03667880583088845 Engine time: 0.053766694269143045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.561954654054716,
    "estimated_duration": 3600.009466090059,
    "input_throughput": 6243.930248442763,
    "output_throughput": 5492.897501038214,
    "total_throughput": 11736.827749480977,
    "itl": 156.2575653405597,
    "ttft": 1976793.6867740992,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 633808,
    "finished_requests": 90516,
    "scheduler_time": 55.53081134478849
}
#Debug simulation 
Total elapsed time: 6.562067038961686. Arrivals time: 0.3575057872803882 Scheduler time: 6.086734596872702 Scheduler overhead time: 0.035331310238689184 Adapter cache time: 0.02875128376763314 Engine time: 0.03698109183460474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.599292033002712,
    "estimated_duration": 3600.1000583474993,
    "input_throughput": 6243.812848445636,
    "output_throughput": 5492.934273909342,
    "total_throughput": 11736.747122354978,
    "itl": 156.2596284704215,
    "ttft": 1976800.5127487278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 633808,
    "finished_requests": 90519,
    "scheduler_time": 55.53179494401173
}
#Debug simulation 
Total elapsed time: 6.599410869064741. Arrivals time: 0.32148864259943366 Scheduler time: 6.159796569496393 Scheduler overhead time: 0.035376003477722406 Adapter cache time: 0.02893344615586102 Engine time: 0.03711193997878581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.194799787015654,
    "estimated_duration": 3600.0689295544594,
    "input_throughput": 5774.441658419231,
    "output_throughput": 5092.468882885779,
    "total_throughput": 10866.910541305011,
    "itl": 99.96702373689926,
    "ttft": 2024658.879722162,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 633808,
    "finished_requests": 83728,
    "scheduler_time": 35.13349550472364
}
#Debug simulation 
Total elapsed time: 6.194892517058179. Arrivals time: 0.27625458769034594 Scheduler time: 5.752309461240657 Scheduler overhead time: 0.05228955496568233 Adapter cache time: 0.03410470834933221 Engine time: 0.05508298322092742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.575816388009116,
    "estimated_duration": 3600.0202182870953,
    "input_throughput": 6243.911599667411,
    "output_throughput": 5492.881095375843,
    "total_throughput": 11736.792695043254,
    "itl": 156.2579010053778,
    "ttft": 1976801.22082119,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 633808,
    "finished_requests": 90516,
    "scheduler_time": 55.530769386472514
}
#Debug simulation 
Total elapsed time: 6.575941496994346. Arrivals time: 0.3510606257477775 Scheduler time: 6.106442831805907 Scheduler overhead time: 0.03542968479450792 Adapter cache time: 0.028909254702739418 Engine time: 0.03726332064252347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.251511234091595,
    "estimated_duration": 3600.0755496785787,
    "input_throughput": 5774.431039886378,
    "output_throughput": 5092.45951842228,
    "total_throughput": 10866.890558308658,
    "itl": 99.9671697091502,
    "ttft": 2024663.5025051923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978163,
    "arrivals": 633808,
    "finished_requests": 83728,
    "scheduler_time": 35.13345067816342
}
#Debug simulation 
Total elapsed time: 6.251609320053831. Arrivals time: 0.2755922297947109 Scheduler time: 5.810479147010483 Scheduler overhead time: 0.052295642206445336 Adapter cache time: 0.03370210714638233 Engine time: 0.054656053078360856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.616724911960773,
    "estimated_duration": 3600.1639998965857,
    "input_throughput": 6243.722230611075,
    "output_throughput": 5492.927544569649,
    "total_throughput": 11736.649775180724,
    "itl": 156.25769032971948,
    "ttft": 1976793.1918915443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 633808,
    "finished_requests": 90520,
    "scheduler_time": 55.53345104390267
}
#Debug simulation 
Total elapsed time: 6.616837406996638. Arrivals time: 0.30191606329753995 Scheduler time: 6.197022893116809 Scheduler overhead time: 0.03539865300990641 Adapter cache time: 0.028743545641191304 Engine time: 0.03698887792415917 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 540, 135, 135, 135, 34560, 135, 540, 34560, 540, 135, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 135, 34560, 540, 34560, 34560, 34560, 540, 34560, 135, 540, 540, 540, 34560, 135, 135, 34560, 135, 34560, 135, 135, 540, 135, 540, 34560, 135, 135, 540, 135, 135, 135, 135, 135, 34560, 540, 540, 34560, 540, 135, 34560, 34560, 34560, 540, 540, 135, 135, 135, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 135, 135, 34560, 540, 34560, 34560, 135, 34560, 135, 540, 135, 135, 540, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 135, 34560, 540, 34560, 540, 34560, 34560, 34560, 135, 540, 540, 34560, 540, 34560, 135, 135, 135, 540, 540, 540, 34560, 34560, 540, 34560, 135, 540, 540, 135, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 135, 34560, 135, 34560, 540, 135, 540, 135, 135, 540, 540, 540, 34560, 540, 135, 135]
Prompts retrieved: 1902015 . Total input tokens: 423674545 . Total output tokens: 380526369
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.217590698972344,
    "estimated_duration": 3600.082302440272,
    "input_throughput": 5774.42020864602,
    "output_throughput": 5092.4499663724455,
    "total_throughput": 10866.870175018466,
    "itl": 99.96732053052227,
    "ttft": 2024668.1562666572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.536504152864218,
    "arrivals": 633808,
    "finished_requests": 83728,
    "scheduler_time": 35.13341273538941
}
#Debug simulation 
Total elapsed time: 6.217685600044206. Arrivals time: 0.33276710275094956 Scheduler time: 5.719602393684909 Scheduler overhead time: 0.05226711870636791 Adapter cache time: 0.033569728722795844 Engine time: 0.054584591183811426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.582773880916648,
    "estimated_duration": 3600.0126848957852,
    "input_throughput": 6290.383390872844,
    "output_throughput": 5550.241832155772,
    "total_throughput": 11840.625223028615,
    "itl": 154.8800904364142,
    "ttft": 1979630.0565488252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48661755128530726,
    "arrivals": 632594,
    "finished_requests": 91476,
    "scheduler_time": 56.13811390730963
}
#Debug simulation 
Total elapsed time: 6.582868853933178. Arrivals time: 0.29936343792360276 Scheduler time: 6.168243024148978 Scheduler overhead time: 0.03566269902512431 Adapter cache time: 0.02537270332686603 Engine time: 0.0374199952930212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.858884039917029,
    "estimated_duration": 3600.0844372985807,
    "input_throughput": 6290.285240363056,
    "output_throughput": 5550.245653402935,
    "total_throughput": 11840.530893765992,
    "itl": 154.88069907938407,
    "ttft": 1979648.683354235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5191376959625633,
    "arrivals": 632594,
    "finished_requests": 91477,
    "scheduler_time": 56.13903500531921
}
#Debug simulation 
Total elapsed time: 6.858959227916785. Arrivals time: 0.5942865624092519 Scheduler time: 6.14949542703107 Scheduler overhead time: 0.035483119427226484 Adapter cache time: 0.025218954542651772 Engine time: 0.03771877440158278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.136693872977048,
    "estimated_duration": 3600.1035657683656,
    "input_throughput": 5797.893760188284,
    "output_throughput": 5123.877317141289,
    "total_throughput": 10921.771077329573,
    "itl": 99.32529115356452,
    "ttft": 2028771.3031108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5200049788691121,
    "arrivals": 632594,
    "finished_requests": 84287,
    "scheduler_time": 35.32442031510414
}
#Debug simulation 
Total elapsed time: 6.136790600023232. Arrivals time: 0.2660672782221809 Scheduler time: 5.7083536254940554 Scheduler overhead time: 0.052325707860291004 Adapter cache time: 0.030343375052325428 Engine time: 0.054768416564911604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.543591142981313,
    "estimated_duration": 3600.0241101508955,
    "input_throughput": 6290.363427330161,
    "output_throughput": 5550.224217571281,
    "total_throughput": 11840.587644901441,
    "itl": 154.88040834754366,
    "ttft": 1979638.093560512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.497482144085225,
    "arrivals": 632594,
    "finished_requests": 91476,
    "scheduler_time": 56.13809434470452
}
#Debug simulation 
Total elapsed time: 6.543733085040003. Arrivals time: 0.2887300612637773 Scheduler time: 6.1397200979990885 Scheduler overhead time: 0.03572597517631948 Adapter cache time: 0.025323056033812463 Engine time: 0.03737915854435414 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.197188067948446,
    "estimated_duration": 3600.0796963293496,
    "input_throughput": 5797.810259945561,
    "output_throughput": 5123.752959915722,
    "total_throughput": 10921.563219861282,
    "itl": 99.32777562050926,
    "ttft": 2028854.746075285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.526669929549099,
    "arrivals": 632594,
    "finished_requests": 84285,
    "scheduler_time": 35.324777619734704
}
#Debug simulation 
Total elapsed time: 6.197280208929442. Arrivals time: 0.2666379591682926 Scheduler time: 5.76859950972721 Scheduler overhead time: 0.05237831536214799 Adapter cache time: 0.030033562099561095 Engine time: 0.0548569590318948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.566089367959648,
    "estimated_duration": 3600.166897460786,
    "input_throughput": 6290.437817194744,
    "output_throughput": 5550.492676907254,
    "total_throughput": 11840.930494101998,
    "itl": 154.88088580726202,
    "ttft": 1979678.2681276882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4754179968894465,
    "arrivals": 632594,
    "finished_requests": 91482,
    "scheduler_time": 56.14081381612361
}
#Debug simulation 
Total elapsed time: 6.566195043968037. Arrivals time: 0.29746390762738883 Scheduler time: 6.1532396679976955 Scheduler overhead time: 0.035559131065383554 Adapter cache time: 0.025574862374924123 Engine time: 0.03748641686979681 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 540, 66, 66, 66, 34560, 66, 540, 34560, 540, 66, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 66, 34560, 540, 34560, 34560, 34560, 540, 34560, 66, 540, 540, 540, 34560, 66, 66, 34560, 66, 34560, 66, 66, 540, 66, 540, 34560, 66, 66, 540, 66, 66, 66, 66, 66, 34560, 540, 540, 34560, 540, 66, 34560, 34560, 34560, 540, 540, 66, 66, 66, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 66, 66, 34560, 540, 34560, 34560, 66, 34560, 66, 540, 66, 66, 540, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 66, 34560, 540, 34560, 540, 34560, 34560, 34560, 66, 540, 540, 34560, 540, 34560, 66, 66, 66, 540, 540, 540, 34560, 34560, 540, 34560, 66, 540, 540, 66, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 66, 34560, 66, 34560, 540, 66, 540, 66, 66, 540, 540, 540, 34560, 540, 66, 66]
Prompts retrieved: 1898358 . Total input tokens: 422881787 . Total output tokens: 379783575
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.2208970279898494,
    "estimated_duration": 3600.1040328473587,
    "input_throughput": 5797.746345538725,
    "output_throughput": 5123.791932593327,
    "total_throughput": 10921.538278132053,
    "itl": 99.32849570475447,
    "ttft": 2028846.7561690279,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5334606340155006,
    "arrivals": 632594,
    "finished_requests": 84286,
    "scheduler_time": 35.3256806538338
}
#Debug simulation 
Total elapsed time: 6.221018053940497. Arrivals time: 0.2671847516903654 Scheduler time: 5.791403736686334 Scheduler overhead time: 0.05261111434083432 Adapter cache time: 0.030200836597941816 Engine time: 0.05464119650423527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.603535239002667,
    "estimated_duration": 3600.1043213272565,
    "input_throughput": 6342.661479204487,
    "output_throughput": 5578.500567615746,
    "total_throughput": 11921.162046820233,
    "itl": 154.18162888575952,
    "ttft": 1970894.4199573004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.44377072287024877,
    "arrivals": 632045,
    "finished_requests": 92182,
    "scheduler_time": 56.35553230695782
}
#Debug simulation 
Total elapsed time: 6.603633085032925. Arrivals time: 0.35458886716514826 Scheduler time: 6.136186833609827 Scheduler overhead time: 0.03558479668572545 Adapter cache time: 0.02291195432189852 Engine time: 0.037450328352861106 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.884047189028934,
    "estimated_duration": 3600.069716579324,
    "input_throughput": 6342.586060165505,
    "output_throughput": 5578.500579450512,
    "total_throughput": 11921.086639616016,
    "itl": 154.18343262696933,
    "ttft": 1970884.89398879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4727824433310889,
    "arrivals": 632045,
    "finished_requests": 92180,
    "scheduler_time": 56.354533746016514
}
#Debug simulation 
Total elapsed time: 6.884158142027445. Arrivals time: 0.5931287601124495 Scheduler time: 6.179008776904084 Scheduler overhead time: 0.03562427614815533 Adapter cache time: 0.02218205516692251 Engine time: 0.037327528349123895 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.296948131057434,
    "estimated_duration": 3600.0428801834646,
    "input_throughput": 5837.258804797644,
    "output_throughput": 5144.758719945057,
    "total_throughput": 10982.017524742701,
    "itl": 99.60836827468336,
    "ttft": 2019897.905960609,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47030522620305565,
    "arrivals": 632045,
    "finished_requests": 84845,
    "scheduler_time": 35.62058922198305
}
#Debug simulation 
Total elapsed time: 6.297037538024597. Arrivals time: 0.3380332632223144 Scheduler time: 5.799985336954705 Scheduler overhead time: 0.052313178312033415 Adapter cache time: 0.026847356813959777 Engine time: 0.054859688971191645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.609123728936538,
    "estimated_duration": 3600.147725113564,
    "input_throughput": 6342.625287488642,
    "output_throughput": 5578.43386811759,
    "total_throughput": 11921.059155606232,
    "itl": 154.1812355286819,
    "ttft": 1970898.4590973326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45316986804595244,
    "arrivals": 632045,
    "finished_requests": 92183,
    "scheduler_time": 56.35630539199837
}
#Debug simulation 
Total elapsed time: 6.609216562937945. Arrivals time: 0.2891387188574299 Scheduler time: 6.207332317600958 Scheduler overhead time: 0.03583642572630197 Adapter cache time: 0.022400193731300533 Engine time: 0.037517700577154756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.275575577979907,
    "estimated_duration": 3600.067269893483,
    "input_throughput": 5837.112038359703,
    "output_throughput": 5144.847751844374,
    "total_throughput": 10981.959790204077,
    "itl": 99.60834628496332,
    "ttft": 2019893.8290559892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4763414079509682,
    "arrivals": 632045,
    "finished_requests": 84845,
    "scheduler_time": 35.620058918951074
}
#Debug simulation 
Total elapsed time: 6.2756704239873216. Arrivals time: 0.3412233857670799 Scheduler time: 5.774722186848521 Scheduler overhead time: 0.05264533613808453 Adapter cache time: 0.026718109846115112 Engine time: 0.055365382111631334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.644341976032592,
    "estimated_duration": 3600.0885056274815,
    "input_throughput": 6342.689343416595,
    "output_throughput": 5578.5250747605105,
    "total_throughput": 11921.214418177105,
    "itl": 154.18107571017052,
    "ttft": 1970885.7194381733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.433557292760816,
    "arrivals": 632045,
    "finished_requests": 92182,
    "scheduler_time": 56.35552618416175
}
#Debug simulation 
Total elapsed time: 6.644434487097897. Arrivals time: 0.3696391936391592 Scheduler time: 6.161571861244738 Scheduler overhead time: 0.03589446097612381 Adapter cache time: 0.02265930303838104 Engine time: 0.03772772534284741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 540, 33, 33, 33, 34560, 33, 540, 34560, 540, 33, 540, 540, 540, 540, 34560, 34560, 34560, 540, 540, 540, 34560, 34560, 33, 34560, 540, 34560, 34560, 34560, 540, 34560, 33, 540, 540, 540, 34560, 33, 33, 34560, 33, 34560, 33, 33, 540, 33, 540, 34560, 33, 33, 540, 33, 33, 33, 33, 33, 34560, 540, 540, 34560, 540, 33, 34560, 34560, 34560, 540, 540, 33, 33, 33, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 33, 33, 34560, 540, 34560, 34560, 33, 34560, 33, 540, 33, 33, 540, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 33, 34560, 540, 34560, 540, 34560, 34560, 34560, 33, 540, 540, 34560, 540, 34560, 33, 33, 33, 540, 540, 540, 34560, 34560, 540, 34560, 33, 540, 540, 33, 34560, 34560, 34560, 34560, 34560, 34560, 540, 540, 540, 540, 540, 34560, 33, 34560, 33, 34560, 540, 33, 540, 33, 33, 540, 540, 540, 34560, 540, 33, 33]
Prompts retrieved: 1896609 . Total input tokens: 422496618 . Total output tokens: 379418946
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.303795740939677,
    "estimated_duration": 3600.085292282285,
    "input_throughput": 5837.009207823041,
    "output_throughput": 5144.692832612958,
    "total_throughput": 10981.702040436,
    "itl": 99.60885338394023,
    "ttft": 2019960.3734284153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 144,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48225183591246595,
    "arrivals": 632045,
    "finished_requests": 84844,
    "scheduler_time": 35.620143482155356
}
#Debug simulation 
Total elapsed time: 6.303910651942715. Arrivals time: 0.38187817903235555 Scheduler time: 5.762619137880392 Scheduler overhead time: 0.052551901317201555 Adapter cache time: 0.02699943899642676 Engine time: 0.05491385981440544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.743968509021215,
    "estimated_duration": 3600.000195681012,
    "input_throughput": 6450.826593804368,
    "output_throughput": 5694.4444126959315,
    "total_throughput": 12145.2710065003,
    "itl": 151.02779052769534,
    "ttft": 1958462.5808297712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 629108,
    "finished_requests": 94038,
    "scheduler_time": 57.60288168901228
}
#Debug simulation 
Total elapsed time: 6.744061226025224. Arrivals time: 0.3589343490311876 Scheduler time: 6.271136200521141 Scheduler overhead time: 0.03639319655485451 Adapter cache time: 0.022316310787573457 Engine time: 0.038081935374066234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.721889473963529,
    "estimated_duration": 3600.1314780712414,
    "input_throughput": 6450.761351760953,
    "output_throughput": 5694.36980978902,
    "total_throughput": 12145.131161549974,
    "itl": 151.02825656547586,
    "ttft": 1958536.8409142278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 629108,
    "finished_requests": 94041,
    "scheduler_time": 57.60502769640734
}
#Debug simulation 
Total elapsed time: 6.722015829989687. Arrivals time: 0.35734350816346705 Scheduler time: 6.250860434956849 Scheduler overhead time: 0.036358971148729324 Adapter cache time: 0.02226922381669283 Engine time: 0.037985403439961374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.356817731051706,
    "estimated_duration": 3600.037512450842,
    "input_throughput": 5902.598771959369,
    "output_throughput": 5223.986676515722,
    "total_throughput": 11126.58544847509,
    "itl": 97.85936323627142,
    "ttft": 2011652.3201901333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 629108,
    "finished_requests": 86070,
    "scheduler_time": 36.12677063415706
}
#Debug simulation 
Total elapsed time: 6.35691200196743. Arrivals time: 0.3407122087664902 Scheduler time: 5.856306991307065 Scheduler overhead time: 0.053040410159155726 Adapter cache time: 0.025708164903335273 Engine time: 0.05588283215183765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.807392430957407,
    "estimated_duration": 3600.071651297886,
    "input_throughput": 6450.772164945004,
    "output_throughput": 5694.3766640337135,
    "total_throughput": 12145.148828978718,
    "itl": 151.02827998219135,
    "ttft": 1958514.273332804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 629108,
    "finished_requests": 94040,
    "scheduler_time": 57.60369361237993
}
#Debug simulation 
Total elapsed time: 6.807508740923367. Arrivals time: 0.3680724947480485 Scheduler time: 6.325036049354821 Scheduler overhead time: 0.03648022888228297 Adapter cache time: 0.022357634152285755 Engine time: 0.03834891063161194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.3265511859208345,
    "estimated_duration": 3600.0198829469073,
    "input_throughput": 5902.776565390818,
    "output_throughput": 5224.077813871719,
    "total_throughput": 11126.854379262537,
    "itl": 97.85834895173907,
    "ttft": 2011635.5697988486,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978163,
    "arrivals": 629108,
    "finished_requests": 86071,
    "scheduler_time": 36.125343367029735
}
#Debug simulation 
Total elapsed time: 6.326668602996506. Arrivals time: 0.3387725695502013 Scheduler time: 5.828983428189531 Scheduler overhead time: 0.052826149156317115 Adapter cache time: 0.02552888891659677 Engine time: 0.05540415027644485 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.7654116719495505,
    "estimated_duration": 3600.16139674654,
    "input_throughput": 6450.707743543698,
    "output_throughput": 5694.32248746577,
    "total_throughput": 12145.030231009468,
    "itl": 151.02721015839353,
    "ttft": 1958535.7318510795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 629108,
    "finished_requests": 94041,
    "scheduler_time": 57.60562750140716
}
#Debug simulation 
Total elapsed time: 6.76550664997194. Arrivals time: 0.36142132862005383 Scheduler time: 6.290682858671062 Scheduler overhead time: 0.0362538123736158 Adapter cache time: 0.02185572893358767 Engine time: 0.03804905561264604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [53 53 54]
Adapter prompts. [135, 34560, 135, 135, 270, 135, 135, 135, 34560, 135, 270, 34560, 270, 135, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 135, 34560, 270, 34560, 34560, 34560, 270, 34560, 135, 270, 270, 270, 34560, 135, 135, 34560, 135, 34560, 135, 135, 270, 135, 270, 34560, 135, 135, 270, 135, 135, 135, 135, 135, 34560, 270, 270, 34560, 270, 135, 34560, 34560, 34560, 270, 270, 135, 135, 135, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 135, 135, 34560, 270, 34560, 34560, 135, 34560, 135, 270, 135, 135, 270, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 135, 34560, 270, 34560, 270, 34560, 34560, 34560, 135, 270, 270, 34560, 270, 34560, 135, 135, 135, 270, 270, 270, 34560, 34560, 270, 34560, 135, 270, 270, 135, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 135, 34560, 135, 34560, 270, 135, 270, 135, 135, 270, 270, 270, 34560, 270, 135, 135]
Prompts retrieved: 1887705 . Total input tokens: 420506884 . Total output tokens: 377587439
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.270784278050996,
    "estimated_duration": 3600.0267676429917,
    "input_throughput": 5902.76527691289,
    "output_throughput": 5224.067823338206,
    "total_throughput": 11126.833100251095,
    "itl": 97.85849889885075,
    "ttft": 2011640.4343815949,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642178,
    "arrivals": 629108,
    "finished_requests": 86071,
    "scheduler_time": 36.12531037051196
}
#Debug simulation 
Total elapsed time: 6.270906909951009. Arrivals time: 0.33360703580547124 Scheduler time: 5.77991667878814 Scheduler overhead time: 0.05269663513172418 Adapter cache time: 0.024684355477802455 Engine time: 0.05495904735289514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.73500552889891,
    "estimated_duration": 3600.149208370245,
    "input_throughput": 6478.51148662763,
    "output_throughput": 5738.688538787717,
    "total_throughput": 12217.200025415346,
    "itl": 150.23143245121977,
    "ttft": 1956306.6438013515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48661755128530726,
    "arrivals": 627971,
    "finished_requests": 94635,
    "scheduler_time": 58.10900521069516
}
#Debug simulation 
Total elapsed time: 6.73510498192627. Arrivals time: 0.29403174470644444 Scheduler time: 6.329321048571728 Scheduler overhead time: 0.03649215679615736 Adapter cache time: 0.0198131431825459 Engine time: 0.038173897308297455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.803065159008838,
    "estimated_duration": 3600.005084193549,
    "input_throughput": 6478.703628060224,
    "output_throughput": 5738.808561885147,
    "total_throughput": 12217.512189945372,
    "itl": 150.23212107705484,
    "ttft": 1956309.3321910845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5187291006441229,
    "arrivals": 627971,
    "finished_requests": 94632,
    "scheduler_time": 58.106388483659615
}
#Debug simulation 
Total elapsed time: 6.803186158067547. Arrivals time: 0.3073935667052865 Scheduler time: 6.383698389632627 Scheduler overhead time: 0.036627834546379745 Adapter cache time: 0.019462148193269968 Engine time: 0.038662562845274806 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.613833399023861,
    "estimated_duration": 3600.059328771248,
    "input_throughput": 5906.000167851958,
    "output_throughput": 5251.595119254022,
    "total_throughput": 11157.595287105982,
    "itl": 97.85481757074238,
    "ttft": 2010345.1509606414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5162869650311785,
    "arrivals": 627971,
    "finished_requests": 86409,
    "scheduler_time": 36.49289791769278
}
#Debug simulation 
Total elapsed time: 6.613904255093075. Arrivals time: 0.5835745093645528 Scheduler time: 5.8736494107870385 Scheduler overhead time: 0.05304826982319355 Adapter cache time: 0.022449030075222254 Engine time: 0.05584470403846353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.722207261016592,
    "estimated_duration": 3600.1617793413266,
    "input_throughput": 6478.488865094059,
    "output_throughput": 5738.668500552747,
    "total_throughput": 12217.157365646806,
    "itl": 150.2316272005693,
    "ttft": 1956314.70057964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.497482144085225,
    "arrivals": 627971,
    "finished_requests": 94635,
    "scheduler_time": 58.10898419332917
}
#Debug simulation 
Total elapsed time: 6.722300747060217. Arrivals time: 0.2958649538923055 Scheduler time: 6.314301517326385 Scheduler overhead time: 0.036547639523632824 Adapter cache time: 0.019792976323515177 Engine time: 0.03854456264525652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.294235575012863,
    "estimated_duration": 3600.075034947993,
    "input_throughput": 5906.038011318049,
    "output_throughput": 5251.6038739379,
    "total_throughput": 11157.64188525595,
    "itl": 97.8552606328608,
    "ttft": 2010299.0071004757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5227004081383357,
    "arrivals": 627971,
    "finished_requests": 86410,
    "scheduler_time": 36.493378793217275
}
#Debug simulation 
Total elapsed time: 6.294354507001117. Arrivals time: 0.2741100051207468 Scheduler time: 5.864398277248256 Scheduler overhead time: 0.05284369201399386 Adapter cache time: 0.0221576860640198 Engine time: 0.05556912976317108 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.716297129984014,
    "estimated_duration": 3600.1382579157653,
    "input_throughput": 6478.531192161153,
    "output_throughput": 5738.705994019466,
    "total_throughput": 12217.237186180619,
    "itl": 150.2311501089591,
    "ttft": 1956299.054305634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4754179968894465,
    "arrivals": 627971,
    "finished_requests": 94635,
    "scheduler_time": 58.109031358553224
}
#Debug simulation 
Total elapsed time: 6.71639277797658. Arrivals time: 0.2922765673138201 Scheduler time: 6.311930438154377 Scheduler overhead time: 0.03660181618761271 Adapter cache time: 0.019704950507730246 Engine time: 0.03847645572386682 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 270, 66, 66, 66, 34560, 66, 270, 34560, 270, 66, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 66, 34560, 270, 34560, 34560, 34560, 270, 34560, 66, 270, 270, 270, 34560, 66, 66, 34560, 66, 34560, 66, 66, 270, 66, 270, 34560, 66, 66, 270, 66, 66, 66, 66, 66, 34560, 270, 270, 34560, 270, 66, 34560, 34560, 34560, 270, 270, 66, 66, 66, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 66, 66, 34560, 270, 34560, 34560, 66, 34560, 66, 270, 66, 66, 270, 270, 34560, 34560, 34560, 66, 34560, 270, 270, 66, 66, 66, 66, 34560, 270, 34560, 270, 34560, 34560, 34560, 66, 270, 270, 34560, 270, 34560, 66, 66, 66, 270, 270, 270, 34560, 34560, 270, 34560, 66, 270, 270, 66, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 66, 34560, 66, 34560, 270, 66, 270, 66, 66, 270, 270, 270, 34560, 270, 66, 66]
Prompts retrieved: 1884048 . Total input tokens: 419665541 . Total output tokens: 376874901
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.305502201081254,
    "estimated_duration": 3600.0530770199034,
    "input_throughput": 5905.980424488739,
    "output_throughput": 5251.555906406286,
    "total_throughput": 11157.536330895025,
    "itl": 97.85669373113619,
    "ttft": 2010322.4143513094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5294911126047375,
    "arrivals": 627971,
    "finished_requests": 86410,
    "scheduler_time": 36.49358835991051
}
#Debug simulation 
Total elapsed time: 6.305601192987524. Arrivals time: 0.27636635513044894 Scheduler time: 5.873460045433603 Scheduler overhead time: 0.05285572831053287 Adapter cache time: 0.022014289279468358 Engine time: 0.05551073048263788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.845259100897238,
    "estimated_duration": 3600.106763706223,
    "input_throughput": 6539.787996665435,
    "output_throughput": 5781.194938389439,
    "total_throughput": 12320.982935054873,
    "itl": 148.9741701085852,
    "ttft": 1949025.5497176694,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4590731615899125,
    "arrivals": 627360,
    "finished_requests": 95526,
    "scheduler_time": 58.51157154553185
}
#Debug simulation 
Total elapsed time: 6.845352237927727. Arrivals time: 0.3648785366676748 Scheduler time: 6.367881278623827 Scheduler overhead time: 0.03691758087370545 Adapter cache time: 0.019628989160992205 Engine time: 0.03859729296527803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.8266772569622844,
    "estimated_duration": 3600.1381601541166,
    "input_throughput": 6539.730963822822,
    "output_throughput": 5781.144521161663,
    "total_throughput": 12320.875484984485,
    "itl": 148.9750689703561,
    "ttft": 1949046.0272445166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49059286203468216,
    "arrivals": 627360,
    "finished_requests": 95526,
    "scheduler_time": 58.5114482929787
}
#Debug simulation 
Total elapsed time: 6.826771622989327. Arrivals time: 0.36563803639728576 Scheduler time: 6.350466643925756 Scheduler overhead time: 0.036839926266111434 Adapter cache time: 0.017787575954571366 Engine time: 0.03869805810973048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.357232382986695,
    "estimated_duration": 3600.061381472904,
    "input_throughput": 5949.280229004674,
    "output_throughput": 5272.565933926942,
    "total_throughput": 11221.846162931615,
    "itl": 97.03517501647646,
    "ttft": 2008130.5483581878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4882208004035071,
    "arrivals": 627360,
    "finished_requests": 86958,
    "scheduler_time": 36.50859952484492
}
#Debug simulation 
Total elapsed time: 6.357323998003267. Arrivals time: 0.34666827123146504 Scheduler time: 5.855505253421143 Scheduler overhead time: 0.05332106782589108 Adapter cache time: 0.020670528523623943 Engine time: 0.05577547533903271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.802875326015055,
    "estimated_duration": 3600.117411036248,
    "input_throughput": 6539.768655273711,
    "output_throughput": 5781.177840533058,
    "total_throughput": 12320.94649580677,
    "itl": 148.97450061372868,
    "ttft": 1949031.9768735904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4697545007942247,
    "arrivals": 627360,
    "finished_requests": 95526,
    "scheduler_time": 58.51153753634997
}
#Debug simulation 
Total elapsed time: 6.803000921034254. Arrivals time: 0.36752132896799594 Scheduler time: 6.325755692902021 Scheduler overhead time: 0.03663708223029971 Adapter cache time: 0.017440162715502083 Engine time: 0.038305978989228606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.369657502975315,
    "estimated_duration": 3600.0677559898377,
    "input_throughput": 5949.269694817504,
    "output_throughput": 5272.556597974647,
    "total_throughput": 11221.826292792151,
    "itl": 97.03530272344014,
    "ttft": 2008134.7223334603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4946342435106643,
    "arrivals": 627360,
    "finished_requests": 86958,
    "scheduler_time": 36.50856059867288
}
#Debug simulation 
Total elapsed time: 6.369750543963164. Arrivals time: 0.34021926019340754 Scheduler time: 5.874114038189873 Scheduler overhead time: 0.053299888502806425 Adapter cache time: 0.020766825880855322 Engine time: 0.05575392011087388 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.909322025952861,
    "estimated_duration": 3600.0962377935684,
    "input_throughput": 6539.807117609066,
    "output_throughput": 5781.211841368954,
    "total_throughput": 12321.01895897802,
    "itl": 148.97386362622785,
    "ttft": 1949018.583548834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4485075442353269,
    "arrivals": 627360,
    "finished_requests": 95526,
    "scheduler_time": 58.51161125022733
}
#Debug simulation 
Total elapsed time: 6.909434765926562. Arrivals time: 0.3757357037393376 Scheduler time: 6.423379052546807 Scheduler overhead time: 0.0366824158700183 Adapter cache time: 0.017797054955735803 Engine time: 0.03836886689532548 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 2.500e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 270, 33, 33, 33, 34560, 33, 270, 34560, 270, 33, 270, 270, 270, 270, 34560, 34560, 34560, 270, 270, 270, 34560, 34560, 33, 34560, 270, 34560, 34560, 34560, 270, 34560, 33, 270, 270, 270, 34560, 33, 33, 34560, 33, 34560, 33, 33, 270, 33, 270, 34560, 33, 33, 270, 33, 33, 33, 33, 33, 34560, 270, 270, 34560, 270, 33, 34560, 34560, 34560, 270, 270, 33, 33, 33, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 33, 33, 34560, 270, 34560, 34560, 33, 34560, 33, 270, 33, 33, 270, 270, 34560, 34560, 34560, 33, 34560, 270, 270, 33, 33, 33, 33, 34560, 270, 34560, 270, 34560, 34560, 34560, 33, 270, 270, 34560, 270, 34560, 33, 33, 33, 270, 270, 270, 34560, 34560, 270, 34560, 33, 270, 270, 33, 34560, 34560, 34560, 34560, 34560, 34560, 270, 270, 270, 270, 270, 34560, 33, 34560, 33, 34560, 270, 33, 270, 33, 33, 270, 270, 270, 34560, 270, 33, 33]
Prompts retrieved: 1882299 . Total input tokens: 419280143 . Total output tokens: 376530547
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.391901978990063,
    "estimated_duration": 3600.0742647423644,
    "input_throughput": 5949.258938838236,
    "output_throughput": 5272.547065458494,
    "total_throughput": 11221.80600429673,
    "itl": 97.03544367804396,
    "ttft": 2008138.6709224542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 149,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5011734404042362,
    "arrivals": 627360,
    "finished_requests": 86958,
    "scheduler_time": 36.50853015430885
}
#Debug simulation 
Total elapsed time: 6.392023029038683. Arrivals time: 0.33966244652401656 Scheduler time: 5.896871103905141 Scheduler overhead time: 0.05318823177367449 Adapter cache time: 0.020572639885358512 Engine time: 0.056184271001257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-8/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.942084046895616,
    "estimated_duration": 3600.138847418034,
    "input_throughput": 6666.963697029039,
    "output_throughput": 5873.846508771758,
    "total_throughput": 12540.810205800797,
    "itl": 146.57047185489498,
    "ttft": 1945593.9768012953,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48661755128530726,
    "arrivals": 625657,
    "finished_requests": 96775,
    "scheduler_time": 59.35716716368833
}
#Debug simulation 
Total elapsed time: 6.942178976954892. Arrivals time: 0.3703770647989586 Scheduler time: 6.461587412864901 Scheduler overhead time: 0.03742057515773922 Adapter cache time: 0.015809559728950262 Engine time: 0.039387982222251594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.917456849012524,
    "estimated_duration": 3600.0291163982192,
    "input_throughput": 6666.951078443748,
    "output_throughput": 5873.79916003461,
    "total_throughput": 12540.750238478358,
    "itl": 146.57075707366454,
    "ttft": 1945559.6541893608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.518729100644123,
    "arrivals": 625657,
    "finished_requests": 96773,
    "scheduler_time": 59.355125042417384
}
#Debug simulation 
Total elapsed time: 6.9175529750064015. Arrivals time: 0.3647614544024691 Scheduler time: 6.442294441978447 Scheduler overhead time: 0.037692580837756395 Adapter cache time: 0.01564002363011241 Engine time: 0.039449422736652195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.427767483983189,
    "estimated_duration": 3600.0216754823036,
    "input_throughput": 6027.270932220991,
    "output_throughput": 5319.726303435181,
    "total_throughput": 11346.997235656172,
    "itl": 95.9572790333685,
    "ttft": 2007196.788484259,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.519667731374504,
    "arrivals": 625657,
    "finished_requests": 87507,
    "scheduler_time": 36.691246997675194
}
#Debug simulation 
Total elapsed time: 6.427859885036014. Arrivals time: 0.2844438027823344 Scheduler time: 5.988965032272972 Scheduler overhead time: 0.053923469968140125 Adapter cache time: 0.018282293458469212 Engine time: 0.05654060957022011 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.9383603079477325,
    "estimated_duration": 3600.1499764876517,
    "input_throughput": 6666.943087581209,
    "output_throughput": 5873.828351070788,
    "total_throughput": 12540.771438651997,
    "itl": 146.57072909970927,
    "ttft": 1945601.2351699038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49707354876678467,
    "arrivals": 625657,
    "finished_requests": 96775,
    "scheduler_time": 59.35712009521645
}
#Debug simulation 
Total elapsed time: 6.938480698969215. Arrivals time: 0.37785477621946484 Scheduler time: 6.450093689840287 Scheduler overhead time: 0.037599915522150695 Adapter cache time: 0.015780399786308408 Engine time: 0.03948274126742035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.441189064993523,
    "estimated_duration": 3600.0035884981626,
    "input_throughput": 6027.34704746561,
    "output_throughput": 5319.988585897427,
    "total_throughput": 11347.335633363036,
    "itl": 95.95726644616867,
    "ttft": 2007200.6186431109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5263326820544909,
    "arrivals": 625657,
    "finished_requests": 87509,
    "scheduler_time": 36.690779997749566
}
#Debug simulation 
Total elapsed time: 6.441284503904171. Arrivals time: 0.28035061166156083 Scheduler time: 6.005803438369185 Scheduler overhead time: 0.05407632875721902 Adapter cache time: 0.01811951061245054 Engine time: 0.05717640626244247 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.982494339928962,
    "estimated_duration": 3600.095789754087,
    "input_throughput": 6667.04343487469,
    "output_throughput": 5873.916760821654,
    "total_throughput": 12540.960195696343,
    "itl": 146.56976703386127,
    "ttft": 1945585.0954045379,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4754179968894465,
    "arrivals": 625657,
    "finished_requests": 96775,
    "scheduler_time": 59.35684306986085
}
#Debug simulation 
Total elapsed time: 6.982603024924174. Arrivals time: 0.36667204124387354 Scheduler time: 6.505959682632238 Scheduler overhead time: 0.03742687776684761 Adapter cache time: 0.015698843053542078 Engine time: 0.03913082543294877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_160_slots_160_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.403824370936491,
    "estimated_duration": 3600.0176918206525,
    "input_throughput": 6027.396767882606,
    "output_throughput": 5319.924411347619,
    "total_throughput": 11347.321179230225,
    "itl": 95.95606367529743,
    "ttft": 2007153.3182445043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5329976327344778,
    "arrivals": 625657,
    "finished_requests": 87509,
    "scheduler_time": 36.690116227700635
}
#Debug simulation 
Total elapsed time: 6.403942334000021. Arrivals time: 0.34202664357144386 Scheduler time: 5.907462829607539 Scheduler overhead time: 0.05406791449058801 Adapter cache time: 0.01822723657824099 Engine time: 0.05640183005016297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.444052387028933,
    "estimated_duration": 3600.009481563919,
    "input_throughput": 6642.08445073659,
    "output_throughput": 5904.795837028012,
    "total_throughput": 12546.880287764601,
    "itl": 146.53439741499213,
    "ttft": 1944776.1626061762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46825462482171076,
    "arrivals": 625114,
    "finished_requests": 96899,
    "scheduler_time": 59.80061182378718
}
#Debug simulation 
Total elapsed time: 7.444149618968368. Arrivals time: 0.3050104967551306 Scheduler time: 7.0307980926008895 Scheduler overhead time: 0.03749714407604188 Adapter cache time: 0.013787288451567292 Engine time: 0.03935201745480299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.9495812819805,
    "estimated_duration": 3600.0990031673073,
    "input_throughput": 6641.927063384362,
    "output_throughput": 5904.764280453898,
    "total_throughput": 12546.69134383826,
    "itl": 146.5347848570876,
    "ttft": 1944810.5139853337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49915441760094836,
    "arrivals": 625114,
    "finished_requests": 96900,
    "scheduler_time": 59.802156082499714
}
#Debug simulation 
Total elapsed time: 6.949706144980155. Arrivals time: 0.35268536140210927 Scheduler time: 6.488853723392822 Scheduler overhead time: 0.03745886276010424 Adapter cache time: 0.013662224286235869 Engine time: 0.03936103195883334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.714195807930082,
    "estimated_duration": 3600.0071740589897,
    "input_throughput": 5992.14611444175,
    "output_throughput": 5340.201580299679,
    "total_throughput": 11332.34769474143,
    "itl": 95.83886943346052,
    "ttft": 2005033.6280742304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49329609561711707,
    "arrivals": 625114,
    "finished_requests": 87326,
    "scheduler_time": 36.923553466276495
}
#Debug simulation 
Total elapsed time: 6.714261173969135. Arrivals time: 0.5854362131794915 Scheduler time: 5.976923226495273 Scheduler overhead time: 0.053754103602841496 Adapter cache time: 0.016095834434963763 Engine time: 0.05638645542785525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.94725413096603,
    "estimated_duration": 3600.0208253946744,
    "input_throughput": 6642.06352122381,
    "output_throughput": 5904.777230745473,
    "total_throughput": 12546.840751969285,
    "itl": 146.5345830932992,
    "ttft": 1944781.626942297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47872465167893125,
    "arrivals": 625114,
    "finished_requests": 96899,
    "scheduler_time": 59.80076288065645
}
#Debug simulation 
Total elapsed time: 6.9473745269933715. Arrivals time: 0.3356133853085339 Scheduler time: 6.502578492742032 Scheduler overhead time: 0.03790228767320514 Adapter cache time: 0.013945314451120794 Engine time: 0.039455140475183725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.373076740070246,
    "estimated_duration": 3600.009976490328,
    "input_throughput": 5992.1414498496615,
    "output_throughput": 5340.19742321446,
    "total_throughput": 11332.338873064122,
    "itl": 95.83913024946152,
    "ttft": 2005046.9788568395,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4995837849378593,
    "arrivals": 625114,
    "finished_requests": 87326,
    "scheduler_time": 36.92349283509455
}
#Debug simulation 
Total elapsed time: 6.373171317973174. Arrivals time: 0.2722751629771665 Scheduler time: 5.949129996239208 Scheduler overhead time: 0.05365207197610289 Adapter cache time: 0.016117192804813385 Engine time: 0.056350399274379015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.909045828972012,
    "estimated_duration": 3600.145560734873,
    "input_throughput": 6641.841169088477,
    "output_throughput": 5904.687919246467,
    "total_throughput": 12546.529088334943,
    "itl": 146.53404631479765,
    "ttft": 1944829.4123611806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45747769512003345,
    "arrivals": 625114,
    "finished_requests": 96900,
    "scheduler_time": 59.80318639038719
}
#Debug simulation 
Total elapsed time: 6.909158392925747. Arrivals time: 0.3045292398892343 Scheduler time: 6.496309181326069 Scheduler overhead time: 0.03743208758533001 Adapter cache time: 0.013846390298567712 Engine time: 0.03930871409829706 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.350134074920788,
    "estimated_duration": 3600.064799777765,
    "input_throughput": 5991.847702666796,
    "output_throughput": 5339.809994860841,
    "total_throughput": 11331.657697527637,
    "itl": 95.84115243511202,
    "ttft": 2005073.3070660841,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 151,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5058714742586015,
    "arrivals": 625114,
    "finished_requests": 87323,
    "scheduler_time": 36.924741246597385
}
#Debug simulation 
Total elapsed time: 6.350227450951934. Arrivals time: 0.2724302824353799 Scheduler time: 5.925508343963884 Scheduler overhead time: 0.05398536042775959 Adapter cache time: 0.01612962002400309 Engine time: 0.0565432506846264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.093143051024526,
    "estimated_duration": 3600.0971894852573,
    "input_throughput": 6706.28394992081,
    "output_throughput": 5971.0040781075495,
    "total_throughput": 12677.28802802836,
    "itl": 145.0182883793337,
    "ttft": 1934181.092799952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.465194137077778,
    "arrivals": 623927,
    "finished_requests": 97813,
    "scheduler_time": 60.548203600070195
}
#Debug simulation 
Total elapsed time: 7.093244318966754. Arrivals time: 0.3673382792621851 Scheduler time: 6.6187565920408815 Scheduler overhead time: 0.03796194726601243 Adapter cache time: 0.011186661198735237 Engine time: 0.03989470319356769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.025839393027127,
    "estimated_duration": 3600.03032168623,
    "input_throughput": 6706.28907055751,
    "output_throughput": 5971.11442937273,
    "total_throughput": 12677.40349993024,
    "itl": 145.0201479600323,
    "ttft": 1934159.389124637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49493858135072516,
    "arrivals": 623927,
    "finished_requests": 97812,
    "scheduler_time": 60.54661956639667
}
#Debug simulation 
Total elapsed time: 7.0259369449922815. Arrivals time: 0.3602297325851396 Scheduler time: 6.5592878453899175 Scheduler overhead time: 0.03773109521716833 Adapter cache time: 0.01112668844871223 Engine time: 0.03977770730853081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.751342689967714,
    "estimated_duration": 3600.050870702364,
    "input_throughput": 6022.5993405948575,
    "output_throughput": 5374.100726589295,
    "total_throughput": 11396.700067184152,
    "itl": 95.23269531590701,
    "ttft": 1997650.6467195465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4895780817791835,
    "arrivals": 623927,
    "finished_requests": 87843,
    "scheduler_time": 37.197315981362685
}
#Debug simulation 
Total elapsed time: 6.751413626014255. Arrivals time: 0.6514450294198468 Scheduler time: 5.949706199578941 Scheduler overhead time: 0.054155618883669376 Adapter cache time: 0.013857666170224547 Engine time: 0.05646460922434926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 7.0419997069984674,
    "estimated_duration": 3600.107663962629,
    "input_throughput": 6706.264438054489,
    "output_throughput": 5970.986705530689,
    "total_throughput": 12677.251143585177,
    "itl": 145.01863775845803,
    "ttft": 1934187.7511413363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.475734601384029,
    "arrivals": 623927,
    "finished_requests": 97813,
    "scheduler_time": 60.54816456268223
}
#Debug simulation 
Total elapsed time: 7.042119323043153. Arrivals time: 0.30667628871742636 Scheduler time: 6.6285061307717115 Scheduler overhead time: 0.037968384684063494 Adapter cache time: 0.0111295374808833 Engine time: 0.039919140515848994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.509028443950228,
    "estimated_duration": 3600.0728817719387,
    "input_throughput": 6022.531129794363,
    "output_throughput": 5373.897039128215,
    "total_throughput": 11396.428168922577,
    "itl": 95.23313696609067,
    "ttft": 1997685.195334738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4954885097406812,
    "arrivals": 623927,
    "finished_requests": 87842,
    "scheduler_time": 37.19781344001975
}
#Debug simulation 
Total elapsed time: 6.509122211951762. Arrivals time: 0.3392526573734358 Scheduler time: 6.018302943557501 Scheduler overhead time: 0.05439026444219053 Adapter cache time: 0.014022889430634677 Engine time: 0.05709012399893254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 7.080365865025669,
    "estimated_duration": 3600.057356861607,
    "input_throughput": 6706.358151206566,
    "output_throughput": 5971.070143932253,
    "total_throughput": 12677.428295138818,
    "itl": 145.01774114699194,
    "ttft": 1934158.4409566096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 152,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45448764482513127,
    "arrivals": 623927,
    "finished_requests": 97813,
    "scheduler_time": 60.54771750317387
}
#Debug simulation 
Total elapsed time: 7.080471895984374. Arrivals time: 0.37820267805363983 Scheduler time: 6.595562526490539 Scheduler overhead time: 0.037922751158475876 Adapter cache time: 0.011107140220701694 Engine time: 0.03975630924105644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_160_slots_160_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.510015190113336,
    "estimated_duration": 3600.003941110945,
    "input_throughput": 6022.677295544609,
    "output_throughput": 5373.840228083183,
    "total_throughput": 11396.517523627792,
    "itl": 95.23133075407101,
    "ttft": 1997631.0535716524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 150,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5019019528478383,
    "arrivals": 623927,
    "finished_requests": 87841,
    "scheduler_time": 37.1953552152314
}
#Debug simulation 
Total elapsed time: 6.510130930109881. Arrivals time: 0.34040945256128907 Scheduler time: 6.019183660042472 Scheduler overhead time: 0.05415003397502005 Adapter cache time: 0.013731760438531637 Engine time: 0.0567204924300313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.538886360009201,
    "estimated_duration": 3600.1927941014055,
    "input_throughput": 4222.901069328615,
    "output_throughput": 3716.8523368871256,
    "total_throughput": 7939.753406215741,
    "itl": 230.1682823790851,
    "ttft": 2166579.7258074414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 540089,
    "finished_requests": 61640,
    "scheduler_time": 37.78374419711488
}
#Debug simulation 
Total elapsed time: 4.538979614037089. Arrivals time: 0.27788682805839926 Scheduler time: 4.177715807920322 Scheduler overhead time: 0.024252442177385092 Adapter cache time: 0.022246750071644783 Engine time: 0.025401918916031718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.528514567995444,
    "estimated_duration": 3600.046440290408,
    "input_throughput": 4222.964134533114,
    "output_throughput": 3716.8356636312174,
    "total_throughput": 7939.799798164331,
    "itl": 230.1680649194378,
    "ttft": 2166573.1494877576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 540089,
    "finished_requests": 61638,
    "scheduler_time": 37.78203173641124
}
#Debug simulation 
Total elapsed time: 4.5286072680028155. Arrivals time: 0.27505610964726657 Scheduler time: 4.169620153028518 Scheduler overhead time: 0.02432812750339508 Adapter cache time: 0.02277955028694123 Engine time: 0.02533451304771006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.390755363972858,
    "estimated_duration": 3600.105229376633,
    "input_throughput": 3889.5732507309604,
    "output_throughput": 3446.156211980565,
    "total_throughput": 7335.729462711525,
    "itl": 148.03103037054848,
    "ttft": 2214990.942634892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 540089,
    "finished_requests": 56796,
    "scheduler_time": 24.003868187376465
}
#Debug simulation 
Total elapsed time: 4.390872450079769. Arrivals time: 0.26893576385919005 Scheduler time: 3.9633626642171293 Scheduler overhead time: 0.03596961416769773 Adapter cache time: 0.0679886220023036 Engine time: 0.03762196737807244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.543198902043514,
    "estimated_duration": 3600.2041754223046,
    "input_throughput": 4222.887719476814,
    "output_throughput": 3716.8405868065415,
    "total_throughput": 7939.728306283356,
    "itl": 230.1683519474853,
    "ttft": 2166587.684923688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 540089,
    "finished_requests": 61640,
    "scheduler_time": 37.78368927901601
}
#Debug simulation 
Total elapsed time: 4.543347009108402. Arrivals time: 0.2807302362052724 Scheduler time: 4.1788275903090835 Scheduler overhead time: 0.024267155211418867 Adapter cache time: 0.02275547804310918 Engine time: 0.02526893955655396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.398452184977941,
    "estimated_duration": 3600.1028582179506,
    "input_throughput": 3888.9211090291597,
    "output_throughput": 3445.484334339276,
    "total_throughput": 7334.405443368436,
    "itl": 147.92862188999453,
    "ttft": 2215005.9213470044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978163,
    "arrivals": 540089,
    "finished_requests": 56787,
    "scheduler_time": 23.973961028743137
}
#Debug simulation 
Total elapsed time: 4.398559827008285. Arrivals time: 0.2219179345993325 Scheduler time: 4.01734919718001 Scheduler overhead time: 0.035878299036994576 Adapter cache time: 0.06866544845979661 Engine time: 0.0375696507981047 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.530671571032144,
    "estimated_duration": 3600.1461614189543,
    "input_throughput": 4222.955490787024,
    "output_throughput": 3716.867704817277,
    "total_throughput": 7939.823195604301,
    "itl": 230.16757754208726,
    "ttft": 2166591.385569775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 540089,
    "finished_requests": 61639,
    "scheduler_time": 37.78316358873603
}
#Debug simulation 
Total elapsed time: 4.530766902025789. Arrivals time: 0.22838164283894002 Scheduler time: 4.218591348966584 Scheduler overhead time: 0.0243188712047413 Adapter cache time: 0.02258819353301078 Engine time: 0.02549287606962025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.421087092952803,
    "estimated_duration": 3600.109602825634,
    "input_throughput": 3888.913823348976,
    "output_throughput": 3445.4778794135436,
    "total_throughput": 7334.39170276252,
    "itl": 147.928789817887,
    "ttft": 2215010.5262065222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 540089,
    "finished_requests": 56787,
    "scheduler_time": 23.973914931960458
}
#Debug simulation 
Total elapsed time: 4.421180980978534. Arrivals time: 0.21739020850509405 Scheduler time: 4.0442856089212 Scheduler overhead time: 0.035956995212472975 Adapter cache time: 0.06884675065521151 Engine time: 0.03762105747591704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.922693899017759,
    "estimated_duration": 3600.1169220749102,
    "input_throughput": 4562.986523929954,
    "output_throughput": 4043.527839537512,
    "total_throughput": 8606.514363467466,
    "itl": 212.77714862022614,
    "ttft": 2100037.9126439183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 483089,
    "finished_requests": 66667,
    "scheduler_time": 41.23144589431966
}
#Debug simulation 
Total elapsed time: 4.922799941035919. Arrivals time: 0.24117598356679082 Scheduler time: 4.582897079177201 Scheduler overhead time: 0.0264579284703359 Adapter cache time: 0.031887452001683414 Engine time: 0.0279118629405275 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.8704934099223465,
    "estimated_duration": 3600.0789675561623,
    "input_throughput": 4563.034352313476,
    "output_throughput": 4043.5260257309637,
    "total_throughput": 8606.56037804444,
    "itl": 212.77850039417643,
    "ttft": 2100052.8592529404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 483089,
    "finished_requests": 66666,
    "scheduler_time": 41.23092322746051
}
#Debug simulation 
Total elapsed time: 4.870588058955036. Arrivals time: 0.2570654700975865 Scheduler time: 4.5151362277101725 Scheduler overhead time: 0.02623260859400034 Adapter cache time: 0.03222213208209723 Engine time: 0.0275248596444726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.710741120972671,
    "estimated_duration": 3600.1077499238945,
    "input_throughput": 4252.98881688296,
    "output_throughput": 3783.4489815722714,
    "total_throughput": 8036.437798455231,
    "itl": 135.3449213066307,
    "ttft": 2148232.2509860327,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 483089,
    "finished_requests": 62215,
    "scheduler_time": 26.49738230797223
}
#Debug simulation 
Total elapsed time: 4.710832456010394. Arrivals time: 0.22314786177594215 Scheduler time: 4.33110547659453 Scheduler overhead time: 0.03943633520975709 Adapter cache time: 0.057517939480021596 Engine time: 0.04102995968423784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.897247315966524,
    "estimated_duration": 3600.190621129402,
    "input_throughput": 4562.893115600267,
    "output_throughput": 4043.445064981955,
    "total_throughput": 8606.338180582223,
    "itl": 212.7790593718118,
    "ttft": 2100047.9982041717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 483089,
    "finished_requests": 66667,
    "scheduler_time": 41.23192404204564
}
#Debug simulation 
Total elapsed time: 4.897339261951856. Arrivals time: 0.23557198769412935 Scheduler time: 4.5627494843211025 Scheduler overhead time: 0.026532098650932312 Adapter cache time: 0.032263312372379005 Engine time: 0.027786572463810444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.702092036954127,
    "estimated_duration": 3600.0361899161994,
    "input_throughput": 4253.099189082433,
    "output_throughput": 3783.4114107383602,
    "total_throughput": 8036.510599820794,
    "itl": 135.3477060176725,
    "ttft": 2148228.50549536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978161,
    "arrivals": 483089,
    "finished_requests": 62213,
    "scheduler_time": 26.498321438592054
}
#Debug simulation 
Total elapsed time: 4.702229178976268. Arrivals time: 0.22074949147645384 Scheduler time: 4.325162827502936 Scheduler overhead time: 0.03899834689218551 Adapter cache time: 0.05790751404128969 Engine time: 0.04079056007321924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.863643978023902,
    "estimated_duration": 3600.0819032468403,
    "input_throughput": 4563.030631382183,
    "output_throughput": 4043.522728433297,
    "total_throughput": 8606.55335981548,
    "itl": 212.77680594569023,
    "ttft": 2100035.806801999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 483089,
    "finished_requests": 66666,
    "scheduler_time": 41.231136021846254
}
#Debug simulation 
Total elapsed time: 4.863738250918686. Arrivals time: 0.23358136718161404 Scheduler time: 4.530923667480238 Scheduler overhead time: 0.026403671130537987 Adapter cache time: 0.032387325656600296 Engine time: 0.027970872586593032 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.013556084944867,
    "estimated_duration": 3600.0399078947944,
    "input_throughput": 4253.094796650084,
    "output_throughput": 3783.4075033809418,
    "total_throughput": 8036.502300031027,
    "itl": 135.34690886321988,
    "ttft": 2148282.7171122828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642178,
    "arrivals": 483089,
    "finished_requests": 62213,
    "scheduler_time": 26.49685933113538
}
#Debug simulation 
Total elapsed time: 5.013621600926854. Arrivals time: 0.2269245721399784 Scheduler time: 4.6267802064539865 Scheduler overhead time: 0.041357103968039155 Adapter cache time: 0.05903672694694251 Engine time: 0.04085641459096223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.118132577044889,
    "estimated_duration": 3600.154010341996,
    "input_throughput": 4829.3361756344375,
    "output_throughput": 4255.632107956568,
    "total_throughput": 9084.968283591006,
    "itl": 201.42261084473628,
    "ttft": 2066470.3652560383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 473564,
    "finished_requests": 70437,
    "scheduler_time": 43.287106826421315
}
#Debug simulation 
Total elapsed time: 5.1182539149886. Arrivals time: 0.2666805551853031 Scheduler time: 4.753499616752379 Scheduler overhead time: 0.027676051598973572 Adapter cache time: 0.028343976009637117 Engine time: 0.02895754750352353 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.0784927878994495,
    "estimated_duration": 3600.125216777227,
    "input_throughput": 4829.327301999741,
    "output_throughput": 4255.53781535261,
    "total_throughput": 9084.865117352352,
    "itl": 201.4247709693994,
    "ttft": 2066504.1343782532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574657,
    "arrivals": 473564,
    "finished_requests": 70435,
    "scheduler_time": 43.286307464748916
}
#Debug simulation 
Total elapsed time: 5.0785865699872375. Arrivals time: 0.24681695539038628 Scheduler time: 4.734786631423049 Scheduler overhead time: 0.027630937402136624 Adapter cache time: 0.027350100106559694 Engine time: 0.02904349402524531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.888084506965242,
    "estimated_duration": 3600.119010546638,
    "input_throughput": 4437.4077504661,
    "output_throughput": 3929.6009266793462,
    "total_throughput": 8367.008677145446,
    "itl": 129.4601808859067,
    "ttft": 2121036.839896892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 473564,
    "finished_requests": 64825,
    "scheduler_time": 27.330279797820584
}
#Debug simulation 
Total elapsed time: 4.888191826990806. Arrivals time: 0.23541231139097363 Scheduler time: 4.501753571210429 Scheduler overhead time: 0.040914203971624374 Adapter cache time: 0.04781802522484213 Engine time: 0.04282201698515564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.07876541709993,
    "estimated_duration": 3600.1651339292034,
    "input_throughput": 4829.3212542238625,
    "output_throughput": 4255.618959144468,
    "total_throughput": 9084.94021336833,
    "itl": 201.4229406304724,
    "ttft": 2066478.2340732731,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 473564,
    "finished_requests": 70437,
    "scheduler_time": 43.28706264375094
}
#Debug simulation 
Total elapsed time: 5.078885212074965. Arrivals time: 0.24266721727326512 Scheduler time: 4.738897411734797 Scheduler overhead time: 0.027848608093336225 Adapter cache time: 0.027394835487939417 Engine time: 0.028960196999832988 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.839531617006287,
    "estimated_duration": 3600.034330208555,
    "input_throughput": 4439.087112559342,
    "output_throughput": 3930.709738309699,
    "total_throughput": 8369.796850869041,
    "itl": 129.627726084664,
    "ttft": 2121079.0871459427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 473564,
    "finished_requests": 64847,
    "scheduler_time": 27.389516347396466
}
#Debug simulation 
Total elapsed time: 4.839625074993819. Arrivals time: 0.22556618764065206 Scheduler time: 4.46422415249981 Scheduler overhead time: 0.04084743023850024 Adapter cache time: 0.046950797783210874 Engine time: 0.04262558196205646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.1027804700424895,
    "estimated_duration": 3600.1433927439944,
    "input_throughput": 4829.350418386611,
    "output_throughput": 4255.644658731922,
    "total_throughput": 9084.995077118532,
    "itl": 201.42241007493797,
    "ttft": 2066466.2392863242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 473564,
    "finished_requests": 70437,
    "scheduler_time": 43.287139354681784
}
#Debug simulation 
Total elapsed time: 5.1028722079936415. Arrivals time: 0.24215346504934132 Scheduler time: 4.763550904579461 Scheduler overhead time: 0.027598238782957196 Adapter cache time: 0.02745630231220275 Engine time: 0.029068796196952462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.888008555048145,
    "estimated_duration": 3600.0791869795557,
    "input_throughput": 4437.103510881943,
    "output_throughput": 3929.3080138796213,
    "total_throughput": 8366.411524761565,
    "itl": 129.43558375956886,
    "ttft": 2121037.238859055,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.536504152864218,
    "arrivals": 473564,
    "finished_requests": 64820,
    "scheduler_time": 27.3208563877968
}
#Debug simulation 
Total elapsed time: 4.888097668997943. Arrivals time: 0.253084777854383 Scheduler time: 4.484501222614199 Scheduler overhead time: 0.04092279076576233 Adapter cache time: 0.04708116967231035 Engine time: 0.0429543029749766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.574802852002904,
    "estimated_duration": 3600.2061069734877,
    "input_throughput": 4985.850661502749,
    "output_throughput": 4385.044225501687,
    "total_throughput": 9370.894887004435,
    "itl": 195.47267338725823,
    "ttft": 2045998.8582765001,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 468761,
    "finished_requests": 72331,
    "scheduler_time": 44.60347121042566
}
#Debug simulation 
Total elapsed time: 5.574868853087537. Arrivals time: 0.5296260664472356 Scheduler time: 4.954382476280443 Scheduler overhead time: 0.02853460400365293 Adapter cache time: 0.01908750447910279 Engine time: 0.029775106231682003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.2981421550503,
    "estimated_duration": 3600.066659764442,
    "input_throughput": 4985.970732323741,
    "output_throughput": 4385.015471083784,
    "total_throughput": 9370.986203407525,
    "itl": 195.47411562061785,
    "ttft": 2045940.4776457176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574656,
    "arrivals": 468761,
    "finished_requests": 72329,
    "scheduler_time": 44.600955887600655
}
#Debug simulation 
Total elapsed time: 5.298234145040624. Arrivals time: 0.28912768338341266 Scheduler time: 4.917261547641829 Scheduler overhead time: 0.0284890946932137 Adapter cache time: 0.02001411363016814 Engine time: 0.02985363034531474 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.010910901008174,
    "estimated_duration": 3600.0382921046,
    "input_throughput": 4545.0624888866005,
    "output_throughput": 4007.0309895417263,
    "total_throughput": 8552.093478428327,
    "itl": 126.89318626232202,
    "ttft": 2107447.0417542025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 468761,
    "finished_requests": 65983,
    "scheduler_time": 27.843485853979775
}
#Debug simulation 
Total elapsed time: 5.011027239030227. Arrivals time: 0.27151637291535735 Scheduler time: 4.596704137860797 Scheduler overhead time: 0.041711570927873254 Adapter cache time: 0.037696574348956347 Engine time: 0.04358393920119852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.257406826945953,
    "estimated_duration": 3600.040292591062,
    "input_throughput": 4986.007250235787,
    "output_throughput": 4385.047587519658,
    "total_throughput": 9371.054837755444,
    "itl": 195.4736954691244,
    "ttft": 2045923.2775216245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 468761,
    "finished_requests": 72329,
    "scheduler_time": 44.60107383098362
}
#Debug simulation 
Total elapsed time: 5.257496380945668. Arrivals time: 0.2729445059085265 Scheduler time: 4.893820208264515 Scheduler overhead time: 0.028461548849008977 Adapter cache time: 0.019035666598938406 Engine time: 0.029745724285021424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.974044633912854,
    "estimated_duration": 3600.060477228717,
    "input_throughput": 4543.866722092497,
    "output_throughput": 4005.9346478261327,
    "total_throughput": 8549.80136991863,
    "itl": 126.73814099449793,
    "ttft": 2107765.6535150316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978162,
    "arrivals": 468761,
    "finished_requests": 65968,
    "scheduler_time": 27.786490706208237
}
#Debug simulation 
Total elapsed time: 4.974145272979513. Arrivals time: 0.26454973535146564 Scheduler time: 4.568342560436577 Scheduler overhead time: 0.041307664941996336 Adapter cache time: 0.037065817741677165 Engine time: 0.043307507992722094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.255503125023097,
    "estimated_duration": 3600.1395284420155,
    "input_throughput": 4985.903979051601,
    "output_throughput": 4384.961159222543,
    "total_throughput": 9370.865138274145,
    "itl": 195.47195843830207,
    "ttft": 2045950.7506997678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 468761,
    "finished_requests": 72330,
    "scheduler_time": 44.60291396359877
}
#Debug simulation 
Total elapsed time: 5.255614219000563. Arrivals time: 0.27600907580927014 Scheduler time: 4.888076125178486 Scheduler overhead time: 0.028768286807462573 Adapter cache time: 0.01908381306566298 Engine time: 0.0299587098415941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.95478421705775,
    "estimated_duration": 3600.009793110101,
    "input_throughput": 4542.101810749133,
    "output_throughput": 4004.5771618708186,
    "total_throughput": 8546.678972619951,
    "itl": 126.63342423775553,
    "ttft": 2107942.708662498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5365041528642179,
    "arrivals": 468761,
    "finished_requests": 65942,
    "scheduler_time": 27.73699155940162
}
#Debug simulation 
Total elapsed time: 4.9549246079986915. Arrivals time: 0.2575250123627484 Scheduler time: 4.556137139443308 Scheduler overhead time: 0.04154475894756615 Adapter cache time: 0.03666965663433075 Engine time: 0.04327961662784219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.317078517982736,
    "estimated_duration": 3600.1001978744316,
    "input_throughput": 5053.460737215447,
    "output_throughput": 4472.515517625492,
    "total_throughput": 9525.976254840938,
    "itl": 191.9249834033762,
    "ttft": 2036111.1459938188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48661755128530726,
    "arrivals": 466310,
    "finished_requests": 73625,
    "scheduler_time": 45.57119762863758
}
#Debug simulation 
Total elapsed time: 5.317163637955673. Arrivals time: 0.2755369743099436 Scheduler time: 4.954368496662937 Scheduler overhead time: 0.02887036674655974 Adapter cache time: 0.01451925269793719 Engine time: 0.030266932561062276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.34158456500154,
    "estimated_duration": 3600.183072104729,
    "input_throughput": 5053.344409334184,
    "output_throughput": 4472.412562783032,
    "total_throughput": 9525.756972117217,
    "itl": 191.9266014930224,
    "ttft": 2036151.6235574654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5191376959625633,
    "arrivals": 466310,
    "finished_requests": 73625,
    "scheduler_time": 45.5715930507826
}
#Debug simulation 
Total elapsed time: 5.341672900947742. Arrivals time: 0.2749654420185834 Scheduler time: 4.97905345086474 Scheduler overhead time: 0.029041633824817836 Adapter cache time: 0.014689769712276757 Engine time: 0.03026842069812119 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.015242211055011,
    "estimated_duration": 3600.058289505175,
    "input_throughput": 4568.353253597051,
    "output_throughput": 4057.7742984288984,
    "total_throughput": 8626.12755202595,
    "itl": 125.29654485409056,
    "ttft": 2101616.498512331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5200049788691121,
    "arrivals": 466310,
    "finished_requests": 66524,
    "scheduler_time": 28.30604211162779
}
#Debug simulation 
Total elapsed time: 5.015326880966313. Arrivals time: 0.2571637282380834 Scheduler time: 4.620434119482525 Scheduler overhead time: 0.04202178749255836 Adapter cache time: 0.031930537428706884 Engine time: 0.04381756449583918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.315271437051706,
    "estimated_duration": 3600.1123045348486,
    "input_throughput": 5053.443743153067,
    "output_throughput": 4472.500477198416,
    "total_throughput": 9525.944220351483,
    "itl": 191.92527283093816,
    "ttft": 2036119.718814328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.497482144085225,
    "arrivals": 466310,
    "finished_requests": 73625,
    "scheduler_time": 45.571168249609414
}
#Debug simulation 
Total elapsed time: 5.315359532018192. Arrivals time: 0.2768359227338806 Scheduler time: 4.951336978701875 Scheduler overhead time: 0.02898851817008108 Adapter cache time: 0.01447171822655946 Engine time: 0.030121083254925907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.0421904880786315,
    "estimated_duration": 3600.064833306413,
    "input_throughput": 4568.344949747798,
    "output_throughput": 4057.7669226538196,
    "total_throughput": 8626.111872401618,
    "itl": 125.29664583090542,
    "ttft": 2101621.02899488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.526669929549099,
    "arrivals": 466310,
    "finished_requests": 66524,
    "scheduler_time": 28.30599201101199
}
#Debug simulation 
Total elapsed time: 5.042277395026758. Arrivals time: 0.257500157575123 Scheduler time: 4.647697563748807 Scheduler overhead time: 0.04217241902370006 Adapter cache time: 0.03131773485802114 Engine time: 0.043657968868501484 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.368323547067121,
    "estimated_duration": 3600.0011140843894,
    "input_throughput": 5053.599824962034,
    "output_throughput": 4472.6386158619825,
    "total_throughput": 9526.238440824018,
    "itl": 191.92464308344626,
    "ttft": 2036095.054623852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4754179968894465,
    "arrivals": 466310,
    "finished_requests": 73625,
    "scheduler_time": 45.56971018728452
}
#Debug simulation 
Total elapsed time: 5.368410455062985. Arrivals time: 0.2767906767548993 Scheduler time: 5.004047255264595 Scheduler overhead time: 0.028995080618187785 Adapter cache time: 0.014527327963151038 Engine time: 0.03033964359201491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.036805878975429,
    "estimated_duration": 3600.0055539838754,
    "input_throughput": 4569.137950855971,
    "output_throughput": 4058.3629055327397,
    "total_throughput": 8627.500856388711,
    "itl": 125.34512199056422,
    "ttft": 2101531.1733122603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 159,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5334606340155006,
    "arrivals": 466310,
    "finished_requests": 66538,
    "scheduler_time": 28.328302531850486
}
#Debug simulation 
Total elapsed time: 5.036907127010636. Arrivals time: 0.2610144871287048 Scheduler time: 4.638364061363973 Scheduler overhead time: 0.042054922436363995 Adapter cache time: 0.03153130738064647 Engine time: 0.043969023739919066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.404310452984646,
    "estimated_duration": 3600.2173124564606,
    "input_throughput": 5161.698694049246,
    "output_throughput": 4511.539329529412,
    "total_throughput": 9673.238023578659,
    "itl": 189.15276193324559,
    "ttft": 2027755.5922076884,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4835570635413745,
    "arrivals": 465116,
    "finished_requests": 74656,
    "scheduler_time": 45.82482311267153
}
#Debug simulation 
Total elapsed time: 5.404422951978631. Arrivals time: 0.25982300215400755 Scheduler time: 5.059509590850212 Scheduler overhead time: 0.029609116027131677 Adapter cache time: 0.01095289969816804 Engine time: 0.030638508033007383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.626580100972205,
    "estimated_duration": 3600.1337330496967,
    "input_throughput": 5161.603534171681,
    "output_throughput": 4511.533794118584,
    "total_throughput": 9673.137328290264,
    "itl": 189.1527389664708,
    "ttft": 2027753.5126643039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5157390503492209,
    "arrivals": 465116,
    "finished_requests": 74655,
    "scheduler_time": 45.82365067766881
}
#Debug simulation 
Total elapsed time: 5.6266411419492215. Arrivals time: 0.5203377333236858 Scheduler time: 5.02124833187554 Scheduler overhead time: 0.029475551447831094 Adapter cache time: 0.011086979764513671 Engine time: 0.030761233647353947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.014746444998309,
    "estimated_duration": 3600.026727193231,
    "input_throughput": 4658.655968667146,
    "output_throughput": 4078.782773773516,
    "total_throughput": 8737.438742440663,
    "itl": 124.33413178810656,
    "ttft": 2095584.916078581,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5166242125257867,
    "arrivals": 465116,
    "finished_requests": 67340,
    "scheduler_time": 28.340097711441622
}
#Debug simulation 
Total elapsed time: 5.014830418978818. Arrivals time: 0.23989392863586545 Scheduler time: 4.641700136708096 Scheduler overhead time: 0.04221138474531472 Adapter cache time: 0.026724563562311232 Engine time: 0.04422791628167033 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.370506851002574,
    "estimated_duration": 3600.0093032891887,
    "input_throughput": 5161.667216532553,
    "output_throughput": 4511.46056349381,
    "total_throughput": 9673.127780026362,
    "itl": 189.15183212579126,
    "ttft": 2027749.738450631,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4940834984718825,
    "arrivals": 465116,
    "finished_requests": 74653,
    "scheduler_time": 45.821972621106106
}
#Debug simulation 
Total elapsed time: 5.370593133033253. Arrivals time: 0.23885433201212436 Scheduler time: 5.046503582270816 Scheduler overhead time: 0.029541742522269487 Adapter cache time: 0.01098411320708692 Engine time: 0.030828187940642238 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.003242156933993,
    "estimated_duration": 3600.0671635393865,
    "input_throughput": 4658.582531419074,
    "output_throughput": 4078.5319642660497,
    "total_throughput": 8737.114495685124,
    "itl": 124.31104602090424,
    "ttft": 2095456.9402585425,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5232891632057736,
    "arrivals": 465116,
    "finished_requests": 67338,
    "scheduler_time": 28.330276659887833
}
#Debug simulation 
Total elapsed time: 5.003327328944579. Arrivals time: 0.22023542365059257 Scheduler time: 4.6492051522945985 Scheduler overhead time: 0.04240305977873504 Adapter cache time: 0.027186489081941545 Engine time: 0.04419453756418079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.360641328967176,
    "estimated_duration": 3600.171043456652,
    "input_throughput": 5161.550041844212,
    "output_throughput": 4511.487038795068,
    "total_throughput": 9673.037080639278,
    "itl": 189.15139572336687,
    "ttft": 2027758.1509998704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.47242794659454435,
    "arrivals": 465116,
    "finished_requests": 74655,
    "scheduler_time": 45.82435708766086
}
#Debug simulation 
Total elapsed time: 5.360725820995867. Arrivals time: 0.2388274995610118 Scheduler time: 5.0368447116343305 Scheduler overhead time: 0.029457940137945116 Adapter cache time: 0.010949123534373939 Engine time: 0.030817745020613074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.987865842063911,
    "estimated_duration": 3600.0806109907217,
    "input_throughput": 4658.541519542442,
    "output_throughput": 4078.475897227025,
    "total_throughput": 8737.017416769468,
    "itl": 124.31053755291344,
    "ttft": 2095519.3161626153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 158,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5299541138857605,
    "arrivals": 465116,
    "finished_requests": 67337,
    "scheduler_time": 28.33089936712418
}
#Debug simulation 
Total elapsed time: 4.987952132010832. Arrivals time: 0.21659217565320432 Scheduler time: 4.637861929950304 Scheduler overhead time: 0.04213635460473597 Adapter cache time: 0.027188961626961827 Engine time: 0.044158658129163086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.414381999056786,
    "estimated_duration": 3600.216642410428,
    "input_throughput": 5150.322283823875,
    "output_throughput": 4536.149799325947,
    "total_throughput": 9686.472083149823,
    "itl": 188.82944740309597,
    "ttft": 2023720.1433732286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.46825462482171076,
    "arrivals": 464531,
    "finished_requests": 75027,
    "scheduler_time": 46.13238449360184
}
#Debug simulation 
Total elapsed time: 5.414472921052948. Arrivals time: 0.2786825392395258 Scheduler time: 5.052169199101627 Scheduler overhead time: 0.029571752063930035 Adapter cache time: 0.00937346916180104 Engine time: 0.0308004398830235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.444342648028396,
    "estimated_duration": 3600.144824127133,
    "input_throughput": 5150.290864894725,
    "output_throughput": 4536.060852485116,
    "total_throughput": 9686.351717379841,
    "itl": 188.83009982993315,
    "ttft": 2023743.6525983152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.49915441760094836,
    "arrivals": 464531,
    "finished_requests": 75025,
    "scheduler_time": 46.13134076075623
}
#Debug simulation 
Total elapsed time: 5.44444191607181. Arrivals time: 0.2846315369242802 Scheduler time: 5.076459552976303 Scheduler overhead time: 0.029485505656339228 Adapter cache time: 0.009275409858673811 Engine time: 0.030703453929163516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.03288197692018,
    "estimated_duration": 3600.034849643525,
    "input_throughput": 4645.8197485660785,
    "output_throughput": 4101.916958237844,
    "total_throughput": 8747.736706803922,
    "itl": 124.80149836559285,
    "ttft": 2092381.8739801277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4841655390709654,
    "arrivals": 464531,
    "finished_requests": 67702,
    "scheduler_time": 28.799359708521457
}
#Debug simulation 
Total elapsed time: 5.032990692998283. Arrivals time: 0.2590468870475888 Scheduler time: 4.644121407880448 Scheduler overhead time: 0.04206672077998519 Adapter cache time: 0.02415155724156648 Engine time: 0.04366308101452887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.408587989048101,
    "estimated_duration": 3600.062057987273,
    "input_throughput": 5150.408715555966,
    "output_throughput": 4536.028473111874,
    "total_throughput": 9686.43718866784,
    "itl": 188.83094994035736,
    "ttft": 2023705.0995450886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4783160563604909,
    "arrivals": 464531,
    "finished_requests": 75023,
    "scheduler_time": 46.130338971448275
}
#Debug simulation 
Total elapsed time: 5.408677952946164. Arrivals time: 0.27767255844082683 Scheduler time: 5.047691263956949 Scheduler overhead time: 0.029375166865065694 Adapter cache time: 0.009246051544323564 Engine time: 0.03076065704226494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.07316131296102,
    "estimated_duration": 3600.080791768143,
    "input_throughput": 4645.675741011851,
    "output_throughput": 4101.794613544615,
    "total_throughput": 8747.470354556466,
    "itl": 124.79664443457895,
    "ttft": 2092419.553501326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4904532283917078,
    "arrivals": 464531,
    "finished_requests": 67701,
    "scheduler_time": 28.79922605058958
}
#Debug simulation 
Total elapsed time: 5.073261681012809. Arrivals time: 0.26898585667368025 Scheduler time: 4.674137869151309 Scheduler overhead time: 0.04212026868481189 Adapter cache time: 0.02417305833660066 Engine time: 0.04381686623673886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.40645636501722,
    "estimated_duration": 3600.1994557439834,
    "input_throughput": 5150.346870481438,
    "output_throughput": 4536.171454041055,
    "total_throughput": 9686.518324522493,
    "itl": 188.82918338753618,
    "ttft": 2023710.3388721205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 153,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.45747769512003345,
    "arrivals": 464531,
    "finished_requests": 75027,
    "scheduler_time": 46.132400268969285
}
#Debug simulation 
Total elapsed time: 5.406569274025969. Arrivals time: 0.2783627613680437 Scheduler time: 5.044802958378568 Scheduler overhead time: 0.02939901116769761 Adapter cache time: 0.009239176055416465 Engine time: 0.03088861214928329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_160_slots_160_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.0477416350040585,
    "estimated_duration": 3600.016822368092,
    "input_throughput": 4645.664680255199,
    "output_throughput": 4102.062220444275,
    "total_throughput": 8747.726900699474,
    "itl": 124.79987760792855,
    "ttft": 2092366.20302343,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 148,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4967409177124498,
    "arrivals": 464531,
    "finished_requests": 67702,
    "scheduler_time": 28.798691542363485
}
#Debug simulation 
Total elapsed time: 5.047827607020736. Arrivals time: 0.22278619289863855 Scheduler time: 4.694252004148439 Scheduler overhead time: 0.04218562156893313 Adapter cache time: 0.024454571306705475 Engine time: 0.044013331178575754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.87938549707178,
    "estimated_duration": 3600.232548659274,
    "input_throughput": 4585.051320128556,
    "output_throughput": 4020.306411981673,
    "total_throughput": 8605.35773211023,
    "itl": 212.1884315836489,
    "ttft": 2064534.0281655262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 406769,
    "finished_requests": 66733,
    "scheduler_time": 40.99274437271716
}
#Debug simulation 
Total elapsed time: 4.879469228093512. Arrivals time: 0.23550757172051817 Scheduler time: 4.536232746904716 Scheduler overhead time: 0.026498374762013555 Adapter cache time: 0.04117506509646773 Engine time: 0.027653038850985467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.848412574036047,
    "estimated_duration": 3600.2006191704477,
    "input_throughput": 4584.983656772851,
    "output_throughput": 4020.3415117836084,
    "total_throughput": 8605.32516855646,
    "itl": 212.19231059591363,
    "ttft": 2064541.5511839206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 406769,
    "finished_requests": 66732,
    "scheduler_time": 40.99217780697848
}
#Debug simulation 
Total elapsed time: 4.848499272018671. Arrivals time: 0.2514527292223647 Scheduler time: 4.489377194782719 Scheduler overhead time: 0.026445183204486966 Adapter cache time: 0.04117535799741745 Engine time: 0.027627165312878788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.78921283595264,
    "estimated_duration": 3600.014798376836,
    "input_throughput": 4355.826816898148,
    "output_throughput": 3830.779808521338,
    "total_throughput": 8186.606625419486,
    "itl": 133.0017810035847,
    "ttft": 2103224.3049253714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 406769,
    "finished_requests": 63418,
    "scheduler_time": 26.89336441692564
}
#Debug simulation 
Total elapsed time: 4.789343666983768. Arrivals time: 0.24409489601384848 Scheduler time: 4.366535090142861 Scheduler overhead time: 0.04004939796868712 Adapter cache time: 0.0776860814075917 Engine time: 0.042056675418280065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.91770891204942,
    "estimated_duration": 3600.072867745,
    "input_throughput": 4585.135803192639,
    "output_throughput": 4020.438066595604,
    "total_throughput": 8605.573869788243,
    "itl": 212.19289162045234,
    "ttft": 2064527.1541871899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801272,
    "arrivals": 406769,
    "finished_requests": 66731,
    "scheduler_time": 40.99052557356814
}
#Debug simulation 
Total elapsed time: 4.917798519949429. Arrivals time: 0.2183805921813473 Scheduler time: 4.5916418097913265 Scheduler overhead time: 0.02657711401116103 Adapter cache time: 0.04111370327882469 Engine time: 0.02761398209258914 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_160_slots_160_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.789570868946612,
    "estimated_duration": 3600.0796488410283,
    "input_throughput": 4355.941959519816,
    "output_throughput": 3830.618026587156,
    "total_throughput": 8186.559986106971,
    "itl": 133.00259169054758,
    "ttft": 2103266.479342461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5297134483978164,
    "arrivals": 406769,
    "finished_requests": 63418,
    "scheduler_time": 26.894755368446855
}
#Debug simulation 
Total elapsed time: 4.789658191963099. Arrivals time: 0.24412337655667216 Scheduler time: 4.365349772037007 Scheduler overhead time: 0.040059159393422306 Adapter cache time: 0.0793767359573394 Engine time: 0.041767384856939316 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_160_slots_160_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_160_slots_160_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.854275924968533,
    "estimated_duration": 3600.19648915651,
    "input_throughput": 4585.097243919452,
    "output_throughput": 4020.3466792978074,
    "total_throughput": 8605.443923217259,
    "itl": 212.18872219156916,
    "ttft": 2064539.513033254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.4784080471843487,
    "arrivals": 406769,
    "finished_requests": 66733,
    "scheduler_time": 40.99225601052778
}
#Debug simulation 
Total elapsed time: 4.854360357043333. Arrivals time: 0.25301658164244145 Scheduler time: 4.492990650236607 Scheduler overhead time: 0.02645108464639634 Adapter cache time: 0.04171719099394977 Engine time: 0.02774696215055883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_160_slots_160_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_160_slots_160_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.803879818995483,
    "estimated_duration": 3600.1301346000737,
    "input_throughput": 4355.8203214068,
    "output_throughput": 3830.5020886507255,
    "total_throughput": 8186.322410057526,
    "itl": 133.003843524134,
    "ttft": 2103218.5330117834,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.536504152864218,
    "arrivals": 406769,
    "finished_requests": 63417,
    "scheduler_time": 26.894485051149008
}
#Debug simulation 
Total elapsed time: 4.803964731981978. Arrivals time: 0.22839965810999274 Scheduler time: 4.396909864852205 Scheduler overhead time: 0.03995866794139147 Adapter cache time: 0.07800868130289018 Engine time: 0.041649283142760396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_160_slots_160_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_160_slots_160_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.360757249989547,
    "estimated_duration": 3600.102923861776,
    "input_throughput": 4789.143911892774,
    "output_throughput": 4227.993288500885,
    "total_throughput": 9017.137200393658,
    "itl": 202.93505337245628,
    "ttft": 2031657.0405870555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.48967803902924,
    "arrivals": 397242,
    "finished_requests": 69807,
    "scheduler_time": 43.177549030166105
}
#Debug simulation 
Total elapsed time: 5.360821646056138. Arrivals time: 0.527923752204515 Scheduler time: 4.728071117424406 Scheduler overhead time: 0.02758778480347246 Adapter cache time: 0.03553358418866992 Engine time: 0.028690909151919186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_160_slots_160_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_160_slots_160_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.121459408896044,
    "estimated_duration": 3600.2034640687602,
    "input_throughput": 4788.3610390500835,
    "output_throughput": 4226.78666688527,
    "total_throughput": 9015.147705935355,
    "itl": 202.9744119216013,
    "ttft": 2031746.400805825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5221277462574655,
    "arrivals": 397242,
    "finished_requests": 69794,
    "scheduler_time": 43.16992690402176
}
#Debug simulation 
Total elapsed time: 5.121551291900687. Arrivals time: 0.26918037433642894 Scheduler time: 4.746063649887219 Scheduler overhead time: 0.027494411449879408 Adapter cache time: 0.036967638647183776 Engine time: 0.02874571119900793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_160_slots_160_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_160_slots_160_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.938979831989855,
    "estimated_duration": 3600.115111715523,
    "input_throughput": 4494.157130517466,
    "output_throughput": 3980.737991783533,
    "total_throughput": 8474.895122300997,
    "itl": 127.75537483281025,
    "ttft": 2078434.4334785661,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5230484977178294,
    "arrivals": 397242,
    "finished_requests": 65479,
    "scheduler_time": 27.903340836655648
}
#Debug simulation 
Total elapsed time: 4.939099226961844. Arrivals time: 0.24680432502646 Scheduler time: 4.522907213773578 Scheduler overhead time: 0.04164488660171628 Adapter cache time: 0.06467385240830481 Engine time: 0.043330375920049846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_160_slots_160_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_160_slots_160_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.0771224070340395,
    "estimated_duration": 3600.1141044598667,
    "input_throughput": 4789.12903861606,
    "output_throughput": 4227.9801579465975,
    "total_throughput": 9017.109196562658,
    "itl": 202.93551591324223,
    "ttft": 2031665.0188159293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 160,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5004721943801271,
    "arrivals": 397242,
    "finished_requests": 69807,
    "scheduler_time": 43.17749496083168
}
#Debug simulation 
Total elapsed time: 5.077215738943778. Arrivals time: 0.2609772678697482 Scheduler time: 4.711112079210579 Scheduler overhead time: 0.027520916191861033 Adapter cache time: 0.03558787330985069 Engine time: 0.02902334078680724 
