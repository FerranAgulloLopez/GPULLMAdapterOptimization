INFO 06-01 00:47:06 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:07 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216811905 . Total output tokens: 194671193
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.257688749115914,
    "estimated_duration": 3600.1930091596187,
    "input_throughput": 5388.731923716665,
    "output_throughput": 4767.685498063525,
    "total_throughput": 10156.41742178019,
    "itl": 180.07146520194314,
    "ttft": 1897203.0003787337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 620,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9241095776110726,
    "arrivals": 324936,
    "finished_requests": 78496,
    "scheduler_time": 77.97549963950742
}
#Debug simulation 
Total elapsed time: 6.25782840885222. Arrivals time: 0.24659702833741903 Scheduler time: 5.917698775418103 Scheduler overhead time: 0.03183693066239357 Adapter cache time: 0.014657642226666212 Engine time: 0.03216658579185605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216811905 . Total output tokens: 194671193
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.21485355310142,
    "estimated_duration": 3600.0942276069427,
    "input_throughput": 5377.055370261129,
    "output_throughput": 4756.643553572463,
    "total_throughput": 10133.698923833592,
    "itl": 177.84749246397482,
    "ttft": 1899179.1469879237,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 658,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.185658029597256,
    "arrivals": 324936,
    "finished_requests": 78314,
    "scheduler_time": 77.87402979149738
}
#Debug simulation 
Total elapsed time: 6.214996390044689. Arrivals time: 0.2537729609757662 Scheduler time: 5.866204572375864 Scheduler overhead time: 0.032397928182035685 Adapter cache time: 0.014789546374231577 Engine time: 0.03268937673419714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216811905 . Total output tokens: 194671193
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.574933952186257,
    "estimated_duration": 3600.1621583251986,
    "input_throughput": 5388.883374360367,
    "output_throughput": 4767.969953882689,
    "total_throughput": 10156.853328243056,
    "itl": 180.06818258014147,
    "ttft": 1897169.8163809818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 619,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8508411325444178,
    "arrivals": 324936,
    "finished_requests": 78498,
    "scheduler_time": 77.97590532195014
}
#Debug simulation 
Total elapsed time: 6.574999216012657. Arrivals time: 0.3100062571465969 Scheduler time: 6.17133657913655 Scheduler overhead time: 0.031942302361130714 Adapter cache time: 0.014583799988031387 Engine time: 0.032317386008799076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 4320, 4320, 4320, 1080, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 1080, 17280, 17280, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 1080, 4320, 17280, 17280, 4320, 1080, 1080, 1080, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 4320, 1080, 17280, 17280, 1080, 1080, 4320, 4320, 4320, 1080, 17280, 1080, 17280, 4320, 1080, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 4320, 17280, 17280, 1080, 1080, 17280, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 974160 . Total input tokens: 216811905 . Total output tokens: 194671193
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.222482901997864,
    "estimated_duration": 3600.098845493697,
    "input_throughput": 5375.49796006952,
    "output_throughput": 4755.539982306293,
    "total_throughput": 10131.037942375811,
    "itl": 177.80522695930935,
    "ttft": 1899194.2894803365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 655,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1953734044358146,
    "arrivals": 324936,
    "finished_requests": 78300,
    "scheduler_time": 77.87223490542081
}
#Debug simulation 
Total elapsed time: 6.222576225642115. Arrivals time: 0.2793617700226605 Scheduler time: 5.8479094142094254 Scheduler overhead time: 0.03254645084962249 Adapter cache time: 0.014931728597730398 Engine time: 0.03279327321797609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211698148 . Total output tokens: 190168025
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.713875578250736,
    "estimated_duration": 3600.1410884894344,
    "input_throughput": 5401.390812758162,
    "output_throughput": 4767.069561487928,
    "total_throughput": 10168.46037424609,
    "itl": 180.3278349951041,
    "ttft": 1893534.2977690487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4728740970977148,
    "arrivals": 317347,
    "finished_requests": 78374,
    "scheduler_time": 77.93993249070257
}
#Debug simulation 
Total elapsed time: 5.713975380174816. Arrivals time: 0.2468233536928892 Scheduler time: 5.371686258353293 Scheduler overhead time: 0.0317314094863832 Adapter cache time: 0.01655850000679493 Engine time: 0.03225445933640003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211698148 . Total output tokens: 190168025
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.741869972087443,
    "estimated_duration": 3600.1860416797304,
    "input_throughput": 5401.194486862531,
    "output_throughput": 4766.867267779569,
    "total_throughput": 10168.061754642102,
    "itl": 180.33537597351724,
    "ttft": 1893554.6710316888,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.64937950205524,
    "arrivals": 317347,
    "finished_requests": 78372,
    "scheduler_time": 77.93733622649282
}
#Debug simulation 
Total elapsed time: 5.741976521909237. Arrivals time: 0.25161108979955316 Scheduler time: 5.393896936438978 Scheduler overhead time: 0.03229895140975714 Adapter cache time: 0.01675558602437377 Engine time: 0.032326805870980024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211698148 . Total output tokens: 190168025
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.697824071161449,
    "estimated_duration": 3600.018856468651,
    "input_throughput": 5388.748440898321,
    "output_throughput": 4755.657312526937,
    "total_throughput": 10144.405753425257,
    "itl": 178.34906469809795,
    "ttft": 1896081.760595035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 847,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.768742546811682,
    "arrivals": 317347,
    "finished_requests": 78189,
    "scheduler_time": 77.845231591793
}
#Debug simulation 
Total elapsed time: 5.697946892119944. Arrivals time: 0.2478151610121131 Scheduler time: 5.353767158463597 Scheduler overhead time: 0.03196400264278054 Adapter cache time: 0.017057667952030897 Engine time: 0.032314067240804434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211698148 . Total output tokens: 190168025
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.710733102168888,
    "estimated_duration": 3600.025792834227,
    "input_throughput": 5401.067691987881,
    "output_throughput": 4767.030012440315,
    "total_throughput": 10168.097704428195,
    "itl": 180.3311340721305,
    "ttft": 1893562.4273703229,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.499722678561199,
    "arrivals": 317347,
    "finished_requests": 78369,
    "scheduler_time": 77.93693497207633
}
#Debug simulation 
Total elapsed time: 5.710831349249929. Arrivals time: 0.24872304080054164 Scheduler time: 5.367073178756982 Scheduler overhead time: 0.031589061953127384 Adapter cache time: 0.016621616668999195 Engine time: 0.03196809580549598 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211698148 . Total output tokens: 190168025
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.948890075087547,
    "estimated_duration": 3600.006754076778,
    "input_throughput": 5388.766556626927,
    "output_throughput": 4755.673299949278,
    "total_throughput": 10144.439856576204,
    "itl": 178.35798666847603,
    "ttft": 1896068.0857087693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 846,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.809124098140754,
    "arrivals": 317347,
    "finished_requests": 78189,
    "scheduler_time": 77.84447075568958
}
#Debug simulation 
Total elapsed time: 5.9489918779581785. Arrivals time: 0.5064523695036769 Scheduler time: 5.346055808477104 Scheduler overhead time: 0.031917986925691366 Adapter cache time: 0.017192354425787926 Engine time: 0.03238763101398945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211698148 . Total output tokens: 190168025
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.7112380266189575,
    "estimated_duration": 3600.0743572516953,
    "input_throughput": 5401.096497030095,
    "output_throughput": 4767.038760027522,
    "total_throughput": 10168.135257057616,
    "itl": 180.32723314969004,
    "ttft": 1893539.2416018627,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 808,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.415960638280909,
    "arrivals": 317347,
    "finished_requests": 78370,
    "scheduler_time": 77.939598278928
}
#Debug simulation 
Total elapsed time: 5.71136058261618. Arrivals time: 0.25988470623269677 Scheduler time: 5.35595035366714 Scheduler overhead time: 0.031659544445574284 Adapter cache time: 0.01691274344921112 Engine time: 0.0320547572337091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 540, 540, 17280, 540, 4320, 4320, 4320, 540, 17280, 4320, 540, 17280, 4320, 540, 540, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 4320, 540, 17280, 17280, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 540, 4320, 17280, 17280, 4320, 540, 540, 540, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 540, 540, 4320, 540, 17280, 17280, 540, 540, 4320, 4320, 4320, 540, 17280, 540, 17280, 4320, 540, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 4320, 17280, 17280, 4320, 17280, 540, 4320, 17280, 540, 17280, 4320, 540, 4320, 4320, 17280, 17280, 540, 540, 17280, 540, 540, 540, 4320, 540]
Prompts retrieved: 951480 . Total input tokens: 211698148 . Total output tokens: 190168025
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.703409007750452,
    "estimated_duration": 3600.0073417314934,
    "input_throughput": 5388.828176852906,
    "output_throughput": 4755.805023376801,
    "total_throughput": 10144.633200229708,
    "itl": 178.37532574770844,
    "ttft": 1896044.0846279655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 852,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8574517905712367,
    "arrivals": 317347,
    "finished_requests": 78189,
    "scheduler_time": 77.84418365102292
}
#Debug simulation 
Total elapsed time: 5.703504441771656. Arrivals time: 0.25888171419501305 Scheduler time: 5.348034393507987 Scheduler overhead time: 0.03182274382561445 Adapter cache time: 0.017316109500825405 Engine time: 0.03244765009731054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209202086 . Total output tokens: 187871543
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.684136930387467,
    "estimated_duration": 3600.01182652383,
    "input_throughput": 5408.062233728644,
    "output_throughput": 4795.473412838724,
    "total_throughput": 10203.535646567367,
    "itl": 179.44539416176968,
    "ttft": 1879852.4095520298,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 857,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.622837996550425,
    "arrivals": 313505,
    "finished_requests": 79105,
    "scheduler_time": 78.37053880557944
}
#Debug simulation 
Total elapsed time: 5.6842502392828465. Arrivals time: 0.25334419729188085 Scheduler time: 5.334660472813994 Scheduler overhead time: 0.031040172558277845 Adapter cache time: 0.018473395612090826 Engine time: 0.03187389858067036 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209202086 . Total output tokens: 187871543
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.462714578025043,
    "estimated_duration": 3600.145286201746,
    "input_throughput": 5408.2411825501285,
    "output_throughput": 4795.464801425943,
    "total_throughput": 10203.705983976071,
    "itl": 179.4473533742145,
    "ttft": 1879904.8349743714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 859,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8029869161057346,
    "arrivals": 313505,
    "finished_requests": 79108,
    "scheduler_time": 78.37207881392872
}
#Debug simulation 
Total elapsed time: 5.462839982006699. Arrivals time: 0.24613815313205123 Scheduler time: 5.121633009985089 Scheduler overhead time: 0.03080553514882922 Adapter cache time: 0.018252891022711992 Engine time: 0.03149036457762122 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209202086 . Total output tokens: 187871543
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.460262423846871,
    "estimated_duration": 3600.029909005167,
    "input_throughput": 5398.766535628831,
    "output_throughput": 4788.278829817705,
    "total_throughput": 10187.045365446535,
    "itl": 177.80514301231707,
    "ttft": 1881232.7976254232,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 860,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.811680766791086,
    "arrivals": 313505,
    "finished_requests": 78974,
    "scheduler_time": 78.32655823831938
}
#Debug simulation 
Total elapsed time: 5.460363497957587. Arrivals time: 0.2431468404829502 Scheduler time: 5.121109493076801 Scheduler overhead time: 0.031109038274735212 Adapter cache time: 0.01838313741609454 Engine time: 0.03183255763724446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209202086 . Total output tokens: 187871543
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.477011190727353,
    "estimated_duration": 3600.174708721207,
    "input_throughput": 5408.703903404902,
    "output_throughput": 4795.827813071014,
    "total_throughput": 10204.531716475916,
    "itl": 179.4409508863075,
    "ttft": 1879856.1018237704,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 858,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.658214290311993,
    "arrivals": 313505,
    "finished_requests": 79113,
    "scheduler_time": 78.3756224322102
}
#Debug simulation 
Total elapsed time: 5.477116970811039. Arrivals time: 0.24807199276983738 Scheduler time: 5.133718469180167 Scheduler overhead time: 0.030752908904105425 Adapter cache time: 0.01839814940467477 Engine time: 0.03156346036121249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209202086 . Total output tokens: 187871543
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.45065014436841,
    "estimated_duration": 3600.0881426334317,
    "input_throughput": 5398.679207277116,
    "output_throughput": 4788.201376478132,
    "total_throughput": 10186.880583755248,
    "itl": 177.80691343994505,
    "ttft": 1881257.9848919997,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 860,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8558203458227314,
    "arrivals": 313505,
    "finished_requests": 78974,
    "scheduler_time": 78.32669364034794
}
#Debug simulation 
Total elapsed time: 5.4507676642388105. Arrivals time: 0.24285448528826237 Scheduler time: 5.112069230061024 Scheduler overhead time: 0.030943005345761776 Adapter cache time: 0.018097654450684786 Engine time: 0.03209459176287055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209202086 . Total output tokens: 187871543
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.481797823216766,
    "estimated_duration": 3600.1133851266923,
    "input_throughput": 5408.48854384479,
    "output_throughput": 4795.762286634873,
    "total_throughput": 10204.250830479663,
    "itl": 179.44164023252688,
    "ttft": 1879825.0354530076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 856,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5594830524362084,
    "arrivals": 313505,
    "finished_requests": 79111,
    "scheduler_time": 78.37416196819761
}
#Debug simulation 
Total elapsed time: 5.481892311945558. Arrivals time: 0.25591609301045537 Scheduler time: 5.130614103283733 Scheduler overhead time: 0.03084063669666648 Adapter cache time: 0.018079995643347502 Engine time: 0.03175595076754689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 270, 270, 17280, 270, 4320, 4320, 4320, 270, 17280, 4320, 270, 17280, 4320, 270, 270, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 4320, 270, 17280, 17280, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 270, 4320, 17280, 17280, 4320, 270, 270, 270, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 270, 270, 4320, 270, 17280, 17280, 270, 270, 4320, 4320, 4320, 270, 17280, 270, 17280, 4320, 270, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 4320, 17280, 17280, 4320, 17280, 270, 4320, 17280, 270, 17280, 4320, 270, 4320, 4320, 17280, 17280, 270, 270, 17280, 270, 270, 270, 4320, 270]
Prompts retrieved: 940140 . Total input tokens: 209202086 . Total output tokens: 187871543
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.482679807115346,
    "estimated_duration": 3600.113653645258,
    "input_throughput": 5398.640951326789,
    "output_throughput": 4788.167446476556,
    "total_throughput": 10186.808397803345,
    "itl": 177.80583101045204,
    "ttft": 1881267.646242062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 860,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.8841149477660912,
    "arrivals": 313505,
    "finished_requests": 78974,
    "scheduler_time": 78.32681215736652
}
#Debug simulation 
Total elapsed time: 5.482775546144694. Arrivals time: 0.243531737010926 Scheduler time: 5.142722163815051 Scheduler overhead time: 0.031093649566173553 Adapter cache time: 0.018426199909299612 Engine time: 0.032203310169279575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207913638 . Total output tokens: 186743648
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.567491257097572,
    "estimated_duration": 3600.156662142888,
    "input_throughput": 5551.114264039995,
    "output_throughput": 4897.646867818495,
    "total_throughput": 10448.76113185849,
    "itl": 175.24639631884315,
    "ttft": 1861834.283490786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.377219484769751,
    "arrivals": 311621,
    "finished_requests": 80911,
    "scheduler_time": 80.00838312341627
}
#Debug simulation 
Total elapsed time: 5.567592306993902. Arrivals time: 0.2462430135346949 Scheduler time: 5.229342516046017 Scheduler overhead time: 0.030901416670531034 Adapter cache time: 0.014161327853798866 Engine time: 0.032137694768607616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207913638 . Total output tokens: 186743648
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.58904752926901,
    "estimated_duration": 3600.1189746353384,
    "input_throughput": 5551.172375358595,
    "output_throughput": 4897.698138374997,
    "total_throughput": 10448.870513733591,
    "itl": 175.2497450473003,
    "ttft": 1861819.0415181804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4701442048302906,
    "arrivals": 311621,
    "finished_requests": 80911,
    "scheduler_time": 80.0059508479898
}
#Debug simulation 
Total elapsed time: 5.589149446226656. Arrivals time: 0.24704074766486883 Scheduler time: 5.249873235821724 Scheduler overhead time: 0.031101529486477375 Adapter cache time: 0.014153315220028162 Engine time: 0.03218510327860713 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207913638 . Total output tokens: 186743648
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.593105557374656,
    "estimated_duration": 3600.0421903905826,
    "input_throughput": 5541.86616291723,
    "output_throughput": 4889.345476834622,
    "total_throughput": 10431.211639751853,
    "itl": 173.57738815794417,
    "ttft": 1863362.9315744007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4727812152728526,
    "arrivals": 311621,
    "finished_requests": 80759,
    "scheduler_time": 79.95697064015916
}
#Debug simulation 
Total elapsed time: 5.593201122246683. Arrivals time: 0.25007322849705815 Scheduler time: 5.2499651578255 Scheduler overhead time: 0.0313007365912199 Adapter cache time: 0.014300821349024773 Engine time: 0.03252247115597129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207913638 . Total output tokens: 186743648
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.587436428759247,
    "estimated_duration": 3600.0038279318064,
    "input_throughput": 5551.050486377023,
    "output_throughput": 4897.501181305404,
    "total_throughput": 10448.551667682428,
    "itl": 175.2475357702451,
    "ttft": 1861790.1677312767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3941454756003744,
    "arrivals": 311621,
    "finished_requests": 80908,
    "scheduler_time": 80.00451309555996
}
#Debug simulation 
Total elapsed time: 5.587532507721335. Arrivals time: 0.24668354960158467 Scheduler time: 5.2491613095626235 Scheduler overhead time: 0.030889477115124464 Adapter cache time: 0.014093971345573664 Engine time: 0.03186903987079859 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207913638 . Total output tokens: 186743648
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.545177380088717,
    "estimated_duration": 3600.066063363365,
    "input_throughput": 5541.829413363821,
    "output_throughput": 4889.313054315302,
    "total_throughput": 10431.142467679123,
    "itl": 173.57794738791736,
    "ttft": 1863372.8120288835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4962971733324264,
    "arrivals": 311621,
    "finished_requests": 80759,
    "scheduler_time": 79.95698943998877
}
#Debug simulation 
Total elapsed time: 5.545303449966013. Arrivals time: 0.27105902833864093 Scheduler time: 5.1822054157964885 Scheduler overhead time: 0.03121664747595787 Adapter cache time: 0.01407591812312603 Engine time: 0.031985870096832514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207913638 . Total output tokens: 186743648
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.584030047059059,
    "estimated_duration": 3600.0324643714284,
    "input_throughput": 5551.3057723187485,
    "output_throughput": 4897.8158320799,
    "total_throughput": 10449.121604398648,
    "itl": 175.2432039704042,
    "ttft": 1861779.6605724348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3455226327059682,
    "arrivals": 311621,
    "finished_requests": 80911,
    "scheduler_time": 80.00623377803473
}
#Debug simulation 
Total elapsed time: 5.584125416819006. Arrivals time: 0.24892716202884912 Scheduler time: 5.242742921691388 Scheduler overhead time: 0.03144481452181935 Adapter cache time: 0.014215575531125069 Engine time: 0.03207160625606775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 135, 135, 17280, 135, 4320, 4320, 4320, 135, 17280, 4320, 135, 17280, 4320, 135, 135, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 4320, 135, 17280, 17280, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 135, 4320, 17280, 17280, 4320, 135, 135, 135, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 135, 135, 4320, 135, 17280, 17280, 135, 135, 4320, 4320, 4320, 135, 17280, 135, 17280, 4320, 135, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 4320, 17280, 17280, 4320, 17280, 135, 4320, 17280, 135, 17280, 4320, 135, 4320, 4320, 17280, 17280, 135, 135, 17280, 135, 135, 135, 4320, 135]
Prompts retrieved: 934470 . Total input tokens: 207913638 . Total output tokens: 186743648
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.574997251853347,
    "estimated_duration": 3600.085762545088,
    "input_throughput": 5541.799089223817,
    "output_throughput": 4889.286300656442,
    "total_throughput": 10431.08538988026,
    "itl": 173.57777720481772,
    "ttft": 1863383.1774294113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 450,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5112618739157917,
    "arrivals": 311621,
    "finished_requests": 80759,
    "scheduler_time": 79.95708694750664
}
#Debug simulation 
Total elapsed time: 5.575092259794474. Arrivals time: 0.27651358814910054 Scheduler time: 5.20634772349149 Scheduler overhead time: 0.03112039901316166 Adapter cache time: 0.014249929692596197 Engine time: 0.03204052243381739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207261530 . Total output tokens: 186189423
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.589420969132334,
    "estimated_duration": 3600.1334460588223,
    "input_throughput": 5592.056600579428,
    "output_throughput": 4957.591230274741,
    "total_throughput": 10549.64783085417,
    "itl": 173.8758625450196,
    "ttft": 1854458.6814521132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8630575437890354,
    "arrivals": 310685,
    "finished_requests": 81666,
    "scheduler_time": 80.91439372112043
}
#Debug simulation 
Total elapsed time: 5.589545060880482. Arrivals time: 0.24872361589223146 Scheduler time: 5.250474789645523 Scheduler overhead time: 0.031241061631590128 Adapter cache time: 0.011903793085366488 Engine time: 0.032367267180234194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207261530 . Total output tokens: 186189423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.610763947945088,
    "estimated_duration": 3600.1306140808283,
    "input_throughput": 5591.909616073728,
    "output_throughput": 4957.519855028041,
    "total_throughput": 10549.429471101768,
    "itl": 173.8795014545045,
    "ttft": 1854470.7643839407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9220530796214056,
    "arrivals": 310685,
    "finished_requests": 81663,
    "scheduler_time": 80.91333666170978
}
#Debug simulation 
Total elapsed time: 5.610857624094933. Arrivals time: 0.24871543562039733 Scheduler time: 5.272008188534528 Scheduler overhead time: 0.031044496223330498 Adapter cache time: 0.011879234109073877 Engine time: 0.03227465273812413 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207261530 . Total output tokens: 186189423
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.640983372926712,
    "estimated_duration": 3600.0796997286116,
    "input_throughput": 5580.278125929941,
    "output_throughput": 4947.317972250071,
    "total_throughput": 10527.596098180013,
    "itl": 171.8689450975006,
    "ttft": 1856289.0528960826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9233610817976343,
    "arrivals": 310685,
    "finished_requests": 81495,
    "scheduler_time": 80.85037055862594
}
#Debug simulation 
Total elapsed time: 5.641077636741102. Arrivals time: 0.25106626749038696 Scheduler time: 5.298267237376422 Scheduler overhead time: 0.03150248900055885 Adapter cache time: 0.01210502814501524 Engine time: 0.03300071693956852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207261530 . Total output tokens: 186189423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.605581088922918,
    "estimated_duration": 3600.1894286691436,
    "input_throughput": 5591.970200146416,
    "output_throughput": 4957.844956118928,
    "total_throughput": 10549.815156265344,
    "itl": 173.87665556879378,
    "ttft": 1854456.9087911244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8762904039560842,
    "arrivals": 310685,
    "finished_requests": 81668,
    "scheduler_time": 80.91570808565193
}
#Debug simulation 
Total elapsed time: 5.605705919675529. Arrivals time: 0.2495732200331986 Scheduler time: 5.26603311765939 Scheduler overhead time: 0.0311382501386106 Adapter cache time: 0.01183090778067708 Engine time: 0.032289417926222086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207261530 . Total output tokens: 186189423
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.609747332986444,
    "estimated_duration": 3600.119186689474,
    "input_throughput": 5580.216920116318,
    "output_throughput": 4947.263708893494,
    "total_throughput": 10527.480629009813,
    "itl": 171.86094205979632,
    "ttft": 1856315.054125256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9374455058760989,
    "arrivals": 310685,
    "finished_requests": 81495,
    "scheduler_time": 80.85103418617403
}
#Debug simulation 
Total elapsed time: 5.609842132311314. Arrivals time: 0.24795693205669522 Scheduler time: 5.270627456251532 Scheduler overhead time: 0.031402465887367725 Adapter cache time: 0.012150451075285673 Engine time: 0.03264310257509351 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207261530 . Total output tokens: 186189423
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.617245611734688,
    "estimated_duration": 3600.106111728395,
    "input_throughput": 5592.099059084301,
    "output_throughput": 4957.628871508807,
    "total_throughput": 10549.727930593108,
    "itl": 173.8755084786829,
    "ttft": 1854448.1474969578,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8431941831624147,
    "arrivals": 310685,
    "finished_requests": 81666,
    "scheduler_time": 80.91421818872381
}
#Debug simulation 
Total elapsed time: 5.617339630611241. Arrivals time: 0.2515262537635863 Scheduler time: 5.276070660445839 Scheduler overhead time: 0.03096508653834462 Adapter cache time: 0.011874943505972624 Engine time: 0.03205349529162049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 66, 66, 17280, 66, 4320, 4320, 4320, 66, 17280, 4320, 66, 17280, 4320, 66, 66, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 4320, 66, 17280, 17280, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 66, 4320, 17280, 17280, 4320, 66, 66, 66, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 66, 66, 4320, 66, 17280, 17280, 66, 66, 4320, 4320, 4320, 66, 17280, 66, 17280, 4320, 66, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 4320, 17280, 17280, 4320, 17280, 66, 4320, 17280, 66, 17280, 4320, 66, 4320, 4320, 17280, 17280, 66, 66, 17280, 66, 66, 66, 4320, 66]
Prompts retrieved: 931572 . Total input tokens: 207261530 . Total output tokens: 186189423
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.6039427812211215,
    "estimated_duration": 3600.12720959017,
    "input_throughput": 5580.204484576237,
    "output_throughput": 4947.252683892671,
    "total_throughput": 10527.457168468907,
    "itl": 171.86042322581255,
    "ttft": 1856318.902204587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9476315625757028,
    "arrivals": 310685,
    "finished_requests": 81495,
    "scheduler_time": 80.85102012938736
}
#Debug simulation 
Total elapsed time: 5.604062598198652. Arrivals time: 0.24817753862589598 Scheduler time: 5.264584943186492 Scheduler overhead time: 0.03158741584047675 Adapter cache time: 0.012103344313800335 Engine time: 0.032605175860226154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206946348 . Total output tokens: 185901105
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.678693812806159,
    "estimated_duration": 3600.1116664567844,
    "input_throughput": 5633.003606235066,
    "output_throughput": 4986.061451162344,
    "total_throughput": 10619.06505739741,
    "itl": 172.70959981278773,
    "ttft": 1850761.8458511124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5723112081154242,
    "arrivals": 310219,
    "finished_requests": 82100,
    "scheduler_time": 81.43110681447887
}
#Debug simulation 
Total elapsed time: 5.6787891667336226. Arrivals time: 0.2749794493429363 Scheduler time: 5.314988418482244 Scheduler overhead time: 0.03137578954920173 Adapter cache time: 0.01016219612210989 Engine time: 0.032327751629054546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206946348 . Total output tokens: 185901105
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.928085091989487,
    "estimated_duration": 3600.0081704812665,
    "input_throughput": 5632.794438155157,
    "output_throughput": 4986.10868363678,
    "total_throughput": 10618.903121791936,
    "itl": 172.7101039715134,
    "ttft": 1850729.349856822,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6110310105886323,
    "arrivals": 310219,
    "finished_requests": 82097,
    "scheduler_time": 81.42866703858546
}
#Debug simulation 
Total elapsed time: 5.928152330685407. Arrivals time: 0.2850858476012945 Scheduler time: 5.554068422410637 Scheduler overhead time: 0.0312540945596993 Adapter cache time: 0.010317816399037838 Engine time: 0.032362251076847315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206946348 . Total output tokens: 185901105
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.665676934178919,
    "estimated_duration": 3600.054111233826,
    "input_throughput": 5623.355198142219,
    "output_throughput": 4977.128244847156,
    "total_throughput": 10600.483442989374,
    "itl": 171.12179373208943,
    "ttft": 1852388.6245414682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6052069238387072,
    "arrivals": 310219,
    "finished_requests": 81949,
    "scheduler_time": 81.3737705228943
}
#Debug simulation 
Total elapsed time: 5.665812022052705. Arrivals time: 0.25147978495806456 Scheduler time: 5.324041414540261 Scheduler overhead time: 0.03164862375706434 Adapter cache time: 0.010630990844219923 Engine time: 0.03296795627102256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206946348 . Total output tokens: 185901105
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.695999383926392,
    "estimated_duration": 3600.1672097465394,
    "input_throughput": 5633.431676477008,
    "output_throughput": 4986.395340583043,
    "total_throughput": 10619.827017060052,
    "itl": 172.71096600028687,
    "ttft": 1850715.1002253394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5840637195715688,
    "arrivals": 310219,
    "finished_requests": 82106,
    "scheduler_time": 81.43280953962075
}
#Debug simulation 
Total elapsed time: 5.696098008658737. Arrivals time: 0.280881620477885 Scheduler time: 5.3257788233459 Scheduler overhead time: 0.03150113811716437 Adapter cache time: 0.010412820614874363 Engine time: 0.0325579852797091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206946348 . Total output tokens: 185901105
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.705207830294967,
    "estimated_duration": 3600.116793665143,
    "input_throughput": 5623.25728865867,
    "output_throughput": 4977.041586964302,
    "total_throughput": 10600.298875622973,
    "itl": 171.12444508524882,
    "ttft": 1852417.6626702792,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6132551661692577,
    "arrivals": 310219,
    "finished_requests": 81949,
    "scheduler_time": 81.375075244424
}
#Debug simulation 
Total elapsed time: 5.705304352100939. Arrivals time: 0.2890338241122663 Scheduler time: 5.326101959683001 Scheduler overhead time: 0.03166458196938038 Adapter cache time: 0.010585898533463478 Engine time: 0.03286552941426635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206946348 . Total output tokens: 185901105
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.648878009989858,
    "estimated_duration": 3600.0109393911894,
    "input_throughput": 5632.79010575154,
    "output_throughput": 4986.104848624597,
    "total_throughput": 10618.894954376137,
    "itl": 172.70849512193723,
    "ttft": 1850739.0474495518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5591394051467076,
    "arrivals": 310219,
    "finished_requests": 82097,
    "scheduler_time": 81.42920117089757
}
#Debug simulation 
Total elapsed time: 5.649001305922866. Arrivals time: 0.25043465103954077 Scheduler time: 5.309840757399797 Scheduler overhead time: 0.031195770483464003 Adapter cache time: 0.010266509372740984 Engine time: 0.03235074784606695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 4320, 17280, 17280, 33, 33, 17280, 33, 4320, 4320, 4320, 33, 17280, 4320, 33, 17280, 4320, 33, 33, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 4320, 33, 17280, 17280, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 17280, 17280, 17280, 17280, 4320, 4320, 33, 4320, 17280, 17280, 4320, 33, 33, 33, 17280, 4320, 4320, 4320, 4320, 17280, 4320, 17280, 33, 33, 4320, 33, 17280, 17280, 33, 33, 4320, 4320, 4320, 33, 17280, 33, 17280, 4320, 33, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 4320, 17280, 17280, 4320, 17280, 33, 4320, 17280, 33, 17280, 4320, 33, 4320, 4320, 17280, 17280, 33, 33, 17280, 33, 33, 33, 4320, 33]
Prompts retrieved: 930186 . Total input tokens: 206946348 . Total output tokens: 185901105
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.67780599091202,
    "estimated_duration": 3600.124611164526,
    "input_throughput": 5623.245078022893,
    "output_throughput": 4977.030779555188,
    "total_throughput": 10600.27585757808,
    "itl": 171.12414873333898,
    "ttft": 1852422.2954587045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6209261471405637,
    "arrivals": 310219,
    "finished_requests": 81949,
    "scheduler_time": 81.37509743443957
}
#Debug simulation 
Total elapsed time: 5.677900127135217. Arrivals time: 0.2741925888694823 Scheduler time: 5.313748872373253 Scheduler overhead time: 0.031669643707573414 Adapter cache time: 0.010502108372747898 Engine time: 0.03270241152495146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180789837 . Total output tokens: 162214036
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.558056976646185,
    "estimated_duration": 3600.0538258124266,
    "input_throughput": 5480.83471933846,
    "output_throughput": 4824.791472688665,
    "total_throughput": 10305.626192027124,
    "itl": 177.76075184364504,
    "ttft": 1822843.692118305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.9503676664714895,
    "arrivals": 270597,
    "finished_requests": 79611,
    "scheduler_time": 78.67790506211375
}
#Debug simulation 
Total elapsed time: 5.558151465840638. Arrivals time: 0.23895767098292708 Scheduler time: 5.203922925516963 Scheduler overhead time: 0.031125687528401613 Adapter cache time: 0.0372515213675797 Engine time: 0.03203139500692487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180789837 . Total output tokens: 162214036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.709892166312784,
    "estimated_duration": 3600.0148151644735,
    "input_throughput": 5480.052447811573,
    "output_throughput": 4824.407090448741,
    "total_throughput": 10304.459538260315,
    "itl": 177.7864700126029,
    "ttft": 1822956.1206129375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.404244271085763,
    "arrivals": 270597,
    "finished_requests": 79599,
    "scheduler_time": 78.66797224159528
}
#Debug simulation 
Total elapsed time: 5.709989855065942. Arrivals time: 0.26392837241292 Scheduler time: 5.330609408207238 Scheduler overhead time: 0.031430228147655725 Adapter cache time: 0.03723192913457751 Engine time: 0.031961125787347555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180789837 . Total output tokens: 162214036
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.5438422546721995,
    "estimated_duration": 3600.1564089704193,
    "input_throughput": 5474.953241166793,
    "output_throughput": 4819.782539659813,
    "total_throughput": 10294.735780826606,
    "itl": 175.6495939687586,
    "ttft": 1824178.3610798495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.455911011006636,
    "arrivals": 270597,
    "finished_requests": 79521,
    "scheduler_time": 78.6941398661121
}
#Debug simulation 
Total elapsed time: 5.543935104738921. Arrivals time: 0.2640588544309139 Scheduler time: 5.16375058144331 Scheduler overhead time: 0.03150258585810661 Adapter cache time: 0.037001773715019226 Engine time: 0.03258855966851115 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180789837 . Total output tokens: 162214036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.563724671024829,
    "estimated_duration": 3600.1516902032154,
    "input_throughput": 5480.685731574338,
    "output_throughput": 4824.660318415515,
    "total_throughput": 10305.346049989854,
    "itl": 177.7636652698324,
    "ttft": 1822881.5663458477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.046593484384704,
    "arrivals": 270597,
    "finished_requests": 79611,
    "scheduler_time": 78.678883334525
}
#Debug simulation 
Total elapsed time: 5.563834530767053. Arrivals time: 0.2610749904997647 Scheduler time: 5.186868246644735 Scheduler overhead time: 0.031576951034367085 Adapter cache time: 0.03728166176006198 Engine time: 0.032130174338817596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180789837 . Total output tokens: 162214036
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.532217971049249,
    "estimated_duration": 3600.13151215507,
    "input_throughput": 5474.339182737915,
    "output_throughput": 4819.4992159088215,
    "total_throughput": 10293.838398646736,
    "itl": 175.64970184980095,
    "ttft": 1824229.949626959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.585807184763057,
    "arrivals": 270597,
    "finished_requests": 79516,
    "scheduler_time": 78.69063178065045
}
#Debug simulation 
Total elapsed time: 5.532346602994949. Arrivals time: 0.26322568114846945 Scheduler time: 5.153597122523934 Scheduler overhead time: 0.03137776721268892 Adapter cache time: 0.036893697921186686 Engine time: 0.03219417203217745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180789837 . Total output tokens: 162214036
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.7880091168917716,
    "estimated_duration": 3600.089313114416,
    "input_throughput": 5481.079296593794,
    "output_throughput": 4824.8727987732445,
    "total_throughput": 10305.952095367038,
    "itl": 177.7534332899076,
    "ttft": 1822813.337150737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.793394270017538,
    "arrivals": 270597,
    "finished_requests": 79615,
    "scheduler_time": 78.68209617109093
}
#Debug simulation 
Total elapsed time: 5.788073563948274. Arrivals time: 0.2518664342351258 Scheduler time: 5.421800930984318 Scheduler overhead time: 0.031059428583830595 Adapter cache time: 0.036577708553522825 Engine time: 0.03198634414002299 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 540, 540, 17280, 540, 1080, 1080, 1080, 540, 17280, 1080, 540, 17280, 1080, 540, 540, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 1080, 540, 17280, 17280, 540, 1080, 1080, 540, 1080, 540, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 540, 1080, 17280, 17280, 1080, 540, 540, 540, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 540, 540, 1080, 540, 17280, 17280, 540, 540, 1080, 1080, 1080, 540, 17280, 540, 17280, 1080, 540, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 540, 17280, 540, 540, 17280, 1080, 17280, 17280, 1080, 17280, 540, 1080, 17280, 540, 17280, 1080, 540, 1080, 1080, 17280, 17280, 540, 540, 17280, 540, 540, 540, 1080, 540]
Prompts retrieved: 812160 . Total input tokens: 180789837 . Total output tokens: 162214036
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.574246183037758,
    "estimated_duration": 3600.003971372282,
    "input_throughput": 5474.061183462541,
    "output_throughput": 4819.5288499601975,
    "total_throughput": 10293.590033422737,
    "itl": 175.66362348342636,
    "ttft": 1824227.0101355966,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.66528357777714,
    "arrivals": 270597,
    "finished_requests": 79510,
    "scheduler_time": 78.686181076196
}
#Debug simulation 
Total elapsed time: 5.574361179023981. Arrivals time: 0.2689518928527832 Scheduler time: 5.189094341360033 Scheduler overhead time: 0.03147193416953087 Adapter cache time: 0.037350615952163935 Engine time: 0.032531576696783304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178238268 . Total output tokens: 159946151
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.71886428585276,
    "estimated_duration": 3600.095399989989,
    "input_throughput": 5654.580709182487,
    "output_throughput": 5016.263180150787,
    "total_throughput": 10670.843889333273,
    "itl": 171.88255213292638,
    "ttft": 1788794.6583864791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.40404186351935,
    "arrivals": 266811,
    "finished_requests": 82394,
    "scheduler_time": 81.69896073079138
}
#Debug simulation 
Total elapsed time: 5.718988684006035. Arrivals time: 0.24750196607783437 Scheduler time: 5.360619344748557 Scheduler overhead time: 0.03175510838627815 Adapter cache time: 0.03125138208270073 Engine time: 0.032775257248431444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178238268 . Total output tokens: 159946151
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.749627321027219,
    "estimated_duration": 3600.050144571963,
    "input_throughput": 5654.134576622732,
    "output_throughput": 5016.228739821382,
    "total_throughput": 10670.363316444114,
    "itl": 171.8972006635625,
    "ttft": 1788823.981444067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1435,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.671124414652527,
    "arrivals": 266811,
    "finished_requests": 82389,
    "scheduler_time": 81.6921707237234
}
#Debug simulation 
Total elapsed time: 5.749720172956586. Arrivals time: 0.25223255157470703 Scheduler time: 5.38557109516114 Scheduler overhead time: 0.03195309918373823 Adapter cache time: 0.03169323969632387 Engine time: 0.03306510904803872 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178238268 . Total output tokens: 159946151
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.760065061040223,
    "estimated_duration": 3600.1076109045616,
    "input_throughput": 5650.610536858001,
    "output_throughput": 5013.127647999754,
    "total_throughput": 10663.738184857755,
    "itl": 170.00589532422413,
    "ttft": 1789658.9358548697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.7017115634492965,
    "arrivals": 266811,
    "finished_requests": 82331,
    "scheduler_time": 81.7315041477782
}
#Debug simulation 
Total elapsed time: 5.76017375709489. Arrivals time: 0.2526026973500848 Scheduler time: 5.394217818975449 Scheduler overhead time: 0.03216154593974352 Adapter cache time: 0.03251422056928277 Engine time: 0.03333541518077254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178238268 . Total output tokens: 159946151
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.739246859680861,
    "estimated_duration": 3600.1837431638005,
    "input_throughput": 5654.441954151616,
    "output_throughput": 5016.140088486132,
    "total_throughput": 10670.582042637749,
    "itl": 171.88538459502772,
    "ttft": 1788840.2572105986,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.46366892982964,
    "arrivals": 266811,
    "finished_requests": 82394,
    "scheduler_time": 81.699287975428
}
#Debug simulation 
Total elapsed time: 5.739363214932382. Arrivals time: 0.2473472417332232 Scheduler time: 5.381063192617148 Scheduler overhead time: 0.031583935022354126 Adapter cache time: 0.03157012164592743 Engine time: 0.03262292267754674 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178238268 . Total output tokens: 159946151
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.723014151211828,
    "estimated_duration": 3600.1768435834865,
    "input_throughput": 5650.50187361116,
    "output_throughput": 5013.0312437752,
    "total_throughput": 10663.53311738636,
    "itl": 170.00898184857604,
    "ttft": 1789686.8418467154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1441,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.769870115686176,
    "arrivals": 266811,
    "finished_requests": 82331,
    "scheduler_time": 81.73157543589784
}
#Debug simulation 
Total elapsed time: 5.723112490028143. Arrivals time: 0.24786871206015348 Scheduler time: 5.3621111530810595 Scheduler overhead time: 0.03215646045282483 Adapter cache time: 0.032397360540926456 Engine time: 0.03331309510394931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178238268 . Total output tokens: 159946151
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.737203430850059,
    "estimated_duration": 3600.1882064261504,
    "input_throughput": 5654.777148500612,
    "output_throughput": 5016.487740214052,
    "total_throughput": 10671.264888714664,
    "itl": 171.8788895143536,
    "ttft": 1788781.671521591,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.302682374364115,
    "arrivals": 266811,
    "finished_requests": 82398,
    "scheduler_time": 81.70326501882177
}
#Debug simulation 
Total elapsed time: 5.73729870095849. Arrivals time: 0.24899668525904417 Scheduler time: 5.376115515828133 Scheduler overhead time: 0.03178260521963239 Adapter cache time: 0.0320159662514925 Engine time: 0.033234247006475925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 270, 270, 17280, 270, 1080, 1080, 1080, 270, 17280, 1080, 270, 17280, 1080, 270, 270, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 1080, 270, 17280, 17280, 270, 1080, 1080, 270, 1080, 270, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 270, 1080, 17280, 17280, 1080, 270, 270, 270, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 270, 270, 1080, 270, 17280, 17280, 270, 270, 1080, 1080, 1080, 270, 17280, 270, 17280, 1080, 270, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 1080, 17280, 17280, 1080, 17280, 270, 1080, 17280, 270, 17280, 1080, 270, 1080, 1080, 17280, 17280, 270, 270, 17280, 270, 270, 270, 1080, 270]
Prompts retrieved: 800820 . Total input tokens: 178238268 . Total output tokens: 159946151
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.7256014589220285,
    "estimated_duration": 3600.071489974605,
    "input_throughput": 5650.44751379187,
    "output_throughput": 5013.07905975148,
    "total_throughput": 10663.52657354335,
    "itl": 170.0024612294453,
    "ttft": 1789660.9249657968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.822586380168774,
    "arrivals": 266811,
    "finished_requests": 82328,
    "scheduler_time": 81.72944095444231
}
#Debug simulation 
Total elapsed time: 5.725732712075114. Arrivals time: 0.24721640208736062 Scheduler time: 5.365423355717212 Scheduler overhead time: 0.032097130082547665 Adapter cache time: 0.03218726580962539 Engine time: 0.03347762441262603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176973240 . Total output tokens: 158826613
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.89256135514006,
    "estimated_duration": 3600.139403947849,
    "input_throughput": 5805.315754462588,
    "output_throughput": 5153.90514590995,
    "total_throughput": 10959.220900372538,
    "itl": 167.5358019540524,
    "ttft": 1762710.5515711708,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 822,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.515720925512775,
    "arrivals": 264887,
    "finished_requests": 84750,
    "scheduler_time": 83.89022425599477
}
#Debug simulation 
Total elapsed time: 5.8926550121977925. Arrivals time: 0.27890717750415206 Scheduler time: 5.503170987591147 Scheduler overhead time: 0.03260505013167858 Adapter cache time: 0.028649743180721998 Engine time: 0.03377616545185447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176973240 . Total output tokens: 158826613
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.875809909775853,
    "estimated_duration": 3600.045004658171,
    "input_throughput": 5805.10992861445,
    "output_throughput": 5153.904458414334,
    "total_throughput": 10959.014387028783,
    "itl": 167.5469156950587,
    "ttft": 1762717.7040753227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 819,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6699012588011155,
    "arrivals": 264887,
    "finished_requests": 84746,
    "scheduler_time": 83.88387583597675
}
#Debug simulation 
Total elapsed time: 5.875904774758965. Arrivals time: 0.28869295539334416 Scheduler time: 5.477147101890296 Scheduler overhead time: 0.03251088224351406 Adapter cache time: 0.028508178424090147 Engine time: 0.033543739933520555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176973240 . Total output tokens: 158826613
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.861822119913995,
    "estimated_duration": 3600.0745304272973,
    "input_throughput": 5800.307416836046,
    "output_throughput": 5148.533132673797,
    "total_throughput": 10948.840549509843,
    "itl": 165.3806071556518,
    "ttft": 1763890.7231541972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 820,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6784735980257266,
    "arrivals": 264887,
    "finished_requests": 84660,
    "scheduler_time": 83.9153376139508
}
#Debug simulation 
Total elapsed time: 5.861943756695837. Arrivals time: 0.27422717958688736 Scheduler time: 5.475515252910554 Scheduler overhead time: 0.032839763443917036 Adapter cache time: 0.029155262280255556 Engine time: 0.03445153636857867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176973240 . Total output tokens: 158826613
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.886838840786368,
    "estimated_duration": 3600.0650509727166,
    "input_throughput": 5805.435652989925,
    "output_throughput": 5154.011590703509,
    "total_throughput": 10959.447243693434,
    "itl": 167.5385970389962,
    "ttft": 1762668.8217075167,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 819,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5399679475370505,
    "arrivals": 264887,
    "finished_requests": 84750,
    "scheduler_time": 83.88743901305615
}
#Debug simulation 
Total elapsed time: 5.886934237089008. Arrivals time: 0.2751082736067474 Scheduler time: 5.501654535066336 Scheduler overhead time: 0.03259169217199087 Adapter cache time: 0.028261601459234953 Engine time: 0.03379777958616614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176973240 . Total output tokens: 158826613
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.890838707797229,
    "estimated_duration": 3600.1150106416735,
    "input_throughput": 5800.242197339729,
    "output_throughput": 5148.475241821889,
    "total_throughput": 10948.717439161617,
    "itl": 165.3822954488157,
    "ttft": 1763906.9469624753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 820,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.718211794532841,
    "arrivals": 264887,
    "finished_requests": 84660,
    "scheduler_time": 83.9153928351414
}
#Debug simulation 
Total elapsed time: 5.890939446166158. Arrivals time: 0.2612586561590433 Scheduler time: 5.517486952710897 Scheduler overhead time: 0.03306619729846716 Adapter cache time: 0.028942706994712353 Engine time: 0.03430159296840429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176973240 . Total output tokens: 158826613
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.888865045737475,
    "estimated_duration": 3600.030706189498,
    "input_throughput": 5805.491037636686,
    "output_throughput": 5154.0607606759995,
    "total_throughput": 10959.551798312687,
    "itl": 167.53282801793836,
    "ttft": 1762666.8355163864,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 819,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.448851191524832,
    "arrivals": 264887,
    "finished_requests": 84750,
    "scheduler_time": 83.88945669304904
}
#Debug simulation 
Total elapsed time: 5.888991250656545. Arrivals time: 0.25235577672719955 Scheduler time: 5.5261270347982645 Scheduler overhead time: 0.0325766927562654 Adapter cache time: 0.028408426325768232 Engine time: 0.03395582688972354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 135, 135, 17280, 135, 1080, 1080, 1080, 135, 17280, 1080, 135, 17280, 1080, 135, 135, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 1080, 135, 17280, 17280, 135, 1080, 1080, 135, 1080, 135, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 135, 1080, 17280, 17280, 1080, 135, 135, 135, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 135, 135, 1080, 135, 17280, 17280, 135, 135, 1080, 1080, 1080, 135, 17280, 135, 17280, 1080, 135, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 1080, 17280, 17280, 1080, 17280, 135, 1080, 17280, 135, 17280, 1080, 135, 1080, 1080, 17280, 17280, 135, 135, 17280, 135, 135, 135, 1080, 135]
Prompts retrieved: 795150 . Total input tokens: 176973240 . Total output tokens: 158826613
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.8872655080631375,
    "estimated_duration": 3600.1131337478537,
    "input_throughput": 5800.245221255458,
    "output_throughput": 5148.477925943471,
    "total_throughput": 10948.723147198929,
    "itl": 165.3892309634227,
    "ttft": 1763896.3201883389,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 820,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7466321502626125,
    "arrivals": 264887,
    "finished_requests": 84660,
    "scheduler_time": 83.91531485350347
}
#Debug simulation 
Total elapsed time: 5.887360174674541. Arrivals time: 0.2735120845027268 Scheduler time: 5.501883645541966 Scheduler overhead time: 0.03306247852742672 Adapter cache time: 0.02876655338332057 Engine time: 0.034279386047273874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176311732 . Total output tokens: 158243409
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.936175067909062,
    "estimated_duration": 3600.1657782096686,
    "input_throughput": 5879.929787713005,
    "output_throughput": 5217.131142594326,
    "total_throughput": 11097.06093030733,
    "itl": 164.92768733062482,
    "ttft": 1745023.9174831484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.114017538791525,
    "arrivals": 263940,
    "finished_requests": 85947,
    "scheduler_time": 84.9477611343537
}
#Debug simulation 
Total elapsed time: 5.936271336860955. Arrivals time: 0.2559952586889267 Scheduler time: 5.571798644494265 Scheduler overhead time: 0.0327803217805922 Adapter cache time: 0.02575009409338236 Engine time: 0.03419595118612051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176311732 . Total output tokens: 158243409
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.9378144750371575,
    "estimated_duration": 3600.13722198542,
    "input_throughput": 5879.776157067195,
    "output_throughput": 5216.998086990158,
    "total_throughput": 11096.774244057353,
    "itl": 164.93037372850821,
    "ttft": 1745023.0024816399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1872583744069616,
    "arrivals": 263940,
    "finished_requests": 85945,
    "scheduler_time": 84.9458499946413
}
#Debug simulation 
Total elapsed time: 5.937941248062998. Arrivals time: 0.25770381931215525 Scheduler time: 5.571170142386109 Scheduler overhead time: 0.032885809894651175 Adapter cache time: 0.026005377061665058 Engine time: 0.03437164705246687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176311732 . Total output tokens: 158243409
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.951478074770421,
    "estimated_duration": 3600.1005315028556,
    "input_throughput": 5874.355400617575,
    "output_throughput": 5213.513577123593,
    "total_throughput": 11087.868977741167,
    "itl": 162.86336257101144,
    "ttft": 1746061.9892986994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1799869505874878,
    "arrivals": 263940,
    "finished_requests": 85870,
    "scheduler_time": 84.96392973013413
}
#Debug simulation 
Total elapsed time: 5.951575797051191. Arrivals time: 0.2562815146520734 Scheduler time: 5.585214812774211 Scheduler overhead time: 0.03334426833316684 Adapter cache time: 0.025863334070891142 Engine time: 0.03499923972412944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176311732 . Total output tokens: 158243409
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.913671263959259,
    "estimated_duration": 3600.005581950716,
    "input_throughput": 5879.834494181565,
    "output_throughput": 5217.1882994200105,
    "total_throughput": 11097.022793601574,
    "itl": 164.92709964158962,
    "ttft": 1744973.602635447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.130872220462189,
    "arrivals": 263940,
    "finished_requests": 85944,
    "scheduler_time": 84.94496639844708
}
#Debug simulation 
Total elapsed time: 5.913770494051278. Arrivals time: 0.2761099128983915 Scheduler time: 5.529607784468681 Scheduler overhead time: 0.032952697947621346 Adapter cache time: 0.025250611826777458 Engine time: 0.03415570734068751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176311732 . Total output tokens: 158243409
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.931289192289114,
    "estimated_duration": 3600.1174261564247,
    "input_throughput": 5874.327833405818,
    "output_throughput": 5213.489111114478,
    "total_throughput": 11087.816944520295,
    "itl": 162.86390812480371,
    "ttft": 1746069.6540827812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 361,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1972152193263224,
    "arrivals": 263940,
    "finished_requests": 85870,
    "scheduler_time": 84.96396694927107
}
#Debug simulation 
Total elapsed time: 5.931385121308267. Arrivals time: 0.27598552824929357 Scheduler time: 5.545386415440589 Scheduler overhead time: 0.03334641922265291 Adapter cache time: 0.025939074344933033 Engine time: 0.03472433192655444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176311732 . Total output tokens: 158243409
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.9489702908322215,
    "estimated_duration": 3600.1349545876524,
    "input_throughput": 5879.980130473913,
    "output_throughput": 5217.175810608269,
    "total_throughput": 11097.155941082183,
    "itl": 164.92736826738647,
    "ttft": 1744997.1183800723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 364,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0883783073443902,
    "arrivals": 263940,
    "finished_requests": 85947,
    "scheduler_time": 84.9475035902746
}
#Debug simulation 
Total elapsed time: 5.949094069190323. Arrivals time: 0.2553565767593682 Scheduler time: 5.584443190135062 Scheduler overhead time: 0.033010365441441536 Adapter cache time: 0.02560407016426325 Engine time: 0.03447220101952553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 66, 66, 17280, 66, 1080, 1080, 1080, 66, 17280, 1080, 66, 17280, 1080, 66, 66, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 1080, 66, 17280, 17280, 66, 1080, 1080, 66, 1080, 66, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 66, 1080, 17280, 17280, 1080, 66, 66, 66, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 66, 66, 1080, 66, 17280, 17280, 66, 66, 1080, 1080, 1080, 66, 17280, 66, 17280, 1080, 66, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 1080, 17280, 17280, 1080, 17280, 66, 1080, 17280, 66, 17280, 1080, 66, 1080, 1080, 17280, 17280, 66, 66, 17280, 66, 66, 66, 1080, 66]
Prompts retrieved: 792252 . Total input tokens: 176311732 . Total output tokens: 158243409
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.923881887923926,
    "estimated_duration": 3600.057761400578,
    "input_throughput": 5874.313247622051,
    "output_throughput": 5213.424129255691,
    "total_throughput": 11087.737376877742,
    "itl": 162.88358894411328,
    "ttft": 1746042.1863772012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 362,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2138001332432065,
    "arrivals": 263940,
    "finished_requests": 85868,
    "scheduler_time": 84.96170898467824
}
#Debug simulation 
Total elapsed time: 5.923978971783072. Arrivals time: 0.27342656161636114 Scheduler time: 5.541074467822909 Scheduler overhead time: 0.0332881435751915 Adapter cache time: 0.025847116019576788 Engine time: 0.03446640493348241 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 33, 33, 17280, 33, 1080, 1080, 1080, 33, 17280, 1080, 33, 17280, 1080, 33, 33, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 33, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 33, 33, 1080, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 33, 17280, 33, 17280, 1080, 33, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 1080, 17280, 17280, 1080, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 1080, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33]
Prompts retrieved: 790866 . Total input tokens: 176006423 . Total output tokens: 157966563
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.976225195918232,
    "estimated_duration": 3600.0701849513753,
    "input_throughput": 6005.930687235209,
    "output_throughput": 5265.1176299930985,
    "total_throughput": 11271.048317228307,
    "itl": 162.3031881530889,
    "ttft": 1737327.3186560364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6947307178727342,
    "arrivals": 263449,
    "finished_requests": 87196,
    "scheduler_time": 85.78577485460055
}
#Debug simulation 
Total elapsed time: 5.976321221794933. Arrivals time: 0.2595414347015321 Scheduler time: 5.608783326577395 Scheduler overhead time: 0.03340062499046326 Adapter cache time: 0.02375839464366436 Engine time: 0.03461507707834244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 33, 33, 17280, 33, 1080, 1080, 1080, 33, 17280, 1080, 33, 17280, 1080, 33, 33, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 33, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 33, 33, 1080, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 33, 17280, 33, 17280, 1080, 33, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 1080, 17280, 17280, 1080, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 1080, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33]
Prompts retrieved: 790866 . Total input tokens: 176006423 . Total output tokens: 157966563
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.950234825722873,
    "estimated_duration": 3600.0316541123825,
    "input_throughput": 6005.921913297999,
    "output_throughput": 5265.172871007286,
    "total_throughput": 11271.094784305284,
    "itl": 162.30767183012318,
    "ttft": 1737278.1314357943,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7404393100272891,
    "arrivals": 263449,
    "finished_requests": 87194,
    "scheduler_time": 85.78312607602182
}
#Debug simulation 
Total elapsed time: 5.9503570310771465. Arrivals time: 0.2546702725812793 Scheduler time: 5.587534479331225 Scheduler overhead time: 0.03335250308737159 Adapter cache time: 0.024004078470170498 Engine time: 0.034799027256667614 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 33, 33, 17280, 33, 1080, 1080, 1080, 33, 17280, 1080, 33, 17280, 1080, 33, 33, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 33, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 33, 33, 1080, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 33, 17280, 33, 17280, 1080, 33, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 1080, 17280, 17280, 1080, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 1080, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33]
Prompts retrieved: 790866 . Total input tokens: 176006423 . Total output tokens: 157966563
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.949901508167386,
    "estimated_duration": 3600.06542504202,
    "input_throughput": 6000.075679110159,
    "output_throughput": 5259.486916068754,
    "total_throughput": 11259.562595178913,
    "itl": 160.4604844997761,
    "ttft": 1738668.101792916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7418031503446434,
    "arrivals": 263449,
    "finished_requests": 87104,
    "scheduler_time": 85.78910341457001
}
#Debug simulation 
Total elapsed time: 5.949998891912401. Arrivals time: 0.2579192775301635 Scheduler time: 5.58354030456394 Scheduler overhead time: 0.033512236550450325 Adapter cache time: 0.024331186432391405 Engine time: 0.03475573752075434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 33, 33, 17280, 33, 1080, 1080, 1080, 33, 17280, 1080, 33, 17280, 1080, 33, 33, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 33, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 33, 33, 1080, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 33, 17280, 33, 17280, 1080, 33, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 1080, 17280, 17280, 1080, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 1080, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33]
Prompts retrieved: 790866 . Total input tokens: 176006423 . Total output tokens: 157966563
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.976107689086348,
    "estimated_duration": 3600.1285861474084,
    "input_throughput": 6005.936866587986,
    "output_throughput": 5265.089161796923,
    "total_throughput": 11271.026028384907,
    "itl": 162.30402243360658,
    "ttft": 1737307.0092791195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7032571360492159,
    "arrivals": 263449,
    "finished_requests": 87198,
    "scheduler_time": 85.78698351391317
}
#Debug simulation 
Total elapsed time: 5.976206791121513. Arrivals time: 0.25766827957704663 Scheduler time: 5.610580913256854 Scheduler overhead time: 0.03343350440263748 Adapter cache time: 0.023938749451190233 Engine time: 0.034679064992815256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 33, 33, 17280, 33, 1080, 1080, 1080, 33, 17280, 1080, 33, 17280, 1080, 33, 33, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 33, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 33, 33, 1080, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 33, 17280, 33, 17280, 1080, 33, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 1080, 17280, 17280, 1080, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 1080, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33]
Prompts retrieved: 790866 . Total input tokens: 176006423 . Total output tokens: 157966563
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.952168261632323,
    "estimated_duration": 3600.1511585196054,
    "input_throughput": 6000.040845196741,
    "output_throughput": 5259.481106839444,
    "total_throughput": 11259.521952036184,
    "itl": 160.457553046465,
    "ttft": 1738697.2717499607,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7532467449083954,
    "arrivals": 263449,
    "finished_requests": 87105,
    "scheduler_time": 85.79137237640467
}
#Debug simulation 
Total elapsed time: 5.952288537751883. Arrivals time: 0.25415033707395196 Scheduler time: 5.589583655819297 Scheduler overhead time: 0.03378511918708682 Adapter cache time: 0.023920339066535234 Engine time: 0.03484887955710292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 33, 33, 17280, 33, 1080, 1080, 1080, 33, 17280, 1080, 33, 17280, 1080, 33, 33, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 33, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 33, 33, 1080, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 33, 17280, 33, 17280, 1080, 33, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 1080, 17280, 17280, 1080, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 1080, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33]
Prompts retrieved: 790866 . Total input tokens: 176006423 . Total output tokens: 157966563
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.994163969997317,
    "estimated_duration": 3600.0594081119357,
    "input_throughput": 6005.94866609149,
    "output_throughput": 5265.133391212817,
    "total_throughput": 11271.082057304306,
    "itl": 162.30303426793972,
    "ttft": 1737322.6758510089,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6787414169427948,
    "arrivals": 263449,
    "finished_requests": 87196,
    "scheduler_time": 85.78558173207763
}
#Debug simulation 
Total elapsed time: 5.994285485241562. Arrivals time: 0.254240601323545 Scheduler time: 5.632545160129666 Scheduler overhead time: 0.03323444304987788 Adapter cache time: 0.023689595982432365 Engine time: 0.03470369195565581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 1080, 17280, 17280, 33, 33, 17280, 33, 1080, 1080, 1080, 33, 17280, 1080, 33, 17280, 1080, 33, 33, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 1080, 33, 17280, 17280, 33, 1080, 1080, 33, 1080, 33, 1080, 1080, 17280, 17280, 17280, 17280, 1080, 1080, 33, 1080, 17280, 17280, 1080, 33, 33, 33, 17280, 1080, 1080, 1080, 1080, 17280, 1080, 17280, 33, 33, 1080, 33, 17280, 17280, 33, 33, 1080, 1080, 1080, 33, 17280, 33, 17280, 1080, 33, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 1080, 17280, 17280, 1080, 17280, 33, 1080, 17280, 33, 17280, 1080, 33, 1080, 1080, 17280, 17280, 33, 33, 17280, 33, 33, 33, 1080, 33]
Prompts retrieved: 790866 . Total input tokens: 176006423 . Total output tokens: 157966563
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.9569205129519105,
    "estimated_duration": 3600.160539739784,
    "input_throughput": 6000.025210420562,
    "output_throughput": 5259.467401797754,
    "total_throughput": 11259.492612218315,
    "itl": 160.4574825503857,
    "ttft": 1738703.1316273168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.760791972093287,
    "arrivals": 263449,
    "finished_requests": 87105,
    "scheduler_time": 85.79143842543844
}
#Debug simulation 
Total elapsed time: 5.957017197739333. Arrivals time: 0.25423594238236547 Scheduler time: 5.5939686819911 Scheduler overhead time: 0.033612617291510105 Adapter cache time: 0.024170248303562403 Engine time: 0.03500996483489871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 270, 270, 17280, 270, 540, 540, 540, 270, 17280, 540, 270, 17280, 540, 270, 270, 270, 270, 540, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 17280, 270, 540, 540, 270, 540, 270, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 270, 540, 17280, 17280, 540, 270, 270, 270, 17280, 540, 540, 540, 540, 17280, 540, 17280, 270, 270, 540, 270, 17280, 17280, 270, 270, 540, 540, 540, 270, 17280, 270, 17280, 540, 270, 17280, 17280, 540, 540, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 540, 17280, 17280, 540, 17280, 270, 540, 17280, 270, 17280, 540, 270, 540, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270]
Prompts retrieved: 777600 . Total input tokens: 173088458 . Total output tokens: 155300705
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.020931619219482,
    "estimated_duration": 3600.1613381418706,
    "input_throughput": 6040.523175893021,
    "output_throughput": 5290.9859339335435,
    "total_throughput": 11331.509109826564,
    "itl": 161.50791068362622,
    "ttft": 1719137.1100557446,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.094932601382131,
    "arrivals": 259027,
    "finished_requests": 87574,
    "scheduler_time": 86.12655561258917
}
#Debug simulation 
Total elapsed time: 6.021050253883004. Arrivals time: 0.25505154952406883 Scheduler time: 5.65052717924118 Scheduler overhead time: 0.033472257666289806 Adapter cache time: 0.031427749432623386 Engine time: 0.03462136955931783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 270, 270, 17280, 270, 540, 540, 540, 270, 17280, 540, 270, 17280, 540, 270, 270, 270, 270, 540, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 17280, 270, 540, 540, 270, 540, 270, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 270, 540, 17280, 17280, 540, 270, 270, 270, 17280, 540, 540, 540, 540, 17280, 540, 17280, 270, 270, 540, 270, 17280, 17280, 270, 270, 540, 540, 540, 270, 17280, 270, 17280, 540, 270, 17280, 17280, 540, 540, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 540, 17280, 17280, 540, 17280, 270, 540, 17280, 270, 17280, 540, 270, 540, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270]
Prompts retrieved: 777600 . Total input tokens: 173088458 . Total output tokens: 155300705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.009412046056241,
    "estimated_duration": 3600.0950303002387,
    "input_throughput": 6040.358328592913,
    "output_throughput": 5290.828391941501,
    "total_throughput": 11331.186720534413,
    "itl": 161.51814733215974,
    "ttft": 1719218.198393774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1339,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.359563867235526,
    "arrivals": 259027,
    "finished_requests": 87570,
    "scheduler_time": 86.11891210848569
}
#Debug simulation 
Total elapsed time: 6.009509130846709. Arrivals time: 0.2774014840833843 Scheduler time: 5.616226739715785 Scheduler overhead time: 0.03363950503990054 Adapter cache time: 0.031447021290659904 Engine time: 0.034777123015373945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 270, 270, 17280, 270, 540, 540, 540, 270, 17280, 540, 270, 17280, 540, 270, 270, 270, 270, 540, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 17280, 270, 540, 540, 270, 540, 270, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 270, 540, 17280, 17280, 540, 270, 270, 270, 17280, 540, 540, 540, 540, 17280, 540, 17280, 270, 270, 540, 270, 17280, 17280, 270, 270, 540, 540, 540, 270, 17280, 270, 17280, 540, 270, 17280, 17280, 540, 540, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 540, 17280, 17280, 540, 17280, 270, 540, 17280, 270, 17280, 540, 270, 540, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270]
Prompts retrieved: 777600 . Total input tokens: 173088458 . Total output tokens: 155300705
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.003180616069585,
    "estimated_duration": 3600.022759789091,
    "input_throughput": 6036.246837859741,
    "output_throughput": 5288.473509849528,
    "total_throughput": 11324.72034770927,
    "itl": 159.57414420222216,
    "ttft": 1719442.1423078163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.376787581406473,
    "arrivals": 259027,
    "finished_requests": 87511,
    "scheduler_time": 86.17251353542008
}
#Debug simulation 
Total elapsed time: 6.00327806128189. Arrivals time: 0.28013831889256835 Scheduler time: 5.605560074560344 Scheduler overhead time: 0.03381639579311013 Adapter cache time: 0.032331278547644615 Engine time: 0.03528191428631544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 270, 270, 17280, 270, 540, 540, 540, 270, 17280, 540, 270, 17280, 540, 270, 270, 270, 270, 540, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 17280, 270, 540, 540, 270, 540, 270, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 270, 540, 17280, 17280, 540, 270, 270, 270, 17280, 540, 540, 540, 540, 17280, 540, 17280, 270, 270, 540, 270, 17280, 17280, 270, 270, 540, 540, 540, 270, 17280, 270, 17280, 540, 270, 17280, 17280, 540, 540, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 540, 17280, 17280, 540, 17280, 270, 540, 17280, 270, 17280, 540, 270, 540, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270]
Prompts retrieved: 777600 . Total input tokens: 173088458 . Total output tokens: 155300705
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.025705308653414,
    "estimated_duration": 3600.0269762031353,
    "input_throughput": 6040.615568646488,
    "output_throughput": 5291.068407517732,
    "total_throughput": 11331.68397616422,
    "itl": 161.50880440854814,
    "ttft": 1719147.1072820465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1338,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.152276157720412,
    "arrivals": 259027,
    "finished_requests": 87572,
    "scheduler_time": 86.12216867637558
}
#Debug simulation 
Total elapsed time: 6.025798971764743. Arrivals time: 0.27862634928897023 Scheduler time: 5.631180852651596 Scheduler overhead time: 0.033506084233522415 Adapter cache time: 0.03168578166514635 Engine time: 0.03466325905174017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_128_slots_96_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 270, 270, 17280, 270, 540, 540, 540, 270, 17280, 540, 270, 17280, 540, 270, 270, 270, 270, 540, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 17280, 270, 540, 540, 270, 540, 270, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 270, 540, 17280, 17280, 540, 270, 270, 270, 17280, 540, 540, 540, 540, 17280, 540, 17280, 270, 270, 540, 270, 17280, 17280, 270, 270, 540, 540, 540, 270, 17280, 270, 17280, 540, 270, 17280, 17280, 540, 540, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 540, 17280, 17280, 540, 17280, 270, 540, 17280, 270, 17280, 540, 270, 540, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270]
Prompts retrieved: 777600 . Total input tokens: 173088458 . Total output tokens: 155300705
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.000358048826456,
    "estimated_duration": 3600.086021964538,
    "input_throughput": 6036.140766475844,
    "output_throughput": 5288.380578642612,
    "total_throughput": 11324.521345118455,
    "itl": 159.57653719666226,
    "ttft": 1719466.63019691,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.440041735973195,
    "arrivals": 259027,
    "finished_requests": 87511,
    "scheduler_time": 86.1725909833576
}
#Debug simulation 
Total elapsed time: 6.0004787859506905. Arrivals time: 0.2556126047857106 Scheduler time: 5.627515040338039 Scheduler overhead time: 0.033738574013113976 Adapter cache time: 0.032197856809943914 Engine time: 0.035172657109797 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_128_slots_96_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 270, 270, 17280, 270, 540, 540, 540, 270, 17280, 540, 270, 17280, 540, 270, 270, 270, 270, 540, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 17280, 270, 540, 540, 270, 540, 270, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 270, 540, 17280, 17280, 540, 270, 270, 270, 17280, 540, 540, 540, 540, 17280, 540, 17280, 270, 270, 540, 270, 17280, 17280, 270, 270, 540, 540, 540, 270, 17280, 270, 17280, 540, 270, 17280, 17280, 540, 540, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 540, 17280, 17280, 540, 17280, 270, 540, 17280, 270, 17280, 540, 270, 540, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270]
Prompts retrieved: 777600 . Total input tokens: 173088458 . Total output tokens: 155300705
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.01929542189464,
    "estimated_duration": 3600.0306497222873,
    "input_throughput": 6040.713848277259,
    "output_throughput": 5291.146896615844,
    "total_throughput": 11331.860744893103,
    "itl": 161.50378288168707,
    "ttft": 1719099.7024001493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.009657445463712,
    "arrivals": 259027,
    "finished_requests": 87573,
    "scheduler_time": 86.12528251255627
}
#Debug simulation 
Total elapsed time: 6.01938988501206. Arrivals time: 0.2574237515218556 Scheduler time: 5.64579260814935 Scheduler overhead time: 0.03343788394704461 Adapter cache time: 0.031744238920509815 Engine time: 0.034971967339515686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_128_slots_96_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 270, 270, 17280, 270, 540, 540, 540, 270, 17280, 540, 270, 17280, 540, 270, 270, 270, 270, 540, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 540, 270, 540, 270, 17280, 17280, 270, 540, 540, 270, 540, 270, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 270, 540, 17280, 17280, 540, 270, 270, 270, 17280, 540, 540, 540, 540, 17280, 540, 17280, 270, 270, 540, 270, 17280, 17280, 270, 270, 540, 540, 540, 270, 17280, 270, 17280, 540, 270, 17280, 17280, 540, 540, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 270, 17280, 270, 270, 17280, 540, 17280, 17280, 540, 17280, 270, 540, 17280, 270, 17280, 540, 270, 540, 540, 17280, 17280, 270, 270, 17280, 270, 270, 270, 540, 270]
Prompts retrieved: 777600 . Total input tokens: 173088458 . Total output tokens: 155300705
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.05280497437343,
    "estimated_duration": 3600.1328656951014,
    "input_throughput": 6036.062226221288,
    "output_throughput": 5288.311767994731,
    "total_throughput": 11324.37399421602,
    "itl": 159.57814475464448,
    "ttft": 1719485.369418384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4869478983059645,
    "arrivals": 259027,
    "finished_requests": 87511,
    "scheduler_time": 86.17255100361209
}
#Debug simulation 
Total elapsed time: 6.052903284318745. Arrivals time: 0.25975274108350277 Scheduler time: 5.675200278870761 Scheduler overhead time: 0.03404627833515406 Adapter cache time: 0.03220397885888815 Engine time: 0.03530345717445016 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.1331678931601346,
    "estimated_duration": 3600.1318614322954,
    "input_throughput": 6149.698358879504,
    "output_throughput": 5429.277524358141,
    "total_throughput": 11578.975883237645,
    "itl": 158.35865026464901,
    "ttft": 1705959.0091520443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 902,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7605599450274036,
    "arrivals": 257078,
    "finished_requests": 89183,
    "scheduler_time": 88.33098763300057
}
#Debug simulation 
Total elapsed time: 6.1332940040156245. Arrivals time: 0.2623775959946215 Scheduler time: 5.755893033929169 Scheduler overhead time: 0.034168840385973454 Adapter cache time: 0.028921745717525482 Engine time: 0.035569872707128525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.1133161429315805,
    "estimated_duration": 3600.042856043489,
    "input_throughput": 6149.506515689258,
    "output_throughput": 5428.916482810865,
    "total_throughput": 11578.422998500124,
    "itl": 158.36722394372202,
    "ttft": 1705998.7708528424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 902,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9360536272893683,
    "arrivals": 257078,
    "finished_requests": 89177,
    "scheduler_time": 88.3247756513722
}
#Debug simulation 
Total elapsed time: 6.113414897117764. Arrivals time: 0.26132782036438584 Scheduler time: 5.7369491322897375 Scheduler overhead time: 0.034117869567126036 Adapter cache time: 0.028951166197657585 Engine time: 0.03568537067621946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.145564096979797,
    "estimated_duration": 3600.0097300096395,
    "input_throughput": 6143.238673952357,
    "output_throughput": 5424.251172773278,
    "total_throughput": 11567.489846725635,
    "itl": 155.99481825131946,
    "ttft": 1707309.121167271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 906,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.954717861283549,
    "arrivals": 257078,
    "finished_requests": 89079,
    "scheduler_time": 88.3670474147504
}
#Debug simulation 
Total elapsed time: 6.145660155918449. Arrivals time: 0.2626150664873421 Scheduler time: 5.766559057403356 Scheduler overhead time: 0.03474204055964947 Adapter cache time: 0.02886088564991951 Engine time: 0.03634366486221552 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.347897003870457,
    "estimated_duration": 3600.0826066734844,
    "input_throughput": 6149.679443177279,
    "output_throughput": 5429.023479563913,
    "total_throughput": 11578.702922741191,
    "itl": 158.3624594367598,
    "ttft": 1705971.6568785594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 900,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.799323024798618,
    "arrivals": 257078,
    "finished_requests": 89180,
    "scheduler_time": 88.32811494246094
}
#Debug simulation 
Total elapsed time: 6.347999819088727. Arrivals time: 0.5055676633492112 Scheduler time: 5.728269341867417 Scheduler overhead time: 0.03397613298147917 Adapter cache time: 0.02874564379453659 Engine time: 0.03523281263187528 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.152188661042601,
    "estimated_duration": 3600.1528675890613,
    "input_throughput": 6143.0472019957615,
    "output_throughput": 5424.126340800977,
    "total_throughput": 11567.173542796738,
    "itl": 155.9936826916211,
    "ttft": 1707341.1382900402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 903,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9847767600417208,
    "arrivals": 257078,
    "finished_requests": 89080,
    "scheduler_time": 88.36952257619775
}
#Debug simulation 
Total elapsed time: 6.152283486910164. Arrivals time: 0.26107507664710283 Scheduler time: 5.774477948434651 Scheduler overhead time: 0.03466179966926575 Adapter cache time: 0.02931160479784012 Engine time: 0.03609192464500666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.134060243144631,
    "estimated_duration": 3600.0335779090924,
    "input_throughput": 6149.86597232207,
    "output_throughput": 5429.276304514947,
    "total_throughput": 11579.142276837018,
    "itl": 158.3556063316324,
    "ttft": 1705942.4960804866,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 902,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6970253660017036,
    "arrivals": 257078,
    "finished_requests": 89182,
    "scheduler_time": 88.32991270570024
}
#Debug simulation 
Total elapsed time: 6.134153224993497. Arrivals time: 0.26574455155059695 Scheduler time: 5.7533916546963155 Scheduler overhead time: 0.033979829866439104 Adapter cache time: 0.029165988322347403 Engine time: 0.03558823512867093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 135, 135, 17280, 135, 540, 540, 540, 135, 17280, 540, 135, 17280, 540, 135, 135, 135, 135, 540, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 540, 135, 540, 135, 17280, 17280, 135, 540, 540, 135, 540, 135, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 135, 540, 17280, 17280, 540, 135, 135, 135, 17280, 540, 540, 540, 540, 17280, 540, 17280, 135, 135, 540, 135, 17280, 17280, 135, 135, 540, 540, 540, 135, 17280, 135, 17280, 540, 135, 17280, 17280, 540, 540, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 540, 17280, 17280, 540, 17280, 135, 540, 17280, 135, 17280, 540, 135, 540, 540, 17280, 17280, 135, 135, 17280, 135, 135, 135, 540, 135]
Prompts retrieved: 771930 . Total input tokens: 171836534 . Total output tokens: 154124869
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.376427042298019,
    "estimated_duration": 3600.0204572798802,
    "input_throughput": 6143.220090674233,
    "output_throughput": 5424.202509880869,
    "total_throughput": 11567.422600555103,
    "itl": 155.99433715488033,
    "ttft": 1707330.4747499852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 903,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.018227267228095,
    "arrivals": 257078,
    "finished_requests": 89078,
    "scheduler_time": 88.36526388222708
}
#Debug simulation 
Total elapsed time: 6.376531183253974. Arrivals time: 0.2695814282633364 Scheduler time: 5.988854061812162 Scheduler overhead time: 0.03588670399039984 Adapter cache time: 0.02930813981220126 Engine time: 0.03608569595962763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.308877660892904,
    "estimated_duration": 3600.0242502786687,
    "input_throughput": 6271.179422819838,
    "output_throughput": 5540.272679678783,
    "total_throughput": 11811.45210249862,
    "itl": 155.34043696260176,
    "ttft": 1681436.4063279608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5118809455027968,
    "arrivals": 256097,
    "finished_requests": 90895,
    "scheduler_time": 90.08206710198
}
#Debug simulation 
Total elapsed time: 6.308974019251764. Arrivals time: 0.272289402782917 Scheduler time: 5.922352361027151 Scheduler overhead time: 0.034877573139965534 Adapter cache time: 0.025784510653465986 Engine time: 0.03682236559689045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.290711148176342,
    "estimated_duration": 3600.0564921633427,
    "input_throughput": 6270.876873499822,
    "output_throughput": 5539.840567339383,
    "total_throughput": 11810.717440839206,
    "itl": 155.34496763871275,
    "ttft": 1681507.2865783277,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.614502755743457,
    "arrivals": 256097,
    "finished_requests": 90891,
    "scheduler_time": 90.08010069680273
}
#Debug simulation 
Total elapsed time: 6.290803066920489. Arrivals time: 0.2928290241397917 Scheduler time: 5.884607951622456 Scheduler overhead time: 0.034560817293822765 Adapter cache time: 0.025984425097703934 Engine time: 0.036045122891664505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.27032858831808,
    "estimated_duration": 3600.111599628052,
    "input_throughput": 6264.168866967887,
    "output_throughput": 5533.934004173187,
    "total_throughput": 11798.102871141073,
    "itl": 153.34700677404956,
    "ttft": 1682469.24104039,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6073539568111397,
    "arrivals": 256097,
    "finished_requests": 90783,
    "scheduler_time": 90.1063231882802
}
#Debug simulation 
Total elapsed time: 6.270455673336983. Arrivals time: 0.2925866562873125 Scheduler time: 5.863175828009844 Scheduler overhead time: 0.035257029812783 Adapter cache time: 0.025815236382186413 Engine time: 0.03674597525969148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.272437213920057,
    "estimated_duration": 3600.133477601985,
    "input_throughput": 6271.0955414459195,
    "output_throughput": 5540.105144458493,
    "total_throughput": 11811.200685904412,
    "itl": 155.3419779449011,
    "ttft": 1681478.2705031908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 495,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5389126218319793,
    "arrivals": 256097,
    "finished_requests": 90896,
    "scheduler_time": 90.08405889778814
}
#Debug simulation 
Total elapsed time: 6.272535287775099. Arrivals time: 0.2971592452377081 Scheduler time: 5.862166861072183 Scheduler overhead time: 0.034716911148279905 Adapter cache time: 0.025469114538282156 Engine time: 0.036335432436317205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.2680772240273654,
    "estimated_duration": 3600.136745507091,
    "input_throughput": 6264.125113620794,
    "output_throughput": 5533.8953512983335,
    "total_throughput": 11798.020464919127,
    "itl": 153.34776548705366,
    "ttft": 1682481.1390903422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6304926535114674,
    "arrivals": 256097,
    "finished_requests": 90783,
    "scheduler_time": 90.10641997093066
}
#Debug simulation 
Total elapsed time: 6.26816647592932. Arrivals time: 0.2863481258973479 Scheduler time: 5.867128627374768 Scheduler overhead time: 0.03526598168537021 Adapter cache time: 0.025782655458897352 Engine time: 0.036626535933464766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.23765437817201,
    "estimated_duration": 3600.1196460678416,
    "input_throughput": 6271.24074186243,
    "output_throughput": 5540.4083644236,
    "total_throughput": 11811.64910628603,
    "itl": 155.33730886355303,
    "ttft": 1681425.5223326618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 494,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4770848456816592,
    "arrivals": 256097,
    "finished_requests": 90898,
    "scheduler_time": 90.08593083751754
}
#Debug simulation 
Total elapsed time: 6.237753021996468. Arrivals time: 0.26352093601599336 Scheduler time: 5.862017206847668 Scheduler overhead time: 0.03467975137755275 Adapter cache time: 0.02495357533916831 Engine time: 0.03605493018403649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 66, 66, 17280, 66, 540, 540, 540, 66, 17280, 540, 66, 17280, 540, 66, 66, 66, 66, 540, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 540, 66, 540, 66, 17280, 17280, 66, 540, 540, 66, 540, 66, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 66, 540, 17280, 17280, 540, 66, 66, 66, 17280, 540, 540, 540, 540, 17280, 540, 17280, 66, 66, 540, 66, 17280, 17280, 66, 66, 540, 540, 540, 66, 17280, 66, 17280, 540, 66, 17280, 17280, 540, 540, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 540, 17280, 17280, 540, 17280, 66, 540, 17280, 66, 17280, 540, 66, 540, 540, 17280, 17280, 66, 66, 17280, 66, 66, 66, 540, 66]
Prompts retrieved: 769032 . Total input tokens: 171210139 . Total output tokens: 153556922
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.285344826988876,
    "estimated_duration": 3600.149197881243,
    "input_throughput": 6264.103446954952,
    "output_throughput": 5533.87621038732,
    "total_throughput": 11797.979657342272,
    "itl": 153.34784477987185,
    "ttft": 1682485.043845211,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 492,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.648349691182373,
    "arrivals": 256097,
    "finished_requests": 90783,
    "scheduler_time": 90.10648482513248
}
#Debug simulation 
Total elapsed time: 6.2854384309612215. Arrivals time: 0.2916191224940121 Scheduler time: 5.879289461299777 Scheduler overhead time: 0.03529316931962967 Adapter cache time: 0.02577862050384283 Engine time: 0.03656069142743945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.312157412990928,
    "estimated_duration": 3600.113945265134,
    "input_throughput": 6349.138207156559,
    "output_throughput": 5586.519567374088,
    "total_throughput": 11935.657774530648,
    "itl": 153.66030418573965,
    "ttft": 1665779.4497603804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7375775462877927,
    "arrivals": 255593,
    "finished_requests": 92233,
    "scheduler_time": 90.80576585670367
}
#Debug simulation 
Total elapsed time: 6.312280849087983. Arrivals time: 0.2665935354307294 Scheduler time: 5.933785954955965 Scheduler overhead time: 0.03512714058160782 Adapter cache time: 0.023331711068749428 Engine time: 0.036442015785723925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.290416530799121,
    "estimated_duration": 3600.013985055286,
    "input_throughput": 6370.489974540419,
    "output_throughput": 5606.187665876541,
    "total_throughput": 11976.67764041696,
    "itl": 153.1811420091825,
    "ttft": 1662665.6467699108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7805357498163386,
    "arrivals": 255593,
    "finished_requests": 92546,
    "scheduler_time": 91.08607352769732
}
#Debug simulation 
Total elapsed time: 6.290507189929485. Arrivals time: 0.26999129075556993 Scheduler time: 5.908444568980485 Scheduler overhead time: 0.03514142706990242 Adapter cache time: 0.023582662921398878 Engine time: 0.03656560042873025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.301339094992727,
    "estimated_duration": 3600.018949438117,
    "input_throughput": 6363.256227741636,
    "output_throughput": 5599.897468080408,
    "total_throughput": 11963.153695822044,
    "itl": 151.45942950885023,
    "ttft": 1663858.4679376602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7823806378617919,
    "arrivals": 255593,
    "finished_requests": 92433,
    "scheduler_time": 91.09452680736437
}
#Debug simulation 
Total elapsed time: 6.30143917305395. Arrivals time: 0.2689552800729871 Scheduler time: 5.920001483988017 Scheduler overhead time: 0.03541159024462104 Adapter cache time: 0.023573591373860836 Engine time: 0.03669307380914688 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.310815916862339,
    "estimated_duration": 3600.0565026776435,
    "input_throughput": 6370.415292910625,
    "output_throughput": 5606.244508937145,
    "total_throughput": 11976.659801847769,
    "itl": 153.17803253909133,
    "ttft": 1662653.5363047773,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7437621711567061,
    "arrivals": 255593,
    "finished_requests": 92548,
    "scheduler_time": 91.08826963208739
}
#Debug simulation 
Total elapsed time: 6.310935789253563. Arrivals time: 0.2720010285265744 Scheduler time: 5.926395168993622 Scheduler overhead time: 0.035932053811848164 Adapter cache time: 0.023324084002524614 Engine time: 0.03649712726473808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.275376514066011,
    "estimated_duration": 3600.031814764688,
    "input_throughput": 6363.233487562205,
    "output_throughput": 5599.877455893461,
    "total_throughput": 11963.110943455666,
    "itl": 151.45969991347388,
    "ttft": 1663865.1836428258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7936984786391289,
    "arrivals": 255593,
    "finished_requests": 92433,
    "scheduler_time": 91.09454327823077
}
#Debug simulation 
Total elapsed time: 6.275490355212241. Arrivals time: 0.26419957587495446 Scheduler time: 5.8986105136573315 Scheduler overhead time: 0.03537857253104448 Adapter cache time: 0.02360898070037365 Engine time: 0.036826489958912134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.2777760731987655,
    "estimated_duration": 3600.0823270465557,
    "input_throughput": 6349.193969336804,
    "output_throughput": 5586.568631751157,
    "total_throughput": 11935.762601087961,
    "itl": 153.65984724857216,
    "ttft": 1665776.700853304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 241,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7206021210714253,
    "arrivals": 255593,
    "finished_requests": 92233,
    "scheduler_time": 90.80521785018672
}
#Debug simulation 
Total elapsed time: 6.277870464138687. Arrivals time: 0.27342816861346364 Scheduler time: 5.893332808744162 Scheduler overhead time: 0.03502725996077061 Adapter cache time: 0.023092903196811676 Engine time: 0.03631287533789873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 540, 17280, 17280, 33, 33, 17280, 33, 540, 540, 540, 33, 17280, 540, 33, 17280, 540, 33, 33, 33, 33, 540, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 540, 33, 540, 33, 17280, 17280, 33, 540, 540, 33, 540, 33, 540, 540, 17280, 17280, 17280, 17280, 540, 540, 33, 540, 17280, 17280, 540, 33, 33, 33, 17280, 540, 540, 540, 540, 17280, 540, 17280, 33, 33, 540, 33, 17280, 17280, 33, 33, 540, 540, 540, 33, 17280, 33, 17280, 540, 33, 17280, 17280, 540, 540, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 540, 17280, 17280, 540, 17280, 33, 540, 17280, 33, 17280, 540, 33, 540, 540, 17280, 17280, 33, 33, 17280, 33, 33, 33, 540, 33]
Prompts retrieved: 767646 . Total input tokens: 170893182 . Total output tokens: 153281329
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.297117547132075,
    "estimated_duration": 3600.0012101350394,
    "input_throughput": 6363.28758321187,
    "output_throughput": 5599.925062037351,
    "total_throughput": 11963.212645249221,
    "itl": 151.45744245592198,
    "ttft": 1663848.21132504,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 240,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8017467209696799,
    "arrivals": 255593,
    "finished_requests": 92433,
    "scheduler_time": 91.09464096985222
}
#Debug simulation 
Total elapsed time: 6.297211690805852. Arrivals time: 0.2658946933224797 Scheduler time: 5.9182451968081295 Scheduler overhead time: 0.03549008350819349 Adapter cache time: 0.023663442116230726 Engine time: 0.036829981952905655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.604720239993185,
    "estimated_duration": 3600.032787811194,
    "input_throughput": 6475.992407328793,
    "output_throughput": 5730.026423604302,
    "total_throughput": 12206.018830933095,
    "itl": 150.03534924561203,
    "ttft": 1639862.5125583536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 747,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2861843447178103,
    "arrivals": 253064,
    "finished_requests": 94327,
    "scheduler_time": 93.10121810478476
}
#Debug simulation 
Total elapsed time: 6.604785063769668. Arrivals time: 0.47764148470014334 Scheduler time: 6.01406044466421 Scheduler overhead time: 0.03547752508893609 Adapter cache time: 0.02356524532660842 Engine time: 0.03703978331759572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.374362219125032,
    "estimated_duration": 3600.01957522362,
    "input_throughput": 6475.807009619351,
    "output_throughput": 5729.741344175529,
    "total_throughput": 12205.54835379488,
    "itl": 150.0413366174105,
    "ttft": 1639925.5362346596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 747,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4378652295121057,
    "arrivals": 253064,
    "finished_requests": 94322,
    "scheduler_time": 93.09770744473782
}
#Debug simulation 
Total elapsed time: 6.374459091108292. Arrivals time: 0.25522306002676487 Scheduler time: 6.006968171335757 Scheduler overhead time: 0.035121497232466936 Adapter cache time: 0.02330458862707019 Engine time: 0.03702211380004883 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.374246515799314,
    "estimated_duration": 3600.111282923068,
    "input_throughput": 6469.09336121699,
    "output_throughput": 5723.886119228151,
    "total_throughput": 12192.979480445141,
    "itl": 148.20768701728696,
    "ttft": 1640646.8495526435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 743,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.428946509417142,
    "arrivals": 253064,
    "finished_requests": 94221,
    "scheduler_time": 93.1151296279633
}
#Debug simulation 
Total elapsed time: 6.374340276699513. Arrivals time: 0.2564181797206402 Scheduler time: 6.004082031082362 Scheduler overhead time: 0.03568776044994593 Adapter cache time: 0.023563872557133436 Engine time: 0.03738477639853954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.642501323018223,
    "estimated_duration": 3600.0738927634584,
    "input_throughput": 6475.9184656912885,
    "output_throughput": 5729.960999263127,
    "total_throughput": 12205.879464954414,
    "itl": 150.03645677033492,
    "ttft": 1639877.4018548103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 747,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.321824159075017,
    "arrivals": 253064,
    "finished_requests": 94327,
    "scheduler_time": 93.10145496869643
}
#Debug simulation 
Total elapsed time: 6.642565616872162. Arrivals time: 0.28891529608517885 Scheduler time: 6.240501765627414 Scheduler overhead time: 0.03557023499161005 Adapter cache time: 0.023450672160834074 Engine time: 0.03728011157363653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.3943703710101545,
    "estimated_duration": 3600.1475497013007,
    "input_throughput": 6469.0281935617595,
    "output_throughput": 5723.82845856129,
    "total_throughput": 12192.85665212305,
    "itl": 148.20877429876626,
    "ttft": 1640661.9993785827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 743,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4645348309725534,
    "arrivals": 253064,
    "finished_requests": 94221,
    "scheduler_time": 93.1151423094787
}
#Debug simulation 
Total elapsed time: 6.394463756121695. Arrivals time: 0.2578736315481365 Scheduler time: 6.022223908919841 Scheduler overhead time: 0.035927118733525276 Adapter cache time: 0.02366106677800417 Engine time: 0.03766393940895796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.400938871782273,
    "estimated_duration": 3600.1189135065515,
    "input_throughput": 6476.040530919971,
    "output_throughput": 5730.146002290504,
    "total_throughput": 12206.186533210475,
    "itl": 150.03405504283035,
    "ttft": 1639839.1442936796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 747,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.233567570291883,
    "arrivals": 253064,
    "finished_requests": 94330,
    "scheduler_time": 93.10460820449744
}
#Debug simulation 
Total elapsed time: 6.401038440875709. Arrivals time: 0.2623286577872932 Scheduler time: 6.02643561270088 Scheduler overhead time: 0.035330886486917734 Adapter cache time: 0.023304183967411518 Engine time: 0.03672768920660019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 135, 135, 17280, 135, 270, 270, 270, 135, 17280, 270, 135, 17280, 270, 135, 135, 135, 135, 270, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 270, 135, 270, 135, 17280, 17280, 135, 270, 270, 135, 270, 135, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 135, 270, 17280, 17280, 270, 135, 135, 135, 17280, 270, 270, 270, 270, 17280, 270, 17280, 135, 135, 270, 135, 17280, 17280, 135, 135, 270, 270, 270, 135, 17280, 135, 17280, 270, 135, 17280, 17280, 270, 270, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 135, 17280, 135, 135, 17280, 270, 17280, 17280, 270, 17280, 135, 270, 17280, 135, 17280, 270, 135, 270, 270, 17280, 17280, 135, 135, 17280, 135, 135, 135, 270, 135]
Prompts retrieved: 760320 . Total input tokens: 169260106 . Total output tokens: 151820560
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.657826140057296,
    "estimated_duration": 3600.027159736025,
    "input_throughput": 6468.9606402046265,
    "output_throughput": 5723.5951524073735,
    "total_throughput": 12192.555792612,
    "itl": 148.20184046714985,
    "ttft": 1640688.3318212836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 743,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4914461412653433,
    "arrivals": 253064,
    "finished_requests": 94214,
    "scheduler_time": 93.11135086661953
}
#Debug simulation 
Total elapsed time: 6.657890381291509. Arrivals time: 0.4827073994092643 Scheduler time: 6.060727978590876 Scheduler overhead time: 0.0361579735763371 Adapter cache time: 0.023471778724342585 Engine time: 0.03776393970474601 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.479815582279116,
    "estimated_duration": 3600.155329331127,
    "input_throughput": 6596.572321898197,
    "output_throughput": 5815.835175058975,
    "total_throughput": 12412.407496957172,
    "itl": 147.5726784664956,
    "ttft": 1629805.7140635015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4996389945270654,
    "arrivals": 252140,
    "finished_requests": 96129,
    "scheduler_time": 94.53762562175386
}
#Debug simulation 
Total elapsed time: 6.479919621255249. Arrivals time: 0.2537497212179005 Scheduler time: 6.114031412638724 Scheduler overhead time: 0.03594192396849394 Adapter cache time: 0.02127248840406537 Engine time: 0.037756712175905704 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.486489528790116,
    "estimated_duration": 3600.1058483642973,
    "input_throughput": 6596.526880116582,
    "output_throughput": 5815.859833541565,
    "total_throughput": 12412.386713658148,
    "itl": 147.57529597155764,
    "ttft": 1629811.11280409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.595057955766101,
    "arrivals": 252140,
    "finished_requests": 96128,
    "scheduler_time": 94.53507390047956
}
#Debug simulation 
Total elapsed time: 6.486580674070865. Arrivals time: 0.2596347793005407 Scheduler time: 6.115425604395568 Scheduler overhead time: 0.03602811461314559 Adapter cache time: 0.021046914625912905 Engine time: 0.037286737468093634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.474267574027181,
    "estimated_duration": 3600.153508556053,
    "input_throughput": 6587.238556255792,
    "output_throughput": 5808.882579673032,
    "total_throughput": 12396.121135928823,
    "itl": 145.92880767010087,
    "ttft": 1630745.7174320677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 489,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5951881728135144,
    "arrivals": 252140,
    "finished_requests": 95997,
    "scheduler_time": 94.53151291498608
}
#Debug simulation 
Total elapsed time: 6.474367659073323. Arrivals time: 0.26715244399383664 Scheduler time: 6.094388707075268 Scheduler overhead time: 0.03639100259169936 Adapter cache time: 0.021224510855972767 Engine time: 0.03766546258702874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.63207290135324,
    "estimated_duration": 3600.037563362989,
    "input_throughput": 6596.435615470706,
    "output_throughput": 5815.689317541981,
    "total_throughput": 12412.124933012687,
    "itl": 147.5733805356451,
    "ttft": 1629786.1302951602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5255967516312308,
    "arrivals": 252140,
    "finished_requests": 96125,
    "scheduler_time": 94.53378180264285
}
#Debug simulation 
Total elapsed time: 6.6321385921910405. Arrivals time: 0.2648828011006117 Scheduler time: 6.2566389525309205 Scheduler overhead time: 0.035665323957800865 Adapter cache time: 0.020824311301112175 Engine time: 0.037113229278475046 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.462276794016361,
    "estimated_duration": 3600.1342338681393,
    "input_throughput": 6587.273823542826,
    "output_throughput": 5808.9136797353,
    "total_throughput": 12396.187503278126,
    "itl": 145.92052038002984,
    "ttft": 1630737.0538001661,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6130597963742952,
    "arrivals": 252140,
    "finished_requests": 95997,
    "scheduler_time": 94.53118364434377
}
#Debug simulation 
Total elapsed time: 6.4623717670328915. Arrivals time: 0.26170734921470284 Scheduler time: 6.0876693604514 Scheduler overhead time: 0.036378778982907534 Adapter cache time: 0.021406876388937235 Engine time: 0.03791226865723729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.461672861594707,
    "estimated_duration": 3600.0561970727617,
    "input_throughput": 6596.617858162846,
    "output_throughput": 5815.940044776146,
    "total_throughput": 12412.557902938992,
    "itl": 147.56961786328694,
    "ttft": 1629782.0318234474,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 490,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.465124644502051,
    "arrivals": 252140,
    "finished_requests": 96128,
    "scheduler_time": 94.53675656396516
}
#Debug simulation 
Total elapsed time: 6.461761099752039. Arrivals time: 0.2721578851342201 Scheduler time: 6.07878033304587 Scheduler overhead time: 0.03574906010180712 Adapter cache time: 0.02095228061079979 Engine time: 0.03706388873979449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 66, 66, 17280, 66, 270, 270, 270, 66, 17280, 270, 66, 17280, 270, 66, 66, 66, 66, 270, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 270, 66, 270, 66, 17280, 17280, 66, 270, 270, 66, 270, 66, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 66, 270, 17280, 17280, 270, 66, 66, 66, 17280, 270, 270, 270, 270, 17280, 270, 17280, 66, 66, 270, 66, 17280, 17280, 66, 66, 270, 270, 270, 66, 17280, 66, 17280, 270, 66, 17280, 17280, 270, 270, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 270, 17280, 17280, 270, 17280, 66, 270, 17280, 66, 17280, 270, 66, 270, 270, 17280, 17280, 66, 66, 17280, 66, 66, 66, 270, 66]
Prompts retrieved: 757422 . Total input tokens: 168610071 . Total output tokens: 151252288
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.453785018995404,
    "estimated_duration": 3600.015490537079,
    "input_throughput": 6587.4372102943435,
    "output_throughput": 5808.98417103203,
    "total_throughput": 12396.421381326374,
    "itl": 145.92782578908998,
    "ttft": 1630772.8543460905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 488,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6315456029772748,
    "arrivals": 252140,
    "finished_requests": 95995,
    "scheduler_time": 94.52740943086437
}
#Debug simulation 
Total elapsed time: 6.453883589245379. Arrivals time: 0.2606607414782047 Scheduler time: 6.080289554782212 Scheduler overhead time: 0.036390520632267 Adapter cache time: 0.021114906296133995 Engine time: 0.03782260697335005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.740586783736944,
    "estimated_duration": 3600.0504528427805,
    "input_throughput": 6673.478140015723,
    "output_throughput": 5867.260272233315,
    "total_throughput": 12540.738412249038,
    "itl": 146.1470842777048,
    "ttft": 1621569.9707527175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9609931515948834,
    "arrivals": 251722,
    "finished_requests": 96996,
    "scheduler_time": 95.31608009527929
}
#Debug simulation 
Total elapsed time: 6.740653560962528. Arrivals time: 0.27123012533411384 Scheduler time: 6.358856257516891 Scheduler overhead time: 0.03615517448633909 Adapter cache time: 0.019401211757212877 Engine time: 0.037784550338983536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.577358519192785,
    "estimated_duration": 3600.0325161377305,
    "input_throughput": 6673.469723483147,
    "output_throughput": 5867.288949562337,
    "total_throughput": 12540.758673045484,
    "itl": 146.15106172559496,
    "ttft": 1621579.409553416,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0313482176326276,
    "arrivals": 251722,
    "finished_requests": 96995,
    "scheduler_time": 95.31364970548474
}
#Debug simulation 
Total elapsed time: 6.577448720112443. Arrivals time: 0.2669366323389113 Scheduler time: 6.199768775142729 Scheduler overhead time: 0.036355321761220694 Adapter cache time: 0.019288003910332918 Engine time: 0.03775730822235346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.60749902902171,
    "estimated_duration": 3600.0629884780715,
    "input_throughput": 6663.522576348422,
    "output_throughput": 5859.0963734546,
    "total_throughput": 12522.618949803023,
    "itl": 144.45947052756708,
    "ttft": 1623376.2441250158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0251296109892483,
    "arrivals": 251722,
    "finished_requests": 96854,
    "scheduler_time": 95.300087207065
}
#Debug simulation 
Total elapsed time: 6.607597328256816. Arrivals time: 0.2730898526497185 Scheduler time: 6.22127571515739 Scheduler overhead time: 0.036848148331046104 Adapter cache time: 0.019908226560801268 Engine time: 0.038612102158367634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.770710780285299,
    "estimated_duration": 3600.0669157171205,
    "input_throughput": 6673.447622629629,
    "output_throughput": 5867.23344162965,
    "total_throughput": 12540.681064259279,
    "itl": 146.14717259575124,
    "ttft": 1621576.0037168856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 314,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9801439197617616,
    "arrivals": 251722,
    "finished_requests": 96996,
    "scheduler_time": 95.31622417929097
}
#Debug simulation 
Total elapsed time: 6.770802479237318. Arrivals time: 0.27590610925108194 Scheduler time: 6.384026849642396 Scheduler overhead time: 0.036204750183969736 Adapter cache time: 0.019464469980448484 Engine time: 0.03788474481552839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.572388625238091,
    "estimated_duration": 3600.0770889477976,
    "input_throughput": 6663.496477241088,
    "output_throughput": 5859.073425054054,
    "total_throughput": 12522.569902295141,
    "itl": 144.45995444014034,
    "ttft": 1623382.760195176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.039842803999787,
    "arrivals": 251722,
    "finished_requests": 96854,
    "scheduler_time": 95.30008982766248
}
#Debug simulation 
Total elapsed time: 6.572508737910539. Arrivals time: 0.2644923282787204 Scheduler time: 6.195828008465469 Scheduler overhead time: 0.03694605128839612 Adapter cache time: 0.019552840385586023 Engine time: 0.03812398901209235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.571663998998702,
    "estimated_duration": 3600.000020378679,
    "input_throughput": 6673.571628889285,
    "output_throughput": 5867.342466786476,
    "total_throughput": 12540.91409567576,
    "itl": 146.14606594412524,
    "ttft": 1621554.8836084637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9358857423043823,
    "arrivals": 251722,
    "finished_requests": 96996,
    "scheduler_time": 95.31570259180869
}
#Debug simulation 
Total elapsed time: 6.571781015954912. Arrivals time: 0.2668068348430097 Scheduler time: 6.194344107527286 Scheduler overhead time: 0.03628481086343527 Adapter cache time: 0.01926280837506056 Engine time: 0.03770831413567066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 270, 17280, 17280, 33, 33, 17280, 33, 270, 270, 270, 33, 17280, 270, 33, 17280, 270, 33, 33, 33, 33, 270, 270, 17280, 270, 33, 270, 270, 270, 270, 17280, 270, 33, 270, 33, 17280, 17280, 33, 270, 270, 33, 270, 33, 270, 270, 17280, 17280, 17280, 17280, 270, 270, 33, 270, 17280, 17280, 270, 33, 33, 33, 17280, 270, 270, 270, 270, 17280, 270, 17280, 33, 33, 270, 33, 17280, 17280, 33, 33, 270, 270, 270, 33, 17280, 33, 17280, 270, 33, 17280, 17280, 270, 270, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 270, 17280, 17280, 270, 17280, 33, 270, 17280, 33, 17280, 270, 33, 270, 270, 17280, 17280, 33, 33, 17280, 33, 33, 33, 270, 33]
Prompts retrieved: 756036 . Total input tokens: 168301394 . Total output tokens: 150981039
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.764776058960706,
    "estimated_duration": 3600.100577304688,
    "input_throughput": 6663.453002182535,
    "output_throughput": 5859.035198342132,
    "total_throughput": 12522.488200524667,
    "itl": 144.4597504879927,
    "ttft": 1623397.233074303,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0521666750684426,
    "arrivals": 251722,
    "finished_requests": 96854,
    "scheduler_time": 95.30034329016901
}
#Debug simulation 
Total elapsed time: 6.764860262628645. Arrivals time: 0.4631556891836226 Scheduler time: 6.189418334979564 Scheduler overhead time: 0.03695947350934148 Adapter cache time: 0.019541193265467882 Engine time: 0.03833915526047349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.712911261245608,
    "estimated_duration": 3600.090504647506,
    "input_throughput": 6760.4853179609245,
    "output_throughput": 6022.266376917881,
    "total_throughput": 12782.751694878805,
    "itl": 143.63177287173662,
    "ttft": 1595194.5318862393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3649775337940195,
    "arrivals": 250208,
    "finished_requests": 98893,
    "scheduler_time": 97.68326851092559
}
#Debug simulation 
Total elapsed time: 6.713024102151394. Arrivals time: 0.26811570627614856 Scheduler time: 6.334163681138307 Scheduler overhead time: 0.0369871249422431 Adapter cache time: 0.01733248494565487 Engine time: 0.03873215150088072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.708957592956722,
    "estimated_duration": 3600.066903957194,
    "input_throughput": 6760.369362371545,
    "output_throughput": 6022.095860543424,
    "total_throughput": 12782.46522291497,
    "itl": 143.63459534266042,
    "ttft": 1595204.0834301435,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4536894551478379,
    "arrivals": 250208,
    "finished_requests": 98891,
    "scheduler_time": 97.68056576239663
}
#Debug simulation 
Total elapsed time: 6.709042933769524. Arrivals time: 0.27797267492860556 Scheduler time: 6.319869409780949 Scheduler overhead time: 0.03717993386089802 Adapter cache time: 0.017435331363230944 Engine time: 0.03863069927319884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.8835988524369895,
    "estimated_duration": 3600.038611506018,
    "input_throughput": 6747.699572543708,
    "output_throughput": 6010.496646020162,
    "total_throughput": 12758.19621856387,
    "itl": 141.79686071454475,
    "ttft": 1597298.3783751533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4531794035993602,
    "arrivals": 250208,
    "finished_requests": 98695,
    "scheduler_time": 97.64360137946454
}
#Debug simulation 
Total elapsed time: 6.883660468272865. Arrivals time: 0.46539911022409797 Scheduler time: 6.305859681684524 Scheduler overhead time: 0.037978265434503555 Adapter cache time: 0.01746641704812646 Engine time: 0.03881931211799383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.70127713913098,
    "estimated_duration": 3600.1155759927983,
    "input_throughput": 6760.43823767748,
    "output_throughput": 6022.224437619935,
    "total_throughput": 12782.662675297415,
    "itl": 143.63221797272445,
    "ttft": 1595203.733777703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.390357180789575,
    "arrivals": 250208,
    "finished_requests": 98893,
    "scheduler_time": 97.68348311882755
}
#Debug simulation 
Total elapsed time: 6.701362869236618. Arrivals time: 0.2671173866838217 Scheduler time: 6.323491968214512 Scheduler overhead time: 0.03708855248987675 Adapter cache time: 0.017385495826601982 Engine time: 0.038663031067699194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.694998093880713,
    "estimated_duration": 3600.0580759571976,
    "input_throughput": 6747.663089724227,
    "output_throughput": 6010.464149039261,
    "total_throughput": 12758.127238763489,
    "itl": 141.79745518130989,
    "ttft": 1597307.4852645083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4726712404936597,
    "arrivals": 250208,
    "finished_requests": 98695,
    "scheduler_time": 97.64362265407452
}
#Debug simulation 
Total elapsed time: 6.695087381172925. Arrivals time: 0.2955254619009793 Scheduler time: 6.28809313243255 Scheduler overhead time: 0.037469839211553335 Adapter cache time: 0.017200664151459932 Engine time: 0.03890377841889858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.904668357688934,
    "estimated_duration": 3600.0132421343383,
    "input_throughput": 6760.470132485088,
    "output_throughput": 6022.185625946925,
    "total_throughput": 12782.655758432013,
    "itl": 143.62982107458706,
    "ttft": 1595199.0402657443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 446,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.33356243152636,
    "arrivals": 250208,
    "finished_requests": 98891,
    "scheduler_time": 97.68160336258148
}
#Debug simulation 
Total elapsed time: 6.904733733739704. Arrivals time: 0.4679344678297639 Scheduler time: 6.325751863885671 Scheduler overhead time: 0.03763638250529766 Adapter cache time: 0.017202771734446287 Engine time: 0.03862766642123461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_128_slots_96_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 66, 66, 17280, 66, 135, 135, 135, 66, 17280, 135, 66, 17280, 135, 66, 66, 66, 66, 135, 135, 17280, 135, 66, 135, 135, 135, 135, 17280, 135, 66, 135, 66, 17280, 17280, 66, 135, 135, 66, 135, 66, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 66, 135, 17280, 17280, 135, 66, 66, 66, 17280, 135, 135, 135, 135, 17280, 135, 17280, 66, 66, 135, 66, 17280, 17280, 66, 66, 135, 135, 135, 66, 17280, 66, 17280, 135, 66, 17280, 17280, 135, 135, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 66, 17280, 66, 66, 17280, 135, 17280, 17280, 135, 17280, 66, 135, 17280, 66, 17280, 135, 66, 135, 135, 17280, 17280, 66, 66, 17280, 66, 66, 66, 135, 66]
Prompts retrieved: 751617 . Total input tokens: 167279045 . Total output tokens: 150123066
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.690031968988478,
    "estimated_duration": 3600.074052438547,
    "input_throughput": 6747.6331448086685,
    "output_throughput": 6010.4374756800535,
    "total_throughput": 12758.070620488723,
    "itl": 141.79772939850182,
    "ttft": 1597314.6670458063,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4900252630189066,
    "arrivals": 250208,
    "finished_requests": 98695,
    "scheduler_time": 97.64359346385308
}
#Debug simulation 
Total elapsed time: 6.69012304302305. Arrivals time: 0.26721238624304533 Scheduler time: 6.311062632594258 Scheduler overhead time: 0.037341808434575796 Adapter cache time: 0.017347011249512434 Engine time: 0.03925208281725645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.729336727876216,
    "estimated_duration": 3600.1028072249487,
    "input_throughput": 6838.630260944562,
    "output_throughput": 6066.75119281817,
    "total_throughput": 12905.381453762731,
    "itl": 142.2955893855963,
    "ttft": 1581687.4456257045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9273277864116232,
    "arrivals": 249718,
    "finished_requests": 99826,
    "scheduler_time": 98.42865744332168
}
#Debug simulation 
Total elapsed time: 6.729427655693144. Arrivals time: 0.2752091232687235 Scheduler time: 6.344219487160444 Scheduler overhead time: 0.03743811743333936 Adapter cache time: 0.015675486531108618 Engine time: 0.038699228782206774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.729663631413132,
    "estimated_duration": 3600.1025100236407,
    "input_throughput": 6838.453608321929,
    "output_throughput": 6066.701972843706,
    "total_throughput": 12905.155581165636,
    "itl": 142.2989130506769,
    "ttft": 1581694.7351919108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9901558749540762,
    "arrivals": 249718,
    "finished_requests": 99824,
    "scheduler_time": 98.42773716761461
}
#Debug simulation 
Total elapsed time: 6.7297555902041495. Arrivals time: 0.27517571300268173 Scheduler time: 6.345285501796752 Scheduler overhead time: 0.037206889130175114 Adapter cache time: 0.015700260177254677 Engine time: 0.03879360668361187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.758682986255735,
    "estimated_duration": 3600.0499774124146,
    "input_throughput": 6825.885238866201,
    "output_throughput": 6055.720097438673,
    "total_throughput": 12881.605336304874,
    "itl": 140.64838269395452,
    "ttft": 1583626.17993315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.988278428707277,
    "arrivals": 249718,
    "finished_requests": 99643,
    "scheduler_time": 98.38279164699799
}
#Debug simulation 
Total elapsed time: 6.758771502878517. Arrivals time: 0.27901091799139977 Scheduler time: 6.367499134968966 Scheduler overhead time: 0.03800578787922859 Adapter cache time: 0.01604975713416934 Engine time: 0.03974583186209202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.7222866881638765,
    "estimated_duration": 3600.0375404168276,
    "input_throughput": 6838.504244361164,
    "output_throughput": 6066.772291899823,
    "total_throughput": 12905.276536260986,
    "itl": 142.29733288156672,
    "ttft": 1581690.3237454158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9443931992887545,
    "arrivals": 249718,
    "finished_requests": 99823,
    "scheduler_time": 98.42671018719898
}
#Debug simulation 
Total elapsed time: 6.722377680242062. Arrivals time: 0.26865848898887634 Scheduler time: 6.344672234728932 Scheduler overhead time: 0.037137930281460285 Adapter cache time: 0.015488287899643183 Engine time: 0.038683872669935226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.771233472973108,
    "estimated_duration": 3600.0723021315825,
    "input_throughput": 6825.842910279927,
    "output_throughput": 6055.682544789951,
    "total_throughput": 12881.52545506988,
    "itl": 140.64918015034067,
    "ttft": 1583637.6826361543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0023628527857416,
    "arrivals": 249718,
    "finished_requests": 99643,
    "scheduler_time": 98.3829519517453
}
#Debug simulation 
Total elapsed time: 6.771332522854209. Arrivals time: 0.28591436287388206 Scheduler time: 6.374310959596187 Scheduler overhead time: 0.03811289556324482 Adapter cache time: 0.015899259597063065 Engine time: 0.03920350829139352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.750823410693556,
    "estimated_duration": 3600.0723419877877,
    "input_throughput": 6838.549246042766,
    "output_throughput": 6066.754477478245,
    "total_throughput": 12905.30372352101,
    "itl": 142.2948129966055,
    "ttft": 1581691.6178577596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 303,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9059852393553605,
    "arrivals": 249718,
    "finished_requests": 99825,
    "scheduler_time": 98.42814907434659
}
#Debug simulation 
Total elapsed time: 6.750935675110668. Arrivals time: 0.2713492475450039 Scheduler time: 6.369208253454417 Scheduler overhead time: 0.03728586947545409 Adapter cache time: 0.015636108350008726 Engine time: 0.039555273950099945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 135, 17280, 17280, 33, 33, 17280, 33, 135, 135, 135, 33, 17280, 135, 33, 17280, 135, 33, 33, 33, 33, 135, 135, 17280, 135, 33, 135, 135, 135, 135, 17280, 135, 33, 135, 33, 17280, 17280, 33, 135, 135, 33, 135, 33, 135, 135, 17280, 17280, 17280, 17280, 135, 135, 33, 135, 17280, 17280, 135, 33, 33, 33, 17280, 135, 135, 135, 135, 17280, 135, 17280, 33, 33, 135, 33, 17280, 17280, 33, 33, 135, 135, 135, 33, 17280, 33, 17280, 135, 33, 17280, 17280, 135, 135, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 135, 17280, 17280, 135, 17280, 33, 135, 17280, 33, 17280, 135, 33, 135, 135, 17280, 17280, 33, 33, 17280, 33, 33, 33, 135, 33]
Prompts retrieved: 750231 . Total input tokens: 166961878 . Total output tokens: 149848924
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.701412737369537,
    "estimated_duration": 3600.0809379054217,
    "input_throughput": 6825.826536638154,
    "output_throughput": 6055.668018587402,
    "total_throughput": 12881.494555225556,
    "itl": 140.64907479325845,
    "ttft": 1583641.1430968326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0140579549223236,
    "arrivals": 249718,
    "finished_requests": 99643,
    "scheduler_time": 98.38292260505375
}
#Debug simulation 
Total elapsed time: 6.7015222371555865. Arrivals time: 0.2713338718749583 Scheduler time: 6.319796342402697 Scheduler overhead time: 0.037510198540985584 Adapter cache time: 0.015702559147030115 Engine time: 0.039281693287193775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.919543195981532,
    "estimated_duration": 3600.001748478587,
    "input_throughput": 7056.761572612082,
    "output_throughput": 6234.513638635111,
    "total_throughput": 13291.275211247194,
    "itl": 137.98573571983601,
    "ttft": 1554828.8134725674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8477551050693717,
    "arrivals": 248697,
    "finished_requests": 102676,
    "scheduler_time": 101.10922344498334
}
#Debug simulation 
Total elapsed time: 6.919631764758378. Arrivals time: 0.27637457475066185 Scheduler time: 6.534206803422421 Scheduler overhead time: 0.03822681633755565 Adapter cache time: 0.012717614881694317 Engine time: 0.039855155162513256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.93532553082332,
    "estimated_duration": 3600.071751320568,
    "input_throughput": 7056.6796316437785,
    "output_throughput": 6234.480185503788,
    "total_throughput": 13291.159817147565,
    "itl": 137.9866919223536,
    "ttft": 1554884.6111082456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9046512562362524,
    "arrivals": 248697,
    "finished_requests": 102677,
    "scheduler_time": 101.11000638239614
}
#Debug simulation 
Total elapsed time: 6.935416082851589. Arrivals time: 0.279082381632179 Scheduler time: 6.546982722822577 Scheduler overhead time: 0.03843513922765851 Adapter cache time: 0.012461001519113779 Engine time: 0.04005034686997533 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.900805191136897,
    "estimated_duration": 3600.058142343406,
    "input_throughput": 7044.39817282843,
    "output_throughput": 6224.25502978517,
    "total_throughput": 13268.6532026136,
    "itl": 136.58520537326916,
    "ttft": 1557051.425336908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9030764837376819,
    "arrivals": 248697,
    "finished_requests": 102495,
    "scheduler_time": 101.03705068063006
}
#Debug simulation 
Total elapsed time: 6.900895989034325. Arrivals time: 0.27571326959878206 Scheduler time: 6.514535869471729 Scheduler overhead time: 0.038814958184957504 Adapter cache time: 0.012707471381872892 Engine time: 0.04049966158345342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.902950828894973,
    "estimated_duration": 3600.0162505143685,
    "input_throughput": 7056.788423210648,
    "output_throughput": 6234.576301368954,
    "total_throughput": 13291.364724579602,
    "itl": 137.98555664557622,
    "ttft": 1554821.0271880513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8642003197106559,
    "arrivals": 248697,
    "finished_requests": 102677,
    "scheduler_time": 101.10942422655265
}
#Debug simulation 
Total elapsed time: 6.9030432449653745. Arrivals time: 0.2779318210668862 Scheduler time: 6.516221598256379 Scheduler overhead time: 0.038320292718708515 Adapter cache time: 0.012622691225260496 Engine time: 0.039793175645172596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.925099292770028,
    "estimated_duration": 3600.090516166321,
    "input_throughput": 7044.334826060352,
    "output_throughput": 6224.199058156344,
    "total_throughput": 13268.533884216695,
    "itl": 136.58027839161093,
    "ttft": 1557070.8569324308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9155261085927532,
    "arrivals": 248697,
    "finished_requests": 102495,
    "scheduler_time": 101.03738943193241
}
#Debug simulation 
Total elapsed time: 6.92518532788381. Arrivals time: 0.2870666398666799 Scheduler time: 6.528010577894747 Scheduler overhead time: 0.03871857514604926 Adapter cache time: 0.01269759051501751 Engine time: 0.04016949934884906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.9501955937594175,
    "estimated_duration": 3600.127850251566,
    "input_throughput": 7057.1629833159,
    "output_throughput": 6234.88301906598,
    "total_throughput": 13292.04600238188,
    "itl": 137.986369566981,
    "ttft": 1554818.9970643655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 277,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8282439316879038,
    "arrivals": 248697,
    "finished_requests": 102684,
    "scheduler_time": 101.11335264017788
}
#Debug simulation 
Total elapsed time: 6.950284414924681. Arrivals time: 0.2787695610895753 Scheduler time: 6.561338774859905 Scheduler overhead time: 0.03842627443373203 Adapter cache time: 0.012764398008584976 Engine time: 0.040378994308412075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_128_slots_96_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [42 43 43]
Adapter prompts. [17280, 66, 17280, 17280, 33, 33, 17280, 33, 66, 66, 66, 33, 17280, 66, 33, 17280, 66, 33, 33, 33, 33, 66, 66, 17280, 66, 33, 66, 66, 66, 66, 17280, 66, 33, 66, 33, 17280, 17280, 33, 66, 66, 33, 66, 33, 66, 66, 17280, 17280, 17280, 17280, 66, 66, 33, 66, 17280, 17280, 66, 33, 33, 33, 17280, 66, 66, 66, 66, 17280, 66, 17280, 33, 33, 66, 33, 17280, 17280, 33, 33, 66, 66, 66, 33, 17280, 33, 17280, 66, 33, 17280, 17280, 66, 66, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 17280, 33, 17280, 33, 33, 17280, 66, 17280, 17280, 66, 17280, 33, 66, 17280, 33, 17280, 66, 33, 66, 66, 17280, 17280, 33, 33, 17280, 33, 33, 33, 66, 33]
Prompts retrieved: 747264 . Total input tokens: 166298103 . Total output tokens: 149246063
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.9115021652542055,
    "estimated_duration": 3600.0802802533435,
    "input_throughput": 7044.354854835447,
    "output_throughput": 6224.216755083899,
    "total_throughput": 13268.571609919347,
    "itl": 136.58439366235737,
    "ttft": 1557060.6896821856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9265924417972612,
    "arrivals": 248697,
    "finished_requests": 102495,
    "scheduler_time": 101.0373155616839
}
#Debug simulation 
Total elapsed time: 6.91160489525646. Arrivals time: 0.2829349231906235 Scheduler time: 6.518246147315949 Scheduler overhead time: 0.03884901059791446 Adapter cache time: 0.012806902639567852 Engine time: 0.04035406652837992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.058337199036032,
    "estimated_duration": 3600.1419125833736,
    "input_throughput": 5381.017601636381,
    "output_throughput": 4770.331397207743,
    "total_throughput": 10151.348998844123,
    "itl": 180.3416115805318,
    "ttft": 1700239.2666820274,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365092496434214,
    "arrivals": 200319,
    "finished_requests": 78263,
    "scheduler_time": 77.30305933170449
}
#Debug simulation 
Total elapsed time: 8.058451506309211. Arrivals time: 0.2674546870402992 Scheduler time: 7.702464169822633 Scheduler overhead time: 0.03224380826577544 Adapter cache time: 0.009102771524339914 Engine time: 0.03236346039921045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 8.2805833700113,
    "estimated_duration": 3600.0271994945783,
    "input_throughput": 5381.637672826471,
    "output_throughput": 4770.695344304958,
    "total_throughput": 10152.333017131428,
    "itl": 180.35551262722632,
    "ttft": 1700053.246297025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9923287346120979,
    "arrivals": 200319,
    "finished_requests": 78266,
    "scheduler_time": 77.30007775826643
}
#Debug simulation 
Total elapsed time: 8.28065058030188. Arrivals time: 0.2592704137787223 Scheduler time: 7.9325538338162005 Scheduler overhead time: 0.032224377151578665 Adapter cache time: 0.009205087553709745 Engine time: 0.032502627931535244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.9848687602207065,
    "estimated_duration": 3600.1363955538145,
    "input_throughput": 5364.323147270414,
    "output_throughput": 4755.936753159069,
    "total_throughput": 10120.259900429483,
    "itl": 177.80716414877386,
    "ttft": 1703791.7877357132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0488032667897698,
    "arrivals": 200319,
    "finished_requests": 78028,
    "scheduler_time": 77.1904438055119
}
#Debug simulation 
Total elapsed time: 7.9849313870072365. Arrivals time: 0.25242815958335996 Scheduler time: 7.642359490506351 Scheduler overhead time: 0.0326772783882916 Adapter cache time: 0.009469705633819103 Engine time: 0.03285415098071098 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 8.289646120741963,
    "estimated_duration": 3600.055369645032,
    "input_throughput": 5381.85778012364,
    "output_throughput": 4771.30383738894,
    "total_throughput": 10153.16161751258,
    "itl": 180.33531326020048,
    "ttft": 1700326.140451654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9369086004910084,
    "arrivals": 200319,
    "finished_requests": 78287,
    "scheduler_time": 77.30072308684932
}
#Debug simulation 
Total elapsed time: 8.289710617624223. Arrivals time: 0.2512571350671351 Scheduler time: 7.949573105666786 Scheduler overhead time: 0.03216940863057971 Adapter cache time: 0.00908824848011136 Engine time: 0.03271205676719546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 7.9671689798124135,
    "estimated_duration": 3600.1308610885003,
    "input_throughput": 5364.3313938207575,
    "output_throughput": 4755.944064439689,
    "total_throughput": 10120.275458260447,
    "itl": 177.80579488062594,
    "ttft": 1703811.4457731477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.064522490091627,
    "arrivals": 200319,
    "finished_requests": 78028,
    "scheduler_time": 77.18981131889177
}
#Debug simulation 
Total elapsed time: 7.967243048828095. Arrivals time: 0.2526301476173103 Scheduler time: 7.624914090614766 Scheduler overhead time: 0.03249961929395795 Adapter cache time: 0.009485526941716671 Engine time: 0.03270025458186865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 8.292698399163783,
    "estimated_duration": 3600.1068585694325,
    "input_throughput": 5381.069996266162,
    "output_throughput": 4770.377845624379,
    "total_throughput": 10151.44784189054,
    "itl": 180.34138093049495,
    "ttft": 1700209.28539873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.914955390240067,
    "arrivals": 200319,
    "finished_requests": 78263,
    "scheduler_time": 77.30273424627825
}
#Debug simulation 
Total elapsed time: 8.292763619218022. Arrivals time: 0.25894087133929133 Scheduler time: 7.944906674791127 Scheduler overhead time: 0.03216838929802179 Adapter cache time: 0.009193463250994682 Engine time: 0.03261134075000882 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 4320, 4320, 4320, 1080, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 1080, 8640, 8640, 1080, 4320, 4320, 1080, 4320, 1080, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 1080, 4320, 8640, 8640, 4320, 1080, 1080, 1080, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 1080, 1080, 4320, 1080, 8640, 8640, 1080, 1080, 4320, 4320, 4320, 1080, 8640, 1080, 8640, 4320, 1080, 8640, 8640, 4320, 4320, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 4320, 8640, 8640, 1080, 1080, 8640, 1080, 1080, 1080, 4320, 1080]
Prompts retrieved: 602640 . Total input tokens: 134238379 . Total output tokens: 120461712
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 7.979811400175095,
    "estimated_duration": 3600.1039457701113,
    "input_throughput": 5364.371498964827,
    "output_throughput": 4755.979621121013,
    "total_throughput": 10120.351120085841,
    "itl": 177.81435061851835,
    "ttft": 1703798.2429572877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0755888232961344,
    "arrivals": 200319,
    "finished_requests": 78028,
    "scheduler_time": 77.18941343830976
}
#Debug simulation 
Total elapsed time: 7.979909994173795. Arrivals time: 0.25260851066559553 Scheduler time: 7.637223977595568 Scheduler overhead time: 0.03263569250702858 Adapter cache time: 0.00950146047398448 Engine time: 0.0328239188529551 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.384624633938074,
    "estimated_duration": 3600.0386245539958,
    "input_throughput": 5388.51996411658,
    "output_throughput": 4765.888311024887,
    "total_throughput": 10154.408275141468,
    "itl": 180.1375258308961,
    "ttft": 1675163.2180166054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2180741220852422,
    "arrivals": 192932,
    "finished_requests": 78605,
    "scheduler_time": 77.1931690182683
}
#Debug simulation 
Total elapsed time: 6.384717829991132. Arrivals time: 0.23843041621148586 Scheduler time: 6.05691492324695 Scheduler overhead time: 0.03201610269024968 Adapter cache time: 0.010583230759948492 Engine time: 0.03194181015715003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.401439862791449,
    "estimated_duration": 3600.0188953598517,
    "input_throughput": 5388.549494838393,
    "output_throughput": 4765.9144295366195,
    "total_throughput": 10154.463924375013,
    "itl": 180.13997774350082,
    "ttft": 1675159.5421454683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.299543562713085,
    "arrivals": 192932,
    "finished_requests": 78605,
    "scheduler_time": 77.19129434245471
}
#Debug simulation 
Total elapsed time: 6.401530934963375. Arrivals time: 0.2375381258316338 Scheduler time: 6.074651420582086 Scheduler overhead time: 0.03202321846038103 Adapter cache time: 0.01041655009612441 Engine time: 0.032013196498155594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.29491079505533,
    "estimated_duration": 3600.1648926539597,
    "input_throughput": 5372.489476653284,
    "output_throughput": 4753.147844677008,
    "total_throughput": 10125.637321330292,
    "itl": 178.018949515573,
    "ttft": 1678375.6476219818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3338242563046592,
    "arrivals": 192932,
    "finished_requests": 78374,
    "scheduler_time": 77.09397152221773
}
#Debug simulation 
Total elapsed time: 6.295001950114965. Arrivals time: 0.25297866482287645 Scheduler time: 5.952261337079108 Scheduler overhead time: 0.03193898685276508 Adapter cache time: 0.010875707492232323 Engine time: 0.03211419563740492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 6.367105136159807,
    "estimated_duration": 3600.0872536623656,
    "input_throughput": 5388.447177291478,
    "output_throughput": 4765.823934557645,
    "total_throughput": 10154.271111849122,
    "itl": 180.13776055399072,
    "ttft": 1675170.1644198685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 398,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2353940977179423,
    "arrivals": 192932,
    "finished_requests": 78605,
    "scheduler_time": 77.19401379444678
}
#Debug simulation 
Total elapsed time: 6.36719634430483. Arrivals time: 0.253083401825279 Scheduler time: 6.024740329477936 Scheduler overhead time: 0.031826477497816086 Adapter cache time: 0.01060414221137762 Engine time: 0.032082523219287395 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 6.344473320990801,
    "estimated_duration": 3600.1490679184803,
    "input_throughput": 5372.513091848997,
    "output_throughput": 4753.168737508365,
    "total_throughput": 10125.681829357361,
    "itl": 178.02512348921658,
    "ttft": 1678360.854631499,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3540706159174483,
    "arrivals": 192932,
    "finished_requests": 78374,
    "scheduler_time": 77.09372371512454
}
#Debug simulation 
Total elapsed time: 6.3445647223852575. Arrivals time: 0.2555262609384954 Scheduler time: 5.998738914262503 Scheduler overhead time: 0.032355731818825006 Adapter cache time: 0.010802860837429762 Engine time: 0.032260637264698744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 6.331304635852575,
    "estimated_duration": 3600.058093921578,
    "input_throughput": 5387.471672400878,
    "output_throughput": 4765.082827125536,
    "total_throughput": 10152.554499526414,
    "itl": 180.1351431385019,
    "ttft": 1675407.305602321,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 405,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.210970369435375,
    "arrivals": 192932,
    "finished_requests": 78599,
    "scheduler_time": 77.19374127188091
}
#Debug simulation 
Total elapsed time: 6.331393908243626. Arrivals time: 0.2385262679308653 Scheduler time: 6.003737873863429 Scheduler overhead time: 0.03179305884987116 Adapter cache time: 0.01053353725001216 Engine time: 0.03198252059519291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 540, 540, 8640, 540, 4320, 4320, 4320, 540, 8640, 4320, 540, 8640, 4320, 540, 540, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 4320, 540, 8640, 8640, 540, 4320, 4320, 540, 4320, 540, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 540, 4320, 8640, 8640, 4320, 540, 540, 540, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 540, 540, 4320, 540, 8640, 8640, 540, 540, 4320, 4320, 4320, 540, 8640, 540, 8640, 4320, 540, 8640, 8640, 4320, 4320, 540, 540, 540, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 540, 8640, 540, 540, 8640, 4320, 8640, 8640, 4320, 8640, 540, 4320, 8640, 540, 8640, 4320, 540, 4320, 4320, 8640, 8640, 540, 540, 8640, 540, 540, 540, 4320, 540]
Prompts retrieved: 579960 . Total input tokens: 129205168 . Total output tokens: 115873450
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 6.299358730204403,
    "estimated_duration": 3600.0693843579043,
    "input_throughput": 5372.352289662763,
    "output_throughput": 4752.816730239704,
    "total_throughput": 10125.169019902467,
    "itl": 178.00866284563907,
    "ttft": 1678414.6634502828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 408,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3681550399959104,
    "arrivals": 192932,
    "finished_requests": 78369,
    "scheduler_time": 77.09103457577442
}
#Debug simulation 
Total elapsed time: 6.299447931814939. Arrivals time: 0.2525855014100671 Scheduler time: 5.956942248158157 Scheduler overhead time: 0.031997017562389374 Adapter cache time: 0.010790509171783924 Engine time: 0.032178898341953754 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.708330690860748,
    "estimated_duration": 3600.1956290926364,
    "input_throughput": 5388.382187689401,
    "output_throughput": 4768.819744477901,
    "total_throughput": 10157.201932167301,
    "itl": 180.2256584075114,
    "ttft": 1667601.3980957658,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.726115087578097,
    "arrivals": 188974,
    "finished_requests": 78531,
    "scheduler_time": 77.1542065161838
}
#Debug simulation 
Total elapsed time: 5.708418207708746. Arrivals time: 0.23493100656196475 Scheduler time: 5.383328580763191 Scheduler overhead time: 0.031252925749868155 Adapter cache time: 0.012740383855998516 Engine time: 0.03144087549299002 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.721812321338803,
    "estimated_duration": 3600.0415259070096,
    "input_throughput": 5387.866739985216,
    "output_throughput": 4768.394163363163,
    "total_throughput": 10156.260903348379,
    "itl": 180.23099759425023,
    "ttft": 1667663.7139707266,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8412459920137247,
    "arrivals": 188974,
    "finished_requests": 78522,
    "scheduler_time": 77.14887179903631
}
#Debug simulation 
Total elapsed time: 5.721903820056468. Arrivals time: 0.23710907669737935 Scheduler time: 5.393787265289575 Scheduler overhead time: 0.03156158048659563 Adapter cache time: 0.012988862581551075 Engine time: 0.03172915615141392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.716323595028371,
    "estimated_duration": 3600.132709487045,
    "input_throughput": 5372.852214316297,
    "output_throughput": 4756.309942374256,
    "total_throughput": 10129.162156690552,
    "itl": 178.08411186740219,
    "ttft": 1670730.2689687412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 580,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8971047026477872,
    "arrivals": 188974,
    "finished_requests": 78311,
    "scheduler_time": 77.04737329228479
}
#Debug simulation 
Total elapsed time: 5.7164258770644665. Arrivals time: 0.24287158017978072 Scheduler time: 5.381467725150287 Scheduler overhead time: 0.031751375179737806 Adapter cache time: 0.013280167244374752 Engine time: 0.03209647350013256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.724510561674833,
    "estimated_duration": 3600.0779342722385,
    "input_throughput": 5388.22890897259,
    "output_throughput": 4768.767874873944,
    "total_throughput": 10156.996783846535,
    "itl": 180.2268764789744,
    "ttft": 1667613.0694571715,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.745913399751285,
    "arrivals": 188974,
    "finished_requests": 78528,
    "scheduler_time": 77.1514551065224
}
#Debug simulation 
Total elapsed time: 5.72461489867419. Arrivals time: 0.2375425104983151 Scheduler time: 5.396121583878994 Scheduler overhead time: 0.03154211165383458 Adapter cache time: 0.0128246177919209 Engine time: 0.0318805486895144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.706581733189523,
    "estimated_duration": 3600.1788766524733,
    "input_throughput": 5372.307227685293,
    "output_throughput": 4756.043681896435,
    "total_throughput": 10128.350909581728,
    "itl": 177.98874728749374,
    "ttft": 1670822.1969950877,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9426346716098504,
    "arrivals": 188974,
    "finished_requests": 78307,
    "scheduler_time": 77.04300771162846
}
#Debug simulation 
Total elapsed time: 5.706673877779394. Arrivals time: 0.2331647858954966 Scheduler time: 5.381110928952694 Scheduler overhead time: 0.03186430782079697 Adapter cache time: 0.013415397610515356 Engine time: 0.03211041493341327 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.742576147895306,
    "estimated_duration": 3600.1541925869883,
    "input_throughput": 5388.444206068895,
    "output_throughput": 4768.874631912078,
    "total_throughput": 10157.318837980974,
    "itl": 180.22411454580205,
    "ttft": 1667585.7168248903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.686388366324804,
    "arrivals": 188974,
    "finished_requests": 78531,
    "scheduler_time": 77.15409837136116
}
#Debug simulation 
Total elapsed time: 5.742686196230352. Arrivals time: 0.234331707470119 Scheduler time: 5.416804319713265 Scheduler overhead time: 0.03171326033771038 Adapter cache time: 0.012893476523458958 Engine time: 0.03207772737368941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 270, 270, 8640, 270, 4320, 4320, 4320, 270, 8640, 4320, 270, 8640, 4320, 270, 270, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 4320, 270, 8640, 8640, 270, 4320, 4320, 270, 4320, 270, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 270, 4320, 8640, 8640, 4320, 270, 270, 270, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 270, 270, 4320, 270, 8640, 8640, 270, 270, 4320, 4320, 4320, 270, 8640, 270, 8640, 4320, 270, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 270, 8640, 270, 270, 8640, 4320, 8640, 8640, 4320, 8640, 270, 4320, 8640, 270, 8640, 4320, 270, 4320, 4320, 8640, 8640, 270, 270, 8640, 270, 270, 270, 4320, 270]
Prompts retrieved: 568620 . Total input tokens: 126706655 . Total output tokens: 113623377
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.717068855185062,
    "estimated_duration": 3600.114330304687,
    "input_throughput": 5372.5607648585565,
    "output_throughput": 4756.159785222949,
    "total_throughput": 10128.720550081505,
    "itl": 178.01318018552905,
    "ttft": 1670769.1910552487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 585,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9628810312226366,
    "arrivals": 188974,
    "finished_requests": 78308,
    "scheduler_time": 77.04229480722428
}
#Debug simulation 
Total elapsed time: 5.717179430183023. Arrivals time: 0.23325807927176356 Scheduler time: 5.39127110876143 Scheduler overhead time: 0.032084309961646795 Adapter cache time: 0.013401811942458153 Engine time: 0.03225583489984274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.459378791972995,
    "estimated_duration": 3600.070088435679,
    "input_throughput": 5463.740293052751,
    "output_throughput": 4792.919464381224,
    "total_throughput": 10256.659757433976,
    "itl": 178.30340433368744,
    "ttft": 1649010.8930501873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1025550800818387,
    "arrivals": 187135,
    "finished_requests": 79375,
    "scheduler_time": 77.57535164231558
}
#Debug simulation 
Total elapsed time: 5.459472519811243. Arrivals time: 0.24950697179883718 Scheduler time: 5.118617589585483 Scheduler overhead time: 0.030619182158261538 Adapter cache time: 0.01485269982367754 Engine time: 0.031397552229464054 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.485292249824852,
    "estimated_duration": 3600.1373652035895,
    "input_throughput": 5463.63819061878,
    "output_throughput": 4792.82989776259,
    "total_throughput": 10256.468088381369,
    "itl": 178.31007713787048,
    "ttft": 1649051.3935861376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2408926131250393,
    "arrivals": 187135,
    "finished_requests": 79375,
    "scheduler_time": 77.57444343234127
}
#Debug simulation 
Total elapsed time: 5.485402274876833. Arrivals time: 0.23606804478913546 Scheduler time: 5.157421660143882 Scheduler overhead time: 0.030717247165739536 Adapter cache time: 0.015026078559458256 Engine time: 0.03157552191987634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.454935857094824,
    "estimated_duration": 3600.0742605093374,
    "input_throughput": 5455.738570576372,
    "output_throughput": 4785.658504044993,
    "total_throughput": 10241.397074621365,
    "itl": 176.75667151031757,
    "ttft": 1650233.9486406036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 693,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2646296571753943,
    "arrivals": 187135,
    "finished_requests": 79247,
    "scheduler_time": 77.5294845199752
}
#Debug simulation 
Total elapsed time: 5.4550285916775465. Arrivals time: 0.23656956385821104 Scheduler time: 5.125563833396882 Scheduler overhead time: 0.03143327636644244 Adapter cache time: 0.015024347230792046 Engine time: 0.03189606824889779 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.495692172087729,
    "estimated_duration": 3600.110327032865,
    "input_throughput": 5463.679224578507,
    "output_throughput": 4792.865893701953,
    "total_throughput": 10256.545118280459,
    "itl": 178.3037296440382,
    "ttft": 1649024.2611740225,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1215827801404292,
    "arrivals": 187135,
    "finished_requests": 79375,
    "scheduler_time": 77.5764364389946
}
#Debug simulation 
Total elapsed time: 5.495779178105295. Arrivals time: 0.23901502089574933 Scheduler time: 5.16454334417358 Scheduler overhead time: 0.030681845732033253 Adapter cache time: 0.015077157411724329 Engine time: 0.03183362865820527 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.437815855257213,
    "estimated_duration": 3600.1063421975437,
    "input_throughput": 5455.689952761474,
    "output_throughput": 4785.615857525864,
    "total_throughput": 10241.305810287338,
    "itl": 176.7578500767523,
    "ttft": 1650245.3712352605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 693,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.301475516594951,
    "arrivals": 187135,
    "finished_requests": 79247,
    "scheduler_time": 77.52947065849911
}
#Debug simulation 
Total elapsed time: 5.43790515512228. Arrivals time: 0.23390704998746514 Scheduler time: 5.111783465836197 Scheduler overhead time: 0.030892128590494394 Adapter cache time: 0.015048825647681952 Engine time: 0.03169598104432225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.458812844008207,
    "estimated_duration": 3600.1790431341537,
    "input_throughput": 5463.673268559446,
    "output_throughput": 4792.8649640097965,
    "total_throughput": 10256.538232569243,
    "itl": 178.30065805858365,
    "ttft": 1649026.9677439637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 687,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0541645525977588,
    "arrivals": 187135,
    "finished_requests": 79376,
    "scheduler_time": 77.57869210959393
}
#Debug simulation 
Total elapsed time: 5.458901012316346. Arrivals time: 0.25175074813887477 Scheduler time: 5.115771495271474 Scheduler overhead time: 0.03059113072231412 Adapter cache time: 0.014850523322820663 Engine time: 0.031377183739095926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 135, 135, 8640, 135, 4320, 4320, 4320, 135, 8640, 4320, 135, 8640, 4320, 135, 135, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 4320, 135, 8640, 8640, 135, 4320, 4320, 135, 4320, 135, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 135, 4320, 8640, 8640, 4320, 135, 135, 135, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 135, 135, 4320, 135, 8640, 8640, 135, 135, 4320, 4320, 4320, 135, 8640, 135, 8640, 4320, 135, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 135, 8640, 135, 135, 8640, 4320, 8640, 8640, 4320, 8640, 135, 4320, 8640, 135, 8640, 4320, 135, 4320, 4320, 8640, 8640, 135, 135, 8640, 135, 135, 135, 4320, 135]
Prompts retrieved: 562950 . Total input tokens: 125473252 . Total output tokens: 112487755
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.445236813277006,
    "estimated_duration": 3600.0892770548417,
    "input_throughput": 5455.715813822247,
    "output_throughput": 4785.638542301502,
    "total_throughput": 10241.35435612375,
    "itl": 176.7741239953891,
    "ttft": 1650244.9180757832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 694,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3256456715613667,
    "arrivals": 187135,
    "finished_requests": 79247,
    "scheduler_time": 77.52852983310729
}
#Debug simulation 
Total elapsed time: 5.44532474828884. Arrivals time: 0.23147926852107048 Scheduler time: 5.1216985723003745 Scheduler overhead time: 0.03080271277576685 Adapter cache time: 0.014993824996054173 Engine time: 0.031732472125440836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.516038751229644,
    "estimated_duration": 3600.0258689748075,
    "input_throughput": 5523.4989202071065,
    "output_throughput": 4898.524522275706,
    "total_throughput": 10422.023442482812,
    "itl": 175.44621161887844,
    "ttft": 1633065.5567012113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2762233892199666,
    "arrivals": 186179,
    "finished_requests": 80512,
    "scheduler_time": 79.13214853963095
}
#Debug simulation 
Total elapsed time: 5.516129989176989. Arrivals time: 0.23601144878193736 Scheduler time: 5.192059576977044 Scheduler overhead time: 0.030610348097980022 Adapter cache time: 0.011430096346884966 Engine time: 0.031535296235233545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.5213890969753265,
    "estimated_duration": 3600.1696919136825,
    "input_throughput": 5523.434088305405,
    "output_throughput": 4898.367996266887,
    "total_throughput": 10421.802084572291,
    "itl": 175.44822334015402,
    "ttft": 1633080.9313951684,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3579888995899878,
    "arrivals": 186179,
    "finished_requests": 80514,
    "scheduler_time": 79.13383137120795
}
#Debug simulation 
Total elapsed time: 5.5214756689965725. Arrivals time: 0.23905496671795845 Scheduler time: 5.1944082719273865 Scheduler overhead time: 0.03063878510147333 Adapter cache time: 0.011383538134396076 Engine time: 0.03146284772083163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.561362822074443,
    "estimated_duration": 3600.1303465942538,
    "input_throughput": 5511.565996150944,
    "output_throughput": 4887.6921961023545,
    "total_throughput": 10399.258192253299,
    "itl": 173.38463873011872,
    "ttft": 1635838.3230802978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3476928605698126,
    "arrivals": 186179,
    "finished_requests": 80331,
    "scheduler_time": 79.06273861233112
}
#Debug simulation 
Total elapsed time: 5.561453307978809. Arrivals time: 0.2376073943451047 Scheduler time: 5.234417384490371 Scheduler overhead time: 0.030994080938398838 Adapter cache time: 0.011391445063054562 Engine time: 0.032215607818216085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.513523699250072,
    "estimated_duration": 3600.046293629587,
    "input_throughput": 5523.4675829549105,
    "output_throughput": 4898.49673077967,
    "total_throughput": 10421.964313734581,
    "itl": 175.44622845660464,
    "ttft": 1633075.2850779647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2909792673657607,
    "arrivals": 186179,
    "finished_requests": 80512,
    "scheduler_time": 79.13221933437099
}
#Debug simulation 
Total elapsed time: 5.513612510170788. Arrivals time: 0.2384298169054091 Scheduler time: 5.186991936061531 Scheduler overhead time: 0.03063831152394414 Adapter cache time: 0.011435477994382381 Engine time: 0.03158532828092575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.51178288878873,
    "estimated_duration": 3600.1459356933556,
    "input_throughput": 5511.54213035493,
    "output_throughput": 4887.671031760858,
    "total_throughput": 10399.213162115788,
    "itl": 173.38411703648018,
    "ttft": 1635843.208401035,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3679392201826013,
    "arrivals": 186179,
    "finished_requests": 80331,
    "scheduler_time": 79.06271320294962
}
#Debug simulation 
Total elapsed time: 5.511871347669512. Arrivals time: 0.2437409763224423 Scheduler time: 5.179026310797781 Scheduler overhead time: 0.031324874609708786 Adapter cache time: 0.01146575016900897 Engine time: 0.03168064868077636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [16]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.545293169096112,
    "estimated_duration": 3600.118306352068,
    "input_throughput": 5523.674031743137,
    "output_throughput": 4898.564574637446,
    "total_throughput": 10422.238606380582,
    "itl": 175.44239622765733,
    "ttft": 1633030.5961075977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 417,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2468509729742,
    "arrivals": 186179,
    "finished_requests": 80515,
    "scheduler_time": 79.13488358274019
}
#Debug simulation 
Total elapsed time: 5.545398272108287. Arrivals time: 0.2376715699210763 Scheduler time: 5.220031084958464 Scheduler overhead time: 0.030404262244701385 Adapter cache time: 0.011356663424521685 Engine time: 0.031476407777518034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_128_slots_96_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 66, 66, 8640, 66, 4320, 4320, 4320, 66, 8640, 4320, 66, 8640, 4320, 66, 66, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 4320, 66, 8640, 8640, 66, 4320, 4320, 66, 4320, 66, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 66, 4320, 8640, 8640, 4320, 66, 66, 66, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 66, 66, 4320, 66, 8640, 8640, 66, 66, 4320, 4320, 4320, 66, 8640, 66, 8640, 4320, 66, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 66, 8640, 66, 66, 8640, 4320, 8640, 8640, 4320, 8640, 66, 4320, 8640, 66, 8640, 4320, 66, 4320, 4320, 8640, 8640, 66, 66, 8640, 66, 66, 66, 4320, 66]
Prompts retrieved: 560052 . Total input tokens: 124825011 . Total output tokens: 111909641
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.529584684874862,
    "estimated_duration": 3600.1603313174846,
    "input_throughput": 5511.520091867314,
    "output_throughput": 4887.651487888206,
    "total_throughput": 10399.171579755519,
    "itl": 173.3836135397656,
    "ttft": 1635850.2681386622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.381520629115404,
    "arrivals": 186179,
    "finished_requests": 80331,
    "scheduler_time": 79.06274827567184
}
#Debug simulation 
Total elapsed time: 5.529686735011637. Arrivals time: 0.23863643454387784 Scheduler time: 5.202306992840022 Scheduler overhead time: 0.03109706938266754 Adapter cache time: 0.01129525387659669 Engine time: 0.03166284039616585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 632384,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [8]. Counts: [128]
---Simulation End---
#Simulation results
{
    "duration": 5.608477327041328,
    "estimated_duration": 3600.0437537738544,
    "input_throughput": 5659.219829937602,
    "output_throughput": 4950.671219291957,
    "total_throughput": 10609.891049229558,
    "itl": 172.49775266420608,
    "ttft": 1617023.5370704352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7039121811045325,
    "arrivals": 185762,
    "finished_requests": 81770,
    "scheduler_time": 80.03098971767196
}
#Debug simulation 
Total elapsed time: 5.608573709148914. Arrivals time: 0.2376594878733158 Scheduler time: 5.283513033762574 Scheduler overhead time: 0.03154502948746085 Adapter cache time: 0.00881644431501627 Engine time: 0.03229396743699908 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.585611274000257,
    "estimated_duration": 3600.1596479245613,
    "input_throughput": 5659.079594357731,
    "output_throughput": 4950.521849850326,
    "total_throughput": 10609.601444208058,
    "itl": 172.4972621684718,
    "ttft": 1617052.404279538,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.751043842185757,
    "arrivals": 185762,
    "finished_requests": 81772,
    "scheduler_time": 80.03268776036445
}
#Debug simulation 
Total elapsed time: 5.585701437201351. Arrivals time: 0.23500026343390346 Scheduler time: 5.264158887788653 Scheduler overhead time: 0.03086922923102975 Adapter cache time: 0.008767765015363693 Engine time: 0.03207602445036173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [86 42]
---Simulation End---
#Simulation results
{
    "duration": 5.752592552918941,
    "estimated_duration": 3600.177456931428,
    "input_throughput": 5648.631558660231,
    "output_throughput": 4940.69395544765,
    "total_throughput": 10589.325514107883,
    "itl": 170.97433727961615,
    "ttft": 1619195.2581764245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7492391780205105,
    "arrivals": 185762,
    "finished_requests": 81606,
    "scheduler_time": 79.96851877917133
}
#Debug simulation 
Total elapsed time: 5.752661141101271. Arrivals time: 0.24174565495923162 Scheduler time: 5.423197780270129 Scheduler overhead time: 0.03121682768687606 Adapter cache time: 0.008937207516282797 Engine time: 0.03263003099709749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 85]
---Simulation End---
#Simulation results
{
    "duration": 5.597357854247093,
    "estimated_duration": 3600.0671045073027,
    "input_throughput": 5659.183123140218,
    "output_throughput": 4950.639108278279,
    "total_throughput": 10609.822231418497,
    "itl": 172.49745000878423,
    "ttft": 1617030.4503586693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 230,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7154960494814456,
    "arrivals": 185762,
    "finished_requests": 81770,
    "scheduler_time": 80.03127843655747
}
#Debug simulation 
Total elapsed time: 5.597447399049997. Arrivals time: 0.23807881632819772 Scheduler time: 5.27232640562579 Scheduler overhead time: 0.03114350140094757 Adapter cache time: 0.008925335481762886 Engine time: 0.03215576522052288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 96,
    "served_adapters": 128,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_128_slots_96_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [42 43 43]
Adapter prompts. [8640, 4320, 8640, 8640, 33, 33, 8640, 33, 4320, 4320, 4320, 33, 8640, 4320, 33, 8640, 4320, 33, 33, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 4320, 33, 8640, 8640, 33, 4320, 4320, 33, 4320, 33, 4320, 4320, 8640, 8640, 8640, 8640, 4320, 4320, 33, 4320, 8640, 8640, 4320, 33, 33, 33, 8640, 4320, 4320, 4320, 4320, 8640, 4320, 8640, 33, 33, 4320, 33, 8640, 8640, 33, 33, 4320, 4320, 4320, 33, 8640, 33, 8640, 4320, 33, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 8640, 33, 8640, 33, 33, 8640, 4320, 8640, 8640, 4320, 8640, 33, 4320, 8640, 33, 8640, 4320, 33, 4320, 4320, 8640, 8640, 33, 33, 8640, 33, 33, 33, 4320, 33]
Prompts retrieved: 558666 . Total input tokens: 124512367 . Total output tokens: 111638255
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [43 43 42]
---Simulation End---
#Simulation results
{
    "duration": 5.611919560004026,
    "estimated_duration": 3600.161753546197,
    "input_throughput": 5648.656197174683,
    "output_throughput": 4940.715506040597,
    "total_throughput": 10589.37170321528,
    "itl": 170.98047790064223,
    "ttft": 1619180.2654923692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7601797574386031,
    "arrivals": 185762,
    "finished_requests": 81606,
    "scheduler_time": 79.96853307340109
}
#Debug simulation 
Total elapsed time: 5.612010115291923. Arrivals time: 0.2403212352655828 Scheduler time: 5.283582772128284 Scheduler overhead time: 0.03163858922198415 Adapter cache time: 0.008946247398853302 Engine time: 0.03260554978623986 
