INFO 06-01 00:46:59 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:46:59 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.568684313911945,
    "estimated_duration": 3600.067714773477,
    "input_throughput": 7089.023324553168,
    "output_throughput": 6328.25124552789,
    "total_throughput": 13417.27457008106,
    "itl": 136.9366525179964,
    "ttft": 1747276.0288311776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 376669,
    "finished_requests": 103659,
    "scheduler_time": 64.67940378341174
}
#Debug simulation 
Total elapsed time: 7.568802000954747. Arrivals time: 0.31052506249397993 Scheduler time: 7.145066285971552 Scheduler overhead time: 0.040983268059790134 Adapter cache time: 0.010690290946513414 Engine time: 0.042462017852813005 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.591206539422274,
    "estimated_duration": 3600.0050003219158,
    "input_throughput": 7089.08015342143,
    "output_throughput": 6328.260376850264,
    "total_throughput": 13417.340530271693,
    "itl": 136.93628329988562,
    "ttft": 1747230.442372223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 376669,
    "finished_requests": 103657,
    "scheduler_time": 64.67836442601262
}
#Debug simulation 
Total elapsed time: 7.5913504101336. Arrivals time: 0.31249918788671494 Scheduler time: 7.165735816583037 Scheduler overhead time: 0.041119631845504045 Adapter cache time: 0.010717050638049841 Engine time: 0.04223096650093794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 17280, 34560, 34560, 270, 34560, 270, 17280, 270, 34560, 17280, 34560, 270, 17280, 34560, 34560, 34560, 270, 34560, 17280, 17280, 270, 270, 270, 34560, 270, 270, 270, 34560, 17280, 270, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 270, 17280, 17280, 34560, 34560, 17280, 34560, 270, 34560, 17280, 17280, 17280, 34560, 17280, 270, 270, 17280, 17280, 270, 17280, 270, 270, 270, 34560, 17280]
Prompts retrieved: 1128870 . Total input tokens: 251650939 . Total output tokens: 226081943
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.568065934348851,
    "estimated_duration": 3600.1070630923236,
    "input_throughput": 7088.945843204642,
    "output_throughput": 6328.1820792382805,
    "total_throughput": 13417.127922442922,
    "itl": 136.93749505640952,
    "ttft": 1747285.3539057344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 376669,
    "finished_requests": 103659,
    "scheduler_time": 64.68047225650464
}
#Debug simulation 
Total elapsed time: 7.568165389355272. Arrivals time: 0.31132199242711067 Scheduler time: 7.14337894320488 Scheduler overhead time: 0.04145191516727209 Adapter cache time: 0.010679042898118496 Engine time: 0.04235072107985616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.6674867030233145,
    "estimated_duration": 3600.1268652591725,
    "input_throughput": 7208.26681148966,
    "output_throughput": 6408.214727827145,
    "total_throughput": 13616.481539316805,
    "itl": 134.83987598485993,
    "ttft": 1735851.6908560337,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 375735,
    "finished_requests": 105164,
    "scheduler_time": 65.49623869666898
}
#Debug simulation 
Total elapsed time: 7.667587640229613. Arrivals time: 0.3162403921596706 Scheduler time: 7.2383084199391305 Scheduler overhead time: 0.041578134521842 Adapter cache time: 0.009183383546769619 Engine time: 0.042965639382600784 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.610476502217352,
    "estimated_duration": 3600.014676376113,
    "input_throughput": 7207.927003820082,
    "output_throughput": 6407.964709527723,
    "total_throughput": 13615.891713347806,
    "itl": 134.83925301610452,
    "ttft": 1735848.934492062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667414,
    "arrivals": 375735,
    "finished_requests": 105158,
    "scheduler_time": 65.49411471341962
}
#Debug simulation 
Total elapsed time: 7.610576991923153. Arrivals time: 0.31602820195257664 Scheduler time: 7.181692163459957 Scheduler overhead time: 0.04155898280441761 Adapter cache time: 0.00915115000680089 Engine time: 0.042890679091215134 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.66827308293432,
    "estimated_duration": 3600.0151395344233,
    "input_throughput": 7207.926076487513,
    "output_throughput": 6407.963885113938,
    "total_throughput": 13615.889961601451,
    "itl": 134.83924201445024,
    "ttft": 1735849.299212699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 375735,
    "finished_requests": 105158,
    "scheduler_time": 65.49411318563882
}
#Debug simulation 
Total elapsed time: 7.668370621278882. Arrivals time: 0.32164316624403 Scheduler time: 7.233893858734518 Scheduler overhead time: 0.041574876755476 Adapter cache time: 0.009173592552542686 Engine time: 0.042741159442812204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.646939515136182,
    "estimated_duration": 3600.144457661543,
    "input_throughput": 7208.380748381549,
    "output_throughput": 6408.273687713485,
    "total_throughput": 13616.654436095034,
    "itl": 134.84043932381596,
    "ttft": 1735860.7853767034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 375735,
    "finished_requests": 105165,
    "scheduler_time": 65.49642821140475
}
#Debug simulation 
Total elapsed time: 7.647065428085625. Arrivals time: 0.32412459375336766 Scheduler time: 7.209411502350122 Scheduler overhead time: 0.04209438478574157 Adapter cache time: 0.009222406893968582 Engine time: 0.04289477504789829 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.646524370182306,
    "estimated_duration": 3600.025816201909,
    "input_throughput": 7208.012754579823,
    "output_throughput": 6407.968214055211,
    "total_throughput": 13615.980968635035,
    "itl": 134.8393189244794,
    "ttft": 1735839.9366283126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 375735,
    "finished_requests": 105159,
    "scheduler_time": 65.49422327624137
}
#Debug simulation 
Total elapsed time: 7.6466274112463. Arrivals time: 0.3227942972443998 Scheduler time: 7.211184049025178 Scheduler overhead time: 0.04148088535293937 Adapter cache time: 0.009051974397152662 Engine time: 0.042890123557299376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.731662378646433,
    "estimated_duration": 3600.10126370583,
    "input_throughput": 7208.220574686713,
    "output_throughput": 6408.156412870581,
    "total_throughput": 13616.376987557294,
    "itl": 134.83965274602755,
    "ttft": 1735847.1168779621,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 375735,
    "finished_requests": 105163,
    "scheduler_time": 65.49554318880834
}
#Debug simulation 
Total elapsed time: 7.731779439840466. Arrivals time: 0.327616014983505 Scheduler time: 7.2902645925059915 Scheduler overhead time: 0.041734491009265184 Adapter cache time: 0.009049610234797001 Engine time: 0.0437926328741014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 17280, 34560, 34560, 135, 34560, 135, 17280, 135, 34560, 17280, 34560, 135, 17280, 34560, 34560, 34560, 135, 34560, 17280, 17280, 135, 135, 135, 34560, 135, 135, 135, 34560, 17280, 135, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 135, 17280, 17280, 34560, 34560, 17280, 34560, 135, 34560, 17280, 17280, 17280, 34560, 17280, 135, 135, 17280, 17280, 135, 17280, 135, 135, 135, 34560, 17280]
Prompts retrieved: 1126035 . Total input tokens: 251026360 . Total output tokens: 225507485
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.6413310850039124,
    "estimated_duration": 3600.030343287026,
    "input_throughput": 7208.003690409761,
    "output_throughput": 6407.960155951593,
    "total_throughput": 13615.963846361354,
    "itl": 134.83938367872904,
    "ttft": 1735842.4839124023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 375735,
    "finished_requests": 105159,
    "scheduler_time": 65.4942235630538
}
#Debug simulation 
Total elapsed time: 7.641483264043927. Arrivals time: 0.3174266233108938 Scheduler time: 7.2112083048559725 Scheduler overhead time: 0.04146568477153778 Adapter cache time: 0.009115136694163084 Engine time: 0.04293117579072714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.794229601044208,
    "estimated_duration": 3600.0806079010863,
    "input_throughput": 7262.347110400687,
    "output_throughput": 6448.27978269475,
    "total_throughput": 13710.626893095436,
    "itl": 133.92930348965686,
    "ttft": 1727645.7338253534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 375251,
    "finished_requests": 106080,
    "scheduler_time": 65.89552670231222
}
#Debug simulation 
Total elapsed time: 7.794330789707601. Arrivals time: 0.3262935820966959 Scheduler time: 7.354347949381918 Scheduler overhead time: 0.04226738587021828 Adapter cache time: 0.008241466246545315 Engine time: 0.04363025678321719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.73543002968654,
    "estimated_duration": 3600.136029667514,
    "input_throughput": 7262.317252610763,
    "output_throughput": 6448.18107113134,
    "total_throughput": 13710.498323742104,
    "itl": 133.92881197486847,
    "ttft": 1727687.6875375265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 375251,
    "finished_requests": 106081,
    "scheduler_time": 65.89636065568538
}
#Debug simulation 
Total elapsed time: 7.735565459821373. Arrivals time: 0.32495273603126407 Scheduler time: 7.297786463052034 Scheduler overhead time: 0.042099534533917904 Adapter cache time: 0.00818715337663889 Engine time: 0.04324200144037604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.677729615941644,
    "estimated_duration": 3600.136371803284,
    "input_throughput": 7262.3165624428775,
    "output_throughput": 6448.180458334165,
    "total_throughput": 13710.497020777044,
    "itl": 133.928805795292,
    "ttft": 1727687.8741305391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 375251,
    "finished_requests": 106081,
    "scheduler_time": 65.89635692696785
}
#Debug simulation 
Total elapsed time: 7.677828062791377. Arrivals time: 0.3554164175875485 Scheduler time: 7.209784291218966 Scheduler overhead time: 0.041779063642024994 Adapter cache time: 0.00815900368615985 Engine time: 0.04336168896406889 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.736817322205752,
    "estimated_duration": 3600.0914714067712,
    "input_throughput": 7262.325195804975,
    "output_throughput": 6448.260324599134,
    "total_throughput": 13710.585520404109,
    "itl": 133.9292742273991,
    "ttft": 1727650.690586652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 375251,
    "finished_requests": 106080,
    "scheduler_time": 65.89550107848899
}
#Debug simulation 
Total elapsed time: 7.736929594073445. Arrivals time: 0.32983150938525796 Scheduler time: 7.294443588703871 Scheduler overhead time: 0.04181531956419349 Adapter cache time: 0.00812818855047226 Engine time: 0.04326193081215024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.758941309060901,
    "estimated_duration": 3600.1473661417826,
    "input_throughput": 7262.294384360024,
    "output_throughput": 6448.160766507291,
    "total_throughput": 13710.455150867314,
    "itl": 133.92892892229318,
    "ttft": 1727689.879948731,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 375251,
    "finished_requests": 106081,
    "scheduler_time": 65.89657088893952
}
#Debug simulation 
Total elapsed time: 7.759062950965017. Arrivals time: 0.34270475478842854 Scheduler time: 7.30301807820797 Scheduler overhead time: 0.04242618242278695 Adapter cache time: 0.008200179319828749 Engine time: 0.043220385909080505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.735872358083725,
    "estimated_duration": 3600.073183229901,
    "input_throughput": 7262.362088023802,
    "output_throughput": 6448.293081412487,
    "total_throughput": 13710.65516943629,
    "itl": 133.92935394662626,
    "ttft": 1727642.896376599,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 375251,
    "finished_requests": 106080,
    "scheduler_time": 65.89544907245988
}
#Debug simulation 
Total elapsed time: 7.735970098059624. Arrivals time: 0.32297468185424805 Scheduler time: 7.299574815202504 Scheduler overhead time: 0.04201299790292978 Adapter cache time: 0.00816177437081933 Engine time: 0.043858985882252455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 17280, 34560, 34560, 66, 34560, 66, 17280, 66, 34560, 17280, 34560, 66, 17280, 34560, 34560, 34560, 66, 34560, 17280, 17280, 66, 66, 66, 34560, 66, 66, 66, 34560, 17280, 66, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 66, 17280, 17280, 34560, 34560, 17280, 34560, 66, 34560, 17280, 17280, 17280, 34560, 17280, 66, 66, 17280, 17280, 66, 17280, 66, 66, 66, 34560, 17280]
Prompts retrieved: 1124586 . Total input tokens: 250714822 . Total output tokens: 225227815
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.736925536300987,
    "estimated_duration": 3600.1465362847775,
    "input_throughput": 7262.296058365737,
    "output_throughput": 6448.162252849951,
    "total_throughput": 13710.458311215689,
    "itl": 133.92870957647483,
    "ttft": 1727703.6155860254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 375251,
    "finished_requests": 106081,
    "scheduler_time": 65.89638904231387
}
#Debug simulation 
Total elapsed time: 7.737023165915161. Arrivals time: 0.3571002376265824 Scheduler time: 7.266609278041869 Scheduler overhead time: 0.04203956061974168 Adapter cache time: 0.008198574185371399 Engine time: 0.04365174565464258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.714246187824756,
    "estimated_duration": 3600.0569057362895,
    "input_throughput": 7304.270651416812,
    "output_throughput": 6477.549830627093,
    "total_throughput": 13781.820482043906,
    "itl": 133.21209946112916,
    "ttft": 1721791.5873927928,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 375015,
    "finished_requests": 106505,
    "scheduler_time": 66.17146745092799
}
#Debug simulation 
Total elapsed time: 7.714344492182136. Arrivals time: 0.3256938583217561 Scheduler time: 7.276495695579797 Scheduler overhead time: 0.04196296073496342 Adapter cache time: 0.00736510893329978 Engine time: 0.04331290954723954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.743404536973685,
    "estimated_duration": 3600.0949515812795,
    "input_throughput": 7304.193459800284,
    "output_throughput": 6477.481375805739,
    "total_throughput": 13781.674835606022,
    "itl": 133.2122153833961,
    "ttft": 1721811.560561758,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 375015,
    "finished_requests": 106505,
    "scheduler_time": 66.17205920140246
}
#Debug simulation 
Total elapsed time: 7.743515461683273. Arrivals time: 0.3294841591268778 Scheduler time: 7.300940684508532 Scheduler overhead time: 0.04215472936630249 Adapter cache time: 0.0075017414055764675 Engine time: 0.043895365204662085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.748434003908187,
    "estimated_duration": 3600.095302088456,
    "input_throughput": 7304.192748660158,
    "output_throughput": 6477.480745154738,
    "total_throughput": 13781.673493814895,
    "itl": 133.21221917451766,
    "ttft": 1721811.8292839131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 375015,
    "finished_requests": 106505,
    "scheduler_time": 66.17205567755951
}
#Debug simulation 
Total elapsed time: 7.748556053265929. Arrivals time: 0.32650219183415174 Scheduler time: 7.308971910271794 Scheduler overhead time: 0.04228569660335779 Adapter cache time: 0.007482301443815231 Engine time: 0.04378719674423337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.720956437755376,
    "estimated_duration": 3600.061859737034,
    "input_throughput": 7304.260600100014,
    "output_throughput": 6477.540916950626,
    "total_throughput": 13781.801517050639,
    "itl": 133.21213147209295,
    "ttft": 1721795.0947283534,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 375015,
    "finished_requests": 106505,
    "scheduler_time": 66.17143532659223
}
#Debug simulation 
Total elapsed time: 7.7210578648373485. Arrivals time: 0.3224826045334339 Scheduler time: 7.28609260590747 Scheduler overhead time: 0.04210219718515873 Adapter cache time: 0.007442108355462551 Engine time: 0.04348448617383838 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.743179430719465,
    "estimated_duration": 3600.1012172552764,
    "input_throughput": 7304.180747464638,
    "output_throughput": 6477.470102293087,
    "total_throughput": 13781.650849757725,
    "itl": 133.21212216709657,
    "ttft": 1721814.0160465636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 375015,
    "finished_requests": 106505,
    "scheduler_time": 66.1720697611985
}
#Debug simulation 
Total elapsed time: 7.743303961120546. Arrivals time: 0.3273470224812627 Scheduler time: 7.303736794739962 Scheduler overhead time: 0.04194357478991151 Adapter cache time: 0.007506194990128279 Engine time: 0.04323562979698181 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.725243684835732,
    "estimated_duration": 3600.049592966333,
    "input_throughput": 7304.285488559911,
    "output_throughput": 6477.562988454665,
    "total_throughput": 13781.848477014575,
    "itl": 133.21212755501418,
    "ttft": 1721788.822199677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 375015,
    "finished_requests": 106505,
    "scheduler_time": 66.17146259835437
}
#Debug simulation 
Total elapsed time: 7.725414462853223. Arrivals time: 0.34189294977113605 Scheduler time: 7.2679951610043645 Scheduler overhead time: 0.04227753868326545 Adapter cache time: 0.0074062589555978775 Engine time: 0.04626430198550224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-1.6-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 17280, 34560, 34560, 33, 34560, 33, 17280, 33, 34560, 17280, 34560, 33, 17280, 34560, 34560, 34560, 33, 34560, 17280, 17280, 33, 33, 33, 34560, 33, 33, 33, 34560, 17280, 33, 34560, 34560, 34560, 34560, 17280, 17280, 17280, 34560, 33, 17280, 17280, 34560, 34560, 17280, 34560, 33, 34560, 17280, 17280, 17280, 34560, 17280, 33, 33, 17280, 17280, 33, 17280, 33, 33, 33, 34560, 17280]
Prompts retrieved: 1123893 . Total input tokens: 250557472 . Total output tokens: 225084752
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.7510154559277,
    "estimated_duration": 3600.1088238935677,
    "input_throughput": 7304.165314525337,
    "output_throughput": 6477.4564161034405,
    "total_throughput": 13781.621730628776,
    "itl": 133.2122164035473,
    "ttft": 1721817.2898877899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 375015,
    "finished_requests": 106505,
    "scheduler_time": 66.17215619480962
}
#Debug simulation 
Total elapsed time: 7.751114876009524. Arrivals time: 0.33215442579239607 Scheduler time: 7.306187354028225 Scheduler overhead time: 0.04215407045558095 Adapter cache time: 0.007498498074710369 Engine time: 0.04362739669159055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.895203482825309,
    "estimated_duration": 3600.0984872741687,
    "input_throughput": 6486.106722508037,
    "output_throughput": 5723.599805071324,
    "total_throughput": 12209.706527579361,
    "itl": 150.36659481568424,
    "ttft": 1788969.4882140176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 344340,
    "finished_requests": 94351,
    "scheduler_time": 58.48530553974825
}
#Debug simulation 
Total elapsed time: 6.895305194891989. Arrivals time: 0.32581926928833127 Scheduler time: 6.467544776853174 Scheduler overhead time: 0.03744639968499541 Adapter cache time: 0.008204804733395576 Engine time: 0.038960233330726624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.876390991732478,
    "estimated_duration": 3600.1624899513877,
    "input_throughput": 6486.108909021855,
    "output_throughput": 5723.582215390016,
    "total_throughput": 12209.691124411871,
    "itl": 150.3656907510961,
    "ttft": 1789014.6414458475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 344340,
    "finished_requests": 94353,
    "scheduler_time": 58.48639017722254
}
#Debug simulation 
Total elapsed time: 6.876485347747803. Arrivals time: 0.29425843665376306 Scheduler time: 6.480544477701187 Scheduler overhead time: 0.03729047626256943 Adapter cache time: 0.008189911488443613 Engine time: 0.03885464323684573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.931578633375466,
    "estimated_duration": 3600.01404514986,
    "input_throughput": 6485.93886222686,
    "output_throughput": 5723.639891838881,
    "total_throughput": 12209.578754065742,
    "itl": 150.3674080638555,
    "ttft": 1788932.2993403804,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 344340,
    "finished_requests": 94347,
    "scheduler_time": 58.48407012331122
}
#Debug simulation 
Total elapsed time: 6.9316938812844455. Arrivals time: 0.29685537656769156 Scheduler time: 6.533172964584082 Scheduler overhead time: 0.03753628861159086 Adapter cache time: 0.008147287648171186 Engine time: 0.03859121724963188 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 6.894877463113517,
    "estimated_duration": 3600.1177361109826,
    "input_throughput": 6486.189261472571,
    "output_throughput": 5723.652810938173,
    "total_throughput": 12209.842072410745,
    "itl": 150.36710468351478,
    "ttft": 1788967.0597177828,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.200352315879427,
    "arrivals": 344340,
    "finished_requests": 94352,
    "scheduler_time": 58.485504367499765
}
#Debug simulation 
Total elapsed time: 6.894998619798571. Arrivals time: 0.3231719648465514 Scheduler time: 6.470194850582629 Scheduler overhead time: 0.03740492230281234 Adapter cache time: 0.008169766515493393 Engine time: 0.03867762768641114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 6.899934318847954,
    "estimated_duration": 3600.0316891898697,
    "input_throughput": 6485.90707412757,
    "output_throughput": 5723.61183982713,
    "total_throughput": 12209.5189139547,
    "itl": 150.36725433754341,
    "ttft": 1788946.5007662836,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 344340,
    "finished_requests": 94347,
    "scheduler_time": 58.48427373340663
}
#Debug simulation 
Total elapsed time: 6.900036802049726. Arrivals time: 0.29203905956819654 Scheduler time: 6.506327961105853 Scheduler overhead time: 0.03742472268640995 Adapter cache time: 0.008191382046788931 Engine time: 0.038648590445518494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 6.939276054035872,
    "estimated_duration": 3600.0957606232278,
    "input_throughput": 6486.111634974308,
    "output_throughput": 5723.604140027901,
    "total_throughput": 12209.715775002209,
    "itl": 150.3668224288442,
    "ttft": 1788966.9834225432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 344340,
    "finished_requests": 94351,
    "scheduler_time": 58.48537307226153
}
#Debug simulation 
Total elapsed time: 6.939388914965093. Arrivals time: 0.3013386274687946 Scheduler time: 6.5360692222602665 Scheduler overhead time: 0.03754604794085026 Adapter cache time: 0.008239576127380133 Engine time: 0.038645653054118156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [4320, 8640, 34560, 34560, 4320, 34560, 4320, 8640, 4320, 34560, 8640, 34560, 4320, 8640, 34560, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 4320, 8640, 8640, 34560, 34560, 8640, 34560, 4320, 34560, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 4320, 34560, 8640]
Prompts retrieved: 1032480 . Total input tokens: 230145331 . Total output tokens: 206814260
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 6.875195165164769,
    "estimated_duration": 3600.0687180944687,
    "input_throughput": 6485.940916249833,
    "output_throughput": 5723.611023432497,
    "total_throughput": 12209.55193968233,
    "itl": 150.36768629977226,
    "ttft": 1788953.388966561,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 344340,
    "finished_requests": 94348,
    "scheduler_time": 58.48489048165121
}
#Debug simulation 
Total elapsed time: 6.875293958000839. Arrivals time: 0.3233193578198552 Scheduler time: 6.450396031606942 Scheduler overhead time: 0.03742730664089322 Adapter cache time: 0.008163309190422297 Engine time: 0.03868134878575802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.161173023283482,
    "estimated_duration": 3600.1620002232253,
    "input_throughput": 6751.229527585969,
    "output_throughput": 5962.158646935637,
    "total_throughput": 12713.388174521606,
    "itl": 144.40958539818726,
    "ttft": 1727738.913548645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 321710,
    "finished_requests": 98032,
    "scheduler_time": 61.087691426706506
}
#Debug simulation 
Total elapsed time: 7.161271742079407. Arrivals time: 0.30273493472486734 Scheduler time: 6.747022047173232 Scheduler overhead time: 0.03898576274514198 Adapter cache time: 0.014022805728018284 Engine time: 0.040396932046860456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.158939888235182,
    "estimated_duration": 3600.081117589371,
    "input_throughput": 6751.068991543813,
    "output_throughput": 5962.142601490189,
    "total_throughput": 12713.211593034002,
    "itl": 144.41042980635575,
    "ttft": 1727714.9065844682,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 321710,
    "finished_requests": 98029,
    "scheduler_time": 61.08663284930452
}
#Debug simulation 
Total elapsed time: 7.1590661709196866. Arrivals time: 0.30366347497329116 Scheduler time: 6.743563154712319 Scheduler overhead time: 0.0391067243181169 Adapter cache time: 0.01400750782340765 Engine time: 0.040609344374388456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.115696030668914,
    "estimated_duration": 3600.081813736162,
    "input_throughput": 6751.067686091533,
    "output_throughput": 5962.141448592379,
    "total_throughput": 12713.209134683912,
    "itl": 144.4103709684061,
    "ttft": 1727715.4489103728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860526,
    "arrivals": 321710,
    "finished_requests": 98029,
    "scheduler_time": 61.086638099942434
}
#Debug simulation 
Total elapsed time: 7.115794545970857. Arrivals time: 0.2996176672168076 Scheduler time: 6.705585942137986 Scheduler overhead time: 0.038734265603125095 Adapter cache time: 0.013704877346754074 Engine time: 0.04015532461926341 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.203014511149377,
    "estimated_duration": 3600.005159708872,
    "input_throughput": 6751.148379455503,
    "output_throughput": 5962.21895463499,
    "total_throughput": 12713.367334090493,
    "itl": 144.40953773669682,
    "ttft": 1727674.5278016692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 321710,
    "finished_requests": 98028,
    "scheduler_time": 61.08509373271938
}
#Debug simulation 
Total elapsed time: 7.203117813915014. Arrivals time: 0.3193559111095965 Scheduler time: 6.772103664465249 Scheduler overhead time: 0.039133650716394186 Adapter cache time: 0.013855856843292713 Engine time: 0.04045574553310871 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.155100754927844,
    "estimated_duration": 3600.087131658805,
    "input_throughput": 6751.057713650756,
    "output_throughput": 5962.132641525814,
    "total_throughput": 12713.19035517657,
    "itl": 144.41043143689876,
    "ttft": 1727719.108438394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 321710,
    "finished_requests": 98029,
    "scheduler_time": 61.08663475169572
}
#Debug simulation 
Total elapsed time: 7.155282082036138. Arrivals time: 0.3163391542620957 Scheduler time: 6.727996214758605 Scheduler overhead time: 0.03891143575310707 Adapter cache time: 0.013882765080779791 Engine time: 0.040056414902210236 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.406297031790018,
    "estimated_duration": 3600.149751740409,
    "input_throughput": 6751.244441498599,
    "output_throughput": 5962.126433663894,
    "total_throughput": 12713.370875162494,
    "itl": 144.40938749968066,
    "ttft": 1727742.7413195786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 321710,
    "finished_requests": 98031,
    "scheduler_time": 61.08768657135431
}
#Debug simulation 
Total elapsed time: 7.406359736807644. Arrivals time: 0.30283478181809187 Scheduler time: 6.99178804596886 Scheduler overhead time: 0.0391494520008564 Adapter cache time: 0.01415754109621048 Engine time: 0.040353898890316486 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 8640, 34560, 34560, 1080, 34560, 1080, 8640, 1080, 34560, 8640, 34560, 1080, 8640, 34560, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 1080, 8640, 8640, 34560, 34560, 8640, 34560, 1080, 34560, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 1080, 34560, 8640]
Prompts retrieved: 964440 . Total input tokens: 214938269 . Total output tokens: 193203261
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.194146387744695,
    "estimated_duration": 3600.0980640108182,
    "input_throughput": 6751.037212837146,
    "output_throughput": 5962.114536426556,
    "total_throughput": 12713.151749263701,
    "itl": 144.41034504438804,
    "ttft": 1727730.055752265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 321710,
    "finished_requests": 98029,
    "scheduler_time": 61.08690544940481
}
#Debug simulation 
Total elapsed time: 7.1942620179615915. Arrivals time: 0.3219685247167945 Scheduler time: 6.760410595685244 Scheduler overhead time: 0.03925897879526019 Adapter cache time: 0.013985756319016218 Engine time: 0.04048071522265673 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.38413455709815,
    "estimated_duration": 3600.1087833926126,
    "input_throughput": 6895.242753361223,
    "output_throughput": 6134.043254990081,
    "total_throughput": 13029.286008351302,
    "itl": 141.00934424444364,
    "ttft": 1707224.4138234248,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 317887,
    "finished_requests": 100487,
    "scheduler_time": 63.01470311979959
}
#Debug simulation 
Total elapsed time: 7.384260017890483. Arrivals time: 0.3034173580817878 Scheduler time: 6.967812064103782 Scheduler overhead time: 0.04001551913097501 Adapter cache time: 0.0132706044241786 Engine time: 0.0412292736582458 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.409688452258706,
    "estimated_duration": 3600.0286809891163,
    "input_throughput": 6894.6709038941235,
    "output_throughput": 6133.706688673672,
    "total_throughput": 13028.377592567796,
    "itl": 141.010004528648,
    "ttft": 1707259.1256803784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 317887,
    "finished_requests": 100480,
    "scheduler_time": 63.01357055925291
}
#Debug simulation 
Total elapsed time: 7.409786012023687. Arrivals time: 0.33168764086440206 Scheduler time: 6.96533478749916 Scheduler overhead time: 0.039918350987136364 Adapter cache time: 0.013321820180863142 Engine time: 0.04109028400853276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.401836551725864,
    "estimated_duration": 3600.0298170654696,
    "input_throughput": 6894.668728114206,
    "output_throughput": 6133.704753034391,
    "total_throughput": 13028.373481148597,
    "itl": 141.00996699535511,
    "ttft": 1707260.0084499032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 317887,
    "finished_requests": 100480,
    "scheduler_time": 63.013576044031495
}
#Debug simulation 
Total elapsed time: 7.401948142796755. Arrivals time: 0.3388330773450434 Scheduler time: 6.949736813083291 Scheduler overhead time: 0.03994005359709263 Adapter cache time: 0.013372744433581829 Engine time: 0.04156083008274436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.337158654816449,
    "estimated_duration": 3600.1224290686982,
    "input_throughput": 6895.216618069716,
    "output_throughput": 6134.020004900951,
    "total_throughput": 13029.236622970668,
    "itl": 141.00958000877873,
    "ttft": 1707231.278097186,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 317887,
    "finished_requests": 100487,
    "scheduler_time": 63.01471442299243
}
#Debug simulation 
Total elapsed time: 7.337282685097307. Arrivals time: 0.33270720206201077 Scheduler time: 6.891779203899205 Scheduler overhead time: 0.03981007309630513 Adapter cache time: 0.013374646659940481 Engine time: 0.0411276207305491 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.356599444989115,
    "estimated_duration": 3600.037460489536,
    "input_throughput": 6894.654089689617,
    "output_throughput": 6133.691730251423,
    "total_throughput": 13028.34581994104,
    "itl": 141.0099379248056,
    "ttft": 1707263.736822175,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 317887,
    "finished_requests": 100480,
    "scheduler_time": 63.013618599378
}
#Debug simulation 
Total elapsed time: 7.356698763091117. Arrivals time: 0.33178285090252757 Scheduler time: 6.912023373879492 Scheduler overhead time: 0.040000815875828266 Adapter cache time: 0.01321544824168086 Engine time: 0.041292074136435986 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.3780428329482675,
    "estimated_duration": 3600.0961409714982,
    "input_throughput": 6895.266967315284,
    "output_throughput": 6134.064795847582,
    "total_throughput": 13029.331763162865,
    "itl": 141.00924549194892,
    "ttft": 1707217.9558757904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 317887,
    "finished_requests": 100487,
    "scheduler_time": 63.014601695241076
}
#Debug simulation 
Total elapsed time: 7.378156390041113. Arrivals time: 0.3523993748240173 Scheduler time: 6.912719894200563 Scheduler overhead time: 0.0401582233607769 Adapter cache time: 0.013269748073071241 Engine time: 0.04117181897163391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 8640, 34560, 34560, 540, 34560, 540, 8640, 540, 34560, 8640, 34560, 540, 8640, 34560, 34560, 34560, 540, 34560, 8640, 8640, 540, 540, 540, 34560, 540, 540, 540, 34560, 8640, 540, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 540, 8640, 8640, 34560, 34560, 8640, 34560, 540, 34560, 8640, 8640, 8640, 34560, 8640, 540, 540, 8640, 8640, 540, 8640, 540, 540, 540, 34560, 8640]
Prompts retrieved: 953100 . Total input tokens: 212431605 . Total output tokens: 190895305
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.352093635126948,
    "estimated_duration": 3600.0455877057943,
    "input_throughput": 6894.6385247909375,
    "output_throughput": 6133.677883249228,
    "total_throughput": 13028.316408040166,
    "itl": 141.01039696539675,
    "ttft": 1707264.98436293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 317887,
    "finished_requests": 100480,
    "scheduler_time": 63.01363337960248
}
#Debug simulation 
Total elapsed time: 7.352222831919789. Arrivals time: 0.33236266346648335 Scheduler time: 6.90618055826053 Scheduler overhead time: 0.04013024875894189 Adapter cache time: 0.013493807520717382 Engine time: 0.04145619459450245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.577586781233549,
    "estimated_duration": 3600.0905652333913,
    "input_throughput": 7116.265698260045,
    "output_throughput": 6294.84164061046,
    "total_throughput": 13411.107338870504,
    "itl": 136.93015101356312,
    "ttft": 1676607.8071405452,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 315978,
    "finished_requests": 103513,
    "scheduler_time": 64.53475600363801
}
#Debug simulation 
Total elapsed time: 7.577686160802841. Arrivals time: 0.3432350275106728 Scheduler time: 7.11825032858178 Scheduler overhead time: 0.041092352475970984 Adapter cache time: 0.011526450514793396 Engine time: 0.0424880632199347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.501724562142044,
    "estimated_duration": 3600.0231378447993,
    "input_throughput": 7116.312873293665,
    "output_throughput": 6294.768153508727,
    "total_throughput": 13411.081026802392,
    "itl": 136.93064753087523,
    "ttft": 1676591.8263481178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 315978,
    "finished_requests": 103511,
    "scheduler_time": 64.53327344725501
}
#Debug simulation 
Total elapsed time: 7.501854605041444. Arrivals time: 0.31327438447624445 Scheduler time: 7.075121910311282 Scheduler overhead time: 0.04088967153802514 Adapter cache time: 0.011490597389638424 Engine time: 0.042205479461699724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.566267731133848,
    "estimated_duration": 3600.0242312852306,
    "input_throughput": 7116.310711845931,
    "output_throughput": 6294.766241589928,
    "total_throughput": 13411.076953435859,
    "itl": 136.9306560398521,
    "ttft": 1676592.6827121198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 315978,
    "finished_requests": 103511,
    "scheduler_time": 64.53328767724835
}
#Debug simulation 
Total elapsed time: 7.566366967745125. Arrivals time: 0.34135688515380025 Scheduler time: 7.110869186930358 Scheduler overhead time: 0.041088133584707975 Adapter cache time: 0.011624342761933804 Engine time: 0.042371591087430716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.592586562037468,
    "estimated_duration": 3600.116518841383,
    "input_throughput": 7116.214396373195,
    "output_throughput": 6294.796260453609,
    "total_throughput": 13411.010656826804,
    "itl": 136.93059988155508,
    "ttft": 1676625.8443076012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 315978,
    "finished_requests": 103513,
    "scheduler_time": 64.53512326747192
}
#Debug simulation 
Total elapsed time: 7.592756263911724. Arrivals time: 0.3199702282436192 Scheduler time: 7.158473466522992 Scheduler overhead time: 0.041088317055255175 Adapter cache time: 0.011438722256571054 Engine time: 0.04266787553206086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.559121946338564,
    "estimated_duration": 3600.022645975474,
    "input_throughput": 7116.313845591996,
    "output_throughput": 6294.769013559807,
    "total_throughput": 13411.082859151804,
    "itl": 136.93040757813165,
    "ttft": 1676592.589850511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 315978,
    "finished_requests": 103511,
    "scheduler_time": 64.53320269529256
}
#Debug simulation 
Total elapsed time: 7.559225275181234. Arrivals time: 0.3397577619180083 Scheduler time: 7.10542006092146 Scheduler overhead time: 0.041128771379590034 Adapter cache time: 0.011505926959216595 Engine time: 0.0425224588252604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.585238432046026,
    "estimated_duration": 3600.059651414915,
    "input_throughput": 7116.326805843621,
    "output_throughput": 6294.89569460141,
    "total_throughput": 13411.22250044503,
    "itl": 136.92962073964284,
    "ttft": 1676584.1168744913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 315978,
    "finished_requests": 103513,
    "scheduler_time": 64.5340565739177
}
#Debug simulation 
Total elapsed time: 7.585337513126433. Arrivals time: 0.3430063840933144 Scheduler time: 7.1276185368187726 Scheduler overhead time: 0.041494958102703094 Adapter cache time: 0.01150227664038539 Engine time: 0.042662831488996744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 8640, 34560, 34560, 270, 34560, 270, 8640, 270, 34560, 8640, 34560, 270, 8640, 34560, 34560, 34560, 270, 34560, 8640, 8640, 270, 270, 270, 34560, 270, 270, 270, 34560, 8640, 270, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 270, 8640, 8640, 34560, 34560, 8640, 34560, 270, 34560, 8640, 8640, 8640, 34560, 8640, 270, 270, 8640, 8640, 270, 8640, 270, 270, 270, 34560, 8640]
Prompts retrieved: 947430 . Total input tokens: 211148508 . Total output tokens: 189758457
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.586049261968583,
    "estimated_duration": 3600.0301117928393,
    "input_throughput": 7116.299087632247,
    "output_throughput": 6294.755959336828,
    "total_throughput": 13411.055046969075,
    "itl": 136.93043459533038,
    "ttft": 1676597.5843176795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 315978,
    "finished_requests": 103511,
    "scheduler_time": 64.53333071256083
}
#Debug simulation 
Total elapsed time: 7.586147396359593. Arrivals time: 0.3516561076976359 Scheduler time: 7.120148702524602 Scheduler overhead time: 0.041193741373717785 Adapter cache time: 0.011584562249481678 Engine time: 0.04244639910757542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.6064398512244225,
    "estimated_duration": 3600.006445026724,
    "input_throughput": 7219.765685671448,
    "output_throughput": 6384.343847981468,
    "total_throughput": 13604.109533652916,
    "itl": 134.94878340675143,
    "ttft": 1665621.8579769433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 314998,
    "finished_requests": 104912,
    "scheduler_time": 65.46422286604721
}
#Debug simulation 
Total elapsed time: 7.60656680027023. Arrivals time: 0.3152004424482584 Scheduler time: 7.177980867680162 Scheduler overhead time: 0.04167110053822398 Adapter cache time: 0.00956709822639823 Engine time: 0.04285452514886856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.621505009941757,
    "estimated_duration": 3600.0598804242795,
    "input_throughput": 7219.658523273465,
    "output_throughput": 6384.249085682234,
    "total_throughput": 13603.907608955698,
    "itl": 134.9488609319432,
    "ttft": 1665638.3535415865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667414,
    "arrivals": 314998,
    "finished_requests": 104912,
    "scheduler_time": 65.46502427918485
}
#Debug simulation 
Total elapsed time: 7.621605254244059. Arrivals time: 0.31759161362424493 Scheduler time: 7.190803578589112 Scheduler overhead time: 0.04144512675702572 Adapter cache time: 0.009600183926522732 Engine time: 0.0428516985848546 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.614197063725442,
    "estimated_duration": 3600.0602311363305,
    "input_throughput": 7219.657819945996,
    "output_throughput": 6384.2484637390035,
    "total_throughput": 13603.906283685,
    "itl": 134.948862235003,
    "ttft": 1665638.6359497083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 314998,
    "finished_requests": 104912,
    "scheduler_time": 65.46502096021648
}
#Debug simulation 
Total elapsed time: 7.6142933536320925. Arrivals time: 0.3158689853735268 Scheduler time: 7.18483244581148 Scheduler overhead time: 0.04158477019518614 Adapter cache time: 0.009604934602975845 Engine time: 0.04315577633678913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.597834012936801,
    "estimated_duration": 3600.0063306640263,
    "input_throughput": 7219.765915024345,
    "output_throughput": 6384.344050795218,
    "total_throughput": 13604.109965819564,
    "itl": 134.9486081344447,
    "ttft": 1665622.1634963672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942695,
    "arrivals": 314998,
    "finished_requests": 104912,
    "scheduler_time": 65.46407927827407
}
#Debug simulation 
Total elapsed time: 7.59795595286414. Arrivals time: 0.31837060768157244 Scheduler time: 7.166157747153193 Scheduler overhead time: 0.041638897731900215 Adapter cache time: 0.009573122952133417 Engine time: 0.04298273799940944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.605451517738402,
    "estimated_duration": 3600.0877353109145,
    "input_throughput": 7219.602662754362,
    "output_throughput": 6384.1996889598195,
    "total_throughput": 13603.802351714181,
    "itl": 134.9493491654539,
    "ttft": 1665641.146408689,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 314998,
    "finished_requests": 104912,
    "scheduler_time": 65.46540970074332
}
#Debug simulation 
Total elapsed time: 7.605551189742982. Arrivals time: 0.3190728514455259 Scheduler time: 7.172963727265596 Scheduler overhead time: 0.04143284633755684 Adapter cache time: 0.009595965500921011 Engine time: 0.04316991288214922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.633701822720468,
    "estimated_duration": 3600.1255957696862,
    "input_throughput": 7219.52673832848,
    "output_throughput": 6384.1325499885015,
    "total_throughput": 13603.659288316981,
    "itl": 134.94842456279878,
    "ttft": 1665649.1486010535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 314998,
    "finished_requests": 104912,
    "scheduler_time": 65.4662622712292
}
#Debug simulation 
Total elapsed time: 7.633822301868349. Arrivals time: 0.3241189825348556 Scheduler time: 7.196222614496946 Scheduler overhead time: 0.04166824044659734 Adapter cache time: 0.00960385799407959 Engine time: 0.0428528543561697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 8640, 34560, 34560, 135, 34560, 135, 8640, 135, 34560, 8640, 34560, 135, 8640, 34560, 34560, 34560, 135, 34560, 8640, 8640, 135, 135, 135, 34560, 135, 135, 135, 34560, 8640, 135, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 135, 8640, 8640, 34560, 34560, 8640, 34560, 135, 34560, 8640, 8640, 8640, 34560, 8640, 135, 135, 8640, 8640, 135, 8640, 135, 135, 135, 34560, 8640]
Prompts retrieved: 944595 . Total input tokens: 210544330 . Total output tokens: 189170588
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.626348989084363,
    "estimated_duration": 3600.0923352874734,
    "input_throughput": 7219.593437990128,
    "output_throughput": 6384.191531622123,
    "total_throughput": 13603.784969612252,
    "itl": 134.94944098584264,
    "ttft": 1665643.457837383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 314998,
    "finished_requests": 104912,
    "scheduler_time": 65.46546404515703
}
#Debug simulation 
Total elapsed time: 7.6264718431048095. Arrivals time: 0.3203750899992883 Scheduler time: 7.192513738293201 Scheduler overhead time: 0.041590225882828236 Adapter cache time: 0.00958371302112937 Engine time: 0.043071340303868055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.75095649715513,
    "estimated_duration": 3600.1343226194,
    "input_throughput": 7305.728798714204,
    "output_throughput": 6437.44008505154,
    "total_throughput": 13743.168883765744,
    "itl": 133.61396888779723,
    "ttft": 1653533.9851772515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 314533,
    "finished_requests": 105880,
    "scheduler_time": 65.97197088066468
}
#Debug simulation 
Total elapsed time: 7.751055602449924. Arrivals time: 0.33028338430449367 Scheduler time: 7.307880953419954 Scheduler overhead time: 0.04176716133952141 Adapter cache time: 0.00848734239116311 Engine time: 0.04318944178521633 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.708394129760563,
    "estimated_duration": 3600.0542064689375,
    "input_throughput": 7305.743883728083,
    "output_throughput": 6437.435847037144,
    "total_throughput": 13743.179730765229,
    "itl": 133.61429418909717,
    "ttft": 1653494.7188540378,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 314533,
    "finished_requests": 105877,
    "scheduler_time": 65.97036422888446
}
#Debug simulation 
Total elapsed time: 7.708519422914833. Arrivals time: 0.3332345592789352 Scheduler time: 7.26209542574361 Scheduler overhead time: 0.042043499648571014 Adapter cache time: 0.008407102432101965 Engine time: 0.0433677863329649 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.708549276925623,
    "estimated_duration": 3600.0548535466633,
    "input_throughput": 7305.742570585832,
    "output_throughput": 6437.434689965512,
    "total_throughput": 13743.177260551343,
    "itl": 133.6142629459031,
    "ttft": 1653495.226453057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 314533,
    "finished_requests": 105877,
    "scheduler_time": 65.97035886117021
}
#Debug simulation 
Total elapsed time: 7.708728163037449. Arrivals time: 0.3200558815151453 Scheduler time: 7.275270013138652 Scheduler overhead time: 0.04203763836994767 Adapter cache time: 0.008464338723570108 Engine time: 0.043421799317002296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.727008922025561,
    "estimated_duration": 3600.1417641234107,
    "input_throughput": 7305.713697750486,
    "output_throughput": 6437.4267788432435,
    "total_throughput": 13743.14047659373,
    "itl": 133.6140026527824,
    "ttft": 1653539.1075803954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 314533,
    "finished_requests": 105880,
    "scheduler_time": 65.97197809807457
}
#Debug simulation 
Total elapsed time: 7.727106384932995. Arrivals time: 0.3257194780744612 Scheduler time: 7.287466802634299 Scheduler overhead time: 0.04224251303821802 Adapter cache time: 0.008486680220812559 Engine time: 0.043648031540215015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.706739998888224,
    "estimated_duration": 3600.0594320657733,
    "input_throughput": 7305.733279216452,
    "output_throughput": 6437.426502901297,
    "total_throughput": 13743.15978211775,
    "itl": 133.61424791470515,
    "ttft": 1653497.1253482774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 314533,
    "finished_requests": 105877,
    "scheduler_time": 65.97035585828048
}
#Debug simulation 
Total elapsed time: 7.706868615932763. Arrivals time: 0.33350020181387663 Scheduler time: 7.2600979707203805 Scheduler overhead time: 0.04204882262274623 Adapter cache time: 0.008433515671640635 Engine time: 0.043326495215296745 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.715774109121412,
    "estimated_duration": 3600.12061712177,
    "input_throughput": 7305.756611295887,
    "output_throughput": 6437.464592097057,
    "total_throughput": 13743.221203392945,
    "itl": 133.61390217797253,
    "ttft": 1653528.0399880267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 314533,
    "finished_requests": 105880,
    "scheduler_time": 65.97191593920829
}
#Debug simulation 
Total elapsed time: 7.715871102176607. Arrivals time: 0.3226628038100898 Scheduler time: 7.280029144603759 Scheduler overhead time: 0.04200077475979924 Adapter cache time: 0.008336168713867664 Engine time: 0.04343695193529129 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 8640, 34560, 34560, 66, 34560, 66, 8640, 66, 34560, 8640, 34560, 66, 8640, 34560, 34560, 34560, 66, 34560, 8640, 8640, 66, 66, 66, 34560, 66, 66, 66, 34560, 8640, 66, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 66, 8640, 8640, 34560, 34560, 8640, 34560, 66, 34560, 8640, 8640, 8640, 34560, 8640, 66, 66, 8640, 8640, 66, 8640, 66, 66, 66, 34560, 8640]
Prompts retrieved: 943146 . Total input tokens: 210242948 . Total output tokens: 188867081
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.716275840997696,
    "estimated_duration": 3600.0634173282165,
    "input_throughput": 7305.725191785459,
    "output_throughput": 6437.4193766840335,
    "total_throughput": 13743.144568469494,
    "itl": 133.61428481082115,
    "ttft": 1653543.1904375723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 314533,
    "finished_requests": 105877,
    "scheduler_time": 65.97032445978456
}
#Debug simulation 
Total elapsed time: 7.716383711900562. Arrivals time: 0.3216951102949679 Scheduler time: 7.281523151323199 Scheduler overhead time: 0.04202592559158802 Adapter cache time: 0.008451757486909628 Engine time: 0.0432130740955472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.747453284915537,
    "estimated_duration": 3600.1112101624567,
    "input_throughput": 7324.782891584629,
    "output_throughput": 6463.588384246037,
    "total_throughput": 13788.371275830666,
    "itl": 133.12788641166387,
    "ttft": 1653087.5077469575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 314275,
    "finished_requests": 106065,
    "scheduler_time": 66.29549245604085
}
#Debug simulation 
Total elapsed time: 7.747578785289079. Arrivals time: 0.31571853160858154 Scheduler time: 7.319156507961452 Scheduler overhead time: 0.042186744045466185 Adapter cache time: 0.007794019300490618 Engine time: 0.043234326876699924 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.713535548187792,
    "estimated_duration": 3600.0504346242133,
    "input_throughput": 7324.870159146142,
    "output_throughput": 6463.581947687055,
    "total_throughput": 13788.452106833198,
    "itl": 133.1286741563889,
    "ttft": 1653090.4638879201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 314275,
    "finished_requests": 106064,
    "scheduler_time": 66.29470388285506
}
#Debug simulation 
Total elapsed time: 7.713636368047446. Arrivals time: 0.3401224254630506 Scheduler time: 7.261013587471098 Scheduler overhead time: 0.04232130106538534 Adapter cache time: 0.007874543312937021 Engine time: 0.0429673264734447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.701170270331204,
    "estimated_duration": 3600.050784276607,
    "input_throughput": 7324.869447723294,
    "output_throughput": 6463.581319916216,
    "total_throughput": 13788.45076763951,
    "itl": 133.12867331098093,
    "ttft": 1653090.7348028673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 314275,
    "finished_requests": 106064,
    "scheduler_time": 66.29470113753557
}
#Debug simulation 
Total elapsed time: 7.701267030090094. Arrivals time: 0.3482402553781867 Scheduler time: 7.24050102988258 Scheduler overhead time: 0.04219964286312461 Adapter cache time: 0.007870097178965807 Engine time: 0.04306984506547451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.661367373075336,
    "estimated_duration": 3600.112124373699,
    "input_throughput": 7324.7810315317665,
    "output_throughput": 6463.586742884613,
    "total_throughput": 13788.36777441638,
    "itl": 133.12763510464927,
    "ttft": 1653088.1289803886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 314275,
    "finished_requests": 106065,
    "scheduler_time": 66.29540565361908
}
#Debug simulation 
Total elapsed time: 7.6614643251523376. Arrivals time: 0.30806124256923795 Scheduler time: 7.242191179189831 Scheduler overhead time: 0.04161935672163963 Adapter cache time: 0.007786271627992392 Engine time: 0.04247159697115421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.645557596348226,
    "estimated_duration": 3600.0548293391403,
    "input_throughput": 7324.86121741671,
    "output_throughput": 6463.5740573627645,
    "total_throughput": 13788.435274779475,
    "itl": 133.12869317303375,
    "ttft": 1653092.4630393537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 314275,
    "finished_requests": 106064,
    "scheduler_time": 66.29474886743064
}
#Debug simulation 
Total elapsed time: 7.645657609216869. Arrivals time: 0.32956146681681275 Scheduler time: 7.205077145714313 Scheduler overhead time: 0.04162681149318814 Adapter cache time: 0.007696223445236683 Engine time: 0.04249202320352197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.640601210761815,
    "estimated_duration": 3600.0985799134555,
    "input_throughput": 7324.808589167556,
    "output_throughput": 6463.611060494735,
    "total_throughput": 13788.419649662292,
    "itl": 133.12794148790834,
    "ttft": 1653082.3153327454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 314275,
    "finished_requests": 106065,
    "scheduler_time": 66.29536332074063
}
#Debug simulation 
Total elapsed time: 7.640705467201769. Arrivals time: 0.30624097026884556 Scheduler time: 7.22327192639932 Scheduler overhead time: 0.04154420644044876 Adapter cache time: 0.007720728404819965 Engine time: 0.04267614055424929 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 8.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 8640, 34560, 34560, 33, 34560, 33, 8640, 33, 34560, 8640, 34560, 33, 8640, 34560, 34560, 34560, 33, 34560, 8640, 8640, 33, 33, 33, 34560, 33, 33, 33, 34560, 8640, 33, 34560, 34560, 34560, 34560, 8640, 8640, 8640, 34560, 33, 8640, 8640, 34560, 34560, 8640, 34560, 33, 34560, 8640, 8640, 8640, 34560, 8640, 33, 33, 8640, 8640, 33, 8640, 33, 33, 33, 34560, 8640]
Prompts retrieved: 942453 . Total input tokens: 210093743 . Total output tokens: 188729713
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.626366863958538,
    "estimated_duration": 3600.0559604673667,
    "input_throughput": 7324.858915964352,
    "output_throughput": 6463.572026524594,
    "total_throughput": 13788.430942488945,
    "itl": 133.1285392384182,
    "ttft": 1653093.166544869,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 314275,
    "finished_requests": 106064,
    "scheduler_time": 66.2947355564309
}
#Debug simulation 
Total elapsed time: 7.626465772278607. Arrivals time: 0.3437833613716066 Scheduler time: 7.171674157958478 Scheduler overhead time: 0.04146187053993344 Adapter cache time: 0.007740096189081669 Engine time: 0.04259514110162854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.032162434887141,
    "estimated_duration": 3600.1205674658586,
    "input_throughput": 6657.897576155916,
    "output_throughput": 5936.720340186965,
    "total_throughput": 12594.61791634288,
    "itl": 145.84921667289115,
    "ttft": 1691100.0732105903,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 291201,
    "finished_requests": 97006,
    "scheduler_time": 61.175662145850474
}
#Debug simulation 
Total elapsed time: 7.032257457729429. Arrivals time: 0.2832042998634279 Scheduler time: 6.640204019844532 Scheduler overhead time: 0.0379175185225904 Adapter cache time: 0.014313471503555775 Engine time: 0.0389735228382051 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.0444940607994795,
    "estimated_duration": 3600.005173549446,
    "input_throughput": 6657.875431986735,
    "output_throughput": 5936.723690574011,
    "total_throughput": 12594.599122560745,
    "itl": 145.84805568879858,
    "ttft": 1691036.7729711675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 291201,
    "finished_requests": 97003,
    "scheduler_time": 61.173835013968876
}
#Debug simulation 
Total elapsed time: 7.044628633651882. Arrivals time: 0.2808723533526063 Scheduler time: 6.654636004939675 Scheduler overhead time: 0.0379135231487453 Adapter cache time: 0.014474106021225452 Engine time: 0.038996600080281496 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.015554953832179,
    "estimated_duration": 3600.0056416756174,
    "input_throughput": 6657.874566230943,
    "output_throughput": 5936.722918593073,
    "total_throughput": 12594.597484824017,
    "itl": 145.84800620706724,
    "ttft": 1691036.778750426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 291201,
    "finished_requests": 97003,
    "scheduler_time": 61.17389720800993
}
#Debug simulation 
Total elapsed time: 7.015651952009648. Arrivals time: 0.2812856938689947 Scheduler time: 6.625181767623872 Scheduler overhead time: 0.037990538869053125 Adapter cache time: 0.014516257215291262 Engine time: 0.03909731050953269 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.029966713860631,
    "estimated_duration": 3600.142528464736,
    "input_throughput": 6657.856962741296,
    "output_throughput": 5936.684125979416,
    "total_throughput": 12594.541088720713,
    "itl": 145.84815241677958,
    "ttft": 1691104.3021925993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 291201,
    "finished_requests": 97006,
    "scheduler_time": 61.17659835780424
}
#Debug simulation 
Total elapsed time: 7.030064773745835. Arrivals time: 0.28380600176751614 Scheduler time: 6.636756820604205 Scheduler overhead time: 0.03805403644219041 Adapter cache time: 0.014677195344120264 Engine time: 0.03908830462023616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.261708423029631,
    "estimated_duration": 3600.035856157057,
    "input_throughput": 6657.818687835409,
    "output_throughput": 5936.67309269922,
    "total_throughput": 12594.491780534629,
    "itl": 145.84873617036402,
    "ttft": 1691059.0555565008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 291201,
    "finished_requests": 97003,
    "scheduler_time": 61.17384563227303
}
#Debug simulation 
Total elapsed time: 7.261791334953159. Arrivals time: 0.2851264076307416 Scheduler time: 6.86752283712849 Scheduler overhead time: 0.038094610907137394 Adapter cache time: 0.01471693255007267 Engine time: 0.03845206182450056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.017433737870306,
    "estimated_duration": 3600.0972244558952,
    "input_throughput": 6657.765195111508,
    "output_throughput": 5936.649392359163,
    "total_throughput": 12594.41458747067,
    "itl": 145.84917170259152,
    "ttft": 1691091.993222882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 291201,
    "finished_requests": 97004,
    "scheduler_time": 61.17504278994177
}
#Debug simulation 
Total elapsed time: 7.017556613776833. Arrivals time: 0.28423892660066485 Scheduler time: 6.624682380817831 Scheduler overhead time: 0.03791655646637082 Adapter cache time: 0.0144203407689929 Engine time: 0.03867145534604788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.1_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 3.2]. Counts: [21 21 22]
Adapter prompts. [1080, 4320, 34560, 34560, 1080, 34560, 1080, 4320, 1080, 34560, 4320, 34560, 1080, 4320, 34560, 34560, 34560, 1080, 34560, 4320, 4320, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 4320, 1080, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 1080, 4320, 4320, 34560, 34560, 4320, 34560, 1080, 34560, 4320, 4320, 4320, 34560, 4320, 1080, 1080, 4320, 4320, 1080, 4320, 1080, 1080, 1080, 34560, 4320]
Prompts retrieved: 873720 . Total input tokens: 194777577 . Total output tokens: 174971506
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.032876443117857,
    "estimated_duration": 3600.042814849503,
    "input_throughput": 6657.805818623849,
    "output_throughput": 5936.6616174239725,
    "total_throughput": 12594.46743604782,
    "itl": 145.8486729101214,
    "ttft": 1691063.1734872125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 291201,
    "finished_requests": 97003,
    "scheduler_time": 61.173884148659816
}
#Debug simulation 
Total elapsed time: 7.032997345086187. Arrivals time: 0.2836900637485087 Scheduler time: 6.640227688010782 Scheduler overhead time: 0.038095979019999504 Adapter cache time: 0.01477054227143526 Engine time: 0.038652113638818264 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.283142887055874,
    "estimated_duration": 3600.035842446331,
    "input_throughput": 6933.152360794056,
    "output_throughput": 6132.99004962036,
    "total_throughput": 13066.142410414417,
    "itl": 140.50393790896845,
    "ttft": 1654206.0083809125,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 287382,
    "finished_requests": 100718,
    "scheduler_time": 63.09835411016695
}
#Debug simulation 
Total elapsed time: 7.283266950864345. Arrivals time: 0.31918192468583584 Scheduler time: 6.8520440249703825 Scheduler overhead time: 0.039486580062657595 Adapter cache time: 0.013993475120514631 Engine time: 0.04018500540405512 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.275409858673811,
    "estimated_duration": 3600.0842446017873,
    "input_throughput": 6933.1658106131,
    "output_throughput": 6132.908148776447,
    "total_throughput": 13066.073959389547,
    "itl": 140.50385845412023,
    "ttft": 1654216.3780194412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 287382,
    "finished_requests": 100719,
    "scheduler_time": 63.099288800140094
}
#Debug simulation 
Total elapsed time: 7.275526362005621. Arrivals time: 0.29182995762676 Scheduler time: 6.872016283683479 Scheduler overhead time: 0.039367716293781996 Adapter cache time: 0.013872974552214146 Engine time: 0.04012517910450697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.3102238602004945,
    "estimated_duration": 3600.0845946172644,
    "input_throughput": 6933.165136541345,
    "output_throughput": 6132.907552509133,
    "total_throughput": 13066.072689050477,
    "itl": 140.50386461124393,
    "ttft": 1654216.65317091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 287382,
    "finished_requests": 100719,
    "scheduler_time": 63.09928478459813
}
#Debug simulation 
Total elapsed time: 7.310317639261484. Arrivals time: 0.3031469127163291 Scheduler time: 6.894261367619038 Scheduler overhead time: 0.039729703683406115 Adapter cache time: 0.014026676770299673 Engine time: 0.040735308080911636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.2682895278558135,
    "estimated_duration": 3600.0522446494547,
    "input_throughput": 6933.227437767479,
    "output_throughput": 6132.962662643215,
    "total_throughput": 13066.190100410693,
    "itl": 140.50373064795002,
    "ttft": 1654215.724469728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 287382,
    "finished_requests": 100719,
    "scheduler_time": 63.09847746213094
}
#Debug simulation 
Total elapsed time: 7.268383433111012. Arrivals time: 0.3146622544154525 Scheduler time: 6.842352551873773 Scheduler overhead time: 0.03939958056434989 Adapter cache time: 0.013743309304118156 Engine time: 0.03997234255075455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.259387758094817,
    "estimated_duration": 3600.1096413190453,
    "input_throughput": 6933.116901088297,
    "output_throughput": 6132.864884612368,
    "total_throughput": 13065.981785700666,
    "itl": 140.50439439232804,
    "ttft": 1654237.7073977757,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 287382,
    "finished_requests": 100719,
    "scheduler_time": 63.099396897738345
}
#Debug simulation 
Total elapsed time: 7.259481228888035. Arrivals time: 0.3153409888036549 Scheduler time: 6.832468098029494 Scheduler overhead time: 0.039513752330094576 Adapter cache time: 0.013993358239531517 Engine time: 0.039896424394100904 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.507705134805292,
    "estimated_duration": 3600.0419225729697,
    "input_throughput": 6933.140651362537,
    "output_throughput": 6132.979691586488,
    "total_throughput": 13066.120342949025,
    "itl": 140.50472798291537,
    "ttft": 1654209.269630633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 287382,
    "finished_requests": 100718,
    "scheduler_time": 63.09829421966337
}
#Debug simulation 
Total elapsed time: 7.5077720996923745. Arrivals time: 0.5102779762819409 Scheduler time: 6.88519240450114 Scheduler overhead time: 0.03964623436331749 Adapter cache time: 0.014055696316063404 Engine time: 0.040267331060022116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 4320, 34560, 34560, 540, 34560, 540, 4320, 540, 34560, 4320, 34560, 540, 4320, 34560, 34560, 34560, 540, 34560, 4320, 4320, 540, 540, 540, 34560, 540, 540, 540, 34560, 4320, 540, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 540, 4320, 4320, 34560, 34560, 4320, 34560, 540, 34560, 4320, 4320, 4320, 34560, 4320, 540, 540, 4320, 4320, 540, 4320, 540, 540, 540, 34560, 4320]
Prompts retrieved: 862380 . Total input tokens: 192265937 . Total output tokens: 172713498
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.290188213810325,
    "estimated_duration": 3600.111350575852,
    "input_throughput": 6933.1136093908735,
    "output_throughput": 6132.861972857695,
    "total_throughput": 13065.975582248568,
    "itl": 140.50427835602127,
    "ttft": 1654240.0260693252,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 287382,
    "finished_requests": 100719,
    "scheduler_time": 63.09934446155215
}
#Debug simulation 
Total elapsed time: 7.290289180818945. Arrivals time: 0.31997201731428504 Scheduler time: 6.856952687725425 Scheduler overhead time: 0.039373706094920635 Adapter cache time: 0.01393720181658864 Engine time: 0.041706965770572424 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.4237054493278265,
    "estimated_duration": 3600.0049415576714,
    "input_throughput": 7061.893084235571,
    "output_throughput": 6281.205266961647,
    "total_throughput": 13343.098351197217,
    "itl": 137.7129728330598,
    "ttft": 1630414.3154498367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 285504,
    "finished_requests": 102820,
    "scheduler_time": 64.76951544845309
}
#Debug simulation 
Total elapsed time: 7.42384553514421. Arrivals time: 0.29697646806016564 Scheduler time: 7.01534223370254 Scheduler overhead time: 0.04014334362000227 Adapter cache time: 0.011856271885335445 Engine time: 0.04076289664953947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.405816533137113,
    "estimated_duration": 3600.0717810426677,
    "input_throughput": 7062.145853279772,
    "output_throughput": 6281.325033316604,
    "total_throughput": 13343.470886596377,
    "itl": 137.71349960942217,
    "ttft": 1630434.3614528093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 285504,
    "finished_requests": 102823,
    "scheduler_time": 64.77044035713006
}
#Debug simulation 
Total elapsed time: 7.405937360133976. Arrivals time: 0.2955097099766135 Scheduler time: 6.998875860590488 Scheduler overhead time: 0.0401820158585906 Adapter cache time: 0.011889001354575157 Engine time: 0.04080280149355531 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.577840812969953,
    "estimated_duration": 3600.071736144238,
    "input_throughput": 7062.1459413555895,
    "output_throughput": 6281.325111654384,
    "total_throughput": 13343.471053009975,
    "itl": 137.7131902563662,
    "ttft": 1630432.233299004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 285504,
    "finished_requests": 102823,
    "scheduler_time": 64.7704301660723
}
#Debug simulation 
Total elapsed time: 7.577914230059832. Arrivals time: 0.29791004629805684 Scheduler time: 7.168778097257018 Scheduler overhead time: 0.040150422137230635 Adapter cache time: 0.011833102442324162 Engine time: 0.040620983112603426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.399003506172448,
    "estimated_duration": 3600.0501354581343,
    "input_throughput": 7061.972484659486,
    "output_throughput": 6281.160580871296,
    "total_throughput": 13343.133065530781,
    "itl": 137.7138264509032,
    "ttft": 1630445.8750948717,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 285504,
    "finished_requests": 102822,
    "scheduler_time": 64.76994385266497
}
#Debug simulation 
Total elapsed time: 7.399093366228044. Arrivals time: 0.2983114686794579 Scheduler time: 6.989109098445624 Scheduler overhead time: 0.040141275618225336 Adapter cache time: 0.011985726654529572 Engine time: 0.04076241189613938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.444555518217385,
    "estimated_duration": 3600.0747748891563,
    "input_throughput": 7062.139980351601,
    "output_throughput": 6281.319809724298,
    "total_throughput": 13343.459790075898,
    "itl": 137.7130373466822,
    "ttft": 1630432.8995851048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 285504,
    "finished_requests": 102823,
    "scheduler_time": 64.77047354091714
}
#Debug simulation 
Total elapsed time: 7.444647944997996. Arrivals time: 0.2979626003652811 Scheduler time: 7.035134005360305 Scheduler overhead time: 0.039961997885257006 Adapter cache time: 0.011807716451585293 Engine time: 0.04108338709920645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.395858779083937,
    "estimated_duration": 3600.001826293616,
    "input_throughput": 7061.7283620029375,
    "output_throughput": 6281.172369093056,
    "total_throughput": 13342.900731095993,
    "itl": 137.71328622454578,
    "ttft": 1630428.4724972858,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 285504,
    "finished_requests": 102819,
    "scheduler_time": 64.76922466136118
}
#Debug simulation 
Total elapsed time: 7.395950216799974. Arrivals time: 0.2963811154477298 Scheduler time: 6.988170732744038 Scheduler overhead time: 0.040168965235352516 Adapter cache time: 0.011887104250490665 Engine time: 0.040758405812084675 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 4320, 34560, 34560, 270, 34560, 270, 4320, 270, 34560, 4320, 34560, 270, 4320, 34560, 34560, 34560, 270, 34560, 4320, 4320, 270, 270, 270, 34560, 270, 270, 270, 34560, 4320, 270, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 270, 4320, 4320, 34560, 34560, 4320, 34560, 270, 34560, 4320, 4320, 4320, 34560, 4320, 270, 270, 4320, 4320, 270, 4320, 270, 270, 270, 34560, 4320]
Prompts retrieved: 856710 . Total input tokens: 190987370 . Total output tokens: 171557076
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.458102995995432,
    "estimated_duration": 3600.101333691249,
    "input_throughput": 7062.08788126863,
    "output_throughput": 6281.273470937069,
    "total_throughput": 13343.361352205698,
    "itl": 137.7132367240695,
    "ttft": 1630449.7253514614,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 285504,
    "finished_requests": 102823,
    "scheduler_time": 64.77080759167372
}
#Debug simulation 
Total elapsed time: 7.458196242339909. Arrivals time: 0.30133201368153095 Scheduler time: 7.045211881399155 Scheduler overhead time: 0.04016908584162593 Adapter cache time: 0.011923572979867458 Engine time: 0.040851314552128315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.543294474016875,
    "estimated_duration": 3600.113634133489,
    "input_throughput": 7232.684477824045,
    "output_throughput": 6379.740845465873,
    "total_throughput": 13612.425323289917,
    "itl": 134.73202564163282,
    "ttft": 1607339.2070804732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 284495,
    "finished_requests": 105011,
    "scheduler_time": 65.73614188602649
}
#Debug simulation 
Total elapsed time: 7.543394310865551. Arrivals time: 0.30247638700529933 Scheduler time: 7.129362316336483 Scheduler overhead time: 0.04072552314028144 Adapter cache time: 0.010393162723630667 Engine time: 0.04133685352280736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.526821990031749,
    "estimated_duration": 3600.005391702799,
    "input_throughput": 7232.507501241406,
    "output_throughput": 6379.4554455200605,
    "total_throughput": 13611.962946761467,
    "itl": 134.7315879287719,
    "ttft": 1607312.101667497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 284495,
    "finished_requests": 105006,
    "scheduler_time": 65.73397267814057
}
#Debug simulation 
Total elapsed time: 7.526930799242109. Arrivals time: 0.3038443303667009 Scheduler time: 7.111046119593084 Scheduler overhead time: 0.04106406308710575 Adapter cache time: 0.010343324393033981 Engine time: 0.041665368247777224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.506194697227329,
    "estimated_duration": 3600.0058878550262,
    "input_throughput": 7232.506504458396,
    "output_throughput": 6379.4545663045465,
    "total_throughput": 13611.961070762942,
    "itl": 134.7315524758081,
    "ttft": 1607312.4472049128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 284495,
    "finished_requests": 105006,
    "scheduler_time": 65.73397188790821
}
#Debug simulation 
Total elapsed time: 7.506314100231975. Arrivals time: 0.30198609456419945 Scheduler time: 7.092500880826265 Scheduler overhead time: 0.040902236476540565 Adapter cache time: 0.010354060679674149 Engine time: 0.041620549745857716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.5243341736495495,
    "estimated_duration": 3600.1453378019382,
    "input_throughput": 7232.620785220229,
    "output_throughput": 6379.684664070518,
    "total_throughput": 13612.305449290747,
    "itl": 134.7321404744699,
    "ttft": 1607345.3747390138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 284495,
    "finished_requests": 105011,
    "scheduler_time": 65.73667639178564
}
#Debug simulation 
Total elapsed time: 7.524455117993057. Arrivals time: 0.3064755075611174 Scheduler time: 7.105381149798632 Scheduler overhead time: 0.04111263155937195 Adapter cache time: 0.010420129168778658 Engine time: 0.04202458681538701 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.531371375080198,
    "estimated_duration": 3600.003371220802,
    "input_throughput": 7232.582115935756,
    "output_throughput": 6379.601803598248,
    "total_throughput": 13612.183919534004,
    "itl": 134.73142593020432,
    "ttft": 1607330.3579009639,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 284495,
    "finished_requests": 105007,
    "scheduler_time": 65.73394836830514
}
#Debug simulation 
Total elapsed time: 7.531466226093471. Arrivals time: 0.3161856038495898 Scheduler time: 7.1034980695694685 Scheduler overhead time: 0.04079422540962696 Adapter cache time: 0.010329678654670715 Engine time: 0.041533710435032845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.50754373986274,
    "estimated_duration": 3600.038961799928,
    "input_throughput": 7232.654778542103,
    "output_throughput": 6379.632899449822,
    "total_throughput": 13612.287677991924,
    "itl": 134.7310853158005,
    "ttft": 1607336.1593703127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 284495,
    "finished_requests": 105008,
    "scheduler_time": 65.73447283214013
}
#Debug simulation 
Total elapsed time: 7.507632362656295. Arrivals time: 0.2996872030198574 Scheduler time: 7.096431736368686 Scheduler overhead time: 0.04083766462281346 Adapter cache time: 0.010278428439050913 Engine time: 0.04138426249846816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 4320, 34560, 34560, 135, 34560, 135, 4320, 135, 34560, 4320, 34560, 135, 4320, 34560, 34560, 34560, 135, 34560, 4320, 4320, 135, 135, 135, 34560, 135, 135, 135, 34560, 4320, 135, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 135, 4320, 4320, 34560, 34560, 4320, 34560, 135, 34560, 4320, 4320, 4320, 34560, 4320, 135, 135, 4320, 4320, 135, 4320, 135, 135, 135, 34560, 4320]
Prompts retrieved: 853875 . Total input tokens: 190361500 . Total output tokens: 170986527
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.483467801939696,
    "estimated_duration": 3600.02950659024,
    "input_throughput": 7232.529609086785,
    "output_throughput": 6379.555489186185,
    "total_throughput": 13612.08509827297,
    "itl": 134.73213800978834,
    "ttft": 1607336.601009563,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 284495,
    "finished_requests": 105007,
    "scheduler_time": 65.73416377915308
}
#Debug simulation 
Total elapsed time: 7.483594383113086. Arrivals time: 0.30635314574465156 Scheduler time: 7.0652724876999855 Scheduler overhead time: 0.04091846663504839 Adapter cache time: 0.010424231644719839 Engine time: 0.04164017038419843 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.6205098307691514,
    "estimated_duration": 3600.0459704655786,
    "input_throughput": 7281.7164600289225,
    "output_throughput": 6434.567555537132,
    "total_throughput": 13716.284015566054,
    "itl": 133.78699594393981,
    "ttft": 1598600.8116881356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 284045,
    "finished_requests": 105750,
    "scheduler_time": 66.29721590865492
}
#Debug simulation 
Total elapsed time: 7.620608090888709. Arrivals time: 0.32526890421286225 Scheduler time: 7.183801116421819 Scheduler overhead time: 0.041208629962056875 Adapter cache time: 0.009185123723000288 Engine time: 0.04190873866900802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.6068870248273015,
    "estimated_duration": 3600.0771707815616,
    "input_throughput": 7281.653352533257,
    "output_throughput": 6434.511789915612,
    "total_throughput": 13716.16514244887,
    "itl": 133.78653065391987,
    "ttft": 1598615.906770307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 284045,
    "finished_requests": 105750,
    "scheduler_time": 66.29781192942761
}
#Debug simulation 
Total elapsed time: 7.607006705831736. Arrivals time: 0.303156313020736 Scheduler time: 7.192607262637466 Scheduler overhead time: 0.041214986238628626 Adapter cache time: 0.009111393708735704 Engine time: 0.04177500493824482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.624947924166918,
    "estimated_duration": 3600.0775050319335,
    "input_throughput": 7281.652676465773,
    "output_throughput": 6434.511192501263,
    "total_throughput": 13716.163868967036,
    "itl": 133.78652894787243,
    "ttft": 1598616.1807369601,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 284045,
    "finished_requests": 105750,
    "scheduler_time": 66.29780684853777
}
#Debug simulation 
Total elapsed time: 7.625067963264883. Arrivals time: 0.30484147602692246 Scheduler time: 7.208788568619639 Scheduler overhead time: 0.04123276472091675 Adapter cache time: 0.009140418842434883 Engine time: 0.04182980442419648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.626236005220562,
    "estimated_duration": 3600.052051010785,
    "input_throughput": 7281.704161093939,
    "output_throughput": 6434.556687450129,
    "total_throughput": 13716.260848544067,
    "itl": 133.7870784253943,
    "ttft": 1598602.6499935805,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 284045,
    "finished_requests": 105750,
    "scheduler_time": 66.29718381944576
}
#Debug simulation 
Total elapsed time: 7.626329337246716. Arrivals time: 0.3291297792457044 Scheduler time: 7.185368231963366 Scheduler overhead time: 0.041415791027247906 Adapter cache time: 0.009133802261203527 Engine time: 0.042050291784107685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.627134062815458,
    "estimated_duration": 3600.085571525239,
    "input_throughput": 7281.76080239492,
    "output_throughput": 6434.550662690434,
    "total_throughput": 13716.311465085355,
    "itl": 133.78670905704445,
    "ttft": 1598609.9060950393,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 284045,
    "finished_requests": 105751,
    "scheduler_time": 66.2979448227111
}
#Debug simulation 
Total elapsed time: 7.627244715113193. Arrivals time: 0.30448259646072984 Scheduler time: 7.210850375704467 Scheduler overhead time: 0.04146861983463168 Adapter cache time: 0.009230806026607752 Engine time: 0.041991923935711384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.639406396076083,
    "estimated_duration": 3600.0342424909036,
    "input_throughput": 7281.740181966127,
    "output_throughput": 6434.588517683671,
    "total_throughput": 13716.328699649797,
    "itl": 133.78702757183808,
    "ttft": 1598583.0001915365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 284045,
    "finished_requests": 105750,
    "scheduler_time": 66.29715905309774
}
#Debug simulation 
Total elapsed time: 7.639502519741654. Arrivals time: 0.317462008446455 Scheduler time: 7.210063026752323 Scheduler overhead time: 0.04134004143998027 Adapter cache time: 0.009188420604914427 Engine time: 0.04213146585971117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 4320, 34560, 34560, 66, 34560, 66, 4320, 66, 34560, 4320, 34560, 66, 4320, 34560, 34560, 34560, 66, 34560, 4320, 4320, 66, 66, 66, 34560, 66, 66, 66, 34560, 4320, 66, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 66, 4320, 4320, 34560, 34560, 4320, 34560, 66, 34560, 4320, 4320, 4320, 34560, 4320, 66, 66, 4320, 4320, 66, 4320, 66, 66, 66, 34560, 4320]
Prompts retrieved: 852426 . Total input tokens: 190026204 . Total output tokens: 170712236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.848679925315082,
    "estimated_duration": 3600.0876304181766,
    "input_throughput": 7281.7566379502105,
    "output_throughput": 6434.546982765868,
    "total_throughput": 13716.303620716079,
    "itl": 133.78647255243325,
    "ttft": 1598609.634014623,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 284045,
    "finished_requests": 105751,
    "scheduler_time": 66.29791985727707
}
#Debug simulation 
Total elapsed time: 7.848747838288546. Arrivals time: 0.30977693665772676 Scheduler time: 7.427249895874411 Scheduler overhead time: 0.04133812431246042 Adapter cache time: 0.009234710596501827 Engine time: 0.04201072594150901 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.6593555230647326,
    "estimated_duration": 3600.063714388965,
    "input_throughput": 7294.200904012892,
    "output_throughput": 6485.7318793212,
    "total_throughput": 13779.932783334092,
    "itl": 133.3297492089666,
    "ttft": 1597491.6653633825,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 283764,
    "finished_requests": 106205,
    "scheduler_time": 66.90647114827969
}
#Debug simulation 
Total elapsed time: 7.659452697262168. Arrivals time: 0.308537888340652 Scheduler time: 7.239630620460957 Scheduler overhead time: 0.041596388444304466 Adapter cache time: 0.008395631797611713 Engine time: 0.04205533070489764 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.658901378978044,
    "estimated_duration": 3600.105032772372,
    "input_throughput": 7294.258017738333,
    "output_throughput": 6485.771605952015,
    "total_throughput": 13780.029623690349,
    "itl": 133.33039968388385,
    "ttft": 1597498.5466638159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 283764,
    "finished_requests": 106207,
    "scheduler_time": 66.9071512671172
}
#Debug simulation 
Total elapsed time: 7.65901322895661. Arrivals time: 0.3052411484532058 Scheduler time: 7.242198221385479 Scheduler overhead time: 0.041482866276055574 Adapter cache time: 0.008408891968429089 Engine time: 0.04237760929390788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.827877282164991,
    "estimated_duration": 3600.1090147438326,
    "input_throughput": 7294.249949780631,
    "output_throughput": 6485.764432236628,
    "total_throughput": 13780.01438201726,
    "itl": 133.33055201708817,
    "ttft": 1597498.9367163603,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 283764,
    "finished_requests": 106207,
    "scheduler_time": 66.90722727555402
}
#Debug simulation 
Total elapsed time: 7.8279457171447575. Arrivals time: 0.3058220911771059 Scheduler time: 7.411379195284098 Scheduler overhead time: 0.04124826053157449 Adapter cache time: 0.008398097474128008 Engine time: 0.041925554629415274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.652960655745119,
    "estimated_duration": 3600.0807303617717,
    "input_throughput": 7294.255314480446,
    "output_throughput": 6485.720390401815,
    "total_throughput": 13779.975704882261,
    "itl": 133.3301842259002,
    "ttft": 1597492.399268615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 283764,
    "finished_requests": 106206,
    "scheduler_time": 66.90689462967738
}
#Debug simulation 
Total elapsed time: 7.653055552858859. Arrivals time: 0.30963180819526315 Scheduler time: 7.231976532842964 Scheduler overhead time: 0.04156988766044378 Adapter cache time: 0.008392061106860638 Engine time: 0.04217720404267311 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.62667596898973,
    "estimated_duration": 3600.115718832831,
    "input_throughput": 7294.236366522576,
    "output_throughput": 6485.752354529862,
    "total_throughput": 13779.988721052438,
    "itl": 133.33020012966773,
    "ttft": 1597498.4280354732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 283764,
    "finished_requests": 106207,
    "scheduler_time": 66.90733508047916
}
#Debug simulation 
Total elapsed time: 7.626794418785721. Arrivals time: 0.31744582392275333 Scheduler time: 7.19834995502606 Scheduler overhead time: 0.04145359946414828 Adapter cache time: 0.008389727212488651 Engine time: 0.04200342018157244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.688496644608676,
    "estimated_duration": 3600.1372312533986,
    "input_throughput": 7294.192780217289,
    "output_throughput": 6485.713599275997,
    "total_throughput": 13779.906379493284,
    "itl": 133.32885955554283,
    "ttft": 1597495.5553014427,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 283764,
    "finished_requests": 106207,
    "scheduler_time": 66.90774860185341
}
#Debug simulation 
Total elapsed time: 7.688614872749895. Arrivals time: 0.32380498526617885 Scheduler time: 7.253587915562093 Scheduler overhead time: 0.04145666677504778 Adapter cache time: 0.008345116395503283 Engine time: 0.04212882276624441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.4-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 4.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 4320, 34560, 34560, 33, 34560, 33, 4320, 33, 34560, 4320, 34560, 33, 4320, 34560, 34560, 34560, 33, 34560, 4320, 4320, 33, 33, 33, 34560, 33, 33, 33, 34560, 4320, 33, 34560, 34560, 34560, 34560, 4320, 4320, 4320, 34560, 33, 4320, 4320, 34560, 34560, 4320, 34560, 33, 34560, 4320, 4320, 4320, 34560, 4320, 33, 33, 4320, 4320, 33, 4320, 33, 33, 33, 34560, 4320]
Prompts retrieved: 851733 . Total input tokens: 189862579 . Total output tokens: 170582154
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.6328464252874255,
    "estimated_duration": 3600.121088846036,
    "input_throughput": 7294.225486292538,
    "output_throughput": 6485.742680250878,
    "total_throughput": 13779.968166543416,
    "itl": 133.33024531331546,
    "ttft": 1597502.4488246606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 283764,
    "finished_requests": 106207,
    "scheduler_time": 66.90733205417158
}
#Debug simulation 
Total elapsed time: 7.632963405922055. Arrivals time: 0.3047831980511546 Scheduler time: 7.217178399208933 Scheduler overhead time: 0.0414534704759717 Adapter cache time: 0.008382386527955532 Engine time: 0.041945663280785084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.472421280108392,
    "estimated_duration": 3600.119687521988,
    "input_throughput": 7101.563619846695,
    "output_throughput": 6304.420955410633,
    "total_throughput": 13405.984575257327,
    "itl": 136.90823991330024,
    "ttft": 1581558.4322033909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 264567,
    "finished_requests": 103574,
    "scheduler_time": 65.21913633503985
}
#Debug simulation 
Total elapsed time: 7.47251690691337. Arrivals time: 0.302584835793823 Scheduler time: 7.049608510918915 Scheduler overhead time: 0.04058104893192649 Adapter cache time: 0.019619534723460674 Engine time: 0.041248347610235214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.703253661748022,
    "estimated_duration": 3600.0920375053224,
    "input_throughput": 7101.617884668372,
    "output_throughput": 6304.4049328604,
    "total_throughput": 13406.022817528772,
    "itl": 136.91061879836897,
    "ttft": 1581540.5859534352,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 264567,
    "finished_requests": 103573,
    "scheduler_time": 65.21815337658866
}
#Debug simulation 
Total elapsed time: 7.703340330161154. Arrivals time: 0.3116388376802206 Scheduler time: 7.269863144960254 Scheduler overhead time: 0.0411811675876379 Adapter cache time: 0.019789656158536673 Engine time: 0.04187915660440922 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.480707689188421,
    "estimated_duration": 3600.09238727495,
    "input_throughput": 7101.617194705456,
    "output_throughput": 6304.404320351295,
    "total_throughput": 13406.021515056751,
    "itl": 136.9106217088299,
    "ttft": 1581540.8664278972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 264567,
    "finished_requests": 103573,
    "scheduler_time": 65.21814911519719
}
#Debug simulation 
Total elapsed time: 7.480799263343215. Arrivals time: 0.31936061568558216 Scheduler time: 7.040727837942541 Scheduler overhead time: 0.040714103262871504 Adapter cache time: 0.01945339934900403 Engine time: 0.04166559362784028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.461219687014818,
    "estimated_duration": 3600.008265473278,
    "input_throughput": 7101.255917988603,
    "output_throughput": 6304.213581303085,
    "total_throughput": 13405.469499291688,
    "itl": 136.90864366678315,
    "ttft": 1581561.1638725093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 264567,
    "finished_requests": 103568,
    "scheduler_time": 65.21725646548896
}
#Debug simulation 
Total elapsed time: 7.461311660241336. Arrivals time: 0.3203349751420319 Scheduler time: 7.020634942688048 Scheduler overhead time: 0.04042864963412285 Adapter cache time: 0.01973361987620592 Engine time: 0.04132994916290045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.460231348872185,
    "estimated_duration": 3600.099827605181,
    "input_throughput": 7101.602517785473,
    "output_throughput": 6304.391291032026,
    "total_throughput": 13405.993808817499,
    "itl": 136.91077564536087,
    "ttft": 1581543.406835569,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 264567,
    "finished_requests": 103573,
    "scheduler_time": 65.21814134165959
}
#Debug simulation 
Total elapsed time: 7.460340551100671. Arrivals time: 0.3177855690009892 Scheduler time: 7.021731127984822 Scheduler overhead time: 0.04081461392343044 Adapter cache time: 0.01979732420295477 Engine time: 0.04135037772357464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.457774288021028,
    "estimated_duration": 3600.1127391371756,
    "input_throughput": 7101.57732619435,
    "output_throughput": 6304.433123235918,
    "total_throughput": 13406.010449430269,
    "itl": 136.9082896349607,
    "ttft": 1581555.8955195432,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 264567,
    "finished_requests": 103574,
    "scheduler_time": 65.21914451249017
}
#Debug simulation 
Total elapsed time: 7.457866721320897. Arrivals time: 0.2966378042474389 Scheduler time: 7.040603815577924 Scheduler overhead time: 0.04075863445177674 Adapter cache time: 0.01958315819501877 Engine time: 0.041376534383744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.05_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  3.2 ]. Counts: [21 21 22]
Adapter prompts. [540, 1080, 34560, 34560, 540, 34560, 540, 1080, 540, 34560, 1080, 34560, 540, 1080, 34560, 34560, 34560, 540, 34560, 1080, 1080, 540, 540, 540, 34560, 540, 540, 540, 34560, 1080, 540, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 540, 1080, 1080, 34560, 34560, 1080, 34560, 540, 34560, 1080, 1080, 1080, 34560, 1080, 540, 540, 1080, 1080, 540, 1080, 540, 540, 540, 34560, 1080]
Prompts retrieved: 794340 . Total input tokens: 177069654 . Total output tokens: 159061870
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.4444704172201455,
    "estimated_duration": 3600.102919526127,
    "input_throughput": 7101.596418628291,
    "output_throughput": 6304.385876553629,
    "total_throughput": 13405.98229518192,
    "itl": 136.91051320720254,
    "ttft": 1581544.998624795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 264567,
    "finished_requests": 103573,
    "scheduler_time": 65.21818856814672
}
#Debug simulation 
Total elapsed time: 7.444562275893986. Arrivals time: 0.3195319166406989 Scheduler time: 7.0049158982001245 Scheduler overhead time: 0.04058466525748372 Adapter cache time: 0.019480703864246607 Engine time: 0.04127614200115204 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.635935801081359,
    "estimated_duration": 3600.0613086808535,
    "input_throughput": 7337.653371708672,
    "output_throughput": 6494.6693945519,
    "total_throughput": 13832.322766260573,
    "itl": 132.6186112267152,
    "ttft": 1552651.9363855333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 262719,
    "finished_requests": 106642,
    "scheduler_time": 67.15835581049562
}
#Debug simulation 
Total elapsed time: 7.636053883936256. Arrivals time: 0.2954526892863214 Scheduler time: 7.218573184218258 Scheduler overhead time: 0.04161447659134865 Adapter cache time: 0.018691519740968943 Engine time: 0.04230171628296375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.662025468889624,
    "estimated_duration": 3600.1038876899597,
    "input_throughput": 7337.627697446869,
    "output_throughput": 6494.712022047521,
    "total_throughput": 13832.33971949439,
    "itl": 132.61929301423223,
    "ttft": 1552648.1950323714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 262719,
    "finished_requests": 106644,
    "scheduler_time": 67.15899567444261
}
#Debug simulation 
Total elapsed time: 7.662115016952157. Arrivals time: 0.3008787063881755 Scheduler time: 7.239046674687415 Scheduler overhead time: 0.04187282733619213 Adapter cache time: 0.018275028094649315 Engine time: 0.04254193417727947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.681942407973111,
    "estimated_duration": 3600.104221249449,
    "input_throughput": 7337.627017595621,
    "output_throughput": 6494.711420294713,
    "total_throughput": 13832.338437890334,
    "itl": 132.61929177961832,
    "ttft": 1552648.4657111368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 262719,
    "finished_requests": 106644,
    "scheduler_time": 67.15899153597589
}
#Debug simulation 
Total elapsed time: 7.682043931912631. Arrivals time: 0.3060748167335987 Scheduler time: 7.253922621719539 Scheduler overhead time: 0.04176991665735841 Adapter cache time: 0.018533818889409304 Engine time: 0.04242978757247329 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.673143774736673,
    "estimated_duration": 3600.072559540194,
    "input_throughput": 7337.630440252539,
    "output_throughput": 6494.649097568822,
    "total_throughput": 13832.279537821361,
    "itl": 132.6189632617087,
    "ttft": 1552673.1743683617,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 262719,
    "finished_requests": 106642,
    "scheduler_time": 67.15835997820442
}
#Debug simulation 
Total elapsed time: 7.673252935055643. Arrivals time: 0.3052079724147916 Scheduler time: 7.245877231936902 Scheduler overhead time: 0.04191080993041396 Adapter cache time: 0.01830971520394087 Engine time: 0.042527837213128805 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.661081505008042,
    "estimated_duration": 3600.103989107733,
    "input_throughput": 7337.6274907401,
    "output_throughput": 6494.7118390863525,
    "total_throughput": 13832.339329826453,
    "itl": 132.61898099708827,
    "ttft": 1552648.85111188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 262719,
    "finished_requests": 106644,
    "scheduler_time": 67.15894649868072
}
#Debug simulation 
Total elapsed time: 7.661224183160812. Arrivals time: 0.30920256255194545 Scheduler time: 7.2299481919035316 Scheduler overhead time: 0.041798063553869724 Adapter cache time: 0.018483056221157312 Engine time: 0.04235971439629793 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.660966082941741,
    "estimated_duration": 3600.0455655054147,
    "input_throughput": 7337.685459625961,
    "output_throughput": 6494.697796059002,
    "total_throughput": 13832.383255684963,
    "itl": 132.61846699144763,
    "ttft": 1552659.5209318153,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 262719,
    "finished_requests": 106642,
    "scheduler_time": 67.15788648129528
}
#Debug simulation 
Total elapsed time: 7.66105864290148. Arrivals time: 0.3031742530874908 Scheduler time: 7.235797873698175 Scheduler overhead time: 0.04168749460950494 Adapter cache time: 0.01843026652932167 Engine time: 0.04241099301725626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 1080, 34560, 34560, 270, 34560, 270, 1080, 270, 34560, 1080, 34560, 270, 1080, 34560, 34560, 34560, 270, 34560, 1080, 1080, 270, 270, 270, 34560, 270, 270, 270, 34560, 1080, 270, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 270, 1080, 1080, 34560, 34560, 1080, 34560, 270, 34560, 1080, 1080, 1080, 34560, 1080, 270, 270, 1080, 1080, 270, 1080, 270, 270, 270, 34560, 1080]
Prompts retrieved: 788670 . Total input tokens: 175797183 . Total output tokens: 157928644
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.653770620934665,
    "estimated_duration": 3600.148011804853,
    "input_throughput": 7337.537766053352,
    "output_throughput": 6494.632421592618,
    "total_throughput": 13832.17018764597,
    "itl": 132.62045749140944,
    "ttft": 1552675.2751587604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 262719,
    "finished_requests": 106644,
    "scheduler_time": 67.15962455126208
}
#Debug simulation 
Total elapsed time: 7.653863225597888. Arrivals time: 0.30313000176101923 Scheduler time: 7.228868709877133 Scheduler overhead time: 0.041748630348593 Adapter cache time: 0.01828298158943653 Engine time: 0.04243888799101114 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.770564236212522,
    "estimated_duration": 3600.0393829679992,
    "input_throughput": 7400.112378225283,
    "output_throughput": 6602.808878274789,
    "total_throughput": 14002.921256500073,
    "itl": 131.26987463783314,
    "ttft": 1544999.2459478497,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 261754,
    "finished_requests": 107751,
    "scheduler_time": 68.451253458555
}
#Debug simulation 
Total elapsed time: 7.770655736327171. Arrivals time: 0.3262939350679517 Scheduler time: 7.323555299080908 Scheduler overhead time: 0.042182630859315395 Adapter cache time: 0.016187720466405153 Engine time: 0.04285986814647913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.783930043224245,
    "estimated_duration": 3600.0755780470317,
    "input_throughput": 7400.03797766158,
    "output_throughput": 6602.7424937825745,
    "total_throughput": 14002.780471444154,
    "itl": 131.26968342918582,
    "ttft": 1545003.0852114484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 261754,
    "finished_requests": 107751,
    "scheduler_time": 68.45151100605882
}
#Debug simulation 
Total elapsed time: 7.784023236948997. Arrivals time: 0.30877836933359504 Scheduler time: 7.354079702869058 Scheduler overhead time: 0.04229414090514183 Adapter cache time: 0.01631624484434724 Engine time: 0.042965652886778116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.758022253867239,
    "estimated_duration": 3600.0786523422403,
    "input_throughput": 7400.031658382615,
    "output_throughput": 6602.736855355869,
    "total_throughput": 14002.768513738485,
    "itl": 131.26977440676598,
    "ttft": 1545005.3526307945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 261754,
    "finished_requests": 107751,
    "scheduler_time": 68.45151727520926
}
#Debug simulation 
Total elapsed time: 7.758126584812999. Arrivals time: 0.304546901024878 Scheduler time: 7.332965292967856 Scheduler overhead time: 0.04221995081752539 Adapter cache time: 0.016255894675850868 Engine time: 0.04248783690854907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.814943226985633,
    "estimated_duration": 3600.048474797407,
    "input_throughput": 7400.0936894326705,
    "output_throughput": 6602.792203051566,
    "total_throughput": 14002.885892484235,
    "itl": 131.26946966882562,
    "ttft": 1544994.2249650883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 261754,
    "finished_requests": 107751,
    "scheduler_time": 68.45152821865392
}
#Debug simulation 
Total elapsed time: 7.815054524689913. Arrivals time: 0.3055271804332733 Scheduler time: 7.388304165564477 Scheduler overhead time: 0.04233498591929674 Adapter cache time: 0.01631005574017763 Engine time: 0.04293733788654208 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.00645386427641,
    "estimated_duration": 3600.1026350162356,
    "input_throughput": 7400.100969588011,
    "output_throughput": 6602.693425681405,
    "total_throughput": 14002.794395269417,
    "itl": 131.26940471633867,
    "ttft": 1545009.6044759809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 261754,
    "finished_requests": 107752,
    "scheduler_time": 68.45192422032034
}
#Debug simulation 
Total elapsed time: 8.006545997224748. Arrivals time: 0.5078474441543221 Scheduler time: 7.377315629273653 Scheduler overhead time: 0.042420586571097374 Adapter cache time: 0.01665049698203802 Engine time: 0.042725805658847094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.80058068735525,
    "estimated_duration": 3600.01800798726,
    "input_throughput": 7400.15631613315,
    "output_throughput": 6602.848082221071,
    "total_throughput": 14003.004398354222,
    "itl": 131.2694749323537,
    "ttft": 1544995.6792946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 261754,
    "finished_requests": 107751,
    "scheduler_time": 68.45088020657099
}
#Debug simulation 
Total elapsed time: 7.800671678036451. Arrivals time: 0.3283721315674484 Scheduler time: 7.349839459639043 Scheduler overhead time: 0.04245501710101962 Adapter cache time: 0.01637576660141349 Engine time: 0.043993230909109116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 1080, 34560, 34560, 135, 34560, 135, 1080, 135, 34560, 1080, 34560, 135, 1080, 34560, 34560, 34560, 135, 34560, 1080, 1080, 135, 135, 135, 34560, 135, 135, 135, 34560, 1080, 135, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 135, 1080, 1080, 34560, 34560, 1080, 34560, 135, 34560, 1080, 1080, 1080, 34560, 1080, 135, 135, 1080, 1080, 135, 1080, 135, 135, 135, 34560, 1080]
Prompts retrieved: 785835 . Total input tokens: 175162241 . Total output tokens: 157358395
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.811533291824162,
    "estimated_duration": 3600.100694927181,
    "input_throughput": 7400.104957491715,
    "output_throughput": 6602.696983863337,
    "total_throughput": 14002.801941355052,
    "itl": 131.26921382844267,
    "ttft": 1545009.370441788,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 261754,
    "finished_requests": 107752,
    "scheduler_time": 68.45195938262448
}
#Debug simulation 
Total elapsed time: 7.811627948656678. Arrivals time: 0.3062018691562116 Scheduler time: 7.384561645798385 Scheduler overhead time: 0.042333101853728294 Adapter cache time: 0.016102845780551434 Engine time: 0.04285829793661833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.88028759509325,
    "estimated_duration": 3600.0260750439,
    "input_throughput": 7593.853886090708,
    "output_throughput": 6673.625273591116,
    "total_throughput": 14267.479159681825,
    "itl": 128.62715542291818,
    "ttft": 1517520.6643242957,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 261307,
    "finished_requests": 109963,
    "scheduler_time": 68.98998356148125
}
#Debug simulation 
Total elapsed time: 7.88038351200521. Arrivals time: 0.30912963720038533 Scheduler time: 7.449107386637479 Scheduler overhead time: 0.043096646666526794 Adapter cache time: 0.015138014685362577 Engine time: 0.043915299233049154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.8646124098449945,
    "estimated_duration": 3600.06417009382,
    "input_throughput": 7593.773807450647,
    "output_throughput": 6673.635486717972,
    "total_throughput": 14267.409294168618,
    "itl": 128.62755035443735,
    "ttft": 1517543.2276649228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 261307,
    "finished_requests": 109964,
    "scheduler_time": 68.99040758134097
}
#Debug simulation 
Total elapsed time: 7.864704365842044. Arrivals time: 0.3101233933120966 Scheduler time: 7.432987409643829 Scheduler overhead time: 0.04309819918125868 Adapter cache time: 0.014968524686992168 Engine time: 0.04359949938952923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.860704619903117,
    "estimated_duration": 3600.0645217482943,
    "input_throughput": 7593.773065690459,
    "output_throughput": 6673.634834836938,
    "total_throughput": 14267.407900527398,
    "itl": 128.62755316196913,
    "ttft": 1517543.4853348013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860526,
    "arrivals": 261307,
    "finished_requests": 109964,
    "scheduler_time": 68.99040520479574
}
#Debug simulation 
Total elapsed time: 7.860797367990017. Arrivals time: 0.30937611497938633 Scheduler time: 7.43032946344465 Scheduler overhead time: 0.042836164589971304 Adapter cache time: 0.015081354882568121 Engine time: 0.043327991385012865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.8847685577347875,
    "estimated_duration": 3600.022359587257,
    "input_throughput": 7593.861723440605,
    "output_throughput": 6673.632161205381,
    "total_throughput": 14267.493884645985,
    "itl": 128.62676935648562,
    "ttft": 1517519.6898419226,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 261307,
    "finished_requests": 109963,
    "scheduler_time": 68.98991846990593
}
#Debug simulation 
Total elapsed time: 7.884890014771372. Arrivals time: 0.312019323464483 Scheduler time: 7.450092168990523 Scheduler overhead time: 0.04323237435892224 Adapter cache time: 0.015431894920766354 Engine time: 0.043795658741146326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.8815452749840915,
    "estimated_duration": 3600.067338402434,
    "input_throughput": 7593.767124403718,
    "output_throughput": 6673.629613456498,
    "total_throughput": 14267.396737860217,
    "itl": 128.6273479308506,
    "ttft": 1517543.865520464,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 261307,
    "finished_requests": 109964,
    "scheduler_time": 68.99050981956141
}
#Debug simulation 
Total elapsed time: 7.881646458059549. Arrivals time: 0.3133935104124248 Scheduler time: 7.445989317260683 Scheduler overhead time: 0.04303267691284418 Adapter cache time: 0.015479516703635454 Engine time: 0.04371221223846078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.901554263196886,
    "estimated_duration": 3600.0988899355607,
    "input_throughput": 7593.7005720666,
    "output_throughput": 6673.571125272629,
    "total_throughput": 14267.271697339229,
    "itl": 128.62624736886698,
    "ttft": 1517544.839464688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 261307,
    "finished_requests": 109964,
    "scheduler_time": 68.9908693769929
}
#Debug simulation 
Total elapsed time: 7.901664779987186. Arrivals time: 0.32179428543895483 Scheduler time: 7.457095828838646 Scheduler overhead time: 0.043360198847949505 Adapter cache time: 0.015329816844314337 Engine time: 0.04399554943665862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 1080, 34560, 34560, 66, 34560, 66, 1080, 66, 34560, 1080, 34560, 66, 1080, 34560, 34560, 34560, 66, 34560, 1080, 1080, 66, 66, 66, 34560, 66, 66, 66, 34560, 1080, 66, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 66, 1080, 1080, 34560, 34560, 1080, 34560, 66, 34560, 1080, 1080, 1080, 34560, 1080, 66, 66, 1080, 1080, 66, 1080, 66, 66, 66, 34560, 1080]
Prompts retrieved: 784386 . Total input tokens: 174840145 . Total output tokens: 157069590
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.880750939249992,
    "estimated_duration": 3600.072194643925,
    "input_throughput": 7593.756880951647,
    "output_throughput": 6673.620611204524,
    "total_throughput": 14267.37749215617,
    "itl": 128.6273241874729,
    "ttft": 1517546.0283363345,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 261307,
    "finished_requests": 109964,
    "scheduler_time": 68.99043936613224
}
#Debug simulation 
Total elapsed time: 7.880868882872164. Arrivals time: 0.32205167412757874 Scheduler time: 7.435614308807999 Scheduler overhead time: 0.04350937716662884 Adapter cache time: 0.015412114560604095 Engine time: 0.0441360087133944 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.914526698179543,
    "estimated_duration": 3600.0305452111156,
    "input_throughput": 7518.283986785666,
    "output_throughput": 6711.099168905296,
    "total_throughput": 14229.383155690963,
    "itl": 129.16621011815153,
    "ttft": 1524444.6273279844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 261056,
    "finished_requests": 109608,
    "scheduler_time": 69.58390208422497
}
#Debug simulation 
Total elapsed time: 7.914621082134545. Arrivals time: 0.3093840624205768 Scheduler time: 7.48412308935076 Scheduler overhead time: 0.04307741345837712 Adapter cache time: 0.014116662088781595 Engine time: 0.0439657811075449 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.120405085384846,
    "estimated_duration": 3600.110195266786,
    "input_throughput": 7518.202369356711,
    "output_throughput": 6711.074575374097,
    "total_throughput": 14229.276944730807,
    "itl": 129.16815045650492,
    "ttft": 1524470.8007942054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 261056,
    "finished_requests": 109609,
    "scheduler_time": 69.58499891189014
}
#Debug simulation 
Total elapsed time: 8.120471955277026. Arrivals time: 0.3369596740230918 Scheduler time: 7.663176222704351 Scheduler overhead time: 0.042771213222295046 Adapter cache time: 0.013926239684224129 Engine time: 0.04378586867824197 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.906181754078716,
    "estimated_duration": 3600.110542823769,
    "input_throughput": 7518.201643544628,
    "output_throughput": 6711.073927482648,
    "total_throughput": 14229.275571027276,
    "itl": 129.16815439378428,
    "ttft": 1524471.125572469,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 261056,
    "finished_requests": 109609,
    "scheduler_time": 69.58499243785312
}
#Debug simulation 
Total elapsed time: 7.906271263025701. Arrivals time: 0.31034897780045867 Scheduler time: 7.4756902772933245 Scheduler overhead time: 0.04294571280479431 Adapter cache time: 0.013958712574094534 Engine time: 0.043498238548636436 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.9333157311193645,
    "estimated_duration": 3600.080462181092,
    "input_throughput": 7518.17974190559,
    "output_throughput": 6711.006116058495,
    "total_throughput": 14229.185857964085,
    "itl": 129.16762325726367,
    "ttft": 1524460.7240172187,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 261056,
    "finished_requests": 109608,
    "scheduler_time": 69.58475101968847
}
#Debug simulation 
Total elapsed time: 7.9334037830121815. Arrivals time: 0.31331456266343594 Scheduler time: 7.499175852630287 Scheduler overhead time: 0.043175566010177135 Adapter cache time: 0.01388609642162919 Engine time: 0.04392403457313776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.9208979569375515,
    "estimated_duration": 3600.116929623976,
    "input_throughput": 7518.188305852338,
    "output_throughput": 6711.062021678146,
    "total_throughput": 14229.250327530484,
    "itl": 129.16821111568603,
    "ttft": 1524473.9938297025,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 261056,
    "finished_requests": 109609,
    "scheduler_time": 69.58500530981364
}
#Debug simulation 
Total elapsed time: 7.920997914858162. Arrivals time: 0.3117089122533798 Scheduler time: 7.488817827776074 Scheduler overhead time: 0.0429662074893713 Adapter cache time: 0.01398425828665495 Engine time: 0.043643987737596035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.961311850231141,
    "estimated_duration": 3600.029127465927,
    "input_throughput": 7518.286947598084,
    "output_throughput": 6711.10181183629,
    "total_throughput": 14229.388759434374,
    "itl": 129.16634017603363,
    "ttft": 1524444.9870993316,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 261056,
    "finished_requests": 109608,
    "scheduler_time": 69.58394064747121
}
#Debug simulation 
Total elapsed time: 7.961421442218125. Arrivals time: 0.3067534430883825 Scheduler time: 7.53359843371436 Scheduler overhead time: 0.04318114137277007 Adapter cache time: 0.014204246457666159 Engine time: 0.043768010567873716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.1-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.000e-01 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 1080, 34560, 34560, 33, 34560, 33, 1080, 33, 34560, 1080, 34560, 33, 1080, 34560, 34560, 34560, 33, 34560, 1080, 1080, 33, 33, 33, 34560, 33, 33, 33, 34560, 1080, 33, 34560, 34560, 34560, 34560, 1080, 1080, 1080, 34560, 33, 1080, 1080, 34560, 34560, 1080, 34560, 33, 34560, 1080, 1080, 1080, 34560, 1080, 33, 33, 1080, 1080, 33, 1080, 33, 33, 33, 34560, 1080]
Prompts retrieved: 783693 . Total input tokens: 174684086 . Total output tokens: 156932475
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.947750453837216,
    "estimated_duration": 3600.1194218526584,
    "input_throughput": 7518.183101290394,
    "output_throughput": 6711.057375859688,
    "total_throughput": 14229.240477150082,
    "itl": 129.16813853424281,
    "ttft": 1524474.7315400431,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 261056,
    "finished_requests": 109609,
    "scheduler_time": 69.58504613251685
}
#Debug simulation 
Total elapsed time: 7.947868524119258. Arrivals time: 0.31481098663061857 Scheduler time: 7.512474176939577 Scheduler overhead time: 0.04284409387037158 Adapter cache time: 0.014066055417060852 Engine time: 0.04372751899063587 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.945087653119117,
    "estimated_duration": 3600.002762708552,
    "input_throughput": 7562.756140641371,
    "output_throughput": 6717.80901129211,
    "total_throughput": 14280.56515193348,
    "itl": 128.70204052656422,
    "ttft": 1512144.3633221632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 258928,
    "finished_requests": 109961,
    "scheduler_time": 69.6319071556799
}
#Debug simulation 
Total elapsed time: 7.945179657079279. Arrivals time: 0.31161901308223605 Scheduler time: 7.507295164745301 Scheduler overhead time: 0.0432286043651402 Adapter cache time: 0.018772831186652184 Engine time: 0.04434588644653559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.958011103793979,
    "estimated_duration": 3600.094466311563,
    "input_throughput": 7562.753492936739,
    "output_throughput": 6717.669835141216,
    "total_throughput": 14280.423328077955,
    "itl": 128.70388474312023,
    "ttft": 1512189.1726457165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 258928,
    "finished_requests": 109962,
    "scheduler_time": 69.63329152760404
}
#Debug simulation 
Total elapsed time: 7.958104914985597. Arrivals time: 0.31018277537077665 Scheduler time: 7.521449509076774 Scheduler overhead time: 0.04334882413968444 Adapter cache time: 0.018866723868995905 Engine time: 0.04422780033200979 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.940251789987087,
    "estimated_duration": 3600.09481624509,
    "input_throughput": 7562.752757828044,
    "output_throughput": 6717.669182175664,
    "total_throughput": 14280.421940003707,
    "itl": 128.70388790959686,
    "ttft": 1512189.4541019392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 258928,
    "finished_requests": 109962,
    "scheduler_time": 69.63328743011225
}
#Debug simulation 
Total elapsed time: 7.940367563161999. Arrivals time: 0.31029554223641753 Scheduler time: 7.504543675109744 Scheduler overhead time: 0.043153772596269846 Adapter cache time: 0.018463510554283857 Engine time: 0.04393273638561368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 7.947973596863449,
    "estimated_duration": 3600.0798525050727,
    "input_throughput": 7562.78419242692,
    "output_throughput": 6717.697104182753,
    "total_throughput": 14280.481296609672,
    "itl": 128.70362889013154,
    "ttft": 1512181.940886515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.200352315879427,
    "arrivals": 258928,
    "finished_requests": 109962,
    "scheduler_time": 69.63334927298428
}
#Debug simulation 
Total elapsed time: 7.948075742926449. Arrivals time: 0.3116947119124234 Scheduler time: 7.510944969486445 Scheduler overhead time: 0.04303903225809336 Adapter cache time: 0.01858467608690262 Engine time: 0.04380830191075802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 7.971858021337539,
    "estimated_duration": 3600.083090158921,
    "input_throughput": 7562.77739100686,
    "output_throughput": 6717.691062772781,
    "total_throughput": 14280.46845377964,
    "itl": 128.7031428477209,
    "ttft": 1512180.1216343557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 258928,
    "finished_requests": 109962,
    "scheduler_time": 69.63342836035277
}
#Debug simulation 
Total elapsed time: 7.971968618221581. Arrivals time: 0.31414713710546494 Scheduler time: 7.531761449296027 Scheduler overhead time: 0.04328040359541774 Adapter cache time: 0.018631163518875837 Engine time: 0.04414721252396703 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 7.954821532126516,
    "estimated_duration": 3600.0062447982573,
    "input_throughput": 7562.74882559981,
    "output_throughput": 6717.802513521825,
    "total_throughput": 14280.551339121634,
    "itl": 128.7025801680922,
    "ttft": 1512144.882824339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 258928,
    "finished_requests": 109961,
    "scheduler_time": 69.63189966319068
}
#Debug simulation 
Total elapsed time: 7.954937841743231. Arrivals time: 0.3124337210319936 Scheduler time: 7.516400497406721 Scheduler overhead time: 0.04328039661049843 Adapter cache time: 0.01872857240960002 Engine time: 0.04406808456405997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.025_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  3.2  ]. Counts: [21 21 22]
Adapter prompts. [270, 540, 34560, 34560, 270, 34560, 270, 540, 270, 34560, 540, 34560, 270, 540, 34560, 34560, 34560, 270, 34560, 540, 540, 270, 270, 270, 34560, 270, 270, 270, 34560, 540, 270, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 270, 540, 540, 34560, 34560, 540, 34560, 270, 34560, 540, 540, 540, 34560, 540, 270, 270, 540, 540, 270, 540, 270, 270, 270, 34560, 540]
Prompts retrieved: 777330 . Total input tokens: 173286899 . Total output tokens: 155656418
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 7.947003641165793,
    "estimated_duration": 3600.0873704964306,
    "input_throughput": 7562.768399214047,
    "output_throughput": 6717.683075748558,
    "total_throughput": 14280.451474962605,
    "itl": 128.7031280229058,
    "ttft": 1512184.940169975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189146,
    "arrivals": 258928,
    "finished_requests": 109962,
    "scheduler_time": 69.63333636582534
}
#Debug simulation 
Total elapsed time: 7.947119271848351. Arrivals time: 0.3337095659226179 Scheduler time: 7.487499626353383 Scheduler overhead time: 0.043276720214635134 Adapter cache time: 0.018518174067139626 Engine time: 0.04407874820753932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.084641668945551,
    "estimated_duration": 3600.0685887773484,
    "input_throughput": 7731.779079646167,
    "output_throughput": 6823.132502689988,
    "total_throughput": 14554.911582336154,
    "itl": 126.06779251166925,
    "ttft": 1497056.3160395492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 257941,
    "finished_requests": 111958,
    "scheduler_time": 70.64153222867678
}
#Debug simulation 
Total elapsed time: 8.084732033777982. Arrivals time: 0.31666699098423123 Scheduler time: 7.642980182543397 Scheduler overhead time: 0.04388835886493325 Adapter cache time: 0.016458308324217796 Engine time: 0.044338579289615154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.081310315057635,
    "estimated_duration": 3600.114273824944,
    "input_throughput": 7731.680964234158,
    "output_throughput": 6823.045917901442,
    "total_throughput": 14554.726882135601,
    "itl": 126.0688243906558,
    "ttft": 1497076.9871856596,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 257941,
    "finished_requests": 111958,
    "scheduler_time": 70.64155315781
}
#Debug simulation 
Total elapsed time: 8.081416483037174. Arrivals time: 0.3332366063259542 Scheduler time: 7.621806567069143 Scheduler overhead time: 0.04427628731355071 Adapter cache time: 0.01658135885372758 Engine time: 0.044977488461881876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.266945559997112,
    "estimated_duration": 3600.107845608232,
    "input_throughput": 7731.6947696319185,
    "output_throughput": 6823.058100874753,
    "total_throughput": 14554.75287050667,
    "itl": 126.06866798870134,
    "ttft": 1497069.375229945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605254,
    "arrivals": 257941,
    "finished_requests": 111958,
    "scheduler_time": 70.6418295042454
}
#Debug simulation 
Total elapsed time: 8.26703669084236. Arrivals time: 0.5152483563870192 Scheduler time: 7.626549190375954 Scheduler overhead time: 0.04387663444504142 Adapter cache time: 0.01662760553881526 Engine time: 0.04444416332989931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.076992061920464,
    "estimated_duration": 3600.095717981188,
    "input_throughput": 7731.720815359013,
    "output_throughput": 6823.081085681388,
    "total_throughput": 14554.8019010404,
    "itl": 126.06849350245747,
    "ttft": 1497068.2662104394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 257941,
    "finished_requests": 111958,
    "scheduler_time": 70.64164071258229
}
#Debug simulation 
Total elapsed time: 8.077082660980523. Arrivals time: 0.3147662039846182 Scheduler time: 7.637077380437404 Scheduler overhead time: 0.043834248557686806 Adapter cache time: 0.016403827350586653 Engine time: 0.04467528499662876 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.064919895026833,
    "estimated_duration": 3600.1142045242236,
    "input_throughput": 7731.6811130658425,
    "output_throughput": 6823.046049242275,
    "total_throughput": 14554.727162308118,
    "itl": 126.06875235272713,
    "ttft": 1497074.0698534795,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076452,
    "arrivals": 257941,
    "finished_requests": 111958,
    "scheduler_time": 70.64185370282866
}
#Debug simulation 
Total elapsed time: 8.065010632853955. Arrivals time: 0.31392579851672053 Scheduler time: 7.625680129043758 Scheduler overhead time: 0.04394321283325553 Adapter cache time: 0.016487504355609417 Engine time: 0.044614034704864025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.034204967319965,
    "estimated_duration": 3600.045991876669,
    "input_throughput": 7731.8276107605825,
    "output_throughput": 6823.175330378253,
    "total_throughput": 14555.002941138837,
    "itl": 126.0677023431778,
    "ttft": 1497051.5696117363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 257941,
    "finished_requests": 111958,
    "scheduler_time": 70.64143155924768
}
#Debug simulation 
Total elapsed time: 8.034297698177397. Arrivals time: 0.3088517771102488 Scheduler time: 7.600316000636667 Scheduler overhead time: 0.043788385577499866 Adapter cache time: 0.01651454297825694 Engine time: 0.044504618272185326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.0125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 540, 34560, 34560, 135, 34560, 135, 540, 135, 34560, 540, 34560, 135, 540, 34560, 34560, 34560, 135, 34560, 540, 540, 135, 135, 135, 34560, 135, 135, 135, 34560, 540, 135, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 135, 540, 540, 34560, 34560, 540, 34560, 135, 34560, 540, 540, 540, 34560, 540, 135, 135, 540, 540, 135, 540, 135, 135, 135, 34560, 540]
Prompts retrieved: 774495 . Total input tokens: 172646423 . Total output tokens: 155092013
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.101964042056352,
    "estimated_duration": 3600.117938616295,
    "input_throughput": 7731.6730936593585,
    "output_throughput": 6823.038972284634,
    "total_throughput": 14554.712065943992,
    "itl": 126.06867990426466,
    "ttft": 1497074.59917346,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018914,
    "arrivals": 257941,
    "finished_requests": 111958,
    "scheduler_time": 70.6419163885688
}
#Debug simulation 
Total elapsed time: 8.102059779223055. Arrivals time: 0.32624723576009274 Scheduler time: 7.649038425181061 Scheduler overhead time: 0.04445491125807166 Adapter cache time: 0.016559384763240814 Engine time: 0.045219465624541044 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.145098970271647,
    "estimated_duration": 3600.046842135383,
    "input_throughput": 7798.225476240693,
    "output_throughput": 6910.512860228066,
    "total_throughput": 14708.738336468758,
    "itl": 124.8368510975274,
    "ttft": 1485190.7959859553,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 257455,
    "finished_requests": 113239,
    "scheduler_time": 71.63887779908214
}
#Debug simulation 
Total elapsed time: 8.145189204253256. Arrivals time: 0.31919183768332005 Scheduler time: 7.700777865946293 Scheduler overhead time: 0.04448286956176162 Adapter cache time: 0.015039187856018543 Engine time: 0.045286539010703564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.360670199617743,
    "estimated_duration": 3600.112735443819,
    "input_throughput": 7798.111076801891,
    "output_throughput": 6910.4085977832665,
    "total_throughput": 14708.519674585157,
    "itl": 124.83752295776802,
    "ttft": 1485219.8077017127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 257455,
    "finished_requests": 113241,
    "scheduler_time": 71.63957578293544
}
#Debug simulation 
Total elapsed time: 8.360789556056261. Arrivals time: 0.3260547909885645 Scheduler time: 7.909923837054521 Scheduler overhead time: 0.04435186227783561 Adapter cache time: 0.014999729581177235 Engine time: 0.04495523264631629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.127469270024449,
    "estimated_duration": 3600.1131870432155,
    "input_throughput": 7798.110098604242,
    "output_throughput": 6910.4077309393115,
    "total_throughput": 14708.517829543553,
    "itl": 124.83752019216188,
    "ttft": 1485220.1853853525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2092868485860526,
    "arrivals": 257455,
    "finished_requests": 113241,
    "scheduler_time": 71.63957331273153
}
#Debug simulation 
Total elapsed time: 8.127577598206699. Arrivals time: 0.3181206537410617 Scheduler time: 7.684354032855481 Scheduler overhead time: 0.04428550833836198 Adapter cache time: 0.015221125911921263 Engine time: 0.04507892346009612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.156705655157566,
    "estimated_duration": 3600.046155897735,
    "input_throughput": 7798.226962731609,
    "output_throughput": 6910.51417750398,
    "total_throughput": 14708.74114023559,
    "itl": 124.83672922164963,
    "ttft": 1485192.229376846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 257455,
    "finished_requests": 113239,
    "scheduler_time": 71.63883787780328
}
#Debug simulation 
Total elapsed time: 8.156821930315346. Arrivals time: 0.3189524435438216 Scheduler time: 7.71242659445852 Scheduler overhead time: 0.044548130594193935 Adapter cache time: 0.0151155567727983 Engine time: 0.04515312425792217 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.116389766335487,
    "estimated_duration": 3600.1169164204,
    "input_throughput": 7798.102020507181,
    "output_throughput": 6910.400572417096,
    "total_throughput": 14708.502592924277,
    "itl": 124.83758663889341,
    "ttft": 1485221.245031205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2119276781007646,
    "arrivals": 257455,
    "finished_requests": 113241,
    "scheduler_time": 71.63946088930138
}
#Debug simulation 
Total elapsed time: 8.116485710255802. Arrivals time: 0.3194776070304215 Scheduler time: 7.672050703316927 Scheduler overhead time: 0.044404739048331976 Adapter cache time: 0.015005619265139103 Engine time: 0.045072933193296194 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.36353903496638,
    "estimated_duration": 3600.0336585732057,
    "input_throughput": 7798.19625662234,
    "output_throughput": 6910.493722955872,
    "total_throughput": 14708.689979578212,
    "itl": 124.83669169091532,
    "ttft": 1485185.3530330723,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 257455,
    "finished_requests": 113238,
    "scheduler_time": 71.63877448967736
}
#Debug simulation 
Total elapsed time: 8.36360596632585. Arrivals time: 0.5279696704819798 Scheduler time: 7.7106710290536284 Scheduler overhead time: 0.044343575835227966 Adapter cache time: 0.01503325579687953 Engine time: 0.04510682448744774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.00625_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    3.2    ]. Counts: [21 21 22]
Adapter prompts. [66, 540, 34560, 34560, 66, 34560, 66, 540, 66, 34560, 540, 34560, 66, 540, 34560, 34560, 34560, 66, 34560, 540, 540, 66, 66, 66, 34560, 66, 66, 66, 34560, 540, 66, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 66, 540, 540, 34560, 34560, 540, 34560, 66, 34560, 540, 540, 540, 34560, 540, 66, 66, 540, 540, 66, 540, 66, 66, 66, 34560, 540]
Prompts retrieved: 773046 . Total input tokens: 172316526 . Total output tokens: 154816222
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.140660590026528,
    "estimated_duration": 3600.1198432624083,
    "input_throughput": 7798.0956807702905,
    "output_throughput": 6910.394954367816,
    "total_throughput": 14708.490635138107,
    "itl": 124.83754086817336,
    "ttft": 1485222.8962294925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2146942614018915,
    "arrivals": 257455,
    "finished_requests": 113241,
    "scheduler_time": 71.63953155342963
}
#Debug simulation 
Total elapsed time: 8.140754152089357. Arrivals time: 0.32096529472619295 Scheduler time: 7.694557628128678 Scheduler overhead time: 0.044387588277459145 Adapter cache time: 0.015225183218717575 Engine time: 0.045078710187226534 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.207227465230972,
    "estimated_duration": 3600.0877147986726,
    "input_throughput": 7807.613376879037,
    "output_throughput": 6946.554356773091,
    "total_throughput": 14754.167733652128,
    "itl": 124.59255571237094,
    "ttft": 1478368.9511522537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 257175,
    "finished_requests": 113531,
    "scheduler_time": 72.06623431884427
}
#Debug simulation 
Total elapsed time: 8.207325119990855. Arrivals time: 0.3233055714517832 Scheduler time: 7.758618464693427 Scheduler overhead time: 0.04481044132262468 Adapter cache time: 0.01465152157470584 Engine time: 0.04528148612007499 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.19229657901451,
    "estimated_duration": 3600.0094770883384,
    "input_throughput": 7807.403613485073,
    "output_throughput": 6946.572546310647,
    "total_throughput": 14753.97615979572,
    "itl": 124.59316169547184,
    "ttft": 1478347.1693123982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.2089328175666742,
    "arrivals": 257175,
    "finished_requests": 113528,
    "scheduler_time": 72.06454785071504
}
#Debug simulation 
Total elapsed time: 8.19240747205913. Arrivals time: 0.3191738538444042 Scheduler time: 7.748407369013876 Scheduler overhead time: 0.0445989016443491 Adapter cache time: 0.014631808269768953 Engine time: 0.044961914885789156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.19557917676866,
    "estimated_duration": 3600.010751338274,
    "input_throughput": 7807.4008499978945,
    "output_throughput": 6946.570087520873,
    "total_throughput": 14753.970937518769,
    "itl": 124.59316369254324,
    "ttft": 1478347.8410990832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 257175,
    "finished_requests": 113528,
    "scheduler_time": 72.06457874579115
}
#Debug simulation 
Total elapsed time: 8.195692250970751. Arrivals time: 0.32088166009634733 Scheduler time: 7.74979043751955 Scheduler overhead time: 0.0445140665397048 Adapter cache time: 0.014586882200092077 Engine time: 0.04525336483493447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.218827485106885,
    "estimated_duration": 3600.086185778245,
    "input_throughput": 7807.616692910856,
    "output_throughput": 6946.557307097879,
    "total_throughput": 14754.174000008734,
    "itl": 124.59207191471889,
    "ttft": 1478364.3479396605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 257175,
    "finished_requests": 113531,
    "scheduler_time": 72.06629032259035
}
#Debug simulation 
Total elapsed time: 8.218921616207808. Arrivals time: 0.3216939945705235 Scheduler time: 7.77190082706511 Scheduler overhead time: 0.04461145866662264 Adapter cache time: 0.014844425953924656 Engine time: 0.045115409418940544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.236259996891022,
    "estimated_duration": 3600.0165603114137,
    "input_throughput": 7807.3882520053385,
    "output_throughput": 6946.558878561588,
    "total_throughput": 14753.947130566927,
    "itl": 124.5931933773372,
    "ttft": 1478350.882152526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076454,
    "arrivals": 257175,
    "finished_requests": 113528,
    "scheduler_time": 72.06463997397917
}
#Debug simulation 
Total elapsed time: 8.236353971995413. Arrivals time: 0.34477410977706313 Scheduler time: 7.766251876018941 Scheduler overhead time: 0.04471344733610749 Adapter cache time: 0.014546764083206654 Engine time: 0.04539138404652476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.164285749197006,
    "estimated_duration": 3600.085462420106,
    "input_throughput": 7807.618261680026,
    "output_throughput": 6946.5587028560685,
    "total_throughput": 14754.176964536095,
    "itl": 124.59260936014199,
    "ttft": 1478354.0393744637,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 257175,
    "finished_requests": 113531,
    "scheduler_time": 72.06630949023742
}
#Debug simulation 
Total elapsed time: 8.164375737309456. Arrivals time: 0.32879273500293493 Scheduler time: 7.710752349346876 Scheduler overhead time: 0.044561263639479876 Adapter cache time: 0.01462145708501339 Engine time: 0.045010118279606104 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.05-0.003125_size_16-16-32/adapters_64_slots_64_rate_3.2-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 5.000e-02 3.200e+00]. Counts: [21 21 22]
Adapter prompts. [33, 540, 34560, 34560, 33, 34560, 33, 540, 33, 34560, 540, 34560, 33, 540, 34560, 34560, 34560, 33, 34560, 540, 540, 33, 33, 33, 34560, 33, 33, 33, 34560, 540, 33, 34560, 34560, 34560, 34560, 540, 540, 540, 34560, 33, 540, 540, 34560, 34560, 540, 34560, 33, 34560, 540, 540, 540, 34560, 540, 33, 33, 540, 540, 33, 540, 33, 33, 33, 34560, 540]
Prompts retrieved: 772353 . Total input tokens: 172167437 . Total output tokens: 154670681
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.198949821293354,
    "estimated_duration": 3600.026403773744,
    "input_throughput": 7807.366904458533,
    "output_throughput": 6946.53988475905,
    "total_throughput": 14753.906789217583,
    "itl": 124.59340301350045,
    "ttft": 1478353.9087344732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21469426140189143,
    "arrivals": 257175,
    "finished_requests": 113528,
    "scheduler_time": 72.06455420179138
}
#Debug simulation 
Total elapsed time: 8.19904239103198. Arrivals time: 0.3404248603619635 Scheduler time: 7.733315143734217 Scheduler overhead time: 0.044600358698517084 Adapter cache time: 0.014672632329165936 Engine time: 0.04534429032355547 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-8/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [8]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.295064326841384,
    "estimated_duration": 3600.086090527511,
    "input_throughput": 7982.169114125079,
    "output_throughput": 7055.323223194988,
    "total_throughput": 15037.492337320067,
    "itl": 122.11325690585552,
    "ttft": 1454727.37906024,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19587121561169607,
    "arrivals": 256011,
    "finished_requests": 115698,
    "scheduler_time": 73.189817357039
}
#Debug simulation 
Total elapsed time: 8.295185691211373. Arrivals time: 0.30788699071854353 Scheduler time: 7.859656128566712 Scheduler overhead time: 0.045318339485675097 Adapter cache time: 0.015149217564612627 Engine time: 0.04605274926871061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-16/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.32975693186745,
    "estimated_duration": 3600.103778159941,
    "input_throughput": 7982.268781898239,
    "output_throughput": 7055.326058678846,
    "total_throughput": 15037.594840577085,
    "itl": 122.11346420233104,
    "ttft": 1454721.5122667134,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20893281756667417,
    "arrivals": 256011,
    "finished_requests": 115699,
    "scheduler_time": 73.1898555160097
}
#Debug simulation 
Total elapsed time: 8.329854365903884. Arrivals time: 0.3285431293770671 Scheduler time: 7.872597207780927 Scheduler overhead time: 0.04566920967772603 Adapter cache time: 0.015316939447075129 Engine time: 0.04640965210273862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-8-32/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [43 21]
---Simulation End---
#Simulation results
{
    "duration": 8.316489133052528,
    "estimated_duration": 3600.105331798729,
    "input_throughput": 7982.265337120586,
    "output_throughput": 7055.323013926758,
    "total_throughput": 15037.588351047345,
    "itl": 122.1135046737644,
    "ttft": 1454722.5497471604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20928684858605257,
    "arrivals": 256011,
    "finished_requests": 115699,
    "scheduler_time": 73.18988895737915
}
#Debug simulation 
Total elapsed time: 8.316591487731785. Arrivals time: 0.31269515911117196 Scheduler time: 7.875648953020573 Scheduler overhead time: 0.045453264843672514 Adapter cache time: 0.015171256847679615 Engine time: 0.046363282948732376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-16/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [22 42]
---Simulation End---
#Simulation results
{
    "duration": 8.301685919985175,
    "estimated_duration": 3600.1018780450795,
    "input_throughput": 7982.272994897775,
    "output_throughput": 7055.329782442882,
    "total_throughput": 15037.602777340657,
    "itl": 122.11362032835721,
    "ttft": 1454721.1158432034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.20035231587942698,
    "arrivals": 256011,
    "finished_requests": 115699,
    "scheduler_time": 73.18992761738936
}
#Debug simulation 
Total elapsed time: 8.301791619043797. Arrivals time: 0.3133415332995355 Scheduler time: 7.85972336307168 Scheduler overhead time: 0.045438556000590324 Adapter cache time: 0.015234753955155611 Engine time: 0.04709264822304249 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_8-16-32/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [22 21 21]
---Simulation End---
#Simulation results
{
    "duration": 8.307748136110604,
    "estimated_duration": 3600.111452839862,
    "input_throughput": 7982.251765381182,
    "output_throughput": 7055.311018208587,
    "total_throughput": 15037.562783589769,
    "itl": 122.11356484457136,
    "ttft": 1454725.1564238975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.21192767810076457,
    "arrivals": 256011,
    "finished_requests": 115699,
    "scheduler_time": 73.18992428943129
}
#Debug simulation 
Total elapsed time: 8.307854169979692. Arrivals time: 0.3119694981724024 Scheduler time: 7.8678487334400415 Scheduler overhead time: 0.04545099101960659 Adapter cache time: 0.01527755381539464 Engine time: 0.046127735171467066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 64,
    "served_adapters": 64,
    "served_adapters_rates": [
        3.2,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.025-0.0125_size_16-16-16/adapters_64_slots_64_rate_3.2-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  3.2   ]. Counts: [21 21 22]
Adapter prompts. [135, 270, 34560, 34560, 135, 34560, 135, 270, 135, 34560, 270, 34560, 135, 270, 34560, 34560, 34560, 135, 34560, 270, 270, 135, 135, 135, 34560, 135, 135, 135, 34560, 270, 135, 34560, 34560, 34560, 34560, 270, 270, 270, 34560, 135, 270, 270, 34560, 34560, 270, 34560, 135, 34560, 270, 270, 270, 34560, 270, 135, 135, 270, 270, 135, 270, 135, 135, 135, 34560, 270]
Prompts retrieved: 768825 . Total input tokens: 171371709 . Total output tokens: 153950128
Prompts distributed
Adapter sizes. Values: [16]. Counts: [64]
---Simulation End---
#Simulation results
{
    "duration": 8.317731970921159,
    "estimated_duration": 3600.0700986038687,
    "input_throughput": 7982.204571834367,
    "output_throughput": 7055.3545637486895,
    "total_throughput": 15037.559135583057,
    "itl": 122.11366129541045,
    "ttft": 1454734.3191604677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 64,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.19136321887373942,
    "arrivals": 256011,
    "finished_requests": 115698,
    "scheduler_time": 73.18915141550022
}
#Debug simulation 
Total elapsed time: 8.31781856995076. Arrivals time: 0.30020233476534486 Scheduler time: 7.889276358298957 Scheduler overhead time: 0.045588613487780094 Adapter cache time: 0.015587486326694489 Engine time: 0.04608591506257653 
