INFO 06-01 00:47:15 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:15 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.4-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.4-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 4320, 33, 4320, 4320, 33, 4320, 66, 4320, 33, 66, 33, 4320, 66, 66, 66, 66, 4320, 33, 33, 33, 66, 4320, 33, 4320, 4320, 66, 66, 66, 33, 4320, 33, 66, 66, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 66, 66, 4320, 66, 4320, 33, 4320, 4320, 66, 33, 33, 4320, 66, 33, 66, 66, 4320, 66, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 4320, 4320, 33, 4320, 33, 66, 4320, 4320, 4320, 66, 33, 4320, 66, 33, 33, 33, 33, 33, 4320, 66, 33, 4320, 33, 4320, 33, 33, 66, 66, 4320, 66, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 4320, 66, 4320, 4320, 33, 4320, 66, 66, 33, 66, 4320, 4320, 33, 4320, 66, 33, 66, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 33, 66, 66, 33, 66, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 66, 4320, 66, 33, 66, 4320, 66, 4320, 33, 4320, 33, 66, 4320, 66, 4320, 66, 4320, 66, 4320, 66, 33, 4320, 33, 4320, 66, 66, 33, 66, 4320, 66, 66, 4320, 33, 4320, 4320, 66, 33, 4320, 4320, 66, 66, 4320, 4320, 66, 66, 66, 66, 4320, 4320, 66, 66, 4320, 33, 4320, 66, 4320, 4320, 33, 4320, 66, 66, 4320, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 4320, 66, 33, 33, 33, 33]
Prompts retrieved: 379935 . Total input tokens: 84650362 . Total output tokens: 76040480
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 5.148245059419423,
    "estimated_duration": 3600.0826119976387,
    "input_throughput": 5149.253780516346,
    "output_throughput": 4537.480874900188,
    "total_throughput": 9686.734655416534,
    "itl": 186.3883849866009,
    "ttft": 1452077.299368278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 693,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1664903729269125,
    "arrivals": 126425,
    "finished_requests": 75018,
    "scheduler_time": 71.9011495383108
}
#Debug simulation 
Total elapsed time: 5.148340555373579. Arrivals time: 0.21201257454231381 Scheduler time: 4.830122329760343 Scheduler overhead time: 0.02915919478982687 Adapter cache time: 0.03361056838184595 Engine time: 0.030058144126087427 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.4-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.4-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 4320, 33, 4320, 4320, 33, 4320, 66, 4320, 33, 66, 33, 4320, 66, 66, 66, 66, 4320, 33, 33, 33, 66, 4320, 33, 4320, 4320, 66, 66, 66, 33, 4320, 33, 66, 66, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 66, 66, 4320, 66, 4320, 33, 4320, 4320, 66, 33, 33, 4320, 66, 33, 66, 66, 4320, 66, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 4320, 4320, 33, 4320, 33, 66, 4320, 4320, 4320, 66, 33, 4320, 66, 33, 33, 33, 33, 33, 4320, 66, 33, 4320, 33, 4320, 33, 33, 66, 66, 4320, 66, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 4320, 66, 4320, 4320, 33, 4320, 66, 66, 33, 66, 4320, 4320, 33, 4320, 66, 33, 66, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 33, 66, 66, 33, 66, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 66, 4320, 66, 33, 66, 4320, 66, 4320, 33, 4320, 33, 66, 4320, 66, 4320, 66, 4320, 66, 4320, 66, 33, 4320, 33, 4320, 66, 66, 33, 66, 4320, 66, 66, 4320, 33, 4320, 4320, 66, 33, 4320, 4320, 66, 66, 4320, 4320, 66, 66, 66, 66, 4320, 4320, 66, 66, 4320, 33, 4320, 66, 4320, 4320, 33, 4320, 66, 66, 4320, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 4320, 66, 33, 33, 33, 33]
Prompts retrieved: 379935 . Total input tokens: 84650362 . Total output tokens: 76040480
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 4.6533966031856835,
    "estimated_duration": 3600.0096038499596,
    "input_throughput": 4281.208578865323,
    "output_throughput": 3786.925453037817,
    "total_throughput": 8068.13403190314,
    "itl": 88.90940502388818,
    "ttft": 1687077.3844336646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 576,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.901747361477469,
    "arrivals": 126425,
    "finished_requests": 62369,
    "scheduler_time": 67.75937945343003
}
#Debug simulation 
Total elapsed time: 4.65350753441453. Arrivals time: 0.2079059169627726 Scheduler time: 4.250660326331854 Scheduler overhead time: 0.05524024227634072 Adapter cache time: 0.05760246003046632 Engine time: 0.05627418402582407 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.4-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.4-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 4320, 33, 4320, 4320, 33, 4320, 66, 4320, 33, 66, 33, 4320, 66, 66, 66, 66, 4320, 33, 33, 33, 66, 4320, 33, 4320, 4320, 66, 66, 66, 33, 4320, 33, 66, 66, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 66, 66, 4320, 66, 4320, 33, 4320, 4320, 66, 33, 33, 4320, 66, 33, 66, 66, 4320, 66, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 4320, 4320, 33, 4320, 33, 66, 4320, 4320, 4320, 66, 33, 4320, 66, 33, 33, 33, 33, 33, 4320, 66, 33, 4320, 33, 4320, 33, 33, 66, 66, 4320, 66, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 4320, 66, 4320, 4320, 33, 4320, 66, 66, 33, 66, 4320, 4320, 33, 4320, 66, 33, 66, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 33, 66, 66, 33, 66, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 66, 4320, 66, 33, 66, 4320, 66, 4320, 33, 4320, 33, 66, 4320, 66, 4320, 66, 4320, 66, 4320, 66, 33, 4320, 33, 4320, 66, 66, 33, 66, 4320, 66, 66, 4320, 33, 4320, 4320, 66, 33, 4320, 4320, 66, 66, 4320, 4320, 66, 66, 66, 66, 4320, 4320, 66, 66, 4320, 33, 4320, 66, 4320, 4320, 33, 4320, 66, 66, 4320, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 4320, 66, 33, 33, 33, 33]
Prompts retrieved: 379935 . Total input tokens: 84650362 . Total output tokens: 76040480
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 5.1838267729617655,
    "estimated_duration": 3600.0913138413766,
    "input_throughput": 5131.079572614949,
    "output_throughput": 4521.018102352817,
    "total_throughput": 9652.097674967765,
    "itl": 186.96582282140508,
    "ttft": 1456706.2056020629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 688,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.057154602892661,
    "arrivals": 126425,
    "finished_requests": 74739,
    "scheduler_time": 71.64530954816178
}
#Debug simulation 
Total elapsed time: 5.183914305642247. Arrivals time: 0.2135422332212329 Scheduler time: 4.863271747715771 Scheduler overhead time: 0.029333672486245632 Adapter cache time: 0.03425256488844752 Engine time: 0.03004983626306057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.4-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.4,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.4-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.4     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 4320, 33, 4320, 4320, 33, 4320, 66, 4320, 33, 66, 33, 4320, 66, 66, 66, 66, 4320, 33, 33, 33, 66, 4320, 33, 4320, 4320, 66, 66, 66, 33, 4320, 33, 66, 66, 33, 66, 33, 33, 66, 66, 4320, 66, 33, 66, 4320, 66, 66, 4320, 66, 4320, 33, 4320, 4320, 66, 33, 33, 4320, 66, 33, 66, 66, 4320, 66, 33, 33, 4320, 4320, 4320, 4320, 4320, 4320, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 4320, 4320, 33, 4320, 33, 66, 4320, 4320, 4320, 66, 33, 4320, 66, 33, 33, 33, 33, 33, 4320, 66, 33, 4320, 33, 4320, 33, 33, 66, 66, 4320, 66, 4320, 33, 66, 33, 4320, 4320, 33, 4320, 66, 33, 4320, 33, 33, 4320, 4320, 66, 4320, 4320, 33, 4320, 66, 66, 33, 66, 4320, 4320, 33, 4320, 66, 33, 66, 4320, 33, 33, 33, 33, 33, 33, 4320, 33, 33, 33, 4320, 33, 66, 4320, 4320, 66, 33, 4320, 33, 66, 66, 33, 66, 33, 4320, 33, 33, 4320, 33, 4320, 4320, 66, 4320, 66, 33, 66, 4320, 66, 4320, 33, 4320, 33, 66, 4320, 66, 4320, 66, 4320, 66, 4320, 66, 33, 4320, 33, 4320, 66, 66, 33, 66, 4320, 66, 66, 4320, 33, 4320, 4320, 66, 33, 4320, 4320, 66, 66, 4320, 4320, 66, 66, 66, 66, 4320, 4320, 66, 66, 4320, 33, 4320, 66, 4320, 4320, 33, 4320, 66, 66, 4320, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 4320, 66, 33, 33, 33, 33]
Prompts retrieved: 379935 . Total input tokens: 84650362 . Total output tokens: 76040480
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.622357506770641,
    "estimated_duration": 3600.0159197751987,
    "input_throughput": 4271.094445871269,
    "output_throughput": 3777.824127191432,
    "total_throughput": 8048.918573062701,
    "itl": 88.4713704371686,
    "ttft": 1689348.0041126085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9227228158339817,
    "arrivals": 126425,
    "finished_requests": 62218,
    "scheduler_time": 67.70122015352376
}
#Debug simulation 
Total elapsed time: 4.622455238830298. Arrivals time: 0.20553339179605246 Scheduler time: 4.222813112195581 Scheduler overhead time: 0.055008686147630215 Adapter cache time: 0.056900457944720984 Engine time: 0.056493152398616076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-8/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 270, 1080, 1080, 270, 1080, 540, 1080, 270, 540, 270, 1080, 540, 540, 540, 540, 1080, 270, 270, 270, 540, 1080, 270, 1080, 1080, 540, 540, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 540, 540, 1080, 540, 1080, 270, 1080, 1080, 540, 270, 270, 1080, 540, 270, 540, 540, 1080, 540, 270, 270, 1080, 1080, 1080, 1080, 1080, 1080, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 1080, 1080, 270, 1080, 270, 540, 1080, 1080, 1080, 540, 270, 1080, 540, 270, 270, 270, 270, 270, 1080, 540, 270, 1080, 270, 1080, 270, 270, 540, 540, 1080, 540, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 270, 540, 1080, 1080, 270, 1080, 540, 270, 540, 1080, 270, 270, 270, 270, 270, 270, 1080, 270, 270, 270, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 1080, 270, 270, 1080, 270, 1080, 1080, 540, 1080, 540, 270, 540, 1080, 540, 1080, 270, 1080, 270, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 270, 1080, 270, 1080, 540, 540, 270, 540, 1080, 540, 540, 1080, 270, 1080, 1080, 540, 270, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 270, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 1080, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 1080, 540, 270, 270, 270, 270]
Prompts retrieved: 161730 . Total input tokens: 36052490 . Total output tokens: 32364050
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.7291393699124455,
    "estimated_duration": 3600.060845545103,
    "input_throughput": 3628.064235681642,
    "output_throughput": 3174.563844981219,
    "total_throughput": 6802.628080662861,
    "itl": 245.82150094512394,
    "ttft": 140918.81007590692,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.765179417303427,
    "arrivals": 54189,
    "finished_requests": 52467,
    "scheduler_time": 44.37057106654863
}
#Debug simulation 
Total elapsed time: 4.729233095888048. Arrivals time: 0.12482384545728564 Scheduler time: 4.500843915622681 Scheduler overhead time: 0.02487631095573306 Adapter cache time: 0.04300227062776685 Engine time: 0.024510789662599564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-16/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 270, 1080, 1080, 270, 1080, 540, 1080, 270, 540, 270, 1080, 540, 540, 540, 540, 1080, 270, 270, 270, 540, 1080, 270, 1080, 1080, 540, 540, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 540, 540, 1080, 540, 1080, 270, 1080, 1080, 540, 270, 270, 1080, 540, 270, 540, 540, 1080, 540, 270, 270, 1080, 1080, 1080, 1080, 1080, 1080, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 1080, 1080, 270, 1080, 270, 540, 1080, 1080, 1080, 540, 270, 1080, 540, 270, 270, 270, 270, 270, 1080, 540, 270, 1080, 270, 1080, 270, 270, 540, 540, 1080, 540, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 270, 540, 1080, 1080, 270, 1080, 540, 270, 540, 1080, 270, 270, 270, 270, 270, 270, 1080, 270, 270, 270, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 1080, 270, 270, 1080, 270, 1080, 1080, 540, 1080, 540, 270, 540, 1080, 540, 1080, 270, 1080, 270, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 270, 1080, 270, 1080, 540, 540, 270, 540, 1080, 540, 540, 1080, 270, 1080, 1080, 540, 270, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 270, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 1080, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 1080, 540, 270, 270, 270, 270]
Prompts retrieved: 161730 . Total input tokens: 36052490 . Total output tokens: 32364050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.808029762003571,
    "estimated_duration": 3600.178885541,
    "input_throughput": 3621.7058692184005,
    "output_throughput": 3169.4661189829512,
    "total_throughput": 6791.171988201352,
    "itl": 243.70879692152675,
    "ttft": 150502.24004105447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1590,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.182387862619839,
    "arrivals": 54189,
    "finished_requests": 52357,
    "scheduler_time": 44.276756889406784
}
#Debug simulation 
Total elapsed time: 4.808149813208729. Arrivals time: 0.12496484722942114 Scheduler time: 4.5777240032330155 Scheduler overhead time: 0.024973994586616755 Adapter cache time: 0.04442690499126911 Engine time: 0.024672943633049726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-8-32/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 270, 1080, 1080, 270, 1080, 540, 1080, 270, 540, 270, 1080, 540, 540, 540, 540, 1080, 270, 270, 270, 540, 1080, 270, 1080, 1080, 540, 540, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 540, 540, 1080, 540, 1080, 270, 1080, 1080, 540, 270, 270, 1080, 540, 270, 540, 540, 1080, 540, 270, 270, 1080, 1080, 1080, 1080, 1080, 1080, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 1080, 1080, 270, 1080, 270, 540, 1080, 1080, 1080, 540, 270, 1080, 540, 270, 270, 270, 270, 270, 1080, 540, 270, 1080, 270, 1080, 270, 270, 540, 540, 1080, 540, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 270, 540, 1080, 1080, 270, 1080, 540, 270, 540, 1080, 270, 270, 270, 270, 270, 270, 1080, 270, 270, 270, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 1080, 270, 270, 1080, 270, 1080, 1080, 540, 1080, 540, 270, 540, 1080, 540, 1080, 270, 1080, 270, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 270, 1080, 270, 1080, 540, 540, 270, 540, 1080, 540, 540, 1080, 270, 1080, 1080, 540, 270, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 270, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 1080, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 1080, 540, 270, 270, 270, 270]
Prompts retrieved: 161730 . Total input tokens: 36052490 . Total output tokens: 32364050
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.548766475636512,
    "estimated_duration": 3600.10835073213,
    "input_throughput": 2979.7022630772963,
    "output_throughput": 2627.5686947245013,
    "total_throughput": 5607.2709578017975,
    "itl": 126.80311133900554,
    "ttft": 999627.0016160756,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7036,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 22.990830656867363,
    "arrivals": 54189,
    "finished_requests": 43056,
    "scheduler_time": 36.91531128125312
}
#Debug simulation 
Total elapsed time: 3.548860180657357. Arrivals time: 0.13634643564000726 Scheduler time: 3.0936388564296067 Scheduler overhead time: 0.040831318125128746 Adapter cache time: 0.2164334189146757 Engine time: 0.04261370562016964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-16-16/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 270, 1080, 1080, 270, 1080, 540, 1080, 270, 540, 270, 1080, 540, 540, 540, 540, 1080, 270, 270, 270, 540, 1080, 270, 1080, 1080, 540, 540, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 540, 540, 1080, 540, 1080, 270, 1080, 1080, 540, 270, 270, 1080, 540, 270, 540, 540, 1080, 540, 270, 270, 1080, 1080, 1080, 1080, 1080, 1080, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 1080, 1080, 270, 1080, 270, 540, 1080, 1080, 1080, 540, 270, 1080, 540, 270, 270, 270, 270, 270, 1080, 540, 270, 1080, 270, 1080, 270, 270, 540, 540, 1080, 540, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 270, 540, 1080, 1080, 270, 1080, 540, 270, 540, 1080, 270, 270, 270, 270, 270, 270, 1080, 270, 270, 270, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 1080, 270, 270, 1080, 270, 1080, 1080, 540, 1080, 540, 270, 540, 1080, 540, 1080, 270, 1080, 270, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 270, 1080, 270, 1080, 540, 540, 270, 540, 1080, 540, 540, 1080, 270, 1080, 1080, 540, 270, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 270, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 1080, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 1080, 540, 270, 270, 270, 270]
Prompts retrieved: 161730 . Total input tokens: 36052490 . Total output tokens: 32364050
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.676274374127388,
    "estimated_duration": 3600.2043814879985,
    "input_throughput": 3622.9740364376257,
    "output_throughput": 3170.5163903173398,
    "total_throughput": 6793.4904267549655,
    "itl": 243.96517762533966,
    "ttft": 148450.99426421738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1597,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.004740889922139,
    "arrivals": 54189,
    "finished_requests": 52380,
    "scheduler_time": 44.28863484707662
}
#Debug simulation 
Total elapsed time: 4.67636787192896. Arrivals time: 0.12469725823029876 Scheduler time: 4.4469613041728735 Scheduler overhead time: 0.02471313625574112 Adapter cache time: 0.044447122141718864 Engine time: 0.024302649311721325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_8-16-32/adapters_256_slots_192_rate_0.1-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 270, 1080, 1080, 270, 1080, 540, 1080, 270, 540, 270, 1080, 540, 540, 540, 540, 1080, 270, 270, 270, 540, 1080, 270, 1080, 1080, 540, 540, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 540, 540, 1080, 540, 1080, 270, 1080, 1080, 540, 270, 270, 1080, 540, 270, 540, 540, 1080, 540, 270, 270, 1080, 1080, 1080, 1080, 1080, 1080, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 1080, 1080, 270, 1080, 270, 540, 1080, 1080, 1080, 540, 270, 1080, 540, 270, 270, 270, 270, 270, 1080, 540, 270, 1080, 270, 1080, 270, 270, 540, 540, 1080, 540, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 270, 540, 1080, 1080, 270, 1080, 540, 270, 540, 1080, 270, 270, 270, 270, 270, 270, 1080, 270, 270, 270, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 1080, 270, 270, 1080, 270, 1080, 1080, 540, 1080, 540, 270, 540, 1080, 540, 1080, 270, 1080, 270, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 270, 1080, 270, 1080, 540, 540, 270, 540, 1080, 540, 540, 1080, 270, 1080, 1080, 540, 270, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 270, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 1080, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 1080, 540, 270, 270, 270, 270]
Prompts retrieved: 161730 . Total input tokens: 36052490 . Total output tokens: 32364050
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.5082240011543036,
    "estimated_duration": 3600.1332848281945,
    "input_throughput": 2979.186644338875,
    "output_throughput": 2627.2885617487304,
    "total_throughput": 5606.475206087605,
    "itl": 126.75347556579992,
    "ttft": 1000338.2854051931,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7022,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.22166159141712,
    "arrivals": 54189,
    "finished_requests": 43051,
    "scheduler_time": 36.90860598956311
}
#Debug simulation 
Total elapsed time: 3.5083015612326562. Arrivals time: 0.12869469448924065 Scheduler time: 3.067085792776197 Scheduler overhead time: 0.04071349324658513 Adapter cache time: 0.21139454981312156 Engine time: 0.04148734360933304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_256_slots_192_rate_0.1-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_16-16-16/adapters_256_slots_192_rate_0.1-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 270, 1080, 1080, 270, 1080, 540, 1080, 270, 540, 270, 1080, 540, 540, 540, 540, 1080, 270, 270, 270, 540, 1080, 270, 1080, 1080, 540, 540, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 540, 540, 1080, 540, 1080, 270, 1080, 1080, 540, 270, 270, 1080, 540, 270, 540, 540, 1080, 540, 270, 270, 1080, 1080, 1080, 1080, 1080, 1080, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 1080, 1080, 270, 1080, 270, 540, 1080, 1080, 1080, 540, 270, 1080, 540, 270, 270, 270, 270, 270, 1080, 540, 270, 1080, 270, 1080, 270, 270, 540, 540, 1080, 540, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 270, 540, 1080, 1080, 270, 1080, 540, 270, 540, 1080, 270, 270, 270, 270, 270, 270, 1080, 270, 270, 270, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 1080, 270, 270, 1080, 270, 1080, 1080, 540, 1080, 540, 270, 540, 1080, 540, 1080, 270, 1080, 270, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 270, 1080, 270, 1080, 540, 540, 270, 540, 1080, 540, 540, 1080, 270, 1080, 1080, 540, 270, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 270, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 1080, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 1080, 540, 270, 270, 270, 270]
Prompts retrieved: 161730 . Total input tokens: 36052490 . Total output tokens: 32364050
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.702008299995214,
    "estimated_duration": 3600.2427798335625,
    "input_throughput": 3622.583752699848,
    "output_throughput": 3170.2653676393597,
    "total_throughput": 6792.849120339208,
    "itl": 243.95171004940252,
    "ttft": 148486.88783193662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1595,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.7691302203688375,
    "arrivals": 54189,
    "finished_requests": 52379,
    "scheduler_time": 44.29129617921031
}
#Debug simulation 
Total elapsed time: 4.702135751955211. Arrivals time: 0.13020503614097834 Scheduler time: 4.466921516228467 Scheduler overhead time: 0.02471225755289197 Adapter cache time: 0.04471900733187795 Engine time: 0.024351793341338634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_256_slots_192_rate_0.1-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.025_size_16-16-32/adapters_256_slots_192_rate_0.1-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.1  ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 270, 1080, 1080, 270, 1080, 540, 1080, 270, 540, 270, 1080, 540, 540, 540, 540, 1080, 270, 270, 270, 540, 1080, 270, 1080, 1080, 540, 540, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 270, 540, 540, 1080, 540, 270, 540, 1080, 540, 540, 1080, 540, 1080, 270, 1080, 1080, 540, 270, 270, 1080, 540, 270, 540, 540, 1080, 540, 270, 270, 1080, 1080, 1080, 1080, 1080, 1080, 270, 540, 270, 540, 270, 270, 270, 540, 540, 540, 270, 270, 1080, 1080, 270, 1080, 270, 540, 1080, 1080, 1080, 540, 270, 1080, 540, 270, 270, 270, 270, 270, 1080, 540, 270, 1080, 270, 1080, 270, 270, 540, 540, 1080, 540, 1080, 270, 540, 270, 1080, 1080, 270, 1080, 540, 270, 1080, 270, 270, 1080, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 270, 540, 1080, 1080, 270, 1080, 540, 270, 540, 1080, 270, 270, 270, 270, 270, 270, 1080, 270, 270, 270, 1080, 270, 540, 1080, 1080, 540, 270, 1080, 270, 540, 540, 270, 540, 270, 1080, 270, 270, 1080, 270, 1080, 1080, 540, 1080, 540, 270, 540, 1080, 540, 1080, 270, 1080, 270, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 270, 1080, 270, 1080, 540, 540, 270, 540, 1080, 540, 540, 1080, 270, 1080, 1080, 540, 270, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 270, 1080, 540, 1080, 1080, 270, 1080, 540, 540, 1080, 270, 540, 270, 270, 270, 270, 540, 540, 540, 540, 540, 1080, 540, 270, 270, 270, 270]
Prompts retrieved: 161730 . Total input tokens: 36052490 . Total output tokens: 32364050
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.521533139050007,
    "estimated_duration": 3600.0910012533773,
    "input_throughput": 2979.549404797129,
    "output_throughput": 2627.4255280510533,
    "total_throughput": 5606.974932848182,
    "itl": 126.82293285365209,
    "ttft": 999843.2488981714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 7035,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 23.57522308822534,
    "arrivals": 54189,
    "finished_requests": 43054,
    "scheduler_time": 36.911884576995654
}
#Debug simulation 
Total elapsed time: 3.5216219280846417. Arrivals time: 0.1373667069710791 Scheduler time: 3.0680546960793436 Scheduler overhead time: 0.04064873978495598 Adapter cache time: 0.21492300182580948 Engine time: 0.041775381192564964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 135, 1080, 1080, 135, 1080, 540, 1080, 135, 540, 135, 1080, 540, 540, 540, 540, 1080, 135, 135, 135, 540, 1080, 135, 1080, 1080, 540, 540, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 540, 540, 1080, 540, 1080, 135, 1080, 1080, 540, 135, 135, 1080, 540, 135, 540, 540, 1080, 540, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 1080, 1080, 135, 1080, 135, 540, 1080, 1080, 1080, 540, 135, 1080, 540, 135, 135, 135, 135, 135, 1080, 540, 135, 1080, 135, 1080, 135, 135, 540, 540, 1080, 540, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 135, 540, 1080, 1080, 135, 1080, 540, 135, 540, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 540, 1080, 540, 135, 540, 1080, 540, 1080, 135, 1080, 135, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 135, 1080, 135, 1080, 540, 540, 135, 540, 1080, 540, 540, 1080, 135, 1080, 1080, 540, 135, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 135, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 1080, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 1080, 540, 135, 135, 135, 135]
Prompts retrieved: 150255 . Total input tokens: 33450605 . Total output tokens: 30090347
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.8424252942204475,
    "estimated_duration": 3600.045955750732,
    "input_throughput": 3442.221058374188,
    "output_throughput": 3000.173090220365,
    "total_throughput": 6442.3941485945525,
    "itl": 194.32979444581343,
    "ttft": 45100.55413708066,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3132,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.585447613997221,
    "arrivals": 50289,
    "finished_requests": 49690,
    "scheduler_time": 40.68248087428659
}
#Debug simulation 
Total elapsed time: 3.8425138401798904. Arrivals time: 0.11749538080766797 Scheduler time: 3.570938962046057 Scheduler overhead time: 0.02956802537664771 Adapter cache time: 0.08195595582947135 Engine time: 0.02927033230662346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 135, 1080, 1080, 135, 1080, 540, 1080, 135, 540, 135, 1080, 540, 540, 540, 540, 1080, 135, 135, 135, 540, 1080, 135, 1080, 1080, 540, 540, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 540, 540, 1080, 540, 1080, 135, 1080, 1080, 540, 135, 135, 1080, 540, 135, 540, 540, 1080, 540, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 1080, 1080, 135, 1080, 135, 540, 1080, 1080, 1080, 540, 135, 1080, 540, 135, 135, 135, 135, 135, 1080, 540, 135, 1080, 135, 1080, 135, 135, 540, 540, 1080, 540, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 135, 540, 1080, 1080, 135, 1080, 540, 135, 540, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 540, 1080, 540, 135, 540, 1080, 540, 1080, 135, 1080, 135, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 135, 1080, 135, 1080, 540, 540, 135, 540, 1080, 540, 540, 1080, 135, 1080, 1080, 540, 135, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 135, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 1080, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 1080, 540, 135, 135, 135, 135]
Prompts retrieved: 150255 . Total input tokens: 33450605 . Total output tokens: 30090347
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.79385569691658,
    "estimated_duration": 3600.1128192490855,
    "input_throughput": 3442.2985117963253,
    "output_throughput": 3000.1920890503893,
    "total_throughput": 6442.490600846714,
    "itl": 194.47030281573015,
    "ttft": 44967.92956482814,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3128,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 10.205615752039051,
    "arrivals": 50289,
    "finished_requests": 49692,
    "scheduler_time": 40.68598279546717
}
#Debug simulation 
Total elapsed time: 3.793945020996034. Arrivals time: 0.11694252397865057 Scheduler time: 3.5233350163325667 Scheduler overhead time: 0.029386368114501238 Adapter cache time: 0.08193243062123656 Engine time: 0.02908221771940589 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 135, 1080, 1080, 135, 1080, 540, 1080, 135, 540, 135, 1080, 540, 540, 540, 540, 1080, 135, 135, 135, 540, 1080, 135, 1080, 1080, 540, 540, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 540, 540, 1080, 540, 1080, 135, 1080, 1080, 540, 135, 135, 1080, 540, 135, 540, 540, 1080, 540, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 1080, 1080, 135, 1080, 135, 540, 1080, 1080, 1080, 540, 135, 1080, 540, 135, 135, 135, 135, 135, 1080, 540, 135, 1080, 135, 1080, 135, 135, 540, 540, 1080, 540, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 135, 540, 1080, 1080, 135, 1080, 540, 135, 540, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 540, 1080, 540, 135, 540, 1080, 540, 1080, 135, 1080, 135, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 135, 1080, 135, 1080, 540, 540, 135, 540, 1080, 540, 540, 1080, 135, 1080, 1080, 540, 135, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 135, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 1080, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 1080, 540, 135, 135, 135, 135]
Prompts retrieved: 150255 . Total input tokens: 33450605 . Total output tokens: 30090347
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.635104224085808,
    "estimated_duration": 3600.0516445943654,
    "input_throughput": 3150.3884165189265,
    "output_throughput": 2754.1771560105467,
    "total_throughput": 5904.565572529473,
    "itl": 121.10372486282307,
    "ttft": 482852.08462938154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4458,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.5784005214262,
    "arrivals": 50289,
    "finished_requests": 45489,
    "scheduler_time": 36.615219634018786
}
#Debug simulation 
Total elapsed time: 3.6351968413218856. Arrivals time: 0.12739374302327633 Scheduler time: 3.207732961513102 Scheduler overhead time: 0.042207688093185425 Adapter cache time: 0.19430942693725228 Engine time: 0.04380482342094183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 135, 1080, 1080, 135, 1080, 540, 1080, 135, 540, 135, 1080, 540, 540, 540, 540, 1080, 135, 135, 135, 540, 1080, 135, 1080, 1080, 540, 540, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 540, 540, 1080, 540, 1080, 135, 1080, 1080, 540, 135, 135, 1080, 540, 135, 540, 540, 1080, 540, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 1080, 1080, 135, 1080, 135, 540, 1080, 1080, 1080, 540, 135, 1080, 540, 135, 135, 135, 135, 135, 1080, 540, 135, 1080, 135, 1080, 135, 135, 540, 540, 1080, 540, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 135, 540, 1080, 1080, 135, 1080, 540, 135, 540, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 540, 1080, 540, 135, 540, 1080, 540, 1080, 135, 1080, 135, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 135, 1080, 135, 1080, 540, 540, 135, 540, 1080, 540, 540, 1080, 135, 1080, 1080, 540, 135, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 135, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 1080, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 1080, 540, 135, 135, 135, 135]
Prompts retrieved: 150255 . Total input tokens: 33450605 . Total output tokens: 30090347
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.7728323987685144,
    "estimated_duration": 3600.1802580678905,
    "input_throughput": 3442.2340304290133,
    "output_throughput": 3000.1358892503317,
    "total_throughput": 6442.369919679345,
    "itl": 194.4073239401546,
    "ttft": 44984.3903985842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3107,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.724423089763214,
    "arrivals": 50289,
    "finished_requests": 49692,
    "scheduler_time": 40.686198293241254
}
#Debug simulation 
Total elapsed time: 3.7729492890648544. Arrivals time: 0.11773094814270735 Scheduler time: 3.5022532059811056 Scheduler overhead time: 0.029436359647661448 Adapter cache time: 0.08122054347768426 Engine time: 0.029125033412128687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 135, 1080, 1080, 135, 1080, 540, 1080, 135, 540, 135, 1080, 540, 540, 540, 540, 1080, 135, 135, 135, 540, 1080, 135, 1080, 1080, 540, 540, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 540, 540, 1080, 540, 1080, 135, 1080, 1080, 540, 135, 135, 1080, 540, 135, 540, 540, 1080, 540, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 1080, 1080, 135, 1080, 135, 540, 1080, 1080, 1080, 540, 135, 1080, 540, 135, 135, 135, 135, 135, 1080, 540, 135, 1080, 135, 1080, 135, 135, 540, 540, 1080, 540, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 135, 540, 1080, 1080, 135, 1080, 540, 135, 540, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 540, 1080, 540, 135, 540, 1080, 540, 1080, 135, 1080, 135, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 135, 1080, 135, 1080, 540, 540, 135, 540, 1080, 540, 540, 1080, 135, 1080, 1080, 540, 135, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 135, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 1080, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 1080, 540, 135, 135, 135, 135]
Prompts retrieved: 150255 . Total input tokens: 33450605 . Total output tokens: 30090347
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.655564634129405,
    "estimated_duration": 3600.0494065529865,
    "input_throughput": 3150.0917680067087,
    "output_throughput": 2753.7405408811264,
    "total_throughput": 5903.832308887835,
    "itl": 121.11045468968909,
    "ttft": 483208.79717831535,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4451,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.733008300102144,
    "arrivals": 50289,
    "finished_requests": 45485,
    "scheduler_time": 36.614882636095345
}
#Debug simulation 
Total elapsed time: 3.6556865079328418. Arrivals time: 0.12896815687417984 Scheduler time: 3.2238362468779087 Scheduler overhead time: 0.04278025170788169 Adapter cache time: 0.19653266482055187 Engine time: 0.04378288518637419 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 135, 1080, 1080, 135, 1080, 540, 1080, 135, 540, 135, 1080, 540, 540, 540, 540, 1080, 135, 135, 135, 540, 1080, 135, 1080, 1080, 540, 540, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 540, 540, 1080, 540, 1080, 135, 1080, 1080, 540, 135, 135, 1080, 540, 135, 540, 540, 1080, 540, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 1080, 1080, 135, 1080, 135, 540, 1080, 1080, 1080, 540, 135, 1080, 540, 135, 135, 135, 135, 135, 1080, 540, 135, 1080, 135, 1080, 135, 135, 540, 540, 1080, 540, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 135, 540, 1080, 1080, 135, 1080, 540, 135, 540, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 540, 1080, 540, 135, 540, 1080, 540, 1080, 135, 1080, 135, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 135, 1080, 135, 1080, 540, 540, 135, 540, 1080, 540, 540, 1080, 135, 1080, 1080, 540, 135, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 135, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 1080, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 1080, 540, 135, 135, 135, 135]
Prompts retrieved: 150255 . Total input tokens: 33450605 . Total output tokens: 30090347
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.7609630818478763,
    "estimated_duration": 3600.0468863964543,
    "input_throughput": 3442.324611612093,
    "output_throughput": 3000.2414804134696,
    "total_throughput": 6442.566092025563,
    "itl": 194.32434791019483,
    "ttft": 45050.872288314626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 3110,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.299056417145858,
    "arrivals": 50289,
    "finished_requests": 49691,
    "scheduler_time": 40.68361748281482
}
#Debug simulation 
Total elapsed time: 3.761053599882871. Arrivals time: 0.11646928312256932 Scheduler time: 3.4923286358825862 Scheduler overhead time: 0.029128560796380043 Adapter cache time: 0.08123897714540362 Engine time: 0.028832219541072845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.0125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.1   ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 135, 1080, 1080, 135, 1080, 540, 1080, 135, 540, 135, 1080, 540, 540, 540, 540, 1080, 135, 135, 135, 540, 1080, 135, 1080, 1080, 540, 540, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 135, 540, 540, 1080, 540, 135, 540, 1080, 540, 540, 1080, 540, 1080, 135, 1080, 1080, 540, 135, 135, 1080, 540, 135, 540, 540, 1080, 540, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 540, 135, 540, 135, 135, 135, 540, 540, 540, 135, 135, 1080, 1080, 135, 1080, 135, 540, 1080, 1080, 1080, 540, 135, 1080, 540, 135, 135, 135, 135, 135, 1080, 540, 135, 1080, 135, 1080, 135, 135, 540, 540, 1080, 540, 1080, 135, 540, 135, 1080, 1080, 135, 1080, 540, 135, 1080, 135, 135, 1080, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 135, 540, 1080, 1080, 135, 1080, 540, 135, 540, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 540, 1080, 1080, 540, 135, 1080, 135, 540, 540, 135, 540, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 540, 1080, 540, 135, 540, 1080, 540, 1080, 135, 1080, 135, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 135, 1080, 135, 1080, 540, 540, 135, 540, 1080, 540, 540, 1080, 135, 1080, 1080, 540, 135, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 135, 1080, 540, 1080, 1080, 135, 1080, 540, 540, 1080, 135, 540, 135, 135, 135, 135, 540, 540, 540, 540, 540, 1080, 540, 135, 135, 135, 135]
Prompts retrieved: 150255 . Total input tokens: 33450605 . Total output tokens: 30090347
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.6550447707995772,
    "estimated_duration": 3600.053210239616,
    "input_throughput": 3149.899831409779,
    "output_throughput": 2753.3868032301234,
    "total_throughput": 5903.286634639902,
    "itl": 121.11916629817111,
    "ttft": 483654.4431808711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4455,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.940472293606872,
    "arrivals": 50289,
    "finished_requests": 45480,
    "scheduler_time": 36.61350546241608
}
#Debug simulation 
Total elapsed time: 3.6551384939812124. Arrivals time: 0.12875162344425917 Scheduler time: 3.225555533543229 Scheduler overhead time: 0.04265477089211345 Adapter cache time: 0.1942363502457738 Engine time: 0.044126289431005716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 66, 1080, 1080, 66, 1080, 540, 1080, 66, 540, 66, 1080, 540, 540, 540, 540, 1080, 66, 66, 66, 540, 1080, 66, 1080, 1080, 540, 540, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 540, 540, 1080, 540, 1080, 66, 1080, 1080, 540, 66, 66, 1080, 540, 66, 540, 540, 1080, 540, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 1080, 1080, 66, 1080, 66, 540, 1080, 1080, 1080, 540, 66, 1080, 540, 66, 66, 66, 66, 66, 1080, 540, 66, 1080, 66, 1080, 66, 66, 540, 540, 1080, 540, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 66, 540, 1080, 1080, 66, 1080, 540, 66, 540, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 540, 1080, 540, 66, 540, 1080, 540, 1080, 66, 1080, 66, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 66, 1080, 66, 1080, 540, 540, 66, 540, 1080, 540, 540, 1080, 66, 1080, 1080, 540, 66, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 66, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 1080, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 1080, 540, 66, 66, 66, 66]
Prompts retrieved: 144390 . Total input tokens: 32113289 . Total output tokens: 28897004
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.638381151948124,
    "estimated_duration": 3600.14803546828,
    "input_throughput": 3299.6873692319773,
    "output_throughput": 2918.9077494790113,
    "total_throughput": 6218.595118710989,
    "itl": 133.74519889893836,
    "ttft": 33283.34074624216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2337,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.152359857571058,
    "arrivals": 48457,
    "finished_requests": 48012,
    "scheduler_time": 37.882994318283664
}
#Debug simulation 
Total elapsed time: 3.638470532838255. Arrivals time: 0.11456542136147618 Scheduler time: 3.300475792493671 Scheduler overhead time: 0.03873496875166893 Adapter cache time: 0.12862135795876384 Engine time: 0.038583934772759676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 66, 1080, 1080, 66, 1080, 540, 1080, 66, 540, 66, 1080, 540, 540, 540, 540, 1080, 66, 66, 66, 540, 1080, 66, 1080, 1080, 540, 540, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 540, 540, 1080, 540, 1080, 66, 1080, 1080, 540, 66, 66, 1080, 540, 66, 540, 540, 1080, 540, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 1080, 1080, 66, 1080, 66, 540, 1080, 1080, 1080, 540, 66, 1080, 540, 66, 66, 66, 66, 66, 1080, 540, 66, 1080, 66, 1080, 66, 66, 540, 540, 1080, 540, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 66, 540, 1080, 1080, 66, 1080, 540, 66, 540, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 540, 1080, 540, 66, 540, 1080, 540, 1080, 66, 1080, 66, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 66, 1080, 66, 1080, 540, 540, 66, 540, 1080, 540, 540, 1080, 66, 1080, 1080, 540, 66, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 66, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 1080, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 1080, 540, 66, 66, 66, 66]
Prompts retrieved: 144390 . Total input tokens: 32113289 . Total output tokens: 28897004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.660844181664288,
    "estimated_duration": 3600.053517060209,
    "input_throughput": 3299.746224245179,
    "output_throughput": 2918.8385534281656,
    "total_throughput": 6218.584777673344,
    "itl": 133.86546855776945,
    "ttft": 33136.381251909974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2342,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.641332273382844,
    "arrivals": 48457,
    "finished_requests": 48011,
    "scheduler_time": 37.886326118489386
}
#Debug simulation 
Total elapsed time: 3.660947992000729. Arrivals time: 0.1118296436034143 Scheduler time: 3.325141050852835 Scheduler overhead time: 0.038077641278505325 Adapter cache time: 0.12966620130464435 Engine time: 0.038733418099582195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 66, 1080, 1080, 66, 1080, 540, 1080, 66, 540, 66, 1080, 540, 540, 540, 540, 1080, 66, 66, 66, 540, 1080, 66, 1080, 1080, 540, 540, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 540, 540, 1080, 540, 1080, 66, 1080, 1080, 540, 66, 66, 1080, 540, 66, 540, 540, 1080, 540, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 1080, 1080, 66, 1080, 66, 540, 1080, 1080, 1080, 540, 66, 1080, 540, 66, 66, 66, 66, 66, 1080, 540, 66, 1080, 66, 1080, 66, 66, 540, 540, 1080, 540, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 66, 540, 1080, 1080, 66, 1080, 540, 66, 540, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 540, 1080, 540, 66, 540, 1080, 540, 1080, 66, 1080, 66, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 66, 1080, 66, 1080, 540, 540, 66, 540, 1080, 540, 540, 1080, 66, 1080, 1080, 540, 66, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 66, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 1080, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 1080, 540, 66, 66, 66, 66]
Prompts retrieved: 144390 . Total input tokens: 32113289 . Total output tokens: 28897004
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.7053598975762725,
    "estimated_duration": 3600.0848808873466,
    "input_throughput": 3218.872438680875,
    "output_throughput": 2851.4555460897272,
    "total_throughput": 6070.327984770603,
    "itl": 116.57981238148918,
    "ttft": 159508.53521058598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.4521681229769685,
    "arrivals": 48457,
    "finished_requests": 46870,
    "scheduler_time": 36.74484277022043
}
#Debug simulation 
Total elapsed time: 3.7054556678049266. Arrivals time: 0.12096974393352866 Scheduler time: 3.305066466797143 Scheduler overhead time: 0.04388452973216772 Adapter cache time: 0.16993174515664577 Engine time: 0.04523942619562149 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 66, 1080, 1080, 66, 1080, 540, 1080, 66, 540, 66, 1080, 540, 540, 540, 540, 1080, 66, 66, 66, 540, 1080, 66, 1080, 1080, 540, 540, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 540, 540, 1080, 540, 1080, 66, 1080, 1080, 540, 66, 66, 1080, 540, 66, 540, 540, 1080, 540, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 1080, 1080, 66, 1080, 66, 540, 1080, 1080, 1080, 540, 66, 1080, 540, 66, 66, 66, 66, 66, 1080, 540, 66, 1080, 66, 1080, 66, 66, 540, 540, 1080, 540, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 66, 540, 1080, 1080, 66, 1080, 540, 66, 540, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 540, 1080, 540, 66, 540, 1080, 540, 1080, 66, 1080, 66, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 66, 1080, 66, 1080, 540, 540, 66, 540, 1080, 540, 540, 1080, 66, 1080, 1080, 540, 66, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 66, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 1080, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 1080, 540, 66, 66, 66, 66]
Prompts retrieved: 144390 . Total input tokens: 32113289 . Total output tokens: 28897004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.5937048490159214,
    "estimated_duration": 3600.140465205746,
    "input_throughput": 3299.694307711158,
    "output_throughput": 2918.9138872667418,
    "total_throughput": 6218.6081949779,
    "itl": 133.77899905748257,
    "ttft": 33209.72401221813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2334,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.3064708336905815,
    "arrivals": 48457,
    "finished_requests": 48012,
    "scheduler_time": 37.88410411720668
}
#Debug simulation 
Total elapsed time: 3.5937958359718323. Arrivals time: 0.1132572554051876 Scheduler time: 3.2601791475899518 Scheduler overhead time: 0.03794662607833743 Adapter cache time: 0.12687677098438144 Engine time: 0.03831636207178235 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 66, 1080, 1080, 66, 1080, 540, 1080, 66, 540, 66, 1080, 540, 540, 540, 540, 1080, 66, 66, 66, 540, 1080, 66, 1080, 1080, 540, 540, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 540, 540, 1080, 540, 1080, 66, 1080, 1080, 540, 66, 66, 1080, 540, 66, 540, 540, 1080, 540, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 1080, 1080, 66, 1080, 66, 540, 1080, 1080, 1080, 540, 66, 1080, 540, 66, 66, 66, 66, 66, 1080, 540, 66, 1080, 66, 1080, 66, 66, 540, 540, 1080, 540, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 66, 540, 1080, 1080, 66, 1080, 540, 66, 540, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 540, 1080, 540, 66, 540, 1080, 540, 1080, 66, 1080, 66, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 66, 1080, 66, 1080, 540, 540, 66, 540, 1080, 540, 540, 1080, 66, 1080, 1080, 540, 66, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 66, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 1080, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 1080, 540, 66, 66, 66, 66]
Prompts retrieved: 144390 . Total input tokens: 32113289 . Total output tokens: 28897004
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.675449047703296,
    "estimated_duration": 3600.105423468725,
    "input_throughput": 3218.632966818913,
    "output_throughput": 2851.347611401185,
    "total_throughput": 6069.980578220097,
    "itl": 116.58728745387886,
    "ttft": 160024.95794876857,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.541970912366943,
    "arrivals": 48457,
    "finished_requests": 46868,
    "scheduler_time": 36.745460315234446
}
#Debug simulation 
Total elapsed time: 3.6755395298823714. Arrivals time: 0.12161438586190343 Scheduler time: 3.2760176458396018 Scheduler overhead time: 0.043643351178616285 Adapter cache time: 0.16917639737948775 Engine time: 0.044950305018574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 66, 1080, 1080, 66, 1080, 540, 1080, 66, 540, 66, 1080, 540, 540, 540, 540, 1080, 66, 66, 66, 540, 1080, 66, 1080, 1080, 540, 540, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 540, 540, 1080, 540, 1080, 66, 1080, 1080, 540, 66, 66, 1080, 540, 66, 540, 540, 1080, 540, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 1080, 1080, 66, 1080, 66, 540, 1080, 1080, 1080, 540, 66, 1080, 540, 66, 66, 66, 66, 66, 1080, 540, 66, 1080, 66, 1080, 66, 66, 540, 540, 1080, 540, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 66, 540, 1080, 1080, 66, 1080, 540, 66, 540, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 540, 1080, 540, 66, 540, 1080, 540, 1080, 66, 1080, 66, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 66, 1080, 66, 1080, 540, 540, 66, 540, 1080, 540, 540, 1080, 66, 1080, 1080, 540, 66, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 66, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 1080, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 1080, 540, 66, 66, 66, 66]
Prompts retrieved: 144390 . Total input tokens: 32113289 . Total output tokens: 28897004
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.656705591827631,
    "estimated_duration": 3600.025908293185,
    "input_throughput": 3299.771530153265,
    "output_throughput": 2918.8609381375136,
    "total_throughput": 6218.632468290779,
    "itl": 133.76164589501943,
    "ttft": 33136.26687343811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2334,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.9787773883014665,
    "arrivals": 48457,
    "finished_requests": 48011,
    "scheduler_time": 37.88233246416384
}
#Debug simulation 
Total elapsed time: 3.6567975389771163. Arrivals time: 0.1158270537853241 Scheduler time: 3.3174628573469818 Scheduler overhead time: 0.03856376372277737 Adapter cache time: 0.12893675593659282 Engine time: 0.03854919644072652 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.1-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.1    ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 66, 1080, 1080, 66, 1080, 540, 1080, 66, 540, 66, 1080, 540, 540, 540, 540, 1080, 66, 66, 66, 540, 1080, 66, 1080, 1080, 540, 540, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 66, 540, 540, 1080, 540, 66, 540, 1080, 540, 540, 1080, 540, 1080, 66, 1080, 1080, 540, 66, 66, 1080, 540, 66, 540, 540, 1080, 540, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 540, 66, 540, 66, 66, 66, 540, 540, 540, 66, 66, 1080, 1080, 66, 1080, 66, 540, 1080, 1080, 1080, 540, 66, 1080, 540, 66, 66, 66, 66, 66, 1080, 540, 66, 1080, 66, 1080, 66, 66, 540, 540, 1080, 540, 1080, 66, 540, 66, 1080, 1080, 66, 1080, 540, 66, 1080, 66, 66, 1080, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 66, 540, 1080, 1080, 66, 1080, 540, 66, 540, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 540, 1080, 1080, 540, 66, 1080, 66, 540, 540, 66, 540, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 540, 1080, 540, 66, 540, 1080, 540, 1080, 66, 1080, 66, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 66, 1080, 66, 1080, 540, 540, 66, 540, 1080, 540, 540, 1080, 66, 1080, 1080, 540, 66, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 66, 1080, 540, 1080, 1080, 66, 1080, 540, 540, 1080, 66, 540, 66, 66, 66, 66, 540, 540, 540, 540, 540, 1080, 540, 66, 66, 66, 66]
Prompts retrieved: 144390 . Total input tokens: 32113289 . Total output tokens: 28897004
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.7206069543026388,
    "estimated_duration": 3600.1011776068844,
    "input_throughput": 3218.5612093553405,
    "output_throughput": 2851.2187556971094,
    "total_throughput": 6069.77996505245,
    "itl": 116.59770835139967,
    "ttft": 160185.0442862572,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2288,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.674741093516048,
    "arrivals": 48457,
    "finished_requests": 46866,
    "scheduler_time": 36.744548744197715
}
#Debug simulation 
Total elapsed time: 3.7207002001814544. Arrivals time: 0.12131422106176615 Scheduler time: 3.318125791847706 Scheduler overhead time: 0.04398832703009248 Adapter cache time: 0.17225120915099978 Engine time: 0.04477241775020957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 33, 1080, 1080, 33, 1080, 540, 1080, 33, 540, 33, 1080, 540, 540, 540, 540, 1080, 33, 33, 33, 540, 1080, 33, 1080, 1080, 540, 540, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 540, 540, 1080, 540, 1080, 33, 1080, 1080, 540, 33, 33, 1080, 540, 33, 540, 540, 1080, 540, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 1080, 1080, 33, 1080, 33, 540, 1080, 1080, 1080, 540, 33, 1080, 540, 33, 33, 33, 33, 33, 1080, 540, 33, 1080, 33, 1080, 33, 33, 540, 540, 1080, 540, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 33, 540, 1080, 1080, 33, 1080, 540, 33, 540, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 540, 1080, 540, 33, 540, 1080, 540, 1080, 33, 1080, 33, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 33, 1080, 33, 1080, 540, 540, 33, 540, 1080, 540, 540, 1080, 33, 1080, 1080, 540, 33, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 33, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 1080, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 1080, 540, 33, 33, 33, 33]
Prompts retrieved: 141585 . Total input tokens: 31494608 . Total output tokens: 28327874
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.637134376913309,
    "estimated_duration": 3600.098572896939,
    "input_throughput": 3239.7326806012125,
    "output_throughput": 2893.9129829482727,
    "total_throughput": 6133.645663549485,
    "itl": 108.82034748691635,
    "ttft": 26507.42833856531,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1043,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.192088716921937,
    "arrivals": 47448,
    "finished_requests": 47101,
    "scheduler_time": 36.34729770325796
}
#Debug simulation 
Total elapsed time: 3.6372272917069495. Arrivals time: 0.1127603342756629 Scheduler time: 3.270595689304173 Scheduler overhead time: 0.04497425490990281 Adapter cache time: 0.14274974539875984 Engine time: 0.04549930291250348 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 33, 1080, 1080, 33, 1080, 540, 1080, 33, 540, 33, 1080, 540, 540, 540, 540, 1080, 33, 33, 33, 540, 1080, 33, 1080, 1080, 540, 540, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 540, 540, 1080, 540, 1080, 33, 1080, 1080, 540, 33, 33, 1080, 540, 33, 540, 540, 1080, 540, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 1080, 1080, 33, 1080, 33, 540, 1080, 1080, 1080, 540, 33, 1080, 540, 33, 33, 33, 33, 33, 1080, 540, 33, 1080, 33, 1080, 33, 33, 540, 540, 1080, 540, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 33, 540, 1080, 1080, 33, 1080, 540, 33, 540, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 540, 1080, 540, 33, 540, 1080, 540, 1080, 33, 1080, 33, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 33, 1080, 33, 1080, 540, 540, 33, 540, 1080, 540, 540, 1080, 33, 1080, 1080, 540, 33, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 33, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 1080, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 1080, 540, 33, 33, 33, 33]
Prompts retrieved: 141585 . Total input tokens: 31494608 . Total output tokens: 28327874
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.5965582351200283,
    "estimated_duration": 3600.1159413854616,
    "input_throughput": 3239.716772985788,
    "output_throughput": 2893.864300378799,
    "total_throughput": 6133.581073364587,
    "itl": 109.53016207650037,
    "ttft": 26584.588568275067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1042,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.394703009782829,
    "arrivals": 47448,
    "finished_requests": 47100,
    "scheduler_time": 36.38459931849564
}
#Debug simulation 
Total elapsed time: 3.596650998108089. Arrivals time: 0.11307477205991745 Scheduler time: 3.235826913267374 Scheduler overhead time: 0.04429603833705187 Adapter cache time: 0.13896405650302768 Engine time: 0.04441123176366091 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 33, 1080, 1080, 33, 1080, 540, 1080, 33, 540, 33, 1080, 540, 540, 540, 540, 1080, 33, 33, 33, 540, 1080, 33, 1080, 1080, 540, 540, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 540, 540, 1080, 540, 1080, 33, 1080, 1080, 540, 33, 33, 1080, 540, 33, 540, 540, 1080, 540, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 1080, 1080, 33, 1080, 33, 540, 1080, 1080, 1080, 540, 33, 1080, 540, 33, 33, 33, 33, 33, 1080, 540, 33, 1080, 33, 1080, 33, 33, 540, 540, 1080, 540, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 33, 540, 1080, 1080, 33, 1080, 540, 33, 540, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 540, 1080, 540, 33, 540, 1080, 540, 1080, 33, 1080, 33, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 33, 1080, 33, 1080, 540, 540, 33, 540, 1080, 540, 540, 1080, 33, 1080, 1080, 540, 33, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 33, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 1080, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 1080, 540, 33, 33, 33, 33]
Prompts retrieved: 141585 . Total input tokens: 31494608 . Total output tokens: 28327874
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.737444861792028,
    "estimated_duration": 3600.092782937672,
    "input_throughput": 3230.638958840789,
    "output_throughput": 2884.8587039818544,
    "total_throughput": 6115.497662822644,
    "itl": 108.65321971970184,
    "ttft": 40309.23431084816,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1042,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4013494316860635,
    "arrivals": 47448,
    "finished_requests": 46962,
    "scheduler_time": 36.35232631534889
}
#Debug simulation 
Total elapsed time: 3.7375332480296493. Arrivals time: 0.11591599695384502 Scheduler time: 3.3542126268148422 Scheduler overhead time: 0.0462568299844861 Adapter cache time: 0.15328169241547585 Engine time: 0.046781211625784636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 33, 1080, 1080, 33, 1080, 540, 1080, 33, 540, 33, 1080, 540, 540, 540, 540, 1080, 33, 33, 33, 540, 1080, 33, 1080, 1080, 540, 540, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 540, 540, 1080, 540, 1080, 33, 1080, 1080, 540, 33, 33, 1080, 540, 33, 540, 540, 1080, 540, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 1080, 1080, 33, 1080, 33, 540, 1080, 1080, 1080, 540, 33, 1080, 540, 33, 33, 33, 33, 33, 1080, 540, 33, 1080, 33, 1080, 33, 33, 540, 540, 1080, 540, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 33, 540, 1080, 1080, 33, 1080, 540, 33, 540, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 540, 1080, 540, 33, 540, 1080, 540, 1080, 33, 1080, 33, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 33, 1080, 33, 1080, 540, 540, 33, 540, 1080, 540, 540, 1080, 33, 1080, 1080, 540, 33, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 33, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 1080, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 1080, 540, 33, 33, 33, 33]
Prompts retrieved: 141585 . Total input tokens: 31494608 . Total output tokens: 28327874
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.6585781653411686,
    "estimated_duration": 3600.0307084592564,
    "input_throughput": 3239.793475259463,
    "output_throughput": 2893.9328143839107,
    "total_throughput": 6133.726289643373,
    "itl": 109.5101493937136,
    "ttft": 26584.442868228853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1041,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2527905512181676,
    "arrivals": 47448,
    "finished_requests": 47100,
    "scheduler_time": 36.38258061499442
}
#Debug simulation 
Total elapsed time: 3.6586670372635126. Arrivals time: 0.11428992170840502 Scheduler time: 3.2920014522969723 Scheduler overhead time: 0.044934740755707026 Adapter cache time: 0.14134522527456284 Engine time: 0.04546070983633399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 33, 1080, 1080, 33, 1080, 540, 1080, 33, 540, 33, 1080, 540, 540, 540, 540, 1080, 33, 33, 33, 540, 1080, 33, 1080, 1080, 540, 540, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 540, 540, 1080, 540, 1080, 33, 1080, 1080, 540, 33, 33, 1080, 540, 33, 540, 540, 1080, 540, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 1080, 1080, 33, 1080, 33, 540, 1080, 1080, 1080, 540, 33, 1080, 540, 33, 33, 33, 33, 33, 1080, 540, 33, 1080, 33, 1080, 33, 33, 540, 540, 1080, 540, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 33, 540, 1080, 1080, 33, 1080, 540, 33, 540, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 540, 1080, 540, 33, 540, 1080, 540, 1080, 33, 1080, 33, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 33, 1080, 33, 1080, 540, 540, 33, 540, 1080, 540, 540, 1080, 33, 1080, 1080, 540, 33, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 33, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 1080, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 1080, 540, 33, 33, 33, 33]
Prompts retrieved: 141585 . Total input tokens: 31494608 . Total output tokens: 28327874
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.716426811181009,
    "estimated_duration": 3600.073016389828,
    "input_throughput": 3230.6430861402855,
    "output_throughput": 2884.8489885393496,
    "total_throughput": 6115.492074679635,
    "itl": 108.64027392061709,
    "ttft": 40224.739105235014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1040,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4381444351561403,
    "arrivals": 47448,
    "finished_requests": 46962,
    "scheduler_time": 36.35117028747058
}
#Debug simulation 
Total elapsed time: 3.7165178670547903. Arrivals time: 0.11558830505236983 Scheduler time: 3.3359193028882146 Scheduler overhead time: 0.04591885069385171 Adapter cache time: 0.15137191023677588 Engine time: 0.046496180817484856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 33, 1080, 1080, 33, 1080, 540, 1080, 33, 540, 33, 1080, 540, 540, 540, 540, 1080, 33, 33, 33, 540, 1080, 33, 1080, 1080, 540, 540, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 540, 540, 1080, 540, 1080, 33, 1080, 1080, 540, 33, 33, 1080, 540, 33, 540, 540, 1080, 540, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 1080, 1080, 33, 1080, 33, 540, 1080, 1080, 1080, 540, 33, 1080, 540, 33, 33, 33, 33, 33, 1080, 540, 33, 1080, 33, 1080, 33, 33, 540, 540, 1080, 540, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 33, 540, 1080, 1080, 33, 1080, 540, 33, 540, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 540, 1080, 540, 33, 540, 1080, 540, 1080, 33, 1080, 33, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 33, 1080, 33, 1080, 540, 540, 33, 540, 1080, 540, 540, 1080, 33, 1080, 1080, 540, 33, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 33, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 1080, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 1080, 540, 33, 33, 33, 33]
Prompts retrieved: 141585 . Total input tokens: 31494608 . Total output tokens: 28327874
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.5749373487196863,
    "estimated_duration": 3600.0390482233233,
    "input_throughput": 3239.78624780641,
    "output_throughput": 2893.960832214204,
    "total_throughput": 6133.747080020614,
    "itl": 108.81257606566137,
    "ttft": 26507.70178749582,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1044,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1216125078777974,
    "arrivals": 47448,
    "finished_requests": 47101,
    "scheduler_time": 36.346288195297774
}
#Debug simulation 
Total elapsed time: 3.575042218901217. Arrivals time: 0.10815422609448433 Scheduler time: 3.21663402300328 Scheduler overhead time: 0.04480685479938984 Adapter cache time: 0.13996416982263327 Engine time: 0.045166910625994205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.05-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.1     ]. Counts: [85 85 86]
Adapter prompts. [540, 540, 1080, 33, 1080, 1080, 33, 1080, 540, 1080, 33, 540, 33, 1080, 540, 540, 540, 540, 1080, 33, 33, 33, 540, 1080, 33, 1080, 1080, 540, 540, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 33, 540, 540, 1080, 540, 33, 540, 1080, 540, 540, 1080, 540, 1080, 33, 1080, 1080, 540, 33, 33, 1080, 540, 33, 540, 540, 1080, 540, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 540, 33, 540, 33, 33, 33, 540, 540, 540, 33, 33, 1080, 1080, 33, 1080, 33, 540, 1080, 1080, 1080, 540, 33, 1080, 540, 33, 33, 33, 33, 33, 1080, 540, 33, 1080, 33, 1080, 33, 33, 540, 540, 1080, 540, 1080, 33, 540, 33, 1080, 1080, 33, 1080, 540, 33, 1080, 33, 33, 1080, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 33, 540, 1080, 1080, 33, 1080, 540, 33, 540, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 540, 1080, 1080, 540, 33, 1080, 33, 540, 540, 33, 540, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 540, 1080, 540, 33, 540, 1080, 540, 1080, 33, 1080, 33, 540, 1080, 540, 1080, 540, 1080, 540, 1080, 540, 33, 1080, 33, 1080, 540, 540, 33, 540, 1080, 540, 540, 1080, 33, 1080, 1080, 540, 33, 1080, 1080, 540, 540, 1080, 1080, 540, 540, 540, 540, 1080, 1080, 540, 540, 1080, 33, 1080, 540, 1080, 1080, 33, 1080, 540, 540, 1080, 33, 540, 33, 33, 33, 33, 540, 540, 540, 540, 540, 1080, 540, 33, 33, 33, 33]
Prompts retrieved: 141585 . Total input tokens: 31494608 . Total output tokens: 28327874
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.755433634854853,
    "estimated_duration": 3600.0320497986904,
    "input_throughput": 3230.682071469438,
    "output_throughput": 2884.7370957657795,
    "total_throughput": 6115.419167235217,
    "itl": 108.65037381903876,
    "ttft": 40386.267477584835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1040,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4810264763236485,
    "arrivals": 47448,
    "finished_requests": 46961,
    "scheduler_time": 36.35024403750491
}
#Debug simulation 
Total elapsed time: 3.7555358158424497. Arrivals time: 0.11539970524609089 Scheduler time: 3.371439427603036 Scheduler overhead time: 0.04615673702210188 Adapter cache time: 0.15434604370966554 Engine time: 0.04687764402478933 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 135, 1080, 1080, 135, 1080, 270, 1080, 135, 270, 135, 1080, 270, 270, 270, 270, 1080, 135, 135, 135, 270, 1080, 135, 1080, 1080, 270, 270, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 270, 270, 1080, 270, 1080, 135, 1080, 1080, 270, 135, 135, 1080, 270, 135, 270, 270, 1080, 270, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 1080, 1080, 135, 1080, 135, 270, 1080, 1080, 1080, 270, 135, 1080, 270, 135, 135, 135, 135, 135, 1080, 270, 135, 1080, 135, 1080, 135, 135, 270, 270, 1080, 270, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 135, 270, 1080, 1080, 135, 1080, 270, 135, 270, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 270, 1080, 270, 135, 270, 1080, 270, 1080, 135, 1080, 135, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 135, 1080, 135, 1080, 270, 270, 135, 270, 1080, 270, 270, 1080, 135, 1080, 1080, 270, 135, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 135, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 1080, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 1080, 270, 135, 135, 135, 135]
Prompts retrieved: 127305 . Total input tokens: 28344011 . Total output tokens: 25460583
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.4550642338581383,
    "estimated_duration": 3599.8546781153805,
    "input_throughput": 2948.39151828112,
    "output_throughput": 2598.5535074147174,
    "total_throughput": 5546.945025695837,
    "itl": 71.02461082152834,
    "ttft": 13277.650668382363,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4068,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.450064142317547,
    "arrivals": 42675,
    "finished_requests": 42519,
    "scheduler_time": 28.669081748579156
}
#Debug simulation 
Total elapsed time: 3.4551501446403563. Arrivals time: 0.11010948475450277 Scheduler time: 2.9671079935505986 Scheduler overhead time: 0.059338828548789024 Adapter cache time: 0.23181967996060848 Engine time: 0.05905305268242955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 135, 1080, 1080, 135, 1080, 270, 1080, 135, 270, 135, 1080, 270, 270, 270, 270, 1080, 135, 135, 135, 270, 1080, 135, 1080, 1080, 270, 270, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 270, 270, 1080, 270, 1080, 135, 1080, 1080, 270, 135, 135, 1080, 270, 135, 270, 270, 1080, 270, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 1080, 1080, 135, 1080, 135, 270, 1080, 1080, 1080, 270, 135, 1080, 270, 135, 135, 135, 135, 135, 1080, 270, 135, 1080, 135, 1080, 135, 135, 270, 270, 1080, 270, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 135, 270, 1080, 1080, 135, 1080, 270, 135, 270, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 270, 1080, 270, 135, 270, 1080, 270, 1080, 135, 1080, 135, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 135, 1080, 135, 1080, 270, 270, 135, 270, 1080, 270, 270, 1080, 135, 1080, 1080, 270, 135, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 135, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 1080, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 1080, 270, 135, 135, 135, 135]
Prompts retrieved: 127305 . Total input tokens: 28344011 . Total output tokens: 25460583
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.4188600038178265,
    "estimated_duration": 3599.876745269524,
    "input_throughput": 2948.3734447150755,
    "output_throughput": 2598.537578346903,
    "total_throughput": 5546.911023061979,
    "itl": 71.09883882269222,
    "ttft": 13277.675252969726,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.278432394499557,
    "arrivals": 42675,
    "finished_requests": 42519,
    "scheduler_time": 28.67858848076237
}
#Debug simulation 
Total elapsed time: 3.4189514350146055. Arrivals time: 0.11022592708468437 Scheduler time: 2.934858231805265 Scheduler overhead time: 0.058989393059164286 Adapter cache time: 0.22883219830691814 Engine time: 0.05839395895600319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 135, 1080, 1080, 135, 1080, 270, 1080, 135, 270, 135, 1080, 270, 270, 270, 270, 1080, 135, 135, 135, 270, 1080, 135, 1080, 1080, 270, 270, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 270, 270, 1080, 270, 1080, 135, 1080, 1080, 270, 135, 135, 1080, 270, 135, 270, 270, 1080, 270, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 1080, 1080, 135, 1080, 135, 270, 1080, 1080, 1080, 270, 135, 1080, 270, 135, 135, 135, 135, 135, 1080, 270, 135, 1080, 135, 1080, 135, 135, 270, 270, 1080, 270, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 135, 270, 1080, 1080, 135, 1080, 270, 135, 270, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 270, 1080, 270, 135, 270, 1080, 270, 1080, 135, 1080, 135, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 135, 1080, 135, 1080, 270, 270, 135, 270, 1080, 270, 270, 1080, 135, 1080, 1080, 270, 135, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 135, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 1080, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 1080, 270, 135, 135, 135, 135]
Prompts retrieved: 127305 . Total input tokens: 28344011 . Total output tokens: 25460583
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.388597808778286,
    "estimated_duration": 3599.8387568881094,
    "input_throughput": 2948.404558312804,
    "output_throughput": 2598.5650001964113,
    "total_throughput": 5546.969558509216,
    "itl": 71.1003829371941,
    "ttft": 13277.751320565047,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.309172947331309,
    "arrivals": 42675,
    "finished_requests": 42519,
    "scheduler_time": 28.678566536593596
}
#Debug simulation 
Total elapsed time: 3.3886885456740856. Arrivals time: 0.10959053644910455 Scheduler time: 2.9072439409792423 Scheduler overhead time: 0.05917353741824627 Adapter cache time: 0.22734972182661295 Engine time: 0.057855932507663965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 135, 1080, 1080, 135, 1080, 270, 1080, 135, 270, 135, 1080, 270, 270, 270, 270, 1080, 135, 135, 135, 270, 1080, 135, 1080, 1080, 270, 270, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 270, 270, 1080, 270, 1080, 135, 1080, 1080, 270, 135, 135, 1080, 270, 135, 270, 270, 1080, 270, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 1080, 1080, 135, 1080, 135, 270, 1080, 1080, 1080, 270, 135, 1080, 270, 135, 135, 135, 135, 135, 1080, 270, 135, 1080, 135, 1080, 135, 135, 270, 270, 1080, 270, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 135, 270, 1080, 1080, 135, 1080, 270, 135, 270, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 270, 1080, 270, 135, 270, 1080, 270, 1080, 135, 1080, 135, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 135, 1080, 135, 1080, 270, 270, 135, 270, 1080, 270, 270, 1080, 135, 1080, 1080, 270, 135, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 135, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 1080, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 1080, 270, 135, 135, 135, 135]
Prompts retrieved: 127305 . Total input tokens: 28344011 . Total output tokens: 25460583
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.3921079793944955,
    "estimated_duration": 3599.859779039878,
    "input_throughput": 2948.387340473248,
    "output_throughput": 2598.549825319842,
    "total_throughput": 5546.93716579309,
    "itl": 71.05277520855432,
    "ttft": 13277.837559332236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4070,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.759924935398885,
    "arrivals": 42675,
    "finished_requests": 42519,
    "scheduler_time": 28.672589738556354
}
#Debug simulation 
Total elapsed time: 3.392200356349349. Arrivals time: 0.10982811916619539 Scheduler time: 2.9084309646859765 Scheduler overhead time: 0.05895282234996557 Adapter cache time: 0.22943966835737228 Engine time: 0.0580609068274498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 135, 1080, 1080, 135, 1080, 270, 1080, 135, 270, 135, 1080, 270, 270, 270, 270, 1080, 135, 135, 135, 270, 1080, 135, 1080, 1080, 270, 270, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 270, 270, 1080, 270, 1080, 135, 1080, 1080, 270, 135, 135, 1080, 270, 135, 270, 270, 1080, 270, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 1080, 1080, 135, 1080, 135, 270, 1080, 1080, 1080, 270, 135, 1080, 270, 135, 135, 135, 135, 135, 1080, 270, 135, 1080, 135, 1080, 135, 135, 270, 270, 1080, 270, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 135, 270, 1080, 1080, 135, 1080, 270, 135, 270, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 270, 1080, 270, 135, 270, 1080, 270, 1080, 135, 1080, 135, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 135, 1080, 135, 1080, 270, 270, 135, 270, 1080, 270, 270, 1080, 135, 1080, 1080, 270, 135, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 135, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 1080, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 1080, 270, 135, 135, 135, 135]
Prompts retrieved: 127305 . Total input tokens: 28344011 . Total output tokens: 25460583
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.3968436839058995,
    "estimated_duration": 3599.8425670679153,
    "input_throughput": 2948.401437634247,
    "output_throughput": 2598.5622497983863,
    "total_throughput": 5546.963687432633,
    "itl": 71.38125504061108,
    "ttft": 13278.229722305101,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4067,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.452736659272576,
    "arrivals": 42675,
    "finished_requests": 42519,
    "scheduler_time": 28.714902526417823
}
#Debug simulation 
Total elapsed time: 3.3969333367422223. Arrivals time: 0.1099424664862454 Scheduler time: 2.91407874180004 Scheduler overhead time: 0.058959039859473705 Adapter cache time: 0.2282436671666801 Engine time: 0.05815847031772137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 135, 1080, 1080, 135, 1080, 270, 1080, 135, 270, 135, 1080, 270, 270, 270, 270, 1080, 135, 135, 135, 270, 1080, 135, 1080, 1080, 270, 270, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 270, 270, 1080, 270, 1080, 135, 1080, 1080, 270, 135, 135, 1080, 270, 135, 270, 270, 1080, 270, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 1080, 1080, 135, 1080, 135, 270, 1080, 1080, 1080, 270, 135, 1080, 270, 135, 135, 135, 135, 135, 1080, 270, 135, 1080, 135, 1080, 135, 135, 270, 270, 1080, 270, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 135, 270, 1080, 1080, 135, 1080, 270, 135, 270, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 270, 1080, 270, 135, 270, 1080, 270, 1080, 135, 1080, 135, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 135, 1080, 135, 1080, 270, 270, 135, 270, 1080, 270, 270, 1080, 135, 1080, 1080, 270, 135, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 135, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 1080, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 1080, 270, 135, 135, 135, 135]
Prompts retrieved: 127305 . Total input tokens: 28344011 . Total output tokens: 25460583
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.4042197759263217,
    "estimated_duration": 3599.870901695006,
    "input_throughput": 2948.3782307311303,
    "output_throughput": 2598.541796483717,
    "total_throughput": 5546.920027214847,
    "itl": 71.00070383491878,
    "ttft": 13277.34650290315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4067,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 12.160534549367988,
    "arrivals": 42675,
    "finished_requests": 42519,
    "scheduler_time": 28.665907809697075
}
#Debug simulation 
Total elapsed time: 3.4043108378537. Arrivals time: 0.11156586185097694 Scheduler time: 2.916975968517363 Scheduler overhead time: 0.05902897147461772 Adapter cache time: 0.2308245487511158 Engine time: 0.058392563834786415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.0125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.1   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 135, 1080, 1080, 135, 1080, 270, 1080, 135, 270, 135, 1080, 270, 270, 270, 270, 1080, 135, 135, 135, 270, 1080, 135, 1080, 1080, 270, 270, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 135, 270, 270, 1080, 270, 135, 270, 1080, 270, 270, 1080, 270, 1080, 135, 1080, 1080, 270, 135, 135, 1080, 270, 135, 270, 270, 1080, 270, 135, 135, 1080, 1080, 1080, 1080, 1080, 1080, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 1080, 1080, 135, 1080, 135, 270, 1080, 1080, 1080, 270, 135, 1080, 270, 135, 135, 135, 135, 135, 1080, 270, 135, 1080, 135, 1080, 135, 135, 270, 270, 1080, 270, 1080, 135, 270, 135, 1080, 1080, 135, 1080, 270, 135, 1080, 135, 135, 1080, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 135, 270, 1080, 1080, 135, 1080, 270, 135, 270, 1080, 135, 135, 135, 135, 135, 135, 1080, 135, 135, 135, 1080, 135, 270, 1080, 1080, 270, 135, 1080, 135, 270, 270, 135, 270, 135, 1080, 135, 135, 1080, 135, 1080, 1080, 270, 1080, 270, 135, 270, 1080, 270, 1080, 135, 1080, 135, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 135, 1080, 135, 1080, 270, 270, 135, 270, 1080, 270, 270, 1080, 135, 1080, 1080, 270, 135, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 135, 1080, 270, 1080, 1080, 135, 1080, 270, 270, 1080, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 1080, 270, 135, 135, 135, 135]
Prompts retrieved: 127305 . Total input tokens: 28344011 . Total output tokens: 25460583
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.3985466957092285,
    "estimated_duration": 3599.8776648178005,
    "input_throughput": 2948.372691586227,
    "output_throughput": 2598.5369145796935,
    "total_throughput": 5546.909606165921,
    "itl": 71.3947129385045,
    "ttft": 13278.26459905565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4067,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.634576634428504,
    "arrivals": 42675,
    "finished_requests": 42519,
    "scheduler_time": 28.717059071080644
}
#Debug simulation 
Total elapsed time: 3.398650561925024. Arrivals time: 0.11210705619305372 Scheduler time: 2.914787704590708 Scheduler overhead time: 0.058633286971598864 Adapter cache time: 0.22742112958803773 Engine time: 0.05828429525718093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 66, 1080, 1080, 66, 1080, 270, 1080, 66, 270, 66, 1080, 270, 270, 270, 270, 1080, 66, 66, 66, 270, 1080, 66, 1080, 1080, 270, 270, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 66, 270, 270, 1080, 270, 66, 270, 1080, 270, 270, 1080, 270, 1080, 66, 1080, 1080, 270, 66, 66, 1080, 270, 66, 270, 270, 1080, 270, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 1080, 1080, 66, 1080, 66, 270, 1080, 1080, 1080, 270, 66, 1080, 270, 66, 66, 66, 66, 66, 1080, 270, 66, 1080, 66, 1080, 66, 66, 270, 270, 1080, 270, 1080, 66, 270, 66, 1080, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 66, 270, 1080, 1080, 66, 1080, 270, 66, 270, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 270, 1080, 1080, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 270, 1080, 270, 66, 270, 1080, 270, 1080, 66, 1080, 66, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 66, 1080, 66, 1080, 270, 270, 66, 270, 1080, 270, 270, 1080, 66, 1080, 1080, 270, 66, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 66, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 1080, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 1080, 270, 66, 66, 66, 66]
Prompts retrieved: 121440 . Total input tokens: 27011072 . Total output tokens: 24310264
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.3256031502969563,
    "estimated_duration": 3599.894512829973,
    "input_throughput": 2792.3120980836275,
    "output_throughput": 2485.1703204400387,
    "total_throughput": 5277.482418523666,
    "itl": 57.11209602885405,
    "ttft": 12485.948301516475,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.482892533915807,
    "arrivals": 40673,
    "finished_requests": 40533,
    "scheduler_time": 24.86421855002585
}
#Debug simulation 
Total elapsed time: 3.325694328173995. Arrivals time: 0.10709401313215494 Scheduler time: 2.810774989426136 Scheduler overhead time: 0.06909827189520001 Adapter cache time: 0.23398206010460854 Engine time: 0.0722502269782126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 66, 1080, 1080, 66, 1080, 270, 1080, 66, 270, 66, 1080, 270, 270, 270, 270, 1080, 66, 66, 66, 270, 1080, 66, 1080, 1080, 270, 270, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 66, 270, 270, 1080, 270, 66, 270, 1080, 270, 270, 1080, 270, 1080, 66, 1080, 1080, 270, 66, 66, 1080, 270, 66, 270, 270, 1080, 270, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 1080, 1080, 66, 1080, 66, 270, 1080, 1080, 1080, 270, 66, 1080, 270, 66, 66, 66, 66, 66, 1080, 270, 66, 1080, 66, 1080, 66, 66, 270, 270, 1080, 270, 1080, 66, 270, 66, 1080, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 66, 270, 1080, 1080, 66, 1080, 270, 66, 270, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 270, 1080, 1080, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 270, 1080, 270, 66, 270, 1080, 270, 1080, 66, 1080, 66, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 66, 1080, 66, 1080, 270, 270, 66, 270, 1080, 270, 270, 1080, 66, 1080, 1080, 270, 66, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 66, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 1080, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 1080, 270, 66, 66, 66, 66]
Prompts retrieved: 121440 . Total input tokens: 27011072 . Total output tokens: 24310264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.278742334805429,
    "estimated_duration": 3599.906532262684,
    "input_throughput": 2792.302775061746,
    "output_throughput": 2485.1620229086516,
    "total_throughput": 5277.464797970398,
    "itl": 57.138304754286665,
    "ttft": 12485.988907510402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2442,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.959541282839724,
    "arrivals": 40673,
    "finished_requests": 40533,
    "scheduler_time": 24.869329753872417
}
#Debug simulation 
Total elapsed time: 3.278830141760409. Arrivals time: 0.10326947923749685 Scheduler time: 2.7744116974063218 Scheduler overhead time: 0.06918417289853096 Adapter cache time: 0.2310803891159594 Engine time: 0.06821826752275229 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 66, 1080, 1080, 66, 1080, 270, 1080, 66, 270, 66, 1080, 270, 270, 270, 270, 1080, 66, 66, 66, 270, 1080, 66, 1080, 1080, 270, 270, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 66, 270, 270, 1080, 270, 66, 270, 1080, 270, 270, 1080, 270, 1080, 66, 1080, 1080, 270, 66, 66, 1080, 270, 66, 270, 270, 1080, 270, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 1080, 1080, 66, 1080, 66, 270, 1080, 1080, 1080, 270, 66, 1080, 270, 66, 66, 66, 66, 66, 1080, 270, 66, 1080, 66, 1080, 66, 66, 270, 270, 1080, 270, 1080, 66, 270, 66, 1080, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 66, 270, 1080, 1080, 66, 1080, 270, 66, 270, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 270, 1080, 1080, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 270, 1080, 270, 66, 270, 1080, 270, 1080, 66, 1080, 66, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 66, 1080, 66, 1080, 270, 270, 66, 270, 1080, 270, 270, 1080, 66, 1080, 1080, 270, 66, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 66, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 1080, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 1080, 270, 66, 66, 66, 66]
Prompts retrieved: 121440 . Total input tokens: 27011072 . Total output tokens: 24310264
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.2905725613236427,
    "estimated_duration": 3599.9190930391383,
    "input_throughput": 2792.2930322064085,
    "output_throughput": 2485.1533517236007,
    "total_throughput": 5277.44638393001,
    "itl": 57.13981556644682,
    "ttft": 12486.044667952201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.982003027573182,
    "arrivals": 40673,
    "finished_requests": 40533,
    "scheduler_time": 24.869660620224032
}
#Debug simulation 
Total elapsed time: 3.2906693583354354. Arrivals time: 0.10703608114272356 Scheduler time: 2.7802995750680566 Scheduler overhead time: 0.06861373083665967 Adapter cache time: 0.23492670245468616 Engine time: 0.06750181131064892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 66, 1080, 1080, 66, 1080, 270, 1080, 66, 270, 66, 1080, 270, 270, 270, 270, 1080, 66, 66, 66, 270, 1080, 66, 1080, 1080, 270, 270, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 66, 270, 270, 1080, 270, 66, 270, 1080, 270, 270, 1080, 270, 1080, 66, 1080, 1080, 270, 66, 66, 1080, 270, 66, 270, 270, 1080, 270, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 1080, 1080, 66, 1080, 66, 270, 1080, 1080, 1080, 270, 66, 1080, 270, 66, 66, 66, 66, 66, 1080, 270, 66, 1080, 66, 1080, 66, 66, 270, 270, 1080, 270, 1080, 66, 270, 66, 1080, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 66, 270, 1080, 1080, 66, 1080, 270, 66, 270, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 270, 1080, 1080, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 270, 1080, 270, 66, 270, 1080, 270, 1080, 66, 1080, 66, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 66, 1080, 66, 1080, 270, 270, 66, 270, 1080, 270, 270, 1080, 66, 1080, 1080, 270, 66, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 66, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 1080, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 1080, 270, 66, 66, 66, 66]
Prompts retrieved: 121440 . Total input tokens: 27011072 . Total output tokens: 24310264
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.3140620146878064,
    "estimated_duration": 3599.929409340102,
    "input_throughput": 2792.285030345254,
    "output_throughput": 2485.1462300312,
    "total_throughput": 5277.431260376454,
    "itl": 57.12290747589308,
    "ttft": 12485.93765092219,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.65528659978853,
    "arrivals": 40673,
    "finished_requests": 40533,
    "scheduler_time": 24.86628201194732
}
#Debug simulation 
Total elapsed time: 3.314152472652495. Arrivals time: 0.1080919811502099 Scheduler time: 2.8036160031333566 Scheduler overhead time: 0.06923922151327133 Adapter cache time: 0.23260297439992428 Engine time: 0.06822580425068736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 66, 1080, 1080, 66, 1080, 270, 1080, 66, 270, 66, 1080, 270, 270, 270, 270, 1080, 66, 66, 66, 270, 1080, 66, 1080, 1080, 270, 270, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 66, 270, 270, 1080, 270, 66, 270, 1080, 270, 270, 1080, 270, 1080, 66, 1080, 1080, 270, 66, 66, 1080, 270, 66, 270, 270, 1080, 270, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 1080, 1080, 66, 1080, 66, 270, 1080, 1080, 1080, 270, 66, 1080, 270, 66, 66, 66, 66, 66, 1080, 270, 66, 1080, 66, 1080, 66, 66, 270, 270, 1080, 270, 1080, 66, 270, 66, 1080, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 66, 270, 1080, 1080, 66, 1080, 270, 66, 270, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 270, 1080, 1080, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 270, 1080, 270, 66, 270, 1080, 270, 1080, 66, 1080, 66, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 66, 1080, 66, 1080, 270, 270, 66, 270, 1080, 270, 270, 1080, 66, 1080, 1080, 270, 66, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 66, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 1080, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 1080, 270, 66, 66, 66, 66]
Prompts retrieved: 121440 . Total input tokens: 27011072 . Total output tokens: 24310264
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.2913436330854893,
    "estimated_duration": 3599.924548685466,
    "input_throughput": 2792.2888005168215,
    "output_throughput": 2485.1495855008443,
    "total_throughput": 5277.438386017666,
    "itl": 57.14507083354483,
    "ttft": 12485.950932368043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.07334029037487,
    "arrivals": 40673,
    "finished_requests": 40533,
    "scheduler_time": 24.87066438932566
}
#Debug simulation 
Total elapsed time: 3.291433710139245. Arrivals time: 0.10658270539715886 Scheduler time: 2.7849700013175607 Scheduler overhead time: 0.0685870610177517 Adapter cache time: 0.23195047909393907 Engine time: 0.06717683281749487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 66, 1080, 1080, 66, 1080, 270, 1080, 66, 270, 66, 1080, 270, 270, 270, 270, 1080, 66, 66, 66, 270, 1080, 66, 1080, 1080, 270, 270, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 66, 270, 270, 1080, 270, 66, 270, 1080, 270, 270, 1080, 270, 1080, 66, 1080, 1080, 270, 66, 66, 1080, 270, 66, 270, 270, 1080, 270, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 1080, 1080, 66, 1080, 66, 270, 1080, 1080, 1080, 270, 66, 1080, 270, 66, 66, 66, 66, 66, 1080, 270, 66, 1080, 66, 1080, 66, 66, 270, 270, 1080, 270, 1080, 66, 270, 66, 1080, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 66, 270, 1080, 1080, 66, 1080, 270, 66, 270, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 270, 1080, 1080, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 270, 1080, 270, 66, 270, 1080, 270, 1080, 66, 1080, 66, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 66, 1080, 66, 1080, 270, 270, 66, 270, 1080, 270, 270, 1080, 66, 1080, 1080, 270, 66, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 66, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 1080, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 1080, 270, 66, 66, 66, 66]
Prompts retrieved: 121440 . Total input tokens: 27011072 . Total output tokens: 24310264
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.275606472045183,
    "estimated_duration": 3599.8898322331333,
    "input_throughput": 2792.3157286633927,
    "output_throughput": 2485.173551672351,
    "total_throughput": 5277.489280335743,
    "itl": 57.10457820301965,
    "ttft": 12486.131757953559,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2444,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.307682920740694,
    "arrivals": 40673,
    "finished_requests": 40533,
    "scheduler_time": 24.862469827380753
}
#Debug simulation 
Total elapsed time: 3.275699502788484. Arrivals time: 0.10666757775470614 Scheduler time: 2.7668316564522684 Scheduler overhead time: 0.06888337200507522 Adapter cache time: 0.23282571649178863 Engine time: 0.06815730687230825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.1-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.1    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 66, 1080, 1080, 66, 1080, 270, 1080, 66, 270, 66, 1080, 270, 270, 270, 270, 1080, 66, 66, 66, 270, 1080, 66, 1080, 1080, 270, 270, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 66, 270, 270, 1080, 270, 66, 270, 1080, 270, 270, 1080, 270, 1080, 66, 1080, 1080, 270, 66, 66, 1080, 270, 66, 270, 270, 1080, 270, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 1080, 1080, 66, 1080, 66, 270, 1080, 1080, 1080, 270, 66, 1080, 270, 66, 66, 66, 66, 66, 1080, 270, 66, 1080, 66, 1080, 66, 66, 270, 270, 1080, 270, 1080, 66, 270, 66, 1080, 1080, 66, 1080, 270, 66, 1080, 66, 66, 1080, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 66, 270, 1080, 1080, 66, 1080, 270, 66, 270, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 270, 1080, 1080, 270, 66, 1080, 66, 270, 270, 66, 270, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 270, 1080, 270, 66, 270, 1080, 270, 1080, 66, 1080, 66, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 66, 1080, 66, 1080, 270, 270, 66, 270, 1080, 270, 270, 1080, 66, 1080, 1080, 270, 66, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 66, 1080, 270, 1080, 1080, 66, 1080, 270, 270, 1080, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 1080, 270, 66, 66, 66, 66]
Prompts retrieved: 121440 . Total input tokens: 27011072 . Total output tokens: 24310264
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.325241000857204,
    "estimated_duration": 3599.90645625458,
    "input_throughput": 2792.302834018178,
    "output_throughput": 2485.162075380141,
    "total_throughput": 5277.464909398319,
    "itl": 57.150840786420744,
    "ttft": 12486.075718832224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2445,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.187298648841324,
    "arrivals": 40673,
    "finished_requests": 40533,
    "scheduler_time": 24.87188008345197
}
#Debug simulation 
Total elapsed time: 3.325329339131713. Arrivals time: 0.10719077801331878 Scheduler time: 2.814955458510667 Scheduler overhead time: 0.06879452848806977 Adapter cache time: 0.23375471029430628 Engine time: 0.06821043882519007 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.2339670616202056,
    "estimated_duration": 3599.892354104412,
    "input_throughput": 2726.8401480971856,
    "output_throughput": 2389.355334526351,
    "total_throughput": 5116.195482623536,
    "itl": 50.683909749019165,
    "ttft": 11483.254802428348,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1213,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7123716333905232,
    "arrivals": 39791,
    "finished_requests": 39665,
    "scheduler_time": 22.011517689405952
}
#Debug simulation 
Total elapsed time: 3.2340626688674092. Arrivals time: 0.1057124356739223 Scheduler time: 2.7080224347300828 Scheduler overhead time: 0.07515701279044151 Adapter cache time: 0.23469205619767308 Engine time: 0.07511445181444287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.1911498429253697,
    "estimated_duration": 3599.93574179778,
    "input_throughput": 2726.807005476659,
    "output_throughput": 2389.281814153882,
    "total_throughput": 5116.088819630541,
    "itl": 51.24042534478158,
    "ttft": 11574.617352650399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.969723533766816,
    "arrivals": 39791,
    "finished_requests": 39664,
    "scheduler_time": 22.152877039395438
}
#Debug simulation 
Total elapsed time: 3.191244561690837. Arrivals time: 0.10636247042566538 Scheduler time: 2.6712764045223594 Scheduler overhead time: 0.07418998936191201 Adapter cache time: 0.23153597582131624 Engine time: 0.07292954297736287 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.204128932673484,
    "estimated_duration": 3599.935783467875,
    "input_throughput": 2726.806973913233,
    "output_throughput": 2389.2817864973886,
    "total_throughput": 5116.088760410622,
    "itl": 51.241479067449724,
    "ttft": 11574.630364111514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1216,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9764501231349363,
    "arrivals": 39791,
    "finished_requests": 39664,
    "scheduler_time": 22.15306221442267
}
#Debug simulation 
Total elapsed time: 3.2042377670295537. Arrivals time: 0.10808116337284446 Scheduler time: 2.6799713945947587 Scheduler overhead time: 0.07459635892882943 Adapter cache time: 0.2332189753651619 Engine time: 0.07361664017662406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.212105415761471,
    "estimated_duration": 3599.942092018001,
    "input_throughput": 2726.8024732301487,
    "output_throughput": 2389.3223224538997,
    "total_throughput": 5116.124795684048,
    "itl": 50.68723828934322,
    "ttft": 11483.262227997671,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7998967104823835,
    "arrivals": 39791,
    "finished_requests": 39665,
    "scheduler_time": 22.012565143488324
}
#Debug simulation 
Total elapsed time: 3.2121930001303554. Arrivals time: 0.10598398325964808 Scheduler time: 2.6864773891866207 Scheduler overhead time: 0.0753602241165936 Adapter cache time: 0.23426391370594501 Engine time: 0.0748239504173398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.2174710258841515,
    "estimated_duration": 3599.892057935959,
    "input_throughput": 2726.8378723634023,
    "output_throughput": 2389.2655284035213,
    "total_throughput": 5116.103400766923,
    "itl": 51.24230015572748,
    "ttft": 11665.2032030543,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.023119363784785,
    "arrivals": 39791,
    "finished_requests": 39663,
    "scheduler_time": 22.153044393296852
}
#Debug simulation 
Total elapsed time: 3.217560537159443. Arrivals time: 0.1018629064783454 Scheduler time: 2.700550159905106 Scheduler overhead time: 0.07453119568526745 Adapter cache time: 0.23202270036563277 Engine time: 0.07347210356965661 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.2250564908608794,
    "estimated_duration": 3599.928885348181,
    "input_throughput": 2726.8124767555164,
    "output_throughput": 2389.331087902332,
    "total_throughput": 5116.143564657848,
    "itl": 50.68149103437634,
    "ttft": 11483.223572898622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1214,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.629921058011149,
    "arrivals": 39791,
    "finished_requests": 39665,
    "scheduler_time": 22.01090616237103
}
#Debug simulation 
Total elapsed time: 3.22516251122579. Arrivals time: 0.1064179022796452 Scheduler time: 2.6963375275954604 Scheduler overhead time: 0.07556009897962213 Adapter cache time: 0.23572433134540915 Engine time: 0.0755575355142355 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.025-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.1     ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 1080, 33, 1080, 1080, 33, 1080, 270, 1080, 33, 270, 33, 1080, 270, 270, 270, 270, 1080, 33, 33, 33, 270, 1080, 33, 1080, 1080, 270, 270, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 33, 270, 270, 1080, 270, 33, 270, 1080, 270, 270, 1080, 270, 1080, 33, 1080, 1080, 270, 33, 33, 1080, 270, 33, 270, 270, 1080, 270, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 1080, 1080, 33, 1080, 33, 270, 1080, 1080, 1080, 270, 33, 1080, 270, 33, 33, 33, 33, 33, 1080, 270, 33, 1080, 33, 1080, 33, 33, 270, 270, 1080, 270, 1080, 33, 270, 33, 1080, 1080, 33, 1080, 270, 33, 1080, 33, 33, 1080, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 33, 270, 1080, 1080, 33, 1080, 270, 33, 270, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 270, 1080, 1080, 270, 33, 1080, 33, 270, 270, 33, 270, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 270, 1080, 270, 33, 270, 1080, 270, 1080, 33, 1080, 33, 270, 1080, 270, 1080, 270, 1080, 270, 1080, 270, 33, 1080, 33, 1080, 270, 270, 33, 270, 1080, 270, 270, 1080, 33, 1080, 1080, 270, 33, 1080, 1080, 270, 270, 1080, 1080, 270, 270, 270, 270, 1080, 1080, 270, 270, 1080, 33, 1080, 270, 1080, 1080, 33, 1080, 270, 270, 1080, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 1080, 270, 33, 33, 33, 33]
Prompts retrieved: 118635 . Total input tokens: 26372899 . Total output tokens: 23739096
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.24512467905879,
    "estimated_duration": 3599.896557409708,
    "input_throughput": 2726.8344641167405,
    "output_throughput": 2389.2625420850673,
    "total_throughput": 5116.097006201808,
    "itl": 51.245134442958346,
    "ttft": 11665.160797975434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.075684446506258,
    "arrivals": 39791,
    "finished_requests": 39663,
    "scheduler_time": 22.15363496134925
}
#Debug simulation 
Total elapsed time: 3.2452132259495556. Arrivals time: 0.10688291536644101 Scheduler time: 2.7197089535184205 Scheduler overhead time: 0.07501318538561463 Adapter cache time: 0.2344947070814669 Engine time: 0.07397451810538769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.048394793178886,
    "estimated_duration": 3599.9962144927117,
    "input_throughput": 2540.5290603309095,
    "output_throughput": 2242.705136049066,
    "total_throughput": 4783.234196379975,
    "itl": 43.455594358506765,
    "ttft": 10877.40408762029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2073,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.344391093172783,
    "arrivals": 36981,
    "finished_requests": 36870,
    "scheduler_time": 17.734906522763428
}
#Debug simulation 
Total elapsed time: 3.0485041621141136. Arrivals time: 0.10283233923837543 Scheduler time: 2.508683846332133 Scheduler overhead time: 0.08492110017687082 Adapter cache time: 0.22890949435532093 Engine time: 0.08323053177446127 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.068076857831329,
    "estimated_duration": 3599.993618269893,
    "input_throughput": 2540.5308924951346,
    "output_throughput": 2242.706753430336,
    "total_throughput": 4783.23764592547,
    "itl": 43.46785795242684,
    "ttft": 10877.371973700256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2073,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.7581498475952415,
    "arrivals": 36981,
    "finished_requests": 36870,
    "scheduler_time": 17.73914652395264
}
#Debug simulation 
Total elapsed time: 3.068164528813213. Arrivals time: 0.10074695572257042 Scheduler time: 2.526020842604339 Scheduler overhead time: 0.08510054275393486 Adapter cache time: 0.23203004663810134 Engine time: 0.08420356875285506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.0243160678073764,
    "estimated_duration": 3600.0304427291576,
    "input_throughput": 2540.5049055825657,
    "output_throughput": 2242.6838129400267,
    "total_throughput": 4783.188718522592,
    "itl": 43.468969790063134,
    "ttft": 10877.493019766109,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.767862874660596,
    "arrivals": 36981,
    "finished_requests": 36870,
    "scheduler_time": 17.739716033978826
}
#Debug simulation 
Total elapsed time: 3.0244064670987427. Arrivals time: 0.10005098721012473 Scheduler time: 2.487520277965814 Scheduler overhead time: 0.08487691218033433 Adapter cache time: 0.22889296850189567 Engine time: 0.08305905340239406 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.0767639470286667,
    "estimated_duration": 3600.027912038436,
    "input_throughput": 2540.50669146655,
    "output_throughput": 2242.685389466447,
    "total_throughput": 4783.192080932997,
    "itl": 43.46062271275542,
    "ttft": 10877.484680514915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2073,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.496648843793451,
    "arrivals": 36981,
    "finished_requests": 36870,
    "scheduler_time": 17.736800102747047
}
#Debug simulation 
Total elapsed time: 3.076858120970428. Arrivals time: 0.1006674412637949 Scheduler time: 2.5365512166172266 Scheduler overhead time: 0.08474618103355169 Adapter cache time: 0.23078782577067614 Engine time: 0.0840991628356278 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 3.0649173418059945,
    "estimated_duration": 3600.0295595196644,
    "input_throughput": 2540.505528854684,
    "output_throughput": 2242.6843631465185,
    "total_throughput": 4783.189892001203,
    "itl": 43.470622757626,
    "ttft": 10877.457565864297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.848219544179616,
    "arrivals": 36981,
    "finished_requests": 36870,
    "scheduler_time": 17.74010266757891
}
#Debug simulation 
Total elapsed time: 3.0650066500529647. Arrivals time: 0.10119064664468169 Scheduler time: 2.5204248633235693 Scheduler overhead time: 0.0883607380092144 Adapter cache time: 0.22960377903655171 Engine time: 0.08541540522128344 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.0618166900239885,
    "estimated_duration": 3600.0331284413714,
    "input_throughput": 2540.503010304158,
    "output_throughput": 2242.682139843393,
    "total_throughput": 4783.185150147551,
    "itl": 43.451565528039616,
    "ttft": 10877.397068880764,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2073,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.1983742613320265,
    "arrivals": 36981,
    "finished_requests": 36870,
    "scheduler_time": 17.733738532347502
}
#Debug simulation 
Total elapsed time: 3.0619108099490404. Arrivals time: 0.10165686346590519 Scheduler time: 2.5222637918777764 Scheduler overhead time: 0.08492040215060115 Adapter cache time: 0.22967857867479324 Engine time: 0.08334380760788918 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.1-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.1    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 66, 1080, 1080, 66, 1080, 135, 1080, 66, 135, 66, 1080, 135, 135, 135, 135, 1080, 66, 66, 66, 135, 1080, 66, 1080, 1080, 135, 135, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 66, 135, 135, 1080, 135, 66, 135, 1080, 135, 135, 1080, 135, 1080, 66, 1080, 1080, 135, 66, 66, 1080, 135, 66, 135, 135, 1080, 135, 66, 66, 1080, 1080, 1080, 1080, 1080, 1080, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 1080, 1080, 66, 1080, 66, 135, 1080, 1080, 1080, 135, 66, 1080, 135, 66, 66, 66, 66, 66, 1080, 135, 66, 1080, 66, 1080, 66, 66, 135, 135, 1080, 135, 1080, 66, 135, 66, 1080, 1080, 66, 1080, 135, 66, 1080, 66, 66, 1080, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 66, 135, 1080, 1080, 66, 1080, 135, 66, 135, 1080, 66, 66, 66, 66, 66, 66, 1080, 66, 66, 66, 1080, 66, 135, 1080, 1080, 135, 66, 1080, 66, 135, 135, 66, 135, 66, 1080, 66, 66, 1080, 66, 1080, 1080, 135, 1080, 135, 66, 135, 1080, 135, 1080, 66, 1080, 66, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 66, 1080, 66, 1080, 135, 135, 66, 135, 1080, 135, 135, 1080, 66, 1080, 1080, 135, 66, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 66, 1080, 135, 1080, 1080, 66, 1080, 135, 135, 1080, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 1080, 135, 66, 66, 66, 66]
Prompts retrieved: 109965 . Total input tokens: 24446343 . Total output tokens: 22011364
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.075436525978148,
    "estimated_duration": 3599.9957505435186,
    "input_throughput": 2540.529387741409,
    "output_throughput": 2242.705425077529,
    "total_throughput": 4783.234812818938,
    "itl": 43.47356373292451,
    "ttft": 10877.363314513592,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2072,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.940019808262356,
    "arrivals": 36981,
    "finished_requests": 36870,
    "scheduler_time": 17.74114039015137
}
#Debug simulation 
Total elapsed time: 3.075525193940848. Arrivals time: 0.10060803219676018 Scheduler time: 2.5351697923615575 Scheduler overhead time: 0.08497977675870061 Adapter cache time: 0.23136048344895244 Engine time: 0.08339636633172631 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.0183183648623526,
    "estimated_duration": 3599.992965292615,
    "input_throughput": 2471.4201071436887,
    "output_throughput": 2185.34454812762,
    "total_throughput": 4656.764655271309,
    "itl": 40.879567382503616,
    "ttft": 9582.158944521003,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8990613857704277,
    "arrivals": 35945,
    "finished_requests": 35850,
    "scheduler_time": 15.964797775655754
}
#Debug simulation 
Total elapsed time: 3.018407320138067. Arrivals time: 0.0957152796909213 Scheduler time: 2.483524397481233 Scheduler overhead time: 0.08858815720304847 Adapter cache time: 0.22113100113347173 Engine time: 0.08769851364195347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.981338649056852,
    "estimated_duration": 3599.9850568379143,
    "input_throughput": 2471.425536364548,
    "output_throughput": 2185.349348897093,
    "total_throughput": 4656.7748852616405,
    "itl": 40.88552018849014,
    "ttft": 9582.186001959271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.151726952558373,
    "arrivals": 35945,
    "finished_requests": 35850,
    "scheduler_time": 15.967234902766908
}
#Debug simulation 
Total elapsed time: 2.9814523248933256. Arrivals time: 0.09902301244437695 Scheduler time: 2.4387349733151495 Scheduler overhead time: 0.08988354774191976 Adapter cache time: 0.22298978455364704 Engine time: 0.08882367936894298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.0234574447385967,
    "estimated_duration": 3599.9885213542275,
    "input_throughput": 2471.42315794194,
    "output_throughput": 2185.3472457852567,
    "total_throughput": 4656.770403727197,
    "itl": 40.88566028840745,
    "ttft": 9582.197418620879,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.16005641374731,
    "arrivals": 35945,
    "finished_requests": 35850,
    "scheduler_time": 15.96726321642317
}
#Debug simulation 
Total elapsed time: 3.0235481006093323. Arrivals time: 0.09777623927220702 Scheduler time: 2.4856101851910353 Scheduler overhead time: 0.08916653413325548 Adapter cache time: 0.22208496322855353 Engine time: 0.08690641541033983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.966885518748313,
    "estimated_duration": 3600.008711431245,
    "input_throughput": 2471.40929735773,
    "output_throughput": 2185.3349896123586,
    "total_throughput": 4656.744286970088,
    "itl": 40.88207490872077,
    "ttft": 9582.125232399865,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9907403970928423,
    "arrivals": 35945,
    "finished_requests": 35850,
    "scheduler_time": 15.96568834228891
}
#Debug simulation 
Total elapsed time: 2.9669730379246175. Arrivals time: 0.09763237088918686 Scheduler time: 2.430085150990635 Scheduler overhead time: 0.0896895588375628 Adapter cache time: 0.22122741304337978 Engine time: 0.08665639907121658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.9844617820344865,
    "estimated_duration": 3600.0209129050745,
    "input_throughput": 2471.4009210630934,
    "output_throughput": 2185.3275829032505,
    "total_throughput": 4656.728503966344,
    "itl": 40.88795412647679,
    "ttft": 9582.219457792331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.209603405594811,
    "arrivals": 35945,
    "finished_requests": 35850,
    "scheduler_time": 15.968134366923763
}
#Debug simulation 
Total elapsed time: 2.9845702019520104. Arrivals time: 0.0982027156278491 Scheduler time: 2.446174777112901 Scheduler overhead time: 0.0889604203402996 Adapter cache time: 0.22203645715489984 Engine time: 0.08762989100068808 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.0081037459895015,
    "estimated_duration": 3600.0140101504658,
    "input_throughput": 2471.405659787457,
    "output_throughput": 2185.3317731036227,
    "total_throughput": 4656.737432891079,
    "itl": 40.87702602979998,
    "ttft": 9582.154778525883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8093240757052733,
    "arrivals": 35945,
    "finished_requests": 35850,
    "scheduler_time": 15.963816680395768
}
#Debug simulation 
Total elapsed time: 3.0081940218806267. Arrivals time: 0.09790494432672858 Scheduler time: 2.470399077516049 Scheduler overhead time: 0.08921750634908676 Adapter cache time: 0.22109317546710372 Engine time: 0.08765801461413503 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.0125-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.1     ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 1080, 33, 1080, 1080, 33, 1080, 135, 1080, 33, 135, 33, 1080, 135, 135, 135, 135, 1080, 33, 33, 33, 135, 1080, 33, 1080, 1080, 135, 135, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 33, 135, 135, 1080, 135, 33, 135, 1080, 135, 135, 1080, 135, 1080, 33, 1080, 1080, 135, 33, 33, 1080, 135, 33, 135, 135, 1080, 135, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 1080, 1080, 33, 1080, 33, 135, 1080, 1080, 1080, 135, 33, 1080, 135, 33, 33, 33, 33, 33, 1080, 135, 33, 1080, 33, 1080, 33, 33, 135, 135, 1080, 135, 1080, 33, 135, 33, 1080, 1080, 33, 1080, 135, 33, 1080, 33, 33, 1080, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 33, 135, 1080, 1080, 33, 1080, 135, 33, 135, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 135, 1080, 1080, 135, 33, 1080, 33, 135, 135, 33, 135, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 135, 1080, 135, 33, 135, 1080, 135, 1080, 33, 1080, 33, 135, 1080, 135, 1080, 135, 1080, 135, 1080, 135, 33, 1080, 33, 1080, 135, 135, 33, 135, 1080, 135, 135, 1080, 33, 1080, 1080, 135, 33, 1080, 1080, 135, 135, 1080, 1080, 135, 135, 135, 135, 1080, 1080, 135, 135, 1080, 33, 1080, 135, 1080, 1080, 33, 1080, 135, 135, 1080, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 1080, 135, 33, 33, 33, 33]
Prompts retrieved: 107160 . Total input tokens: 23830713 . Total output tokens: 21444082
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.0166905000805855,
    "estimated_duration": 3600.022976646622,
    "input_throughput": 2471.399504313036,
    "output_throughput": 2185.3263301469888,
    "total_throughput": 4656.725834460024,
    "itl": 40.88864073035708,
    "ttft": 9582.230138030123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1274,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.2654380867630595,
    "arrivals": 35945,
    "finished_requests": 35850,
    "scheduler_time": 15.968541224563955
}
#Debug simulation 
Total elapsed time: 3.0167797477915883. Arrivals time: 0.09853298403322697 Scheduler time: 2.474167368840426 Scheduler overhead time: 0.09077673451974988 Adapter cache time: 0.22264279006049037 Engine time: 0.08842422161251307 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.8420932670123875,
    "estimated_duration": 3600.005253594396,
    "input_throughput": 2329.914655437062,
    "output_throughput": 2082.003906109985,
    "total_throughput": 4411.918561547047,
    "itl": 37.4483756555798,
    "ttft": 7257.136528415791,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1082,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3114477389353185,
    "arrivals": 34024,
    "finished_requests": 33954,
    "scheduler_time": 13.000592722848731
}
#Debug simulation 
Total elapsed time: 2.8422088301740587. Arrivals time: 0.09312916314229369 Scheduler time: 2.3113388838246465 Scheduler overhead time: 0.09512194665148854 Adapter cache time: 0.206067587248981 Engine time: 0.09216424124315381 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.865387488156557,
    "estimated_duration": 3600.0297689497775,
    "input_throughput": 2329.898789266654,
    "output_throughput": 2081.9897281534295,
    "total_throughput": 4411.888517420084,
    "itl": 37.45298635065214,
    "ttft": 7468.479964304727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1082,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5237027139030426,
    "arrivals": 34024,
    "finished_requests": 33954,
    "scheduler_time": 13.0027127860043
}
#Debug simulation 
Total elapsed time: 2.8654795330949128. Arrivals time: 0.09380113938823342 Scheduler time: 2.3342817528173327 Scheduler overhead time: 0.09480972215533257 Adapter cache time: 0.2056690165773034 Engine time: 0.09222475998103619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.897836357355118,
    "estimated_duration": 3600.0093201441223,
    "input_throughput": 2329.91202357893,
    "output_throughput": 2082.001554290403,
    "total_throughput": 4411.9135778693335,
    "itl": 37.45321445219105,
    "ttft": 7257.292038978165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1082,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.531184125505344,
    "arrivals": 34024,
    "finished_requests": 33954,
    "scheduler_time": 13.002756444906689
}
#Debug simulation 
Total elapsed time: 2.897949146106839. Arrivals time: 0.09386741649359465 Scheduler time: 2.3559622950851917 Scheduler overhead time: 0.09488884266465902 Adapter cache time: 0.20753786666318774 Engine time: 0.10096246469765902 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.8715884257107973,
    "estimated_duration": 3600.0309981856058,
    "input_throughput": 2329.7374395462325,
    "output_throughput": 2081.8612405774606,
    "total_throughput": 4411.598680123693,
    "itl": 37.62631126845277,
    "ttft": 7574.3374830770845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1082,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.390092044773011,
    "arrivals": 34024,
    "finished_requests": 33953,
    "scheduler_time": 13.085441201075776
}
#Debug simulation 
Total elapsed time: 2.87166581209749. Arrivals time: 0.09147071093320847 Scheduler time: 2.339827203657478 Scheduler overhead time: 0.09437702130526304 Adapter cache time: 0.2069109627045691 Engine time: 0.09416029509156942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.8515003151260316,
    "estimated_duration": 3600.0318518077665,
    "input_throughput": 2329.8974412651623,
    "output_throughput": 2081.9885235838265,
    "total_throughput": 4411.885964848989,
    "itl": 37.454447305028594,
    "ttft": 7468.340365133917,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1082,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.572305613663042,
    "arrivals": 34024,
    "finished_requests": 33954,
    "scheduler_time": 13.003312960444372
}
#Debug simulation 
Total elapsed time: 2.851628781296313. Arrivals time: 0.09304165793582797 Scheduler time: 2.3207922806032 Scheduler overhead time: 0.09437878709286451 Adapter cache time: 0.20678485557436943 Engine time: 0.09201796678826213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.850741830188781,
    "estimated_duration": 3600.0112027867253,
    "input_throughput": 2329.9108051406,
    "output_throughput": 2082.000465498006,
    "total_throughput": 4411.911270638607,
    "itl": 37.44745273891636,
    "ttft": 7257.266797106736,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1082,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.235234419084076,
    "arrivals": 34024,
    "finished_requests": 33954,
    "scheduler_time": 12.999867993345305
}
#Debug simulation 
Total elapsed time: 2.850833415053785. Arrivals time: 0.09379445761442184 Scheduler time: 2.317224219907075 Scheduler overhead time: 0.09442199300974607 Adapter cache time: 0.208269817288965 Engine time: 0.09254723507910967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.1,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.1-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.1-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.1     ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 1080, 33, 1080, 1080, 33, 1080, 66, 1080, 33, 66, 33, 1080, 66, 66, 66, 66, 1080, 33, 33, 33, 66, 1080, 33, 1080, 1080, 66, 66, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 33, 66, 66, 1080, 66, 33, 66, 1080, 66, 66, 1080, 66, 1080, 33, 1080, 1080, 66, 33, 33, 1080, 66, 33, 66, 66, 1080, 66, 33, 33, 1080, 1080, 1080, 1080, 1080, 1080, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 1080, 1080, 33, 1080, 33, 66, 1080, 1080, 1080, 66, 33, 1080, 66, 33, 33, 33, 33, 33, 1080, 66, 33, 1080, 33, 1080, 33, 33, 66, 66, 1080, 66, 1080, 33, 66, 33, 1080, 1080, 33, 1080, 66, 33, 1080, 33, 33, 1080, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 33, 66, 1080, 1080, 33, 1080, 66, 33, 66, 1080, 33, 33, 33, 33, 33, 33, 1080, 33, 33, 33, 1080, 33, 66, 1080, 1080, 66, 33, 1080, 33, 66, 66, 33, 66, 33, 1080, 33, 33, 1080, 33, 1080, 1080, 66, 1080, 66, 33, 66, 1080, 66, 1080, 33, 1080, 33, 66, 1080, 66, 1080, 66, 1080, 66, 1080, 66, 33, 1080, 33, 1080, 66, 66, 33, 66, 1080, 66, 66, 1080, 33, 1080, 1080, 66, 33, 1080, 1080, 66, 66, 1080, 1080, 66, 66, 66, 66, 1080, 1080, 66, 66, 1080, 33, 1080, 66, 1080, 1080, 33, 1080, 66, 66, 1080, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 1080, 66, 33, 33, 33, 33]
Prompts retrieved: 101295 . Total input tokens: 22527924 . Total output tokens: 20249375
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.8807081822305918,
    "estimated_duration": 3600.00772218448,
    "input_throughput": 2329.9130577726514,
    "output_throughput": 2082.002478442437,
    "total_throughput": 4411.915536215089,
    "itl": 37.45491939309213,
    "ttft": 7257.267767271241,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1082,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.619966298714329,
    "arrivals": 34024,
    "finished_requests": 33954,
    "scheduler_time": 13.003723028906606
}
#Debug simulation 
Total elapsed time: 2.880829710047692. Arrivals time: 0.09310422884300351 Scheduler time: 2.34773863106966 Scheduler overhead time: 0.09514343179762363 Adapter cache time: 0.20685227634385228 Engine time: 0.09333738312125206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-8/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.487240712158382,
    "estimated_duration": 3599.7650458745443,
    "input_throughput": 1857.2901049923155,
    "output_throughput": 1666.0329003619681,
    "total_throughput": 3523.3230053542834,
    "itl": 33.29525158111943,
    "ttft": 7621.800382083718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.778315823184023,
    "arrivals": 27120,
    "finished_requests": 27063,
    "scheduler_time": 4.972055892403449
}
#Debug simulation 
Total elapsed time: 2.487329271156341. Arrivals time: 0.07921083644032478 Scheduler time: 1.890256633516401 Scheduler overhead time: 0.1041237972676754 Adapter cache time: 0.26257002260535955 Engine time: 0.10233957087621093 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-16/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.4678851910866797,
    "estimated_duration": 3599.7681389636527,
    "input_throughput": 1857.2885091218113,
    "output_throughput": 1666.031468828597,
    "total_throughput": 3523.3199779504084,
    "itl": 33.312675163737964,
    "ttft": 7621.876142426911,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4501,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.680324974810627,
    "arrivals": 27120,
    "finished_requests": 27063,
    "scheduler_time": 4.980380482718359
}
#Debug simulation 
Total elapsed time: 2.467989658936858. Arrivals time: 0.07991544948890805 Scheduler time: 1.8715002164244652 Scheduler overhead time: 0.10369695397093892 Adapter cache time: 0.26610457664355636 Engine time: 0.09781240997835994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-8-32/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.4928121459670365,
    "estimated_duration": 3599.7718624242702,
    "input_throughput": 1857.2865880165627,
    "output_throughput": 1666.0297455520122,
    "total_throughput": 3523.316333568575,
    "itl": 33.31358156954575,
    "ttft": 7621.869520187235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.717390645983357,
    "arrivals": 27120,
    "finished_requests": 27063,
    "scheduler_time": 4.980701106783614
}
#Debug simulation 
Total elapsed time: 2.492899073753506. Arrivals time: 0.07820324646309018 Scheduler time: 1.8965646852739155 Scheduler overhead time: 0.10396048193797469 Adapter cache time: 0.26455028587952256 Engine time: 0.10069156251847744 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-16/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.4948239899240434,
    "estimated_duration": 3599.771963399178,
    "input_throughput": 1857.2865359190012,
    "output_throughput": 1666.0296988192742,
    "total_throughput": 3523.3162347382754,
    "itl": 33.301757659721964,
    "ttft": 7621.955775563605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4502,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.101883886965085,
    "arrivals": 27120,
    "finished_requests": 27063,
    "scheduler_time": 4.9749865851393205
}
#Debug simulation 
Total elapsed time: 2.4949414650909603. Arrivals time: 0.07896795403212309 Scheduler time: 1.896360226906836 Scheduler overhead time: 0.10396818164736032 Adapter cache time: 0.264675369951874 Engine time: 0.10186121985316277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_8-16-32/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.4820525581017137,
    "estimated_duration": 3599.756496020496,
    "input_throughput": 1857.2945162793958,
    "output_throughput": 1666.0368573902153,
    "total_throughput": 3523.3313736696114,
    "itl": 33.31748756934551,
    "ttft": 7621.921393934686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4504,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 14.89680128533281,
    "arrivals": 27120,
    "finished_requests": 27063,
    "scheduler_time": 4.98220352366091
}
#Debug simulation 
Total elapsed time: 2.482161497231573. Arrivals time: 0.0783178354613483 Scheduler time: 1.8848561798222363 Scheduler overhead time: 0.10413542902097106 Adapter cache time: 0.2641807799227536 Engine time: 0.10151132382452488 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-16/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.5384022342041135,
    "estimated_duration": 3599.750745828363,
    "input_throughput": 1857.297483096009,
    "output_throughput": 1666.0395186944852,
    "total_throughput": 3523.337001790494,
    "itl": 33.289214648392814,
    "ttft": 7621.829125889824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4499,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 13.452236276766065,
    "arrivals": 27120,
    "finished_requests": 27063,
    "scheduler_time": 4.968985507734102
}
#Debug simulation 
Total elapsed time: 2.5384925291873515. Arrivals time: 0.07900271704420447 Scheduler time: 1.9375939117744565 Scheduler overhead time: 0.10483360663056374 Adapter cache time: 0.2654852420091629 Engine time: 0.1023346739821136 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.0125_size_16-16-32/adapters_256_slots_192_rate_0.05-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.05  ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 135, 540, 540, 135, 540, 270, 540, 135, 270, 135, 540, 270, 270, 270, 270, 540, 135, 135, 135, 270, 540, 135, 540, 540, 270, 270, 270, 135, 540, 135, 270, 270, 135, 270, 135, 135, 270, 270, 540, 270, 135, 270, 540, 270, 270, 540, 270, 540, 135, 540, 540, 270, 135, 135, 540, 270, 135, 270, 270, 540, 270, 135, 135, 540, 540, 540, 540, 540, 540, 135, 270, 135, 270, 135, 135, 135, 270, 270, 270, 135, 135, 540, 540, 135, 540, 135, 270, 540, 540, 540, 270, 135, 540, 270, 135, 135, 135, 135, 135, 540, 270, 135, 540, 135, 540, 135, 135, 270, 270, 540, 270, 540, 135, 270, 135, 540, 540, 135, 540, 270, 135, 540, 135, 135, 540, 540, 270, 540, 540, 135, 540, 270, 270, 135, 270, 540, 540, 135, 540, 270, 135, 270, 540, 135, 135, 135, 135, 135, 135, 540, 135, 135, 135, 540, 135, 270, 540, 540, 270, 135, 540, 135, 270, 270, 135, 270, 135, 540, 135, 135, 540, 135, 540, 540, 270, 540, 270, 135, 270, 540, 270, 540, 135, 540, 135, 270, 540, 270, 540, 270, 540, 270, 540, 270, 135, 540, 135, 540, 270, 270, 135, 270, 540, 270, 270, 540, 135, 540, 540, 270, 135, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 135, 540, 270, 540, 540, 135, 540, 270, 270, 540, 135, 270, 135, 135, 135, 135, 270, 270, 270, 270, 270, 540, 270, 135, 135, 135, 135]
Prompts retrieved: 80865 . Total input tokens: 18012338 . Total output tokens: 16162627
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.5212368248030543,
    "estimated_duration": 3599.758842067667,
    "input_throughput": 1857.2933058370477,
    "output_throughput": 1666.035771594964,
    "total_throughput": 3523.3290774320117,
    "itl": 33.32125574126925,
    "ttft": 7621.981968983598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 4503,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 15.091191211155511,
    "arrivals": 27120,
    "finished_requests": 27063,
    "scheduler_time": 4.984223759221523
}
#Debug simulation 
Total elapsed time: 2.521326582878828. Arrivals time: 0.07919283118098974 Scheduler time: 1.9209081823937595 Scheduler overhead time: 0.10442441469058394 Adapter cache time: 0.26573032932356 Engine time: 0.10207756794989109 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.3559094248339534,
    "estimated_duration": 3600.0175137046995,
    "input_throughput": 1727.0927089467518,
    "output_throughput": 1530.5475540116975,
    "total_throughput": 3257.6402629584495,
    "itl": 30.447184127441382,
    "ttft": 5911.0776224449955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.5655257030019945,
    "arrivals": 25185,
    "finished_requests": 25144,
    "scheduler_time": 2.1916180838347663
}
#Debug simulation 
Total elapsed time: 2.355998951010406. Arrivals time: 0.07456845231354237 Scheduler time: 1.765944393351674 Scheduler overhead time: 0.11113905254751444 Adapter cache time: 0.2441274393349886 Engine time: 0.10798356356099248 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.3389544459059834,
    "estimated_duration": 3600.015138389674,
    "input_throughput": 1727.0938484945327,
    "output_throughput": 1530.548563877618,
    "total_throughput": 3257.642412372151,
    "itl": 30.453797785431576,
    "ttft": 5911.138750061648,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.052102958915874,
    "arrivals": 25185,
    "finished_requests": 25144,
    "scheduler_time": 2.19444615538763
}
#Debug simulation 
Total elapsed time: 2.3390430831350386. Arrivals time: 0.07474660547450185 Scheduler time: 1.7448666328564286 Scheduler overhead time: 0.11202930379658937 Adapter cache time: 0.24516098108142614 Engine time: 0.1095312675461173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.3356210007332265,
    "estimated_duration": 3600.0155601759816,
    "input_throughput": 1727.093646144147,
    "output_throughput": 1530.548384554941,
    "total_throughput": 3257.642030699088,
    "itl": 30.454656063841078,
    "ttft": 5911.025681697097,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.068907792810293,
    "arrivals": 25185,
    "finished_requests": 25144,
    "scheduler_time": 2.194452702153702
}
#Debug simulation 
Total elapsed time: 2.3357120458967984. Arrivals time: 0.07498218538239598 Scheduler time: 1.7418346093036234 Scheduler overhead time: 0.11168581107631326 Adapter cache time: 0.24649991653859615 Engine time: 0.10804420989006758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.34533933037892,
    "estimated_duration": 3600.018251759516,
    "input_throughput": 1727.0923548682438,
    "output_throughput": 1530.5472402277344,
    "total_throughput": 3257.639595095978,
    "itl": 30.450161183207932,
    "ttft": 5911.11126993687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2473,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.7482379250621305,
    "arrivals": 25185,
    "finished_requests": 25144,
    "scheduler_time": 2.192712350465933
}
#Debug simulation 
Total elapsed time: 2.3454259070567787. Arrivals time: 0.07384126540273428 Scheduler time: 1.753330112900585 Scheduler overhead time: 0.11257420340552926 Adapter cache time: 0.2452781843021512 Engine time: 0.10693000676110387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.347385758999735,
    "estimated_duration": 3600.0142427113797,
    "input_throughput": 1727.0942781929639,
    "output_throughput": 1530.548944675869,
    "total_throughput": 3257.643222868833,
    "itl": 30.456190929316055,
    "ttft": 5911.126929645247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2472,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.163474640194123,
    "arrivals": 25185,
    "finished_requests": 25144,
    "scheduler_time": 2.19521433541748
}
#Debug simulation 
Total elapsed time: 2.347473420202732. Arrivals time: 0.0746641862206161 Scheduler time: 1.7564779524691403 Scheduler overhead time: 0.1121354978531599 Adapter cache time: 0.24440828617662191 Engine time: 0.1072121225297451 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.355715516023338,
    "estimated_duration": 3600.0074220066285,
    "input_throughput": 1727.0975504084813,
    "output_throughput": 1530.5518445094624,
    "total_throughput": 3257.649394917944,
    "itl": 30.444216289507377,
    "ttft": 5911.022594962975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2471,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.38841427870305,
    "arrivals": 25185,
    "finished_requests": 25144,
    "scheduler_time": 2.190331834877639
}
#Debug simulation 
Total elapsed time: 2.355828293133527. Arrivals time: 0.07526288973167539 Scheduler time: 1.7580009284429252 Scheduler overhead time: 0.11155988741666079 Adapter cache time: 0.24860341241583228 Engine time: 0.10944786015897989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.05-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.05   ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 66, 540, 540, 66, 540, 270, 540, 66, 270, 66, 540, 270, 270, 270, 270, 540, 66, 66, 66, 270, 540, 66, 540, 540, 270, 270, 270, 66, 540, 66, 270, 270, 66, 270, 66, 66, 270, 270, 540, 270, 66, 270, 540, 270, 270, 540, 270, 540, 66, 540, 540, 270, 66, 66, 540, 270, 66, 270, 270, 540, 270, 66, 66, 540, 540, 540, 540, 540, 540, 66, 270, 66, 270, 66, 66, 66, 270, 270, 270, 66, 66, 540, 540, 66, 540, 66, 270, 540, 540, 540, 270, 66, 540, 270, 66, 66, 66, 66, 66, 540, 270, 66, 540, 66, 540, 66, 66, 270, 270, 540, 270, 540, 66, 270, 66, 540, 540, 66, 540, 270, 66, 540, 66, 66, 540, 540, 270, 540, 540, 66, 540, 270, 270, 66, 270, 540, 540, 66, 540, 270, 66, 270, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 270, 540, 540, 270, 66, 540, 66, 270, 270, 66, 270, 66, 540, 66, 66, 540, 66, 540, 540, 270, 540, 270, 66, 270, 540, 270, 540, 66, 540, 66, 270, 540, 270, 540, 270, 540, 270, 540, 270, 66, 540, 66, 540, 270, 270, 66, 270, 540, 270, 270, 540, 66, 540, 540, 270, 66, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 66, 540, 270, 540, 540, 66, 540, 270, 270, 540, 66, 270, 66, 66, 66, 66, 270, 270, 270, 270, 270, 540, 270, 66, 66, 66, 66]
Prompts retrieved: 75000 . Total input tokens: 16686693 . Total output tokens: 14993236
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.3543387930840254,
    "estimated_duration": 3600.0027574772676,
    "input_throughput": 1727.0997882115541,
    "output_throughput": 1530.553827647948,
    "total_throughput": 3257.653615859502,
    "itl": 30.458272711334256,
    "ttft": 5911.2061199212785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2470,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 8.265701626464361,
    "arrivals": 25185,
    "finished_requests": 25144,
    "scheduler_time": 2.1955342506187385
}
#Debug simulation 
Total elapsed time: 2.35442530317232. Arrivals time: 0.0741982227191329 Scheduler time: 1.7582086464390159 Scheduler overhead time: 0.11165417404845357 Adapter cache time: 0.24723141780123115 Engine time: 0.10968795791268349 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.3192881466820836,
    "estimated_duration": 3599.882481927416,
    "input_throughput": 1634.7708653114466,
    "output_throughput": 1468.6309974122648,
    "total_throughput": 3103.4018627237115,
    "itl": 29.25381641781453,
    "ttft": 5704.030846438924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7490974863177176,
    "arrivals": 24188,
    "finished_requests": 24150,
    "scheduler_time": 1.3544394359286223
}
#Debug simulation 
Total elapsed time: 2.3193763298913836. Arrivals time: 0.07322490401566029 Scheduler time: 1.7292584548704326 Scheduler overhead time: 0.11516486573964357 Adapter cache time: 0.23387539200484753 Engine time: 0.11359335482120514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.285557177849114,
    "estimated_duration": 3599.87495428955,
    "input_throughput": 1634.7742837532605,
    "output_throughput": 1468.6340684417999,
    "total_throughput": 3103.40835219506,
    "itl": 29.257369334010598,
    "ttft": 5704.123606494258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9888706753705656,
    "arrivals": 24188,
    "finished_requests": 24150,
    "scheduler_time": 1.3553467655358697
}
#Debug simulation 
Total elapsed time: 2.2856560931541026. Arrivals time: 0.0707547077909112 Scheduler time: 1.6986665138974786 Scheduler overhead time: 0.11475343117490411 Adapter cache time: 0.23481055162847042 Engine time: 0.11228451365604997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.3186794384382665,
    "estimated_duration": 3599.8738158157535,
    "input_throughput": 1634.77480075685,
    "output_throughput": 1468.634532902914,
    "total_throughput": 3103.4093336597643,
    "itl": 29.25771086576626,
    "ttft": 5704.139586565818,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9940533240325147,
    "arrivals": 24188,
    "finished_requests": 24150,
    "scheduler_time": 1.355254235438935
}
#Debug simulation 
Total elapsed time: 2.3187679522670805. Arrivals time: 0.07204128801822662 Scheduler time: 1.7289662812836468 Scheduler overhead time: 0.11432607239112258 Adapter cache time: 0.23726215586066246 Engine time: 0.11199240200221539 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.2789932303130627,
    "estimated_duration": 3599.8760557184896,
    "input_throughput": 1634.773783572788,
    "output_throughput": 1468.633619094089,
    "total_throughput": 3103.4074026668773,
    "itl": 29.25485691633115,
    "ttft": 5704.091586218634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1225,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8331958590447455,
    "arrivals": 24188,
    "finished_requests": 24150,
    "scheduler_time": 1.3548996681912953
}
#Debug simulation 
Total elapsed time: 2.279079981148243. Arrivals time: 0.07227227976545691 Scheduler time: 1.6892796605825424 Scheduler overhead time: 0.114244076423347 Adapter cache time: 0.23613246949389577 Engine time: 0.11313671711832285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.2920411718077958,
    "estimated_duration": 3599.878253796895,
    "input_throughput": 1634.772785383211,
    "output_throughput": 1468.6327223493615,
    "total_throughput": 3103.4055077325725,
    "itl": 29.257540775227962,
    "ttft": 5704.225264746548,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.041965516656637,
    "arrivals": 24188,
    "finished_requests": 24150,
    "scheduler_time": 1.3554903771553406
}
#Debug simulation 
Total elapsed time: 2.2921489970758557. Arrivals time: 0.0721733127720654 Scheduler time: 1.7033203626051545 Scheduler overhead time: 0.114267248660326 Adapter cache time: 0.23566011106595397 Engine time: 0.11251586442813277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.2660518721677363,
    "estimated_duration": 3599.868288860125,
    "input_throughput": 1634.7773106619525,
    "output_throughput": 1468.6367877292705,
    "total_throughput": 3103.414098391223,
    "itl": 29.251981924232993,
    "ttft": 5704.04931845521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.65982156096017,
    "arrivals": 24188,
    "finished_requests": 24150,
    "scheduler_time": 1.3539471368315943
}
#Debug simulation 
Total elapsed time: 2.266164460219443. Arrivals time: 0.07101435866206884 Scheduler time: 1.6807172005064785 Scheduler overhead time: 0.11439123935997486 Adapter cache time: 0.23646734468638897 Engine time: 0.10939215077087283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.025-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.05-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.05    ]. Counts: [85 85 86]
Adapter prompts. [270, 270, 540, 33, 540, 540, 33, 540, 270, 540, 33, 270, 33, 540, 270, 270, 270, 270, 540, 33, 33, 33, 270, 540, 33, 540, 540, 270, 270, 270, 33, 540, 33, 270, 270, 33, 270, 33, 33, 270, 270, 540, 270, 33, 270, 540, 270, 270, 540, 270, 540, 33, 540, 540, 270, 33, 33, 540, 270, 33, 270, 270, 540, 270, 33, 33, 540, 540, 540, 540, 540, 540, 33, 270, 33, 270, 33, 33, 33, 270, 270, 270, 33, 33, 540, 540, 33, 540, 33, 270, 540, 540, 540, 270, 33, 540, 270, 33, 33, 33, 33, 33, 540, 270, 33, 540, 33, 540, 33, 33, 270, 270, 540, 270, 540, 33, 270, 33, 540, 540, 33, 540, 270, 33, 540, 33, 33, 540, 540, 270, 540, 540, 33, 540, 270, 270, 33, 270, 540, 540, 33, 540, 270, 33, 270, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 270, 540, 540, 270, 33, 540, 33, 270, 270, 33, 270, 33, 540, 33, 33, 540, 33, 540, 540, 270, 540, 270, 33, 270, 540, 270, 540, 33, 540, 33, 270, 540, 270, 540, 270, 540, 270, 540, 270, 33, 540, 33, 540, 270, 270, 33, 270, 540, 270, 270, 540, 33, 540, 540, 270, 33, 540, 540, 270, 270, 540, 540, 270, 270, 270, 270, 540, 540, 270, 270, 540, 33, 540, 270, 540, 540, 33, 540, 270, 270, 540, 33, 270, 33, 33, 33, 33, 270, 270, 270, 270, 270, 540, 270, 33, 33, 33, 33]
Prompts retrieved: 72195 . Total input tokens: 16059542 . Total output tokens: 14433736
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.2841374902054667,
    "estimated_duration": 3599.861667339157,
    "input_throughput": 1634.780317642009,
    "output_throughput": 1468.63948911343,
    "total_throughput": 3103.4198067554394,
    "itl": 29.258690995948804,
    "ttft": 5704.123926756332,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1224,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.094279091805271,
    "arrivals": 24188,
    "finished_requests": 24150,
    "scheduler_time": 1.3557378193098688
}
#Debug simulation 
Total elapsed time: 2.2842275639995933. Arrivals time: 0.0733818574808538 Scheduler time: 1.683841900434345 Scheduler overhead time: 0.12351879244670272 Adapter cache time: 0.23695691768079996 Engine time: 0.11071537062525749 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.108365684747696,
    "estimated_duration": 3599.9442689374364,
    "input_throughput": 1450.996073764713,
    "output_throughput": 1297.4456411178328,
    "total_throughput": 2748.4417148825455,
    "itl": 26.888678156843827,
    "ttft": 6653.124748367421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.815706205738444,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.1664711386518016
}
#Debug simulation 
Total elapsed time: 2.108455994632095. Arrivals time: 0.06565794302150607 Scheduler time: 1.5259065879508853 Scheduler overhead time: 0.12580468086525798 Adapter cache time: 0.21227815560996532 Engine time: 0.12006777664646506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.090130363125354,
    "estimated_duration": 3599.948700384777,
    "input_throughput": 1450.994287624624,
    "output_throughput": 1297.4440439945083,
    "total_throughput": 2748.438331619132,
    "itl": 26.894158794757807,
    "ttft": 6653.234026274353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.261520101446372,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.16691977884638723
}
#Debug simulation 
Total elapsed time: 2.0902220960706472. Arrivals time: 0.06455928646028042 Scheduler time: 1.5139196342788637 Scheduler overhead time: 0.12270710710436106 Adapter cache time: 0.21318487636744976 Engine time: 0.11782913561910391 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.095079705119133,
    "estimated_duration": 3599.9436959512836,
    "input_throughput": 1450.9963047129522,
    "output_throughput": 1297.4458476261698,
    "total_throughput": 2748.442152339122,
    "itl": 26.89434311129266,
    "ttft": 6653.403834636313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.275356530640224,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.16699746418518016
}
#Debug simulation 
Total elapsed time: 2.095184799283743. Arrivals time: 0.06607027584686875 Scheduler time: 1.5127005148679018 Scheduler overhead time: 0.12283003656193614 Adapter cache time: 0.21369145018979907 Engine time: 0.12154274620115757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.104666388127953,
    "estimated_duration": 3599.942282227981,
    "input_throughput": 1450.9968745296678,
    "output_throughput": 1297.4463571425133,
    "total_throughput": 2748.4432316721814,
    "itl": 26.891015906018616,
    "ttft": 6653.228173969113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.984083880225427,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.1666469836136017
}
#Debug simulation 
Total elapsed time: 2.104762217029929. Arrivals time: 0.06542875105515122 Scheduler time: 1.5223132916726172 Scheduler overhead time: 0.12467404920607805 Adapter cache time: 0.21584563935175538 Engine time: 0.117652652785182 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.033934600651264,
    "estimated_duration": 3599.9566387114482,
    "input_throughput": 1450.9910880120149,
    "output_throughput": 1297.4411829781984,
    "total_throughput": 2748.432270990213,
    "itl": 26.895703097070232,
    "ttft": 6653.22622166195,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.360743351615807,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.16691335720232284
}
#Debug simulation 
Total elapsed time: 2.034023799933493. Arrivals time: 0.06411239132285118 Scheduler time: 1.4659723984077573 Scheduler overhead time: 0.12085194699466228 Adapter cache time: 0.21005505276843905 Engine time: 0.11551997018978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.0540561457164586,
    "estimated_duration": 3599.956488855603,
    "input_throughput": 1450.9911484126048,
    "output_throughput": 1297.441236986947,
    "total_throughput": 2748.432385399552,
    "itl": 26.887132562141673,
    "ttft": 6653.151696957875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.658842006746945,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.16634099593047696
}
#Debug simulation 
Total elapsed time: 2.054146390873939. Arrivals time: 0.06408373778685927 Scheduler time: 1.4768490055575967 Scheduler overhead time: 0.12167877843603492 Adapter cache time: 0.21467474196106195 Engine time: 0.11903834622353315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.05-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.05   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 66, 540, 540, 66, 540, 135, 540, 66, 135, 66, 540, 135, 135, 135, 135, 540, 66, 66, 66, 135, 540, 66, 540, 540, 135, 135, 135, 66, 540, 66, 135, 135, 66, 135, 66, 66, 135, 135, 540, 135, 66, 135, 540, 135, 135, 540, 135, 540, 66, 540, 540, 135, 66, 66, 540, 135, 66, 135, 135, 540, 135, 66, 66, 540, 540, 540, 540, 540, 540, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 540, 540, 66, 540, 66, 135, 540, 540, 540, 135, 66, 540, 135, 66, 66, 66, 66, 66, 540, 135, 66, 540, 66, 540, 66, 66, 135, 135, 540, 135, 540, 66, 135, 66, 540, 540, 66, 540, 135, 66, 540, 66, 66, 540, 540, 135, 540, 540, 66, 540, 135, 135, 66, 135, 540, 540, 66, 540, 135, 66, 135, 540, 66, 66, 66, 66, 66, 66, 540, 66, 66, 66, 540, 66, 135, 540, 540, 135, 66, 540, 66, 135, 135, 66, 135, 66, 540, 66, 66, 540, 66, 540, 540, 135, 540, 135, 66, 135, 540, 135, 540, 66, 540, 66, 135, 540, 135, 540, 135, 540, 135, 540, 135, 66, 540, 66, 540, 135, 135, 66, 135, 540, 135, 135, 540, 66, 540, 540, 135, 66, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 66, 540, 135, 540, 540, 66, 540, 135, 135, 540, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 540, 135, 66, 66, 66, 66]
Prompts retrieved: 63525 . Total input tokens: 14099936 . Total output tokens: 12725172
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.0854064631275833,
    "estimated_duration": 3599.9411191339764,
    "input_throughput": 1450.9973433278258,
    "output_throughput": 1297.4467763305026,
    "total_throughput": 2748.4441196583284,
    "itl": 26.896706832222506,
    "ttft": 6653.266118716507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.460843365601913,
    "arrivals": 21245,
    "finished_requests": 21206,
    "scheduler_time": 0.1670812376700436
}
#Debug simulation 
Total elapsed time: 2.085497620049864. Arrivals time: 0.06498033925890923 Scheduler time: 1.5085259363986552 Scheduler overhead time: 0.12151805078610778 Adapter cache time: 0.2141452431678772 Engine time: 0.11867167707532644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.006610181182623,
    "estimated_duration": 3600.020933060403,
    "input_throughput": 1394.9649441984898,
    "output_throughput": 1245.5241464910578,
    "total_throughput": 2640.4890906895475,
    "itl": 26.413721789952525,
    "ttft": 6031.672311481491,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8868194347946963,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.11595159398477534
}
#Debug simulation 
Total elapsed time: 2.0067084413021803. Arrivals time: 0.06218076450750232 Scheduler time: 1.4424528642557561 Scheduler overhead time: 0.122668641153723 Adapter cache time: 0.20257791597396135 Engine time: 0.11836257902905345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.0223817410878837,
    "estimated_duration": 3600.008293812894,
    "input_throughput": 1394.9698417725388,
    "output_throughput": 1245.5285193943073,
    "total_throughput": 2640.498361166846,
    "itl": 26.205645836113455,
    "ttft": 6031.304288197676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.139636868312948,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.10645046212558161
}
#Debug simulation 
Total elapsed time: 2.0224862350150943. Arrivals time: 0.06431810557842255 Scheduler time: 1.4502900312654674 Scheduler overhead time: 0.12400821456685662 Adapter cache time: 0.20468112593516707 Engine time: 0.12022131495177746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.038684908300638,
    "estimated_duration": 3600.0052092720684,
    "input_throughput": 1394.971037004539,
    "output_throughput": 1245.5295865826429,
    "total_throughput": 2640.500623587182,
    "itl": 26.206140107311906,
    "ttft": 6031.456349285896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1471995519659774,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.10648173635989489
}
#Debug simulation 
Total elapsed time: 2.0387725941836834. Arrivals time: 0.06323078414425254 Scheduler time: 1.4696007566526532 Scheduler overhead time: 0.12354399543255568 Adapter cache time: 0.20309702726081014 Engine time: 0.12028018990531564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 2.0384945110417902,
    "estimated_duration": 3600.016157148409,
    "input_throughput": 1395.0981831087806,
    "output_throughput": 1245.6530204998005,
    "total_throughput": 2640.751203608581,
    "itl": 26.204587037815042,
    "ttft": 5855.1132852456285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.9749729549814505,
    "arrivals": 20441,
    "finished_requests": 20408,
    "scheduler_time": 0.1065202662242815
}
#Debug simulation 
Total elapsed time: 2.0385818779468536. Arrivals time: 0.06256882613524795 Scheduler time: 1.4651170894503593 Scheduler overhead time: 0.12572452425956726 Adapter cache time: 0.20516860159114003 Engine time: 0.12031979905441403 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 2.0103701571933925,
    "estimated_duration": 3600.018957633348,
    "input_throughput": 1395.097097850204,
    "output_throughput": 1245.652051495869,
    "total_throughput": 2640.749149346073,
    "itl": 26.20653432674855,
    "ttft": 5855.23359500787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.197878327891219,
    "arrivals": 20441,
    "finished_requests": 20408,
    "scheduler_time": 0.10661604901126501
}
#Debug simulation 
Total elapsed time: 2.0104613709263504. Arrivals time: 0.06320881424471736 Scheduler time: 1.4414971503429115 Scheduler overhead time: 0.12386062275618315 Adapter cache time: 0.20134436944499612 Engine time: 0.1216628854162991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 2.018419895786792,
    "estimated_duration": 3600.012681261453,
    "input_throughput": 1394.9681416789658,
    "output_throughput": 1245.5270014295688,
    "total_throughput": 2640.4951431085346,
    "itl": 26.412705812010035,
    "ttft": 6031.625009251873,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1270,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.797363874525665,
    "arrivals": 20441,
    "finished_requests": 20407,
    "scheduler_time": 0.11586794562191541
}
#Debug simulation 
Total elapsed time: 2.0185186229646206. Arrivals time: 0.06238018162548542 Scheduler time: 1.452823359053582 Scheduler overhead time: 0.12304553017020226 Adapter cache time: 0.2030014991760254 Engine time: 0.11847873171791434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.0125-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.05-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.05    ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 540, 33, 540, 540, 33, 540, 135, 540, 33, 135, 33, 540, 135, 135, 135, 135, 540, 33, 33, 33, 135, 540, 33, 540, 540, 135, 135, 135, 33, 540, 33, 135, 135, 33, 135, 33, 33, 135, 135, 540, 135, 33, 135, 540, 135, 135, 540, 135, 540, 33, 540, 540, 135, 33, 33, 540, 135, 33, 135, 135, 540, 135, 33, 33, 540, 540, 540, 540, 540, 540, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 540, 540, 33, 540, 33, 135, 540, 540, 540, 135, 33, 540, 135, 33, 33, 33, 33, 33, 540, 135, 33, 540, 33, 540, 33, 33, 135, 135, 540, 135, 540, 33, 135, 33, 540, 540, 33, 540, 135, 33, 540, 33, 33, 540, 540, 135, 540, 540, 33, 540, 135, 135, 33, 135, 540, 540, 33, 540, 135, 33, 135, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 135, 540, 540, 135, 33, 540, 33, 135, 135, 33, 135, 33, 540, 33, 33, 540, 33, 540, 540, 135, 540, 135, 33, 135, 540, 135, 540, 33, 540, 33, 135, 540, 135, 540, 135, 540, 135, 540, 135, 33, 540, 33, 540, 135, 135, 33, 135, 540, 135, 135, 540, 33, 540, 540, 135, 33, 540, 540, 135, 135, 540, 540, 135, 135, 135, 135, 540, 540, 135, 135, 540, 33, 540, 135, 540, 540, 33, 540, 135, 135, 540, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 540, 135, 33, 33, 33, 33]
Prompts retrieved: 60720 . Total input tokens: 13472222 . Total output tokens: 12171855
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 2.02166094398126,
    "estimated_duration": 3600.0156462822456,
    "input_throughput": 1395.0983810824914,
    "output_throughput": 1245.6531972662488,
    "total_throughput": 2640.75157834874,
    "itl": 26.207676626406414,
    "ttft": 5855.0788539616315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1269,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.253461501486637,
    "arrivals": 20441,
    "finished_requests": 20408,
    "scheduler_time": 0.10657476694484341
}
#Debug simulation 
Total elapsed time: 2.021748861297965. Arrivals time: 0.06303724506869912 Scheduler time: 1.451488587539643 Scheduler overhead time: 0.12278308160603046 Adapter cache time: 0.2035002587363124 Engine time: 0.12215202115476131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.87745292019099,
    "estimated_duration": 3599.871470869761,
    "input_throughput": 1286.3228694332263,
    "output_throughput": 1128.3886196688493,
    "total_throughput": 2414.7114891020756,
    "itl": 24.71869421960088,
    "ttft": 4502.519529231404,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4246857854608344,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006814050630386982
}
#Debug simulation 
Total elapsed time: 1.877541831228882. Arrivals time: 0.05861669545993209 Scheduler time: 1.3181894128210843 Scheduler overhead time: 0.12956074811518192 Adapter cache time: 0.1829703194089234 Engine time: 0.1267002932727337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.9049157137051225,
    "estimated_duration": 3599.861511450166,
    "input_throughput": 1286.3264281893482,
    "output_throughput": 1128.3917414822008,
    "total_throughput": 2414.718169671549,
    "itl": 24.721534139086813,
    "ttft": 4502.411472732586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6551729360548855,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006781942410064753
}
#Debug simulation 
Total elapsed time: 1.9050069246441126. Arrivals time: 0.05846665846183896 Scheduler time: 1.3447695560753345 Scheduler overhead time: 0.13176759844645858 Adapter cache time: 0.18134601367637515 Engine time: 0.12644028710201383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.8605696810409427,
    "estimated_duration": 3599.863152023645,
    "input_throughput": 1286.3258419690019,
    "output_throughput": 1128.3912272377734,
    "total_throughput": 2414.7170692067752,
    "itl": 24.72178811793935,
    "ttft": 4502.60482050833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.6643747114762144,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006734405689600821
}
#Debug simulation 
Total elapsed time: 1.8606594856828451. Arrivals time: 0.05747469933703542 Scheduler time: 1.3063184167258441 Scheduler overhead time: 0.12955576740205288 Adapter cache time: 0.1819931697100401 Engine time: 0.12363215954974294 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.8950683693401515,
    "estimated_duration": 3599.8580698854857,
    "input_throughput": 1286.3276579532767,
    "output_throughput": 1128.3928202561656,
    "total_throughput": 2414.7204782094423,
    "itl": 24.72029468968934,
    "ttft": 4502.430794305209,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5084872167347587,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006766513909923048
}
#Debug simulation 
Total elapsed time: 1.8952329251915216. Arrivals time: 0.05984273133799434 Scheduler time: 1.3338815150782466 Scheduler overhead time: 0.12972072884440422 Adapter cache time: 0.18433809746056795 Engine time: 0.1258526355959475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.921285032760352,
    "estimated_duration": 3599.870096815853,
    "input_throughput": 1286.323360416767,
    "output_throughput": 1128.3890503696111,
    "total_throughput": 2414.712410786378,
    "itl": 24.722247468110204,
    "ttft": 4502.364939220913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7061395544558744,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006741494329652259
}
#Debug simulation 
Total elapsed time: 1.92137459712103. Arrivals time: 0.05896715959534049 Scheduler time: 1.3628884642384946 Scheduler overhead time: 0.12855863338336349 Adapter cache time: 0.18208264373242855 Engine time: 0.1275582262314856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.8675484177656472,
    "estimated_duration": 3599.849286256116,
    "input_throughput": 1286.33079659173,
    "output_throughput": 1128.3955735337415,
    "total_throughput": 2414.7263701254715,
    "itl": 24.718462983696735,
    "ttft": 4502.406733291507,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1120,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3488563302903547,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006758174049832787
}
#Debug simulation 
Total elapsed time: 1.8676247000694275. Arrivals time: 0.05765086831524968 Scheduler time: 1.3139971168711782 Scheduler overhead time: 0.1288855830207467 Adapter cache time: 0.18190193409100175 Engine time: 0.12393895396962762 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.05,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.05-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.05-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.05    ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 540, 33, 540, 540, 33, 540, 66, 540, 33, 66, 33, 540, 66, 66, 66, 66, 540, 33, 33, 33, 66, 540, 33, 540, 540, 66, 66, 66, 33, 540, 33, 66, 66, 33, 66, 33, 33, 66, 66, 540, 66, 33, 66, 540, 66, 66, 540, 66, 540, 33, 540, 540, 66, 33, 33, 540, 66, 33, 66, 66, 540, 66, 33, 33, 540, 540, 540, 540, 540, 540, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 540, 540, 33, 540, 33, 66, 540, 540, 540, 66, 33, 540, 66, 33, 33, 33, 33, 33, 540, 66, 33, 540, 33, 540, 33, 33, 66, 66, 540, 66, 540, 33, 66, 33, 540, 540, 33, 540, 66, 33, 540, 33, 33, 540, 540, 66, 540, 540, 33, 540, 66, 66, 33, 66, 540, 540, 33, 540, 66, 33, 66, 540, 33, 33, 33, 33, 33, 33, 540, 33, 33, 33, 540, 33, 66, 540, 540, 66, 33, 540, 33, 66, 66, 33, 66, 33, 540, 33, 33, 540, 33, 540, 540, 66, 540, 66, 33, 66, 540, 66, 540, 33, 540, 33, 66, 540, 66, 540, 66, 540, 66, 540, 66, 33, 540, 33, 540, 66, 66, 33, 66, 540, 66, 66, 540, 33, 540, 540, 66, 33, 540, 540, 66, 66, 540, 540, 66, 66, 66, 66, 540, 540, 66, 66, 540, 33, 540, 66, 540, 540, 33, 540, 66, 66, 540, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 540, 66, 33, 33, 33, 33]
Prompts retrieved: 54855 . Total input tokens: 12175253 . Total output tokens: 11005577
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.8658982198685408,
    "estimated_duration": 3599.861916958357,
    "input_throughput": 1286.3262832904838,
    "output_throughput": 1128.391614373966,
    "total_throughput": 2414.7178976644495,
    "itl": 24.72175997287803,
    "ttft": 4502.4935077595155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1119,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7561895614490464,
    "arrivals": 18559,
    "finished_requests": 18536,
    "scheduler_time": 0.0006806961990335546
}
#Debug simulation 
Total elapsed time: 1.865988695062697. Arrivals time: 0.05845006136223674 Scheduler time: 1.3105574240908027 Scheduler overhead time: 0.12922035856172442 Adapter cache time: 0.18103160941973329 Engine time: 0.1254117744974792 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-8/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.5437661320902407,
    "estimated_duration": 3599.6314960947066,
    "input_throughput": 924.0406979460676,
    "output_throughput": 829.5735280791181,
    "total_throughput": 1753.6142260251856,
    "itl": 22.521303488713315,
    "ttft": 5833.106041231227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.029940347813744,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 6.688538869357264e-05
}
#Debug simulation 
Total elapsed time: 1.5438608373515308. Arrivals time: 0.04678109474480152 Scheduler time: 0.9963955818675458 Scheduler overhead time: 0.1376199284568429 Adapter cache time: 0.1644431659951806 Engine time: 0.13267170917242765 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-16/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.5510811968706548,
    "estimated_duration": 3599.619674087629,
    "input_throughput": 924.0437327154766,
    "output_throughput": 829.576252595875,
    "total_throughput": 1753.6199853113517,
    "itl": 22.524569165701962,
    "ttft": 5833.047231641036,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.489080528353496,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 6.688538869357264e-05
}
#Debug simulation 
Total elapsed time: 1.5511771449819207. Arrivals time: 0.04669249011203647 Scheduler time: 1.002117831259966 Scheduler overhead time: 0.13789119757711887 Adapter cache time: 0.16388642927631736 Engine time: 0.13441875390708447 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-8-32/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.5443537281826138,
    "estimated_duration": 3599.6189032291204,
    "input_throughput": 924.0439305994729,
    "output_throughput": 829.5764302496572,
    "total_throughput": 1753.62036084913,
    "itl": 22.524908172071658,
    "ttft": 5832.928055898631,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.506276967264673,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 6.688538869357264e-05
}
#Debug simulation 
Total elapsed time: 1.5444521447643638. Arrivals time: 0.047092349734157324 Scheduler time: 0.9966984102502465 Scheduler overhead time: 0.1371932658366859 Adapter cache time: 0.16209130734205246 Engine time: 0.1354605252854526 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-16/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.5580858141183853,
    "estimated_duration": 3599.6217810032317,
    "input_throughput": 924.0431918580542,
    "output_throughput": 829.5757670317639,
    "total_throughput": 1753.618958889818,
    "itl": 22.521753602654925,
    "ttft": 5832.937084134413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.190248521386858,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 6.605140268454624e-05
}
#Debug simulation 
Total elapsed time: 1.5581841762177646. Arrivals time: 0.04672488430514932 Scheduler time: 1.0099538792856038 Scheduler overhead time: 0.13660287484526634 Adapter cache time: 0.16493093548342586 Engine time: 0.1339356517419219 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_8-16-32/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.552700340282172,
    "estimated_duration": 3599.6262935560662,
    "input_throughput": 924.0420334617695,
    "output_throughput": 829.5747270614521,
    "total_throughput": 1753.6167605232215,
    "itl": 22.52487967956776,
    "ttft": 5832.971040925982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2298,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.60325856456517,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 6.771937470259905e-05
}
#Debug simulation 
Total elapsed time: 1.5527842659503222. Arrivals time: 0.04598397621884942 Scheduler time: 1.0040263603441417 Scheduler overhead time: 0.13698017271235585 Adapter cache time: 0.16509840358048677 Engine time: 0.13476728042587638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-16/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.5453581437468529,
    "estimated_duration": 3599.6383970369147,
    "input_throughput": 924.0389264482804,
    "output_throughput": 829.5719376863221,
    "total_throughput": 1753.6108641346027,
    "itl": 22.519036939878564,
    "ttft": 5832.923855091732,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.86814552739009,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 6.605140268454624e-05
}
#Debug simulation 
Total elapsed time: 1.5454555000178516. Arrivals time: 0.04637570772320032 Scheduler time: 0.9991003051400185 Scheduler overhead time: 0.13754180120304227 Adapter cache time: 0.1638709451071918 Engine time: 0.13255437929183245 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.00625_size_16-16-32/adapters_256_slots_192_rate_0.025-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.025  ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 66, 270, 270, 66, 270, 135, 270, 66, 135, 66, 270, 135, 135, 135, 135, 270, 66, 66, 66, 135, 270, 66, 270, 270, 135, 135, 135, 66, 270, 66, 135, 135, 66, 135, 66, 66, 135, 135, 270, 135, 66, 135, 270, 135, 135, 270, 135, 270, 66, 270, 270, 135, 66, 66, 270, 135, 66, 135, 135, 270, 135, 66, 66, 270, 270, 270, 270, 270, 270, 66, 135, 66, 135, 66, 66, 66, 135, 135, 135, 66, 66, 270, 270, 66, 270, 66, 135, 270, 270, 270, 135, 66, 270, 135, 66, 66, 66, 66, 66, 270, 135, 66, 270, 66, 270, 66, 66, 135, 135, 270, 135, 270, 66, 135, 66, 270, 270, 66, 270, 135, 66, 270, 66, 66, 270, 270, 135, 270, 270, 66, 270, 135, 135, 66, 135, 270, 270, 66, 270, 135, 66, 135, 270, 66, 66, 66, 66, 66, 66, 270, 66, 66, 66, 270, 66, 135, 270, 270, 135, 66, 270, 66, 135, 135, 66, 135, 66, 270, 66, 66, 270, 66, 270, 270, 135, 270, 135, 66, 135, 270, 135, 270, 66, 270, 66, 135, 270, 135, 270, 135, 270, 135, 270, 135, 66, 270, 66, 270, 135, 135, 66, 135, 270, 135, 135, 270, 66, 270, 270, 135, 66, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 66, 270, 135, 270, 270, 66, 270, 135, 135, 270, 66, 135, 66, 66, 66, 66, 135, 135, 135, 135, 135, 270, 135, 66, 66, 66, 66]
Prompts retrieved: 40305 . Total input tokens: 8933800 . Total output tokens: 8066537
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.5388675103895366,
    "estimated_duration": 3599.62622614593,
    "input_throughput": 924.0420507662882,
    "output_throughput": 829.5747425968832,
    "total_throughput": 1753.6167933631714,
    "itl": 22.5262578185916,
    "ttft": 5833.03399602376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 7.697965751625293,
    "arrivals": 13666,
    "finished_requests": 13644,
    "scheduler_time": 6.688538869357264e-05
}
#Debug simulation 
Total elapsed time: 1.5389661863446236. Arrivals time: 0.046793078538030386 Scheduler time: 0.9937724154442549 Scheduler overhead time: 0.13696654420346022 Adapter cache time: 0.16234229318797588 Engine time: 0.133278317283839 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.4648815030232072,
    "estimated_duration": 3599.9636199386414,
    "input_throughput": 865.3937452993204,
    "output_throughput": 760.4837962353248,
    "total_throughput": 1625.8775415346452,
    "itl": 21.77965034873126,
    "ttft": 5144.679531120191,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.36119503510429,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4649935448542237. Arrivals time: 0.044462806079536676 Scheduler time: 0.9250790732912719 Scheduler overhead time: 0.14033214934170246 Adapter cache time: 0.15240473998710513 Engine time: 0.13533430639654398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.5394503991119564,
    "estimated_duration": 3599.9693258229545,
    "input_throughput": 865.3923736663565,
    "output_throughput": 760.4825908826758,
    "total_throughput": 1625.8749645490323,
    "itl": 21.781916769877128,
    "ttft": 5144.774214893575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.648170032116996,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.5395364710129797. Arrivals time: 0.045227888971567154 Scheduler time: 0.99448012560606 Scheduler overhead time: 0.14077208517119288 Adapter cache time: 0.15258256765082479 Engine time: 0.1386801372282207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.4835882792249322,
    "estimated_duration": 3599.9561626925674,
    "input_throughput": 865.395537947291,
    "output_throughput": 760.4853715641754,
    "total_throughput": 1625.8809095114664,
    "itl": 21.781545061990855,
    "ttft": 5144.701987525958,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.65672498431051,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4836783101782203. Arrivals time: 0.043901636730879545 Scheduler time: 0.9343707491643727 Scheduler overhead time: 0.14700837479904294 Adapter cache time: 0.15355405630543828 Engine time: 0.13602548278868198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.4798838938586414,
    "estimated_duration": 3599.949468787761,
    "input_throughput": 865.261589643619,
    "output_throughput": 760.4670631451579,
    "total_throughput": 1625.728652788777,
    "itl": 21.780274995559004,
    "ttft": 5428.444607385991,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.4630763528634985,
    "arrivals": 12685,
    "finished_requests": 12666,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4799711466766894. Arrivals time: 0.04419286549091339 Scheduler time: 0.9351413431577384 Scheduler overhead time: 0.14317840756848454 Adapter cache time: 0.15338629065081477 Engine time: 0.13673299690708518 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.4720692383125424,
    "estimated_duration": 3599.957542330972,
    "input_throughput": 865.3952062953465,
    "output_throughput": 760.4850801177312,
    "total_throughput": 1625.8802864130776,
    "itl": 21.782472854832807,
    "ttft": 5144.668042452525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.713691449556457,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4721560198813677. Arrivals time: 0.04501383192837238 Scheduler time: 0.9295667908154428 Scheduler overhead time: 0.14050059439614415 Adapter cache time: 0.15238500479608774 Engine time: 0.13707032473757863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.4656063728034496,
    "estimated_duration": 3599.9545995042295,
    "input_throughput": 865.2603564580983,
    "output_throughput": 760.4659793145771,
    "total_throughput": 1625.7263357726754,
    "itl": 21.778168725639386,
    "ttft": 5428.513489870231,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.260821670235486,
    "arrivals": 12685,
    "finished_requests": 12666,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4656928009353578. Arrivals time: 0.044709645211696625 Scheduler time: 0.9228754341602325 Scheduler overhead time: 0.14043972454965115 Adapter cache time: 0.15261191874742508 Engine time: 0.1378686148673296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.0125-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.025-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.025   ]. Counts: [85 85 86]
Adapter prompts. [135, 135, 270, 33, 270, 270, 33, 270, 135, 270, 33, 135, 33, 270, 135, 135, 135, 135, 270, 33, 33, 33, 135, 270, 33, 270, 270, 135, 135, 135, 33, 270, 33, 135, 135, 33, 135, 33, 33, 135, 135, 270, 135, 33, 135, 270, 135, 135, 270, 135, 270, 33, 270, 270, 135, 33, 33, 270, 135, 33, 135, 135, 270, 135, 33, 33, 270, 270, 270, 270, 270, 270, 33, 135, 33, 135, 33, 33, 33, 135, 135, 135, 33, 33, 270, 270, 33, 270, 33, 135, 270, 270, 270, 135, 33, 270, 135, 33, 33, 33, 33, 33, 270, 135, 33, 270, 33, 270, 33, 33, 135, 135, 270, 135, 270, 33, 135, 33, 270, 270, 33, 270, 135, 33, 270, 33, 33, 270, 270, 135, 270, 270, 33, 270, 135, 135, 33, 135, 270, 270, 33, 270, 135, 33, 135, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 135, 270, 270, 135, 33, 270, 33, 135, 135, 33, 135, 33, 270, 33, 33, 270, 33, 270, 270, 135, 270, 135, 33, 135, 270, 135, 270, 33, 270, 33, 135, 270, 135, 270, 135, 270, 135, 270, 135, 33, 270, 33, 270, 135, 135, 33, 135, 270, 135, 135, 270, 33, 270, 270, 135, 33, 270, 270, 135, 135, 270, 270, 135, 135, 135, 135, 270, 270, 135, 135, 270, 33, 270, 135, 270, 270, 33, 270, 135, 135, 270, 33, 135, 33, 33, 33, 33, 135, 135, 135, 135, 135, 270, 135, 33, 33, 33, 33]
Prompts retrieved: 37500 . Total input tokens: 8318648 . Total output tokens: 7487535
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.4742967849597335,
    "estimated_duration": 3599.966944423644,
    "input_throughput": 865.3929461285024,
    "output_throughput": 760.4830939463832,
    "total_throughput": 1625.8760400748856,
    "itl": 21.78224940263411,
    "ttft": 5144.685133564891,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1425,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.775939573831842,
    "arrivals": 12685,
    "finished_requests": 12667,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.4743833127431571. Arrivals time: 0.044469802640378475 Scheduler time: 0.9326767679303885 Scheduler overhead time: 0.1400297535583377 Adapter cache time: 0.15322108287364244 Engine time: 0.13663504226133227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.3548522940836847,
    "estimated_duration": 3599.784099827733,
    "input_throughput": 725.3437782907461,
    "output_throughput": 655.0742862920162,
    "total_throughput": 1380.4180645827623,
    "itl": 21.126882466764233,
    "ttft": 2723.0730600340207,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.320629202167117,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.354939240962267. Arrivals time: 0.040048816706985235 Scheduler time: 0.8306008367799222 Scheduler overhead time: 0.143294723238796 Adapter cache time: 0.13179236184805632 Engine time: 0.13984707230702043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.3469675360247493,
    "estimated_duration": 3599.7907502547932,
    "input_throughput": 725.3424382556647,
    "output_throughput": 655.0730760761835,
    "total_throughput": 1380.4155143318483,
    "itl": 21.128667094622077,
    "ttft": 2723.0273119160765,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.528586911603345,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3470854703336954. Arrivals time: 0.03982392279431224 Scheduler time: 0.819555526599288 Scheduler overhead time: 0.1461143409833312 Adapter cache time: 0.13160452526062727 Engine time: 0.14034466492012143 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.3436125232838094,
    "estimated_duration": 3599.7769183278806,
    "input_throughput": 725.3452253404813,
    "output_throughput": 655.0755931551905,
    "total_throughput": 1380.4208184956717,
    "itl": 21.128597401336435,
    "ttft": 2723.0397993437136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.536942207105414,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3437414821237326. Arrivals time: 0.03981089359149337 Scheduler time: 0.8201507451012731 Scheduler overhead time: 0.14286238607019186 Adapter cache time: 0.13122704531997442 Engine time: 0.14044646313413978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.3325951183214784,
    "estimated_duration": 3599.778509767632,
    "input_throughput": 725.3449046698562,
    "output_throughput": 655.0753035503338,
    "total_throughput": 1380.42020822019,
    "itl": 21.127001829808826,
    "ttft": 2723.0698470003626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.389255908015147,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3326651230454445. Arrivals time: 0.03993785893544555 Scheduler time: 0.8075350988656282 Scheduler overhead time: 0.14335482567548752 Adapter cache time: 0.1322544915601611 Engine time: 0.14042450254783034 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.3268040847033262,
    "estimated_duration": 3599.784128252783,
    "input_throughput": 725.343772563199,
    "output_throughput": 655.0742811193395,
    "total_throughput": 1380.4180536825384,
    "itl": 21.129846784848993,
    "ttft": 2723.168874557488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5798242482729266,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3268793816678226. Arrivals time: 0.03937425836920738 Scheduler time: 0.8071522759273648 Scheduler overhead time: 0.14315902534872293 Adapter cache time: 0.12985210539773107 Engine time: 0.13796493923291564 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.3418306480161846,
    "estimated_duration": 3599.784059805982,
    "input_throughput": 725.3437863549875,
    "output_throughput": 655.074293575014,
    "total_throughput": 1380.4180799300016,
    "itl": 21.126128351077888,
    "ttft": 2723.0545937306265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2442045699687823,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3419223381206393. Arrivals time: 0.04006772208958864 Scheduler time: 0.8201775052584708 Scheduler overhead time: 0.1429358054883778 Adapter cache time: 0.13083559181541204 Engine time: 0.13865168392658234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.025,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.025-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.025-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.025   ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 270, 33, 270, 270, 33, 270, 66, 270, 33, 66, 33, 270, 66, 66, 66, 66, 270, 33, 33, 33, 66, 270, 33, 270, 270, 66, 66, 66, 33, 270, 33, 66, 66, 33, 66, 33, 33, 66, 66, 270, 66, 33, 66, 270, 66, 66, 270, 66, 270, 33, 270, 270, 66, 33, 33, 270, 66, 33, 66, 66, 270, 66, 33, 33, 270, 270, 270, 270, 270, 270, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 270, 270, 33, 270, 33, 66, 270, 270, 270, 66, 33, 270, 66, 33, 33, 33, 33, 33, 270, 66, 33, 270, 33, 270, 33, 33, 66, 66, 270, 66, 270, 33, 66, 33, 270, 270, 33, 270, 66, 33, 270, 33, 33, 270, 270, 66, 270, 270, 33, 270, 66, 66, 33, 66, 270, 270, 33, 270, 66, 33, 66, 270, 33, 33, 33, 33, 33, 33, 270, 33, 33, 33, 270, 33, 66, 270, 270, 66, 33, 270, 33, 66, 66, 33, 66, 33, 270, 33, 33, 270, 33, 270, 270, 66, 270, 66, 33, 66, 270, 66, 270, 33, 270, 33, 66, 270, 66, 270, 66, 270, 66, 270, 66, 33, 270, 33, 270, 66, 66, 33, 66, 270, 66, 66, 270, 33, 270, 270, 66, 33, 270, 270, 66, 66, 270, 270, 66, 66, 66, 66, 270, 270, 66, 66, 270, 33, 270, 66, 270, 270, 33, 270, 66, 66, 270, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 270, 66, 33, 33, 33, 33]
Prompts retrieved: 31635 . Total input tokens: 6990941 . Total output tokens: 6335256
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.346814962103963,
    "estimated_duration": 3599.772723220834,
    "input_throughput": 725.346070644088,
    "output_throughput": 655.0763565678968,
    "total_throughput": 1380.4224272119848,
    "itl": 21.12897924363536,
    "ttft": 2723.136700399498,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1085,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.62446684245025,
    "arrivals": 10714,
    "finished_requests": 10706,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.3469069204293191. Arrivals time: 0.03981247125193477 Scheduler time: 0.8181651546619833 Scheduler overhead time: 0.14499561628326774 Adapter cache time: 0.13166125025600195 Engine time: 0.1419715229421854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-8/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.0612087459303439,
    "estimated_duration": 3599.4984854970994,
    "input_throughput": 468.4585941052645,
    "output_throughput": 410.25493022149794,
    "total_throughput": 878.7135243267625,
    "itl": 19.570758901478516,
    "ttft": 3213.8644648160885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5042584668030887,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0613153926096857. Arrivals time: 0.03121888218447566 Scheduler time: 0.5621746936812997 Scheduler overhead time: 0.14845656510442495 Adapter cache time: 0.09754501841962337 Engine time: 0.1486928821541369 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-16/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.0584921669214964,
    "estimated_duration": 3599.507888837705,
    "input_throughput": 468.45737030582967,
    "output_throughput": 410.25385847309144,
    "total_throughput": 878.711228778921,
    "itl": 19.57226909557796,
    "ttft": 3213.922868830085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.730871267130137,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0585642862133682. Arrivals time: 0.03110530972480774 Scheduler time: 0.5617647585459054 Scheduler overhead time: 0.14888204215094447 Adapter cache time: 0.09737774450331926 Engine time: 0.14651897549629211 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-8-32/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.0592714617960155,
    "estimated_duration": 3599.4918823230523,
    "input_throughput": 468.45945348034627,
    "output_throughput": 410.25568282347524,
    "total_throughput": 878.7151363038215,
    "itl": 19.572232812972896,
    "ttft": 3213.8257536545784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.7384391977264944,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0593570698983967. Arrivals time: 0.030612417496740818 Scheduler time: 0.5610465165227652 Scheduler overhead time: 0.14984357682988048 Adapter cache time: 0.09703525481745601 Engine time: 0.14783016592264175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-16/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 1.065705772023648,
    "estimated_duration": 3599.491547389803,
    "input_throughput": 468.4594970705714,
    "output_throughput": 410.25572099783045,
    "total_throughput": 878.7152180684018,
    "itl": 19.571707794083633,
    "ttft": 3213.812877642386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.5878629056759737,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0657633300870657. Arrivals time: 0.030825262889266014 Scheduler time: 0.5681515326723456 Scheduler overhead time: 0.14880767092108727 Adapter cache time: 0.09701679833233356 Engine time: 0.14773153141140938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_8-16-32/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [86 85 85]
---Simulation End---
#Simulation results
{
    "duration": 1.0746165220625699,
    "estimated_duration": 3599.5087836427433,
    "input_throughput": 468.45725385160205,
    "output_throughput": 410.25375648772575,
    "total_throughput": 878.7110103393278,
    "itl": 19.571996891989293,
    "ttft": 3213.7630593194585,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.782453022971747,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.0746994870714843. Arrivals time: 0.03073940332978964 Scheduler time: 0.5736522870138288 Scheduler overhead time: 0.14950095722451806 Adapter cache time: 0.09666348341852427 Engine time: 0.15120048075914383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-16/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 1.0524390861392021,
    "estimated_duration": 3599.507103773994,
    "input_throughput": 468.4574724778413,
    "output_throughput": 410.25394795073584,
    "total_throughput": 878.7114204285771,
    "itl": 19.569850386756016,
    "ttft": 3213.839734905059,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4236075876629064,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.052525743842125. Arrivals time: 0.03068101964890957 Scheduler time: 0.5551170045509934 Scheduler overhead time: 0.14979028888046741 Adapter cache time: 0.09654061729088426 Engine time: 0.14706066250801086 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 256,
    "served_adapters_rates": [
        0.0125,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.0125-0.00625-0.003125_size_16-16-32/adapters_256_slots_192_rate_0.0125-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.0125  ]. Counts: [85 85 86]
Adapter prompts. [66, 66, 135, 33, 135, 135, 33, 135, 66, 135, 33, 66, 33, 135, 66, 66, 66, 66, 135, 33, 33, 33, 66, 135, 33, 135, 135, 66, 66, 66, 33, 135, 33, 66, 66, 33, 66, 33, 33, 66, 66, 135, 66, 33, 66, 135, 66, 66, 135, 66, 135, 33, 135, 135, 66, 33, 33, 135, 66, 33, 66, 66, 135, 66, 33, 33, 135, 135, 135, 135, 135, 135, 33, 66, 33, 66, 33, 33, 33, 66, 66, 66, 33, 33, 135, 135, 33, 135, 33, 66, 135, 135, 135, 66, 33, 135, 66, 33, 33, 33, 33, 33, 135, 66, 33, 135, 33, 135, 33, 33, 66, 66, 135, 66, 135, 33, 66, 33, 135, 135, 33, 135, 66, 33, 135, 33, 33, 135, 135, 66, 135, 135, 33, 135, 66, 66, 33, 66, 135, 135, 33, 135, 66, 33, 66, 135, 33, 33, 33, 33, 33, 33, 135, 33, 33, 33, 135, 33, 66, 135, 135, 66, 33, 135, 33, 66, 66, 33, 66, 33, 135, 33, 33, 135, 33, 135, 135, 66, 135, 66, 33, 66, 135, 66, 135, 33, 135, 33, 66, 135, 66, 135, 66, 135, 66, 135, 66, 33, 135, 33, 135, 66, 66, 33, 66, 135, 66, 66, 135, 33, 135, 135, 66, 33, 135, 135, 66, 66, 135, 135, 66, 66, 66, 66, 135, 135, 66, 66, 135, 33, 135, 66, 135, 135, 33, 135, 66, 66, 135, 33, 66, 33, 33, 33, 33, 66, 66, 66, 66, 66, 135, 66, 33, 33, 33, 33]
Prompts retrieved: 20025 . Total input tokens: 4435016 . Total output tokens: 4009298
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 1.053232628852129,
    "estimated_duration": 3599.5086130983304,
    "input_throughput": 468.457276047067,
    "output_throughput": 410.253775925514,
    "total_throughput": 878.711051972581,
    "itl": 19.57286254407444,
    "ttft": 3213.8897276248344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1145,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.8330060451105807,
    "arrivals": 6790,
    "finished_requests": 6784,
    "scheduler_time": 0.0
}
#Debug simulation 
Total elapsed time: 1.053315119817853. Arrivals time: 0.031083262525498867 Scheduler time: 0.5595812448300421 Scheduler overhead time: 0.14777131006121635 Adapter cache time: 0.09566297335550189 Engine time: 0.14655619394034147 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.8_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.8_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.643531526904553,
    "estimated_duration": 3600.2194449608783,
    "input_throughput": 3237.351272104651,
    "output_throughput": 2849.316592175528,
    "total_throughput": 6086.667864280179,
    "itl": 298.7175728864516,
    "ttft": 2367814.7416739054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 1724092,
    "finished_requests": 47330,
    "scheduler_time": 28.9070216985993
}
#Debug simulation 
Total elapsed time: 3.643625725992024. Arrivals time: 0.2006308794952929 Scheduler time: 3.331835119985044 Scheduler overhead time: 0.019338290207087994 Adapter cache time: 0.06301100039854646 Engine time: 0.01997396908700466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.8_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.8_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.6412152838893235,
    "estimated_duration": 3600.211312611979,
    "input_throughput": 3196.2548863976126,
    "output_throughput": 2821.0855191806463,
    "total_throughput": 6017.3404055782585,
    "itl": 247.6614551232623,
    "ttft": 2377306.362526198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8353226749482605,
    "arrivals": 1724092,
    "finished_requests": 46727,
    "scheduler_time": 25.56718501862776
}
#Debug simulation 
Total elapsed time: 3.641326962970197. Arrivals time: 0.19929228723049164 Scheduler time: 3.2922279397025704 Scheduler overhead time: 0.022723494563251734 Adapter cache time: 0.09304695762693882 Engine time: 0.023537895642220974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.8_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.8_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.646382926031947,
    "estimated_duration": 3600.176717677865,
    "input_throughput": 3196.285599953051,
    "output_throughput": 2821.112627646513,
    "total_throughput": 6017.398227599564,
    "itl": 247.65976227207443,
    "ttft": 2377284.4210109706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8005920728808292,
    "arrivals": 1724092,
    "finished_requests": 46727,
    "scheduler_time": 25.56732068658105
}
#Debug simulation 
Total elapsed time: 3.6464766752906144. Arrivals time: 0.2001407342031598 Scheduler time: 3.296491837594658 Scheduler overhead time: 0.022805253509432077 Adapter cache time: 0.09304993692785501 Engine time: 0.02351253107190132 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.8_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.8
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.8_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.8_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.8 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 8640, 34560, 34560, 8640, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 17280, 17280, 17280, 17280, 34560, 8640, 8640, 8640, 17280, 34560, 8640, 34560, 34560, 17280, 17280, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 8640, 17280, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 8640, 34560, 17280, 8640, 17280, 17280, 34560, 17280, 8640, 8640, 34560, 34560, 34560, 34560, 34560, 34560, 8640, 17280, 8640, 17280, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 34560, 34560, 8640, 34560, 8640, 17280, 34560, 34560, 34560, 17280, 8640, 34560, 17280, 8640, 8640, 8640, 8640, 8640, 34560, 17280, 8640, 34560, 8640, 34560, 8640, 8640, 17280, 17280, 34560, 17280, 34560, 8640, 17280, 8640, 34560, 34560, 8640, 34560, 17280, 8640, 34560, 8640, 8640, 34560, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 34560, 8640, 34560, 17280, 8640, 17280, 34560, 8640, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 8640, 8640, 34560, 8640, 17280, 34560, 34560, 17280, 8640, 34560, 8640, 17280, 17280, 8640, 17280, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 34560, 17280, 34560, 17280, 8640, 17280, 34560, 17280, 34560, 8640, 34560, 8640, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 8640, 34560, 8640, 34560, 17280, 17280, 8640, 17280, 34560, 17280, 17280, 34560, 8640, 34560, 34560, 17280, 8640, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 8640, 34560, 17280, 34560, 34560, 8640, 34560, 17280, 17280, 34560, 8640, 17280, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 8640, 8640, 8640, 8640]
Prompts retrieved: 5175360 . Total input tokens: 1153728886 . Total output tokens: 1034972484
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.6692688637413085,
    "estimated_duration": 3600.021844934814,
    "input_throughput": 3194.1550621919114,
    "output_throughput": 2819.3790030137397,
    "total_throughput": 6013.534065205651,
    "itl": 246.76174431387722,
    "ttft": 2377363.6618553447,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 1724092,
    "finished_requests": 46700,
    "scheduler_time": 25.48769659611954
}
#Debug simulation 
Total elapsed time: 3.669359683059156. Arrivals time: 0.19911531219258904 Scheduler time: 3.3201170871034265 Scheduler overhead time: 0.022914276458323002 Adapter cache time: 0.09289135271683335 Engine time: 0.023756197188049555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.96751761296764,
    "estimated_duration": 3600.1196195054868,
    "input_throughput": 3450.7533951625874,
    "output_throughput": 3012.763781857296,
    "total_throughput": 6463.517177019883,
    "itl": 281.5222537188719,
    "ttft": 2343937.088535633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 1601458,
    "finished_requests": 50215,
    "scheduler_time": 30.45762029476413
}
#Debug simulation 
Total elapsed time: 3.967611651867628. Arrivals time: 0.38536641653627157 Scheduler time: 3.4652694957330823 Scheduler overhead time: 0.02040113927796483 Adapter cache time: 0.0661861696280539 Engine time: 0.021001558285206556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.7177921775728464,
    "estimated_duration": 3600.1185871997723,
    "input_throughput": 3088.6402018909316,
    "output_throughput": 2707.3024857164673,
    "total_throughput": 5795.9426876073985,
    "itl": 188.2967815466331,
    "ttft": 2398203.1270023007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8353226749482601,
    "arrivals": 1601458,
    "finished_requests": 44899,
    "scheduler_time": 18.634905331484305
}
#Debug simulation 
Total elapsed time: 3.7179483086802065. Arrivals time: 0.36076400987803936 Scheduler time: 3.210940381512046 Scheduler overhead time: 0.028665939811617136 Adapter cache time: 0.07493553915992379 Engine time: 0.02922013495117426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.017373553011566,
    "estimated_duration": 3600.1853536764984,
    "input_throughput": 3410.984933722793,
    "output_throughput": 2984.2744038243254,
    "total_throughput": 6395.259337547118,
    "itl": 233.5379164085115,
    "ttft": 2352198.2131571556,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8005920728808292,
    "arrivals": 1601458,
    "finished_requests": 49608,
    "scheduler_time": 26.984566092617708
}
#Debug simulation 
Total elapsed time: 4.017463760916144. Arrivals time: 0.38194309547543526 Scheduler time: 3.4848529528826475 Scheduler overhead time: 0.024443176575005054 Adapter cache time: 0.08967912569642067 Engine time: 0.025437030475586653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.4_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 4320, 34560, 34560, 4320, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 17280, 17280, 17280, 17280, 34560, 4320, 4320, 4320, 17280, 34560, 4320, 34560, 34560, 17280, 17280, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 4320, 17280, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 4320, 34560, 17280, 4320, 17280, 17280, 34560, 17280, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 17280, 34560, 34560, 34560, 17280, 4320, 34560, 17280, 4320, 4320, 4320, 4320, 4320, 34560, 17280, 4320, 34560, 4320, 34560, 4320, 4320, 17280, 17280, 34560, 17280, 34560, 4320, 17280, 4320, 34560, 34560, 4320, 34560, 17280, 4320, 34560, 4320, 4320, 34560, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 34560, 4320, 34560, 17280, 4320, 17280, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 17280, 34560, 34560, 17280, 4320, 34560, 4320, 17280, 17280, 4320, 17280, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 17280, 34560, 17280, 4320, 17280, 34560, 17280, 34560, 4320, 34560, 4320, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 4320, 34560, 4320, 34560, 17280, 17280, 4320, 17280, 34560, 17280, 17280, 34560, 4320, 34560, 34560, 17280, 4320, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 4320, 34560, 17280, 34560, 34560, 4320, 34560, 17280, 17280, 34560, 4320, 17280, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 4320, 4320, 4320, 4320]
Prompts retrieved: 4808160 . Total input tokens: 1071801302 . Total output tokens: 961755331
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.0099938358180225,
    "estimated_duration": 3600.1503535889588,
    "input_throughput": 3411.018094774291,
    "output_throughput": 2984.303416464109,
    "total_throughput": 6395.3215112384005,
    "itl": 233.53639944767713,
    "ttft": 2352175.630564,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 1601458,
    "finished_requests": 49608,
    "scheduler_time": 26.984705202464102
}
#Debug simulation 
Total elapsed time: 4.010110097937286. Arrivals time: 0.38362331688404083 Scheduler time: 3.476954650133848 Scheduler overhead time: 0.024340020958334208 Adapter cache time: 0.08954423433169723 Engine time: 0.024505634792149067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.730910080019385,
    "estimated_duration": 3600.1354216207433,
    "input_throughput": 3751.4841577597676,
    "output_throughput": 3328.721171995524,
    "total_throughput": 7080.205329755291,
    "itl": 257.5797589729611,
    "ttft": 2302507.202328946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 1509498,
    "finished_requests": 55022,
    "scheduler_time": 33.74953043941216
}
#Debug simulation 
Total elapsed time: 4.730987296905369. Arrivals time: 0.8549268119968474 Scheduler time: 3.7782051023095846 Scheduler overhead time: 0.022128582932054996 Adapter cache time: 0.04296660190448165 Engine time: 0.022626806050539017 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.3747160020284355,
    "estimated_duration": 3600.1156581864207,
    "input_throughput": 3675.44802898519,
    "output_throughput": 3266.647829287886,
    "total_throughput": 6942.095858273076,
    "itl": 215.37481314989247,
    "ttft": 2315677.616119705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8353226749482602,
    "arrivals": 1509498,
    "finished_requests": 53915,
    "scheduler_time": 29.633515104400747
}
#Debug simulation 
Total elapsed time: 4.374780168291181. Arrivals time: 0.4855805952101946 Scheduler time: 3.766173522453755 Scheduler overhead time: 0.026065906509757042 Adapter cache time: 0.05835782876238227 Engine time: 0.026572881266474724 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.244783061090857,
    "estimated_duration": 3600.081072922387,
    "input_throughput": 3675.4833382846055,
    "output_throughput": 3266.6792113249544,
    "total_throughput": 6942.16254960956,
    "itl": 215.37343896858133,
    "ttft": 2315654.7866430674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8005920728808293,
    "arrivals": 1509498,
    "finished_requests": 53915,
    "scheduler_time": 29.633660442434664
}
#Debug simulation 
Total elapsed time: 4.244875511154532. Arrivals time: 0.38496071146801114 Scheduler time: 3.7377358665689826 Scheduler overhead time: 0.0258809644728899 Adapter cache time: 0.057867334224283695 Engine time: 0.026490451768040657 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.1_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 1.6 3.2]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 1080, 34560, 34560, 1080, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 17280, 17280, 17280, 17280, 34560, 1080, 1080, 1080, 17280, 34560, 1080, 34560, 34560, 17280, 17280, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 1080, 17280, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 1080, 34560, 17280, 1080, 17280, 17280, 34560, 17280, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 17280, 34560, 34560, 34560, 17280, 1080, 34560, 17280, 1080, 1080, 1080, 1080, 1080, 34560, 17280, 1080, 34560, 1080, 34560, 1080, 1080, 17280, 17280, 34560, 17280, 34560, 1080, 17280, 1080, 34560, 34560, 1080, 34560, 17280, 1080, 34560, 1080, 1080, 34560, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 34560, 1080, 34560, 17280, 1080, 17280, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 17280, 34560, 34560, 17280, 1080, 34560, 1080, 17280, 17280, 1080, 17280, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 17280, 34560, 17280, 1080, 17280, 34560, 17280, 34560, 1080, 34560, 1080, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 1080, 34560, 1080, 34560, 17280, 17280, 1080, 17280, 34560, 17280, 17280, 34560, 1080, 34560, 34560, 17280, 1080, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 1080, 34560, 17280, 34560, 34560, 1080, 34560, 17280, 17280, 34560, 1080, 17280, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 1080, 1080, 1080, 1080]
Prompts retrieved: 4532760 . Total input tokens: 1010348656 . Total output tokens: 906726791
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.267436183989048,
    "estimated_duration": 3600.04606959783,
    "input_throughput": 3675.5190750873317,
    "output_throughput": 3266.7109733164534,
    "total_throughput": 6942.230048403785,
    "itl": 215.37197708629145,
    "ttft": 2315632.1325827655,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 1509498,
    "finished_requests": 53915,
    "scheduler_time": 29.63379631526254
}
#Debug simulation 
Total elapsed time: 4.267539512831718. Arrivals time: 0.3863795236684382 Scheduler time: 3.758292756509036 Scheduler overhead time: 0.026083603501319885 Adapter cache time: 0.05822937469929457 Engine time: 0.026539521291851997 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.195725915953517,
    "estimated_duration": 3600.2556135903455,
    "input_throughput": 3868.9477901012538,
    "output_throughput": 3415.750524373539,
    "total_throughput": 7284.698314474793,
    "itl": 250.92552771994116,
    "ttft": 2294688.0819486803,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 1494384,
    "finished_requests": 56127,
    "scheduler_time": 34.57659742628339
}
#Debug simulation 
Total elapsed time: 4.195818001870066. Arrivals time: 0.21898970706388354 Scheduler time: 3.8865569783374667 Scheduler overhead time: 0.022753975819796324 Adapter cache time: 0.033839779905974865 Engine time: 0.023251410108059645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.170823100022972,
    "estimated_duration": 3600.216065339396,
    "input_throughput": 3769.878183330788,
    "output_throughput": 3332.860801194397,
    "total_throughput": 7102.738984525185,
    "itl": 209.0904966699966,
    "ttft": 2310806.023669893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8353226749482603,
    "arrivals": 1494384,
    "finished_requests": 54705,
    "scheduler_time": 30.010939911544327
}
#Debug simulation 
Total elapsed time: 4.170915563125163. Arrivals time: 0.2234883438795805 Scheduler time: 3.8320025694556534 Scheduler overhead time: 0.026841183193027973 Adapter cache time: 0.0490165650844574 Engine time: 0.0272898911498487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.279886270407587,
    "estimated_duration": 3600.18146712729,
    "input_throughput": 3769.914412350406,
    "output_throughput": 3332.8928304201386,
    "total_throughput": 7102.807242770545,
    "itl": 209.08922624682748,
    "ttft": 2310783.9692229317,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8005920728808293,
    "arrivals": 1494384,
    "finished_requests": 54705,
    "scheduler_time": 30.01107230150418
}
#Debug simulation 
Total elapsed time: 4.280042913276702. Arrivals time: 0.32015278609469533 Scheduler time: 3.8444666331633925 Scheduler overhead time: 0.026600709185004234 Adapter cache time: 0.04933272069320083 Engine time: 0.027127424720674753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.05_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 1.6  3.2 ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 540, 34560, 34560, 540, 34560, 17280, 34560, 540, 17280, 540, 34560, 17280, 17280, 17280, 17280, 34560, 540, 540, 540, 17280, 34560, 540, 34560, 34560, 17280, 17280, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 540, 17280, 17280, 34560, 17280, 540, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 540, 34560, 34560, 17280, 540, 540, 34560, 17280, 540, 17280, 17280, 34560, 17280, 540, 540, 34560, 34560, 34560, 34560, 34560, 34560, 540, 17280, 540, 17280, 540, 540, 540, 17280, 17280, 17280, 540, 540, 34560, 34560, 540, 34560, 540, 17280, 34560, 34560, 34560, 17280, 540, 34560, 17280, 540, 540, 540, 540, 540, 34560, 17280, 540, 34560, 540, 34560, 540, 540, 17280, 17280, 34560, 17280, 34560, 540, 17280, 540, 34560, 34560, 540, 34560, 17280, 540, 34560, 540, 540, 34560, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 34560, 540, 34560, 17280, 540, 17280, 34560, 540, 540, 540, 540, 540, 540, 34560, 540, 540, 540, 34560, 540, 17280, 34560, 34560, 17280, 540, 34560, 540, 17280, 17280, 540, 17280, 540, 34560, 540, 540, 34560, 540, 34560, 34560, 17280, 34560, 17280, 540, 17280, 34560, 17280, 34560, 540, 34560, 540, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 540, 34560, 540, 34560, 17280, 17280, 540, 17280, 34560, 17280, 17280, 34560, 540, 34560, 34560, 17280, 540, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 540, 34560, 17280, 34560, 34560, 540, 34560, 17280, 17280, 34560, 540, 17280, 540, 540, 540, 540, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 540, 540, 540, 540]
Prompts retrieved: 4486860 . Total input tokens: 1000065788 . Total output tokens: 897537807
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.1699622622691095,
    "estimated_duration": 3600.1464519200067,
    "input_throughput": 3769.9510787294967,
    "output_throughput": 3332.925246305122,
    "total_throughput": 7102.876325034619,
    "itl": 209.08786142851872,
    "ttft": 2310761.5127910688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 1494384,
    "finished_requests": 54705,
    "scheduler_time": 30.011196291605852
}
#Debug simulation 
Total elapsed time: 4.170077870134264. Arrivals time: 0.2160215862095356 Scheduler time: 3.8389546517282724 Scheduler overhead time: 0.026679641101509333 Adapter cache time: 0.0489300568588078 Engine time: 0.02714703232049942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.261442490853369,
    "estimated_duration": 3600.115520932595,
    "input_throughput": 3957.777442738559,
    "output_throughput": 3481.020797008652,
    "total_throughput": 7438.7982397472115,
    "itl": 245.31607310914308,
    "ttft": 2286590.849399522,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7804243747028512,
    "arrivals": 1486681,
    "finished_requests": 57706,
    "scheduler_time": 35.17602435697408
}
#Debug simulation 
Total elapsed time: 4.26153497165069. Arrivals time: 0.22617889055982232 Scheduler time: 3.9512273957952857 Scheduler overhead time: 0.023041628766804934 Adapter cache time: 0.02687088865786791 Engine time: 0.023667897563427687 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.635610263794661,
    "estimated_duration": 3600.112360381253,
    "input_throughput": 3844.830553714759,
    "output_throughput": 3387.5614923053354,
    "total_throughput": 7232.392046020094,
    "itl": 205.8547882155125,
    "ttft": 2303125.5601638155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8323326246533582,
    "arrivals": 1486681,
    "finished_requests": 56046,
    "scheduler_time": 30.559185564834124
}
#Debug simulation 
Total elapsed time: 4.63570166984573. Arrivals time: 0.6420110268518329 Scheduler time: 3.885014045983553 Scheduler overhead time: 0.026933616492897272 Adapter cache time: 0.04166993452236056 Engine time: 0.02752310363575816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.354858275968581,
    "estimated_duration": 3600.0934117722627,
    "input_throughput": 3845.0582850808714,
    "output_throughput": 3387.6832084743423,
    "total_throughput": 7232.741493555213,
    "itl": 205.94293819331793,
    "ttft": 2303084.58868027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7976020225859272,
    "arrivals": 1486681,
    "finished_requests": 56049,
    "scheduler_time": 30.573538248404585
}
#Debug simulation 
Total elapsed time: 4.354927088133991. Arrivals time: 0.3871093578636646 Scheduler time: 3.8590814480558038 Scheduler overhead time: 0.02692951774224639 Adapter cache time: 0.04177847458049655 Engine time: 0.027661739382892847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.025_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 1.6   3.2  ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 270, 34560, 34560, 270, 34560, 17280, 34560, 270, 17280, 270, 34560, 17280, 17280, 17280, 17280, 34560, 270, 270, 270, 17280, 34560, 270, 34560, 34560, 17280, 17280, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 270, 17280, 17280, 34560, 17280, 270, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 270, 34560, 34560, 17280, 270, 270, 34560, 17280, 270, 17280, 17280, 34560, 17280, 270, 270, 34560, 34560, 34560, 34560, 34560, 34560, 270, 17280, 270, 17280, 270, 270, 270, 17280, 17280, 17280, 270, 270, 34560, 34560, 270, 34560, 270, 17280, 34560, 34560, 34560, 17280, 270, 34560, 17280, 270, 270, 270, 270, 270, 34560, 17280, 270, 34560, 270, 34560, 270, 270, 17280, 17280, 34560, 17280, 34560, 270, 17280, 270, 34560, 34560, 270, 34560, 17280, 270, 34560, 270, 270, 34560, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 34560, 270, 34560, 17280, 270, 17280, 34560, 270, 270, 270, 270, 270, 270, 34560, 270, 270, 270, 34560, 270, 17280, 34560, 34560, 17280, 270, 34560, 270, 17280, 17280, 270, 17280, 270, 34560, 270, 270, 34560, 270, 34560, 34560, 17280, 34560, 17280, 270, 17280, 34560, 17280, 34560, 270, 34560, 270, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 270, 34560, 270, 34560, 17280, 17280, 270, 17280, 34560, 17280, 17280, 34560, 270, 34560, 34560, 17280, 270, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 270, 34560, 17280, 34560, 34560, 270, 34560, 17280, 17280, 34560, 270, 17280, 270, 270, 270, 270, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 270, 270, 270, 270]
Prompts retrieved: 4463910 . Total input tokens: 994932888 . Total output tokens: 892954293
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.228784033097327,
    "estimated_duration": 3600.2124881108443,
    "input_throughput": 3843.217322780977,
    "output_throughput": 3385.17185867413,
    "total_throughput": 7228.389181455107,
    "itl": 205.76051834637948,
    "ttft": 2303263.560050608,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7624628252000558,
    "arrivals": 1486681,
    "finished_requests": 56016,
    "scheduler_time": 30.521998923861
}
#Debug simulation 
Total elapsed time: 4.228876153938472. Arrivals time: 0.21660584025084972 Scheduler time: 3.904719376936555 Scheduler overhead time: 0.027068393770605326 Adapter cache time: 0.04058712814003229 Engine time: 0.027392767369747162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.447833864949644,
    "estimated_duration": 3600.144539207481,
    "input_throughput": 3951.8410566737716,
    "output_throughput": 3505.6131392930865,
    "total_throughput": 7457.454195966858,
    "itl": 245.05530422065962,
    "ttft": 2283167.776526244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 247,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7559404727513892,
    "arrivals": 1482883,
    "finished_requests": 58010,
    "scheduler_time": 35.49267594842687
}
#Debug simulation 
Total elapsed time: 4.4479380417615175. Arrivals time: 0.40401772782206535 Scheduler time: 3.9654629696160555 Scheduler overhead time: 0.022793450392782688 Adapter cache time: 0.02169886277988553 Engine time: 0.023372675757855177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.8372256169095635,
    "estimated_duration": 3600.022784291419,
    "input_throughput": 3835.8698895618304,
    "output_throughput": 3409.980640557598,
    "total_throughput": 7245.850530119428,
    "itl": 206.85514741319415,
    "ttft": 2299622.5989072844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8029706000885959,
    "arrivals": 1482883,
    "finished_requests": 56306,
    "scheduler_time": 30.941556885937974
}
#Debug simulation 
Total elapsed time: 4.837328719906509. Arrivals time: 0.8416969049721956 Scheduler time: 3.8943942692130804 Scheduler overhead time: 0.02695774519816041 Adapter cache time: 0.03440082399174571 Engine time: 0.02744895499199629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.398826153948903,
    "estimated_duration": 3600.2193160753327,
    "input_throughput": 3835.832705618159,
    "output_throughput": 3410.0339235397605,
    "total_throughput": 7245.866629157919,
    "itl": 206.8502136706659,
    "ttft": 2299668.8939253762,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7694657839764861,
    "arrivals": 1482883,
    "finished_requests": 56312,
    "scheduler_time": 30.943633613361598
}
#Debug simulation 
Total elapsed time: 4.398892645724118. Arrivals time: 0.22168236039578915 Scheduler time: 4.075404601171613 Scheduler overhead time: 0.027061264030635357 Adapter cache time: 0.03501569805666804 Engine time: 0.027334672398865223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.0125_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 1.6    3.2   ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 135, 34560, 34560, 135, 34560, 17280, 34560, 135, 17280, 135, 34560, 17280, 17280, 17280, 17280, 34560, 135, 135, 135, 17280, 34560, 135, 34560, 34560, 17280, 17280, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 135, 17280, 17280, 34560, 17280, 135, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 135, 34560, 34560, 17280, 135, 135, 34560, 17280, 135, 17280, 17280, 34560, 17280, 135, 135, 34560, 34560, 34560, 34560, 34560, 34560, 135, 17280, 135, 17280, 135, 135, 135, 17280, 17280, 17280, 135, 135, 34560, 34560, 135, 34560, 135, 17280, 34560, 34560, 34560, 17280, 135, 34560, 17280, 135, 135, 135, 135, 135, 34560, 17280, 135, 34560, 135, 34560, 135, 135, 17280, 17280, 34560, 17280, 34560, 135, 17280, 135, 34560, 34560, 135, 34560, 17280, 135, 34560, 135, 135, 34560, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 34560, 135, 34560, 17280, 135, 17280, 34560, 135, 135, 135, 135, 135, 135, 34560, 135, 135, 135, 34560, 135, 17280, 34560, 34560, 17280, 135, 34560, 135, 17280, 17280, 135, 17280, 135, 34560, 135, 135, 34560, 135, 34560, 34560, 17280, 34560, 17280, 135, 17280, 34560, 17280, 34560, 135, 34560, 135, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 135, 34560, 135, 34560, 17280, 17280, 135, 17280, 34560, 17280, 17280, 34560, 135, 34560, 34560, 17280, 135, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 135, 34560, 17280, 34560, 34560, 135, 34560, 17280, 17280, 34560, 135, 17280, 135, 135, 135, 135, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 135, 135, 135, 135]
Prompts retrieved: 4452435 . Total input tokens: 992347920 . Total output tokens: 890620743
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.374723557848483,
    "estimated_duration": 3600.185528456903,
    "input_throughput": 3835.8687047773115,
    "output_throughput": 3410.0659265918616,
    "total_throughput": 7245.934631369173,
    "itl": 206.84900457975425,
    "ttft": 2299648.4164023697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 246,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7355523725459362,
    "arrivals": 1482883,
    "finished_requests": 56312,
    "scheduler_time": 30.94375940635966
}
#Debug simulation 
Total elapsed time: 4.3748164917342365. Arrivals time: 0.3872813517227769 Scheduler time: 3.8863744297996163 Scheduler overhead time: 0.026975983288139105 Adapter cache time: 0.0342362062074244 Engine time: 0.027357125654816628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.41817238368094,
    "estimated_duration": 3600.0476960521933,
    "input_throughput": 4021.244222923795,
    "output_throughput": 3519.5219813044123,
    "total_throughput": 7540.766204228207,
    "itl": 242.07334474938355,
    "ttft": 2279302.4086678913,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6763677914091377,
    "arrivals": 1481051,
    "finished_requests": 58054,
    "scheduler_time": 35.54521871441251
}
#Debug simulation 
Total elapsed time: 4.418291836977005. Arrivals time: 0.33013852732256055 Scheduler time: 4.009509294293821 Scheduler overhead time: 0.023478245828300714 Adapter cache time: 0.02051077736541629 Engine time: 0.023824967443943024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.270753987133503,
    "estimated_duration": 3600.0909556714328,
    "input_throughput": 3900.2525694191086,
    "output_throughput": 3421.4171674178574,
    "total_throughput": 7321.669736836966,
    "itl": 203.46899630107228,
    "ttft": 2295869.849720391,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7148845263943101,
    "arrivals": 1481051,
    "finished_requests": 56300,
    "scheduler_time": 30.786597269301442
}
#Debug simulation 
Total elapsed time: 4.270845352206379. Arrivals time: 0.2337372163310647 Scheduler time: 3.9357759742997587 Scheduler overhead time: 0.02721197484061122 Adapter cache time: 0.03378475410863757 Engine time: 0.027722282335162163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.25342956604436,
    "estimated_duration": 3600.0596161420453,
    "input_throughput": 3900.2865222124096,
    "output_throughput": 3421.446951814589,
    "total_throughput": 7321.733474026998,
    "itl": 203.46773149460776,
    "ttft": 2295851.174928333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.683422686874402,
    "arrivals": 1481051,
    "finished_requests": 56300,
    "scheduler_time": 30.78671957943148
}
#Debug simulation 
Total elapsed time: 4.25354982400313. Arrivals time: 0.21979784965515137 Scheduler time: 3.932106966152787 Scheduler overhead time: 0.027350631542503834 Adapter cache time: 0.03378697577863932 Engine time: 0.027860622387379408 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.00625_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 1.6     3.2    ]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 66, 34560, 34560, 66, 34560, 17280, 34560, 66, 17280, 66, 34560, 17280, 17280, 17280, 17280, 34560, 66, 66, 66, 17280, 34560, 66, 34560, 34560, 17280, 17280, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 66, 17280, 17280, 34560, 17280, 66, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 66, 34560, 34560, 17280, 66, 66, 34560, 17280, 66, 17280, 17280, 34560, 17280, 66, 66, 34560, 34560, 34560, 34560, 34560, 34560, 66, 17280, 66, 17280, 66, 66, 66, 17280, 17280, 17280, 66, 66, 34560, 34560, 66, 34560, 66, 17280, 34560, 34560, 34560, 17280, 66, 34560, 17280, 66, 66, 66, 66, 66, 34560, 17280, 66, 34560, 66, 34560, 66, 66, 17280, 17280, 34560, 17280, 34560, 66, 17280, 66, 34560, 34560, 66, 34560, 17280, 66, 34560, 66, 66, 34560, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 34560, 66, 34560, 17280, 66, 17280, 34560, 66, 66, 66, 66, 66, 66, 34560, 66, 66, 66, 34560, 66, 17280, 34560, 34560, 17280, 66, 34560, 66, 17280, 17280, 66, 17280, 66, 34560, 66, 66, 34560, 66, 34560, 34560, 17280, 34560, 17280, 66, 17280, 34560, 17280, 34560, 66, 34560, 66, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 66, 34560, 66, 34560, 17280, 17280, 66, 17280, 34560, 17280, 17280, 34560, 66, 34560, 34560, 17280, 66, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 66, 34560, 17280, 34560, 34560, 66, 34560, 17280, 17280, 34560, 66, 17280, 66, 66, 66, 66, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 66, 66, 66, 66]
Prompts retrieved: 4446570 . Total input tokens: 991018000 . Total output tokens: 889447584
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.2709043230861425,
    "estimated_duration": 3600.1172100878425,
    "input_throughput": 3897.441994578182,
    "output_throughput": 3419.325061280335,
    "total_throughput": 7316.767055858517,
    "itl": 203.02714608152004,
    "ttft": 2295942.7262438233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 219,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6548210145835773,
    "arrivals": 1481051,
    "finished_requests": 56259,
    "scheduler_time": 30.72058191275026
}
#Debug simulation 
Total elapsed time: 4.270991338882595. Arrivals time: 0.22278106678277254 Scheduler time: 3.9458205276168883 Scheduler overhead time: 0.0273943436332047 Adapter cache time: 0.034352114889770746 Engine time: 0.02798450132831931 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-8/adapters_256_slots_256_rate_3.2-1.6-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.29531021323055,
    "estimated_duration": 3600.0119383528763,
    "input_throughput": 4015.030296430976,
    "output_throughput": 3531.5763441098316,
    "total_throughput": 7546.606640540807,
    "itl": 241.85879626922676,
    "ttft": 2276392.7827470065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5937346223229535,
    "arrivals": 1480152,
    "finished_requests": 58501,
    "scheduler_time": 35.68913985283508
}
#Debug simulation 
Total elapsed time: 4.295427720993757. Arrivals time: 0.2271101512014866 Scheduler time: 3.9923392445780337 Scheduler overhead time: 0.023355881217867136 Adapter cache time: 0.017963580321520567 Engine time: 0.02387493196874857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-8-16/adapters_256_slots_256_rate_3.2-1.6-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.294218407943845,
    "estimated_duration": 3600.128195220095,
    "input_throughput": 3886.659374679452,
    "output_throughput": 3428.9106750114806,
    "total_throughput": 7315.570049690933,
    "itl": 203.92065500500266,
    "ttft": 2293191.737107862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6323699579713882,
    "arrivals": 1480152,
    "finished_requests": 56663,
    "scheduler_time": 30.953554184606723
}
#Debug simulation 
Total elapsed time: 4.294314548838884. Arrivals time: 0.23414330556988716 Scheduler time: 3.9618302485905588 Scheduler overhead time: 0.027392453514039516 Adapter cache time: 0.030328721273690462 Engine time: 0.027969864197075367 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_8-16-16/adapters_256_slots_256_rate_3.2-1.6-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.334228238090873,
    "estimated_duration": 3600.1711313952637,
    "input_throughput": 3887.787132656528,
    "output_throughput": 3429.6125237843307,
    "total_throughput": 7317.399656440859,
    "itl": 204.03324970614588,
    "ttft": 2293232.282837521,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6062198575912052,
    "arrivals": 1480152,
    "finished_requests": 56677,
    "scheduler_time": 30.97749837943256
}
#Debug simulation 
Total elapsed time: 4.334360859356821. Arrivals time: 0.32386241806671023 Scheduler time: 3.9126008762978017 Scheduler overhead time: 0.027259827591478825 Adapter cache time: 0.030276946257799864 Engine time: 0.027722730301320553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        1.6,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-1.6-0.003125_size_16-16-16/adapters_256_slots_256_rate_3.2-1.6-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.600e+00 3.200e+00]. Counts: [85 85 86]
Adapter prompts. [17280, 17280, 34560, 33, 34560, 34560, 33, 34560, 17280, 34560, 33, 17280, 33, 34560, 17280, 17280, 17280, 17280, 34560, 33, 33, 33, 17280, 34560, 33, 34560, 34560, 17280, 17280, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 33, 17280, 17280, 34560, 17280, 33, 17280, 34560, 17280, 17280, 34560, 17280, 34560, 33, 34560, 34560, 17280, 33, 33, 34560, 17280, 33, 17280, 17280, 34560, 17280, 33, 33, 34560, 34560, 34560, 34560, 34560, 34560, 33, 17280, 33, 17280, 33, 33, 33, 17280, 17280, 17280, 33, 33, 34560, 34560, 33, 34560, 33, 17280, 34560, 34560, 34560, 17280, 33, 34560, 17280, 33, 33, 33, 33, 33, 34560, 17280, 33, 34560, 33, 34560, 33, 33, 17280, 17280, 34560, 17280, 34560, 33, 17280, 33, 34560, 34560, 33, 34560, 17280, 33, 34560, 33, 33, 34560, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 34560, 33, 34560, 17280, 33, 17280, 34560, 33, 33, 33, 33, 33, 33, 34560, 33, 33, 33, 34560, 33, 17280, 34560, 34560, 17280, 33, 34560, 33, 17280, 17280, 33, 17280, 33, 34560, 33, 33, 34560, 33, 34560, 34560, 17280, 34560, 17280, 33, 17280, 34560, 17280, 34560, 33, 34560, 33, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 34560, 17280, 33, 34560, 33, 34560, 17280, 17280, 33, 17280, 34560, 17280, 17280, 34560, 33, 34560, 34560, 17280, 33, 34560, 34560, 17280, 17280, 34560, 34560, 17280, 17280, 17280, 17280, 34560, 34560, 17280, 17280, 34560, 33, 34560, 17280, 34560, 34560, 33, 34560, 17280, 17280, 34560, 33, 17280, 33, 33, 33, 33, 17280, 17280, 17280, 17280, 17280, 34560, 17280, 33, 33, 33, 33]
Prompts retrieved: 4443765 . Total input tokens: 990406038 . Total output tokens: 888887036
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.288046467117965,
    "estimated_duration": 3600.145110242951,
    "input_throughput": 3887.8152328297265,
    "output_throughput": 3429.637312360103,
    "total_throughput": 7317.452545189829,
    "itl": 204.03246728516706,
    "ttft": 2293215.4119006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 194,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5800697572110228,
    "arrivals": 1480152,
    "finished_requests": 56677,
    "scheduler_time": 30.977627327499306
}
#Debug simulation 
Total elapsed time: 4.28814388718456. Arrivals time: 0.2259218213148415 Scheduler time: 3.963578598573804 Scheduler overhead time: 0.02742301020771265 Adapter cache time: 0.0307110701687634 Engine time: 0.02788866125047207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_256_rate_3.2-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-8/adapters_256_slots_256_rate_3.2-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.81070109969005,
    "estimated_duration": 3600.1267190620706,
    "input_throughput": 3418.024130885555,
    "output_throughput": 3039.4374570378395,
    "total_throughput": 6457.461587923395,
    "itl": 282.8296620179738,
    "ttft": 2341156.188267012,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 1357053,
    "finished_requests": 50116,
    "scheduler_time": 30.902606358836106
}
#Debug simulation 
Total elapsed time: 3.8107945728115737. Arrivals time: 0.20095366798341274 Scheduler time: 3.4759533922187984 Scheduler overhead time: 0.020281873177736998 Adapter cache time: 0.0833252053707838 Engine time: 0.02096179174259305 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_256_rate_3.2-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-8-16/adapters_256_slots_256_rate_3.2-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 3.884085444267839,
    "estimated_duration": 3600.1740265903723,
    "input_throughput": 3408.193023274675,
    "output_throughput": 3038.505894216501,
    "total_throughput": 6446.698917491176,
    "itl": 230.95430195358423,
    "ttft": 2345739.421584147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8353226749482602,
    "arrivals": 1357053,
    "finished_requests": 49977,
    "scheduler_time": 27.551246681482166
}
#Debug simulation 
Total elapsed time: 3.8841999489814043. Arrivals time: 0.2006281274370849 Scheduler time: 3.514663143083453 Scheduler overhead time: 0.024585715495049953 Adapter cache time: 0.10781040927395225 Engine time: 0.025230429600924253 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_256_rate_3.2-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_8-16-16/adapters_256_slots_256_rate_3.2-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 3.910243348684162,
    "estimated_duration": 3600.1477558261363,
    "input_throughput": 3408.4873267053244,
    "output_throughput": 3038.7163922080763,
    "total_throughput": 6447.203718913401,
    "itl": 231.11454384621803,
    "ttft": 2345926.079147314,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8005920728808292,
    "arrivals": 1357053,
    "finished_requests": 49981,
    "scheduler_time": 27.565380288664244
}
#Debug simulation 
Total elapsed time: 3.910337554756552. Arrivals time: 0.2005638536065817 Scheduler time: 3.538527714088559 Scheduler overhead time: 0.024660354014486074 Adapter cache time: 0.10784057155251503 Engine time: 0.027454962022602558 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_256_rate_3.2-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.4_size_16-16-16/adapters_256_slots_256_rate_3.2-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 4320, 34560, 34560, 4320, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 8640, 8640, 8640, 8640, 34560, 4320, 4320, 4320, 8640, 34560, 4320, 34560, 34560, 8640, 8640, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 4320, 34560, 8640, 4320, 8640, 8640, 34560, 8640, 4320, 4320, 34560, 34560, 34560, 34560, 34560, 34560, 4320, 8640, 4320, 8640, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 34560, 34560, 4320, 34560, 4320, 8640, 34560, 34560, 34560, 8640, 4320, 34560, 8640, 4320, 4320, 4320, 4320, 4320, 34560, 8640, 4320, 34560, 4320, 34560, 4320, 4320, 8640, 8640, 34560, 8640, 34560, 4320, 8640, 4320, 34560, 34560, 4320, 34560, 8640, 4320, 34560, 4320, 4320, 34560, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 34560, 4320, 34560, 8640, 4320, 8640, 34560, 4320, 4320, 4320, 4320, 4320, 4320, 34560, 4320, 4320, 4320, 34560, 4320, 8640, 34560, 34560, 8640, 4320, 34560, 4320, 8640, 8640, 4320, 8640, 4320, 34560, 4320, 4320, 34560, 4320, 34560, 34560, 8640, 34560, 8640, 4320, 8640, 34560, 8640, 34560, 4320, 34560, 4320, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 4320, 34560, 4320, 34560, 8640, 8640, 4320, 8640, 34560, 8640, 8640, 34560, 4320, 34560, 34560, 8640, 4320, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 4320, 34560, 8640, 34560, 34560, 4320, 34560, 8640, 8640, 34560, 4320, 8640, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 4320, 4320, 4320, 4320]
Prompts retrieved: 4073760 . Total input tokens: 907952888 . Total output tokens: 814936460
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 3.898116100113839,
    "estimated_duration": 3600.0623552327424,
    "input_throughput": 3410.0512126286035,
    "output_throughput": 3040.295672682146,
    "total_throughput": 6450.346885310749,
    "itl": 231.279809346447,
    "ttft": 2345790.830822283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 1357053,
    "finished_requests": 50001,
    "scheduler_time": 27.59610799302698
}
#Debug simulation 
Total elapsed time: 3.8982323301024735. Arrivals time: 0.20447518164291978 Scheduler time: 3.525949568487704 Scheduler overhead time: 0.02452097460627556 Adapter cache time: 0.10672256862744689 Engine time: 0.025275251362472773 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_256_rate_3.2-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-8/adapters_256_slots_256_rate_3.2-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [8]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.162508236244321,
    "estimated_duration": 3600.228658372838,
    "input_throughput": 3850.510430152902,
    "output_throughput": 3371.0906033077663,
    "total_throughput": 7221.601033460668,
    "itl": 252.56709732387873,
    "ttft": 2282321.6807473223,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 1265213,
    "finished_requests": 56075,
    "scheduler_time": 34.109805030468
}
#Debug simulation 
Total elapsed time: 4.16260394686833. Arrivals time: 0.21772728767246008 Scheduler time: 3.8236139151267707 Scheduler overhead time: 0.022655208595097065 Adapter cache time: 0.06489588087424636 Engine time: 0.023280507419258356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_256_rate_3.2-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-8-16/adapters_256_slots_256_rate_3.2-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [171  85]
---Simulation End---
#Simulation results
{
    "duration": 4.149865662213415,
    "estimated_duration": 3600.1135299172342,
    "input_throughput": 3815.0380219580893,
    "output_throughput": 3347.324160712522,
    "total_throughput": 7162.362182670611,
    "itl": 209.73398733030368,
    "ttft": 2291545.690339622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8353226749482602,
    "arrivals": 1265213,
    "finished_requests": 55573,
    "scheduler_time": 30.308849592618795
}
#Debug simulation 
Total elapsed time: 4.149996488820761. Arrivals time: 0.2135209538973868 Scheduler time: 3.7877304791472852 Scheduler overhead time: 0.026641251053661108 Adapter cache time: 0.08264092355966568 Engine time: 0.027216958813369274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_256_rate_3.2-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_8-16-16/adapters_256_slots_256_rate_3.2-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 86 170]
---Simulation End---
#Simulation results
{
    "duration": 4.198314667213708,
    "estimated_duration": 3600.0789411703327,
    "input_throughput": 3815.074675983381,
    "output_throughput": 3347.3563210484713,
    "total_throughput": 7162.430997031852,
    "itl": 209.73255788445195,
    "ttft": 2291522.765912899,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8005920728808292,
    "arrivals": 1265213,
    "finished_requests": 55573,
    "scheduler_time": 30.30899144778469
}
#Debug simulation 
Total elapsed time: 4.1984379310160875. Arrivals time: 0.21703527588397264 Scheduler time: 3.8313212380744517 Scheduler overhead time: 0.026891410816460848 Adapter cache time: 0.08356171799823642 Engine time: 0.027288923040032387 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_256_rate_3.2-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 256,
    "served_adapters": 256,
    "served_adapters_rates": [
        3.2,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.8-0.1_size_16-16-16/adapters_256_slots_256_rate_3.2-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 3.2]. Counts: [85 85 86]
Adapter prompts. [8640, 8640, 34560, 1080, 34560, 34560, 1080, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 8640, 8640, 8640, 8640, 34560, 1080, 1080, 1080, 8640, 34560, 1080, 34560, 34560, 8640, 8640, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 1080, 34560, 8640, 1080, 8640, 8640, 34560, 8640, 1080, 1080, 34560, 34560, 34560, 34560, 34560, 34560, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 34560, 34560, 1080, 34560, 1080, 8640, 34560, 34560, 34560, 8640, 1080, 34560, 8640, 1080, 1080, 1080, 1080, 1080, 34560, 8640, 1080, 34560, 1080, 34560, 1080, 1080, 8640, 8640, 34560, 8640, 34560, 1080, 8640, 1080, 34560, 34560, 1080, 34560, 8640, 1080, 34560, 1080, 1080, 34560, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 34560, 1080, 34560, 8640, 1080, 8640, 34560, 1080, 1080, 1080, 1080, 1080, 1080, 34560, 1080, 1080, 1080, 34560, 1080, 8640, 34560, 34560, 8640, 1080, 34560, 1080, 8640, 8640, 1080, 8640, 1080, 34560, 1080, 1080, 34560, 1080, 34560, 34560, 8640, 34560, 8640, 1080, 8640, 34560, 8640, 34560, 1080, 34560, 1080, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 34560, 8640, 1080, 34560, 1080, 34560, 8640, 8640, 1080, 8640, 34560, 8640, 8640, 34560, 1080, 34560, 34560, 8640, 1080, 34560, 34560, 8640, 8640, 34560, 34560, 8640, 8640, 8640, 8640, 34560, 34560, 8640, 8640, 34560, 1080, 34560, 8640, 34560, 34560, 1080, 34560, 8640, 8640, 34560, 1080, 8640, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 8640, 34560, 8640, 1080, 1080, 1080, 1080]
Prompts retrieved: 3798360 . Total input tokens: 846500158 . Total output tokens: 759834359
Prompts distributed
Adapter sizes. Values: [16]. Counts: [256]
---Simulation End---
#Simulation results
{
    "duration": 4.269503509160131,
    "estimated_duration": 3600.0439289132432,
    "input_throughput": 3815.1117795237847,
    "output_throughput": 3347.3888757901345,
    "total_throughput": 7162.500655313919,
    "itl": 209.73096267625286,
    "ttft": 2291499.9208977576,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 1265213,
    "finished_requests": 55573,
    "scheduler_time": 30.309118388080453
}
#Debug simulation 
Total elapsed time: 4.269594896119088. Arrivals time: 0.22250524908304214 Scheduler time: 3.8982837116345763 Scheduler overhead time: 0.026525077410042286 Adapter cache time: 0.08283126587048173 Engine time: 0.02723359502851963 
