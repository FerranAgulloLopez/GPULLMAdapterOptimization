INFO 06-01 00:47:09 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-16/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.614754971116781,
    "estimated_duration": 3600.0617509073977,
    "input_throughput": 6614.134047561365,
    "output_throughput": 5825.428687359049,
    "total_throughput": 12439.562734920413,
    "itl": 147.76908842373803,
    "ttft": 1949975.4745050527,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7519909158884576,
    "arrivals": 625657,
    "finished_requests": 95997,
    "scheduler_time": 88.17756016960176
}
#Debug simulation 
Total elapsed time: 6.614856387022883. Arrivals time: 0.28293065587058663 Scheduler time: 6.226503178942949 Scheduler overhead time: 0.036556313280016184 Adapter cache time: 0.01423954265192151 Engine time: 0.03743072086945176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-8-32/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.4179214662872255,
    "estimated_duration": 3600.132514079428,
    "input_throughput": 6390.829479198551,
    "output_throughput": 5631.906303644652,
    "total_throughput": 12022.735782843203,
    "itl": 124.81451081066453,
    "ttft": 1971581.143910306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7374106415174935,
    "arrivals": 625657,
    "finished_requests": 92751,
    "scheduler_time": 85.76518364110319
}
#Debug simulation 
Total elapsed time: 6.418053572066128. Arrivals time: 0.3207347462885082 Scheduler time: 5.976399816107005 Scheduler overhead time: 0.042379966005682945 Adapter cache time: 0.015264406334608793 Engine time: 0.04320779116824269 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-16/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.567153359297663,
    "estimated_duration": 3600.0120605958223,
    "input_throughput": 6614.129508238135,
    "output_throughput": 5825.442983801858,
    "total_throughput": 12439.572492039992,
    "itl": 147.76818142315335,
    "ttft": 1949962.8050918037,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7213462670054304,
    "arrivals": 625657,
    "finished_requests": 95996,
    "scheduler_time": 88.17666215911153
}
#Debug simulation 
Total elapsed time: 6.5672464850358665. Arrivals time: 0.3333183266222477 Scheduler time: 6.12837677821517 Scheduler overhead time: 0.03636107873171568 Adapter cache time: 0.014293213840574026 Engine time: 0.037806269247084856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_8-16-32/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.47740277601406,
    "estimated_duration": 3600.0583589585167,
    "input_throughput": 6390.73362317822,
    "output_throughput": 5631.54759687428,
    "total_throughput": 12022.2812200525,
    "itl": 124.81530500218331,
    "ttft": 1971596.3583945855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7464649141393632,
    "arrivals": 625657,
    "finished_requests": 92745,
    "scheduler_time": 85.76339481561035
}
#Debug simulation 
Total elapsed time: 6.477520687971264. Arrivals time: 0.27969179255887866 Scheduler time: 6.075906106736511 Scheduler overhead time: 0.042611302342265844 Adapter cache time: 0.015255104750394821 Engine time: 0.04395461967214942 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-16/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.6677169520407915,
    "estimated_duration": 3600.1115158686525,
    "input_throughput": 6614.164004376567,
    "output_throughput": 5825.496767963219,
    "total_throughput": 12439.660772339786,
    "itl": 147.76736107501847,
    "ttft": 1949956.801767233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6907016181224035,
    "arrivals": 625657,
    "finished_requests": 95999,
    "scheduler_time": 88.18007266546603
}
#Debug simulation 
Total elapsed time: 6.667817738838494. Arrivals time: 0.3823933699168265 Scheduler time: 6.177876582369208 Scheduler overhead time: 0.037039941642433405 Adapter cache time: 0.014517268631607294 Engine time: 0.03868897818028927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.00625_size_16-16-32/adapters_160_slots_128_rate_3.2-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  3.2    ]. Counts: [53 53 54]
Adapter prompts. [66, 34560, 66, 66, 135, 66, 66, 66, 34560, 66, 135, 34560, 135, 66, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 66, 34560, 135, 34560, 34560, 34560, 135, 34560, 66, 135, 135, 135, 34560, 66, 66, 34560, 66, 34560, 66, 66, 135, 66, 135, 34560, 66, 66, 135, 66, 66, 66, 66, 66, 34560, 135, 135, 34560, 135, 66, 34560, 34560, 34560, 135, 135, 66, 66, 66, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 66, 66, 34560, 135, 34560, 34560, 66, 34560, 66, 135, 66, 66, 135, 135, 34560, 34560, 34560, 66, 34560, 135, 135, 66, 66, 66, 66, 34560, 135, 34560, 135, 34560, 34560, 34560, 66, 135, 135, 34560, 135, 34560, 66, 66, 66, 135, 135, 135, 34560, 34560, 135, 34560, 66, 135, 135, 66, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 66, 34560, 66, 34560, 135, 66, 135, 66, 66, 135, 135, 135, 34560, 135, 66, 66]
Prompts retrieved: 1876893 . Total input tokens: 418028837 . Total output tokens: 375470615
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.489828309044242,
    "estimated_duration": 3600.074189993577,
    "input_throughput": 6390.810796052247,
    "output_throughput": 5631.787827138132,
    "total_throughput": 12022.598623190379,
    "itl": 124.81952431560096,
    "ttft": 1971601.1000819502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 226,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7558964481204775,
    "arrivals": 625657,
    "finished_requests": 92748,
    "scheduler_time": 85.76469097225927
}
#Debug simulation 
Total elapsed time: 6.489922231994569. Arrivals time: 0.3529610550031066 Scheduler time: 6.015038787852973 Scheduler overhead time: 0.04265546053647995 Adapter cache time: 0.015170125290751457 Engine time: 0.04398215934634209 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-8/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.550042876973748,
    "estimated_duration": 3600.026523113871,
    "input_throughput": 6588.584236175016,
    "output_throughput": 5855.187139501323,
    "total_throughput": 12443.771375676339,
    "itl": 147.74177308080223,
    "ttft": 1948468.1866687613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5447668184200295,
    "arrivals": 625114,
    "finished_requests": 96085,
    "scheduler_time": 88.62786422090235
}
#Debug simulation 
Total elapsed time: 6.550166491884738. Arrivals time: 0.2777225528843701 Scheduler time: 6.16857441002503 Scheduler overhead time: 0.036241843830794096 Adapter cache time: 0.012742278166115284 Engine time: 0.03767247358337045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-16/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.59026433667168,
    "estimated_duration": 3600.1200605899357,
    "input_throughput": 6588.806373338845,
    "output_throughput": 5855.393332783933,
    "total_throughput": 12444.19970612278,
    "itl": 147.7423680749158,
    "ttft": 1948493.6903418796,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5808517953869895,
    "arrivals": 625114,
    "finished_requests": 96091,
    "scheduler_time": 88.63017543271694
}
#Debug simulation 
Total elapsed time: 6.590359751600772. Arrivals time: 0.27958026388660073 Scheduler time: 6.206289125606418 Scheduler overhead time: 0.036538629326969385 Adapter cache time: 0.0129327611066401 Engine time: 0.037747261580079794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.46423330437392,
    "estimated_duration": 3600.1121959559705,
    "input_throughput": 6357.960739587999,
    "output_throughput": 5658.973357242881,
    "total_throughput": 12016.93409683088,
    "itl": 124.54337487506798,
    "ttft": 1969790.7873262072,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5717365079000614,
    "arrivals": 625114,
    "finished_requests": 92708,
    "scheduler_time": 86.15065481130948
}
#Debug simulation 
Total elapsed time: 6.464339291211218. Arrivals time: 0.2835437674075365 Scheduler time: 6.061009169090539 Scheduler overhead time: 0.04244416579604149 Adapter cache time: 0.01378205418586731 Engine time: 0.04354775743559003 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.564162347931415,
    "estimated_duration": 3600.040152600904,
    "input_throughput": 6588.559292280057,
    "output_throughput": 5855.16497219379,
    "total_throughput": 12443.724264473847,
    "itl": 147.74202425676606,
    "ttft": 1948474.1268581368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5567446715990086,
    "arrivals": 625114,
    "finished_requests": 96085,
    "scheduler_time": 88.6281238676218
}
#Debug simulation 
Total elapsed time: 6.564280482009053. Arrivals time: 0.28122564358636737 Scheduler time: 6.178558833897114 Scheduler overhead time: 0.036363445688039064 Adapter cache time: 0.012999244499951601 Engine time: 0.03788329195231199 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.544674512930214,
    "estimated_duration": 3600.122257606894,
    "input_throughput": 6358.326568391638,
    "output_throughput": 5659.101981037397,
    "total_throughput": 12017.428549429036,
    "itl": 124.54413163403424,
    "ttft": 1969781.4190954394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5790302275121226,
    "arrivals": 625114,
    "finished_requests": 92713,
    "scheduler_time": 86.15049528269391
}
#Debug simulation 
Total elapsed time: 6.544785398058593. Arrivals time: 0.3276967303827405 Scheduler time: 6.096015660557896 Scheduler overhead time: 0.04279182618483901 Adapter cache time: 0.013954170979559422 Engine time: 0.044018218759447336 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.59188075363636,
    "estimated_duration": 3600.1578677424154,
    "input_throughput": 6588.833009946547,
    "output_throughput": 5855.582109019978,
    "total_throughput": 12444.415118966524,
    "itl": 147.74182448749656,
    "ttft": 1948469.3624580842,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.532228952492588,
    "arrivals": 625114,
    "finished_requests": 96094,
    "scheduler_time": 88.63155493746353
}
#Debug simulation 
Total elapsed time: 6.5919756148941815. Arrivals time: 0.28799154656007886 Scheduler time: 6.1994440080598 Scheduler overhead time: 0.03641702746972442 Adapter cache time: 0.012971432879567146 Engine time: 0.037912927102297544 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_160_slots_128_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 135, 33, 33, 33, 34560, 33, 135, 34560, 135, 33, 135, 135, 135, 135, 34560, 34560, 34560, 135, 135, 135, 34560, 34560, 33, 34560, 135, 34560, 34560, 34560, 135, 34560, 33, 135, 135, 135, 34560, 33, 33, 34560, 33, 34560, 33, 33, 135, 33, 135, 34560, 33, 33, 135, 33, 33, 33, 33, 33, 34560, 135, 135, 34560, 135, 33, 34560, 34560, 34560, 135, 135, 33, 33, 33, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 33, 33, 34560, 135, 34560, 34560, 33, 34560, 33, 135, 33, 33, 135, 135, 34560, 34560, 34560, 33, 34560, 135, 135, 33, 33, 33, 33, 34560, 135, 34560, 135, 34560, 34560, 34560, 33, 135, 135, 34560, 135, 34560, 33, 33, 33, 135, 135, 135, 34560, 34560, 135, 34560, 33, 135, 135, 33, 34560, 34560, 34560, 34560, 34560, 34560, 135, 135, 135, 135, 135, 34560, 33, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 1875144 . Total input tokens: 417624871 . Total output tokens: 375110423
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.496772953309119,
    "estimated_duration": 3600.0768094777413,
    "input_throughput": 6358.20711928661,
    "output_throughput": 5659.050647576458,
    "total_throughput": 12017.257766863067,
    "itl": 124.54681465654889,
    "ttft": 1969724.1353664412,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5863239471241839,
    "arrivals": 625114,
    "finished_requests": 92710,
    "scheduler_time": 86.14986129464322
}
#Debug simulation 
Total elapsed time: 6.4968813941814005. Arrivals time: 0.28463425347581506 Scheduler time: 6.090982206165791 Scheduler overhead time: 0.04291902994737029 Adapter cache time: 0.013913009781390429 Engine time: 0.044027943164110184 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.673182343132794,
    "estimated_duration": 3600.140326358419,
    "input_throughput": 6655.3586327108615,
    "output_throughput": 5923.013012542523,
    "total_throughput": 12578.371645253384,
    "itl": 146.19196756424512,
    "ttft": 1938429.9410940597,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5233434042125003,
    "arrivals": 623927,
    "finished_requests": 97045,
    "scheduler_time": 89.66261580881988
}
#Debug simulation 
Total elapsed time: 6.673272736836225. Arrivals time: 0.3422447042539716 Scheduler time: 6.227745424956083 Scheduler overhead time: 0.036725951824337244 Adapter cache time: 0.010843532159924507 Engine time: 0.03846113849431276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 7.079102172050625,
    "estimated_duration": 3600.080193344167,
    "input_throughput": 6655.152861398917,
    "output_throughput": 5922.683622276079,
    "total_throughput": 12577.836483674997,
    "itl": 146.1934279998527,
    "ttft": 1938442.111927513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5570612760935916,
    "arrivals": 623927,
    "finished_requests": 97040,
    "scheduler_time": 89.66042978202299
}
#Debug simulation 
Total elapsed time: 7.079166730865836. Arrivals time: 0.6670859935693443 Scheduler time: 6.30819909600541 Scheduler overhead time: 0.03683971846476197 Adapter cache time: 0.01088955719023943 Engine time: 0.038532395381480455 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.571945523377508,
    "estimated_duration": 3600.1215105036017,
    "input_throughput": 6411.107217537043,
    "output_throughput": 5710.142543806629,
    "total_throughput": 12121.249761343672,
    "itl": 123.54662131784431,
    "ttft": 1961222.851422923,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5582134425267598,
    "arrivals": 623927,
    "finished_requests": 93504,
    "scheduler_time": 86.93877852085272
}
#Debug simulation 
Total elapsed time: 6.57206852035597. Arrivals time: 0.33629004610702395 Scheduler time: 6.11564219230786 Scheduler overhead time: 0.04352583456784487 Adapter cache time: 0.01202004449442029 Engine time: 0.04421024536713958 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.770755871199071,
    "estimated_duration": 3600.15196005898,
    "input_throughput": 6655.3371262716,
    "output_throughput": 5922.993872639382,
    "total_throughput": 12578.330998910982,
    "itl": 146.19237250967436,
    "ttft": 1938436.855055099,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5349971288978126,
    "arrivals": 623927,
    "finished_requests": 97045,
    "scheduler_time": 89.66262273423936
}
#Debug simulation 
Total elapsed time: 6.7708549979142845. Arrivals time: 0.29877798864617944 Scheduler time: 6.368688913062215 Scheduler overhead time: 0.036776980850845575 Adapter cache time: 0.010761568322777748 Engine time: 0.03839304205030203 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.569861760828644,
    "estimated_duration": 3600.0041866626443,
    "input_throughput": 6411.061988623962,
    "output_throughput": 5709.827248577653,
    "total_throughput": 12120.889237201614,
    "itl": 123.550861965944,
    "ttft": 1961239.2516248375,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5650041469931616,
    "arrivals": 623927,
    "finished_requests": 93498,
    "scheduler_time": 86.93650072610048
}
#Debug simulation 
Total elapsed time: 6.569978387095034. Arrivals time: 0.353740026243031 Scheduler time: 6.097083749715239 Scheduler overhead time: 0.0428964807651937 Adapter cache time: 0.011861821636557579 Engine time: 0.04404643923044205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.684713203925639,
    "estimated_duration": 3600.100661928741,
    "input_throughput": 6655.431680947331,
    "output_throughput": 5922.923551969854,
    "total_throughput": 12578.355232917185,
    "itl": 146.19121845005108,
    "ttft": 1938425.7650113304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5112986004282727,
    "arrivals": 623927,
    "finished_requests": 97044,
    "scheduler_time": 89.66200456155043
}
#Debug simulation 
Total elapsed time: 6.684807375073433. Arrivals time: 0.34266417333856225 Scheduler time: 6.239397060126066 Scheduler overhead time: 0.036702662240713835 Adapter cache time: 0.010750633664429188 Engine time: 0.03793687466531992 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_160_slots_128_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [53 53 54]
Adapter prompts. [33, 34560, 33, 33, 66, 33, 33, 33, 34560, 33, 66, 34560, 66, 33, 66, 66, 66, 66, 34560, 34560, 34560, 66, 66, 66, 34560, 34560, 33, 34560, 66, 34560, 34560, 34560, 66, 34560, 33, 66, 66, 66, 34560, 33, 33, 34560, 33, 34560, 33, 33, 66, 33, 66, 34560, 33, 33, 66, 33, 33, 33, 33, 33, 34560, 66, 66, 34560, 66, 33, 34560, 34560, 34560, 66, 66, 33, 33, 33, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 33, 33, 34560, 66, 34560, 34560, 33, 34560, 33, 66, 33, 33, 66, 66, 34560, 34560, 34560, 33, 34560, 66, 66, 33, 33, 33, 33, 34560, 66, 34560, 66, 34560, 34560, 34560, 33, 66, 66, 34560, 66, 34560, 33, 33, 33, 66, 66, 66, 34560, 34560, 66, 34560, 33, 66, 66, 33, 34560, 34560, 34560, 34560, 34560, 34560, 66, 66, 66, 66, 66, 34560, 33, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 1871487 . Total input tokens: 416814900 . Total output tokens: 374379751
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.871312445960939,
    "estimated_duration": 3600.010900228613,
    "input_throughput": 6411.050032802498,
    "output_throughput": 5709.816600470476,
    "total_throughput": 12120.866633272975,
    "itl": 123.55105568542483,
    "ttft": 1961242.9339741494,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 171,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5722978666052229,
    "arrivals": 623927,
    "finished_requests": 93498,
    "scheduler_time": 86.9364881406829
}
#Debug simulation 
Total elapsed time: 6.871405208949. Arrivals time: 0.6515998649410903 Scheduler time: 6.100705758668482 Scheduler overhead time: 0.042750462889671326 Adapter cache time: 0.012016294989734888 Engine time: 0.044028019066900015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.918852346017957,
    "estimated_duration": 3600.212617947344,
    "input_throughput": 4700.37739316859,
    "output_throughput": 4128.85892513624,
    "total_throughput": 8829.23631830483,
    "itl": 207.11301225204053,
    "ttft": 2109064.281235193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 433,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3251911931228924,
    "arrivals": 540089,
    "finished_requests": 68458,
    "scheduler_time": 62.70896589473186
}
#Debug simulation 
Total elapsed time: 6.918943986762315. Arrivals time: 0.2443606830202043 Scheduler time: 6.591268948279321 Scheduler overhead time: 0.028627832420170307 Adapter cache time: 0.01244426891207695 Engine time: 0.029032903257757425 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.955015618819743,
    "estimated_duration": 3600.090924671385,
    "input_throughput": 4700.441003873989,
    "output_throughput": 4129.458480651291,
    "total_throughput": 8829.89948452528,
    "itl": 207.13079858905843,
    "ttft": 2108903.5157273575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 437,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4206500727171123,
    "arrivals": 540089,
    "finished_requests": 68457,
    "scheduler_time": 62.705270811275554
}
#Debug simulation 
Total elapsed time: 6.955105854663998. Arrivals time: 0.29999957233667374 Scheduler time: 6.572023645509034 Scheduler overhead time: 0.028743347618728876 Adapter cache time: 0.012388799339532852 Engine time: 0.028794444631785154 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.60237737512216,
    "estimated_duration": 3600.155310716247,
    "input_throughput": 4510.7509533440625,
    "output_throughput": 3976.0201337403378,
    "total_throughput": 8486.7710870844,
    "itl": 176.57017726029844,
    "ttft": 2133223.3490665285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 721,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3532206598855563,
    "arrivals": 540089,
    "finished_requests": 65777,
    "scheduler_time": 60.673960543138236
}
#Debug simulation 
Total elapsed time: 5.602496637031436. Arrivals time: 0.2789218001998961 Scheduler time: 5.226105896756053 Scheduler overhead time: 0.03209579223766923 Adapter cache time: 0.018584133591502905 Engine time: 0.03195622703060508 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.937874492723495,
    "estimated_duration": 3600.01181419054,
    "input_throughput": 4700.122353294155,
    "output_throughput": 4128.930338897291,
    "total_throughput": 8829.052692191446,
    "itl": 207.12416766253355,
    "ttft": 2109028.4755105996,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 436,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3653598216618388,
    "arrivals": 540089,
    "finished_requests": 68453,
    "scheduler_time": 62.70472675518155
}
#Debug simulation 
Total elapsed time: 6.938028146978468. Arrivals time: 0.30166934290900826 Scheduler time: 6.55266969371587 Scheduler overhead time: 0.028860867954790592 Adapter cache time: 0.012519415467977524 Engine time: 0.029048229567706585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.68548327870667,
    "estimated_duration": 3600.0056720798584,
    "input_throughput": 4511.15761454271,
    "output_throughput": 3975.9898466300206,
    "total_throughput": 8487.14746117273,
    "itl": 176.5626014424288,
    "ttft": 2133318.8564900174,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 711,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3483477917127344,
    "arrivals": 540089,
    "finished_requests": 65784,
    "scheduler_time": 60.671695611706525
}
#Debug simulation 
Total elapsed time: 5.685582713689655. Arrivals time: 0.2866797395981848 Scheduler time: 5.300803639926016 Scheduler overhead time: 0.032314842101186514 Adapter cache time: 0.018682675436139107 Engine time: 0.032216922380030155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.972678523976356,
    "estimated_duration": 3600.2038429585755,
    "input_throughput": 4700.655779005212,
    "output_throughput": 4129.436456515404,
    "total_throughput": 8830.092235520617,
    "itl": 207.13949326280024,
    "ttft": 2108923.2088606665,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.297681827987535,
    "arrivals": 540089,
    "finished_requests": 68456,
    "scheduler_time": 62.710120952272135
}
#Debug simulation 
Total elapsed time: 6.972790907602757. Arrivals time: 0.2948344978503883 Scheduler time: 6.594902874901891 Scheduler overhead time: 0.02884466340765357 Adapter cache time: 0.012381163891404867 Engine time: 0.0286713857203722 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [4320, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 17280, 8640, 4320, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 4320, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 8640, 17280, 4320, 4320, 17280, 4320, 17280, 4320, 4320, 8640, 4320, 8640, 17280, 4320, 4320, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 8640, 8640, 17280, 8640, 4320, 17280, 17280, 17280, 8640, 8640, 4320, 4320, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 4320, 4320, 17280, 8640, 17280, 17280, 4320, 17280, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 17280, 17280, 4320, 17280, 8640, 8640, 4320, 4320, 4320, 4320, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 8640, 17280, 8640, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 4320, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1620000 . Total input tokens: 360785921 . Total output tokens: 323940616
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.743918460793793,
    "estimated_duration": 3600.037500556838,
    "input_throughput": 4511.11773071476,
    "output_throughput": 3975.9546943013893,
    "total_throughput": 8487.07242501615,
    "itl": 176.5639666592406,
    "ttft": 2133332.3570595062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 711,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.380163499675698,
    "arrivals": 540089,
    "finished_requests": 65784,
    "scheduler_time": 60.671708380747326
}
#Debug simulation 
Total elapsed time: 5.74401972303167. Arrivals time: 0.2926152618601918 Scheduler time: 5.352000483777374 Scheduler overhead time: 0.0327321863733232 Adapter cache time: 0.018958481028676033 Engine time: 0.03272379422560334 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.972162487916648,
    "estimated_duration": 3600.1581482236047,
    "input_throughput": 4652.309512643033,
    "output_throughput": 4126.351229134207,
    "total_throughput": 8778.66074177724,
    "itl": 208.40147726451457,
    "ttft": 2087615.7388997334,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 949,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.904402868992248,
    "arrivals": 483089,
    "finished_requests": 68022,
    "scheduler_time": 62.710202414182206
}
#Debug simulation 
Total elapsed time: 4.972250213846564. Arrivals time: 0.2307050684466958 Scheduler time: 4.651800520252436 Scheduler overhead time: 0.02782588731497526 Adapter cache time: 0.02099854126572609 Engine time: 0.027977668680250645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.001450754702091,
    "estimated_duration": 3600.042678184575,
    "input_throughput": 4652.009850184713,
    "output_throughput": 4126.349970798424,
    "total_throughput": 8778.359820983138,
    "itl": 208.41423328361634,
    "ttft": 2087591.6187305506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 950,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.085296375902836,
    "arrivals": 483089,
    "finished_requests": 68017,
    "scheduler_time": 62.70543583290049
}
#Debug simulation 
Total elapsed time: 5.001541309058666. Arrivals time: 0.22829835303127766 Scheduler time: 4.682869121432304 Scheduler overhead time: 0.02813678840175271 Adapter cache time: 0.020777463912963867 Engine time: 0.028452318161725998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.994131810963154,
    "estimated_duration": 3600.1573077946473,
    "input_throughput": 4477.491015489667,
    "output_throughput": 3974.002460676941,
    "total_throughput": 8451.493476166608,
    "itl": 177.10986443179243,
    "ttft": 2113553.5267256224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.690574104729979,
    "arrivals": 483089,
    "finished_requests": 65425,
    "scheduler_time": 60.646798961649544
}
#Debug simulation 
Total elapsed time: 4.994232211727649. Arrivals time: 0.49899270199239254 Scheduler time: 4.385523732751608 Scheduler overhead time: 0.03191979555413127 Adapter cache time: 0.031180651392787695 Engine time: 0.03188930870965123 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.995251369196922,
    "estimated_duration": 3600.0443903070145,
    "input_throughput": 4652.126525187577,
    "output_throughput": 4126.389952300877,
    "total_throughput": 8778.516477488454,
    "itl": 208.40907659284525,
    "ttft": 2087577.2409226096,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 956,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9908729651011225,
    "arrivals": 483089,
    "finished_requests": 68019,
    "scheduler_time": 62.70674266959665
}
#Debug simulation 
Total elapsed time: 4.995349216274917. Arrivals time: 0.235577420797199 Scheduler time: 4.669755345676094 Scheduler overhead time: 0.027840909082442522 Adapter cache time: 0.021172678098082542 Engine time: 0.028047929983586073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.7881832220591605,
    "estimated_duration": 3600.0995517186834,
    "input_throughput": 4477.562847478617,
    "output_throughput": 3974.0662152439195,
    "total_throughput": 8451.629062722537,
    "itl": 177.11576936566763,
    "ttft": 2113490.70400064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1440,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.742716200035025,
    "arrivals": 483089,
    "finished_requests": 65425,
    "scheduler_time": 60.645495213876
}
#Debug simulation 
Total elapsed time: 4.78834473900497. Arrivals time: 0.23958752863109112 Scheduler time: 4.438207057770342 Scheduler overhead time: 0.03211671393364668 Adapter cache time: 0.031210271175950766 Engine time: 0.03230818919837475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.274960707873106,
    "estimated_duration": 3600.0635891826,
    "input_throughput": 4652.214769295983,
    "output_throughput": 4126.4048903572075,
    "total_throughput": 8778.61965965319,
    "itl": 208.3978182783809,
    "ttft": 2087594.3980385968,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 949,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.837557729862101,
    "arrivals": 483089,
    "finished_requests": 68021,
    "scheduler_time": 62.70964513485824
}
#Debug simulation 
Total elapsed time: 5.275057395920157. Arrivals time: 0.5177038041874766 Scheduler time: 4.667380807455629 Scheduler overhead time: 0.027920030057430267 Adapter cache time: 0.020804256666451693 Engine time: 0.0282871606759727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 17280, 8640, 1080, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 1080, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 8640, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 8640, 1080, 8640, 17280, 1080, 1080, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 8640, 8640, 17280, 8640, 1080, 17280, 17280, 17280, 8640, 8640, 1080, 1080, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 8640, 17280, 17280, 1080, 17280, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 17280, 17280, 1080, 17280, 8640, 8640, 1080, 1080, 1080, 1080, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 8640, 17280, 8640, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 1080, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1448280 . Total input tokens: 322781813 . Total output tokens: 289511553
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.731917636934668,
    "estimated_duration": 3600.06781901773,
    "input_throughput": 4477.512038758794,
    "output_throughput": 3974.0095796038504,
    "total_throughput": 8451.521618362643,
    "itl": 177.11463329382352,
    "ttft": 2113513.9122940525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1434,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.785274193510396,
    "arrivals": 483089,
    "finished_requests": 65424,
    "scheduler_time": 60.64451588121636
}
#Debug simulation 
Total elapsed time: 4.732008961029351. Arrivals time: 0.2285382691770792 Scheduler time: 4.393894626758993 Scheduler overhead time: 0.03190711513161659 Adapter cache time: 0.03075492847710848 Engine time: 0.03204597160220146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.891198233701289,
    "estimated_duration": 3600.03657209864,
    "input_throughput": 4798.6240289579655,
    "output_throughput": 4227.887327024344,
    "total_throughput": 9026.51135598231,
    "itl": 202.75913654265008,
    "ttft": 2069620.718676008,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1000,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.060487743932824,
    "arrivals": 473564,
    "finished_requests": 69993,
    "scheduler_time": 64.14497236364579
}
#Debug simulation 
Total elapsed time: 4.891292379703373. Arrivals time: 0.22623925702646375 Scheduler time: 4.573063038755208 Scheduler overhead time: 0.027254400309175253 Adapter cache time: 0.023609828669577837 Engine time: 0.028284197207540274 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.848396557848901,
    "estimated_duration": 3600.1646953794366,
    "input_throughput": 4798.342426437059,
    "output_throughput": 4227.617980792373,
    "total_throughput": 9025.960407229431,
    "itl": 202.77005175970996,
    "ttft": 2069685.1111327768,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1000,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2437879876536315,
    "arrivals": 473564,
    "finished_requests": 69992,
    "scheduler_time": 64.14382995576595
}
#Debug simulation 
Total elapsed time: 4.848487327806652. Arrivals time: 0.22793679172173142 Scheduler time: 4.529148500878364 Scheduler overhead time: 0.027098156046122313 Adapter cache time: 0.023697263095527887 Engine time: 0.027949681039899588 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.792607942130417,
    "estimated_duration": 3600.0302788316726,
    "input_throughput": 4661.113851921742,
    "output_throughput": 4116.812041039218,
    "total_throughput": 8777.925892960959,
    "itl": 170.31652788380782,
    "ttft": 2088549.4250688697,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 959,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1190595877170293,
    "arrivals": 473564,
    "finished_requests": 68060,
    "scheduler_time": 62.82054106471917
}
#Debug simulation 
Total elapsed time: 4.7926933988928795. Arrivals time: 0.22302271239459515 Scheduler time: 4.462907798122615 Scheduler overhead time: 0.03195085842162371 Adapter cache time: 0.027495597023516893 Engine time: 0.03235831903293729 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.903479163069278,
    "estimated_duration": 3600.102181574037,
    "input_throughput": 4798.536577216519,
    "output_throughput": 4227.810276580892,
    "total_throughput": 9026.34685379741,
    "itl": 202.7625374308181,
    "ttft": 2069648.020089729,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1000,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.125703940624338,
    "arrivals": 473564,
    "finished_requests": 69993,
    "scheduler_time": 64.14499202772696
}
#Debug simulation 
Total elapsed time: 4.903587067965418. Arrivals time: 0.2522091348655522 Scheduler time: 4.559900806285441 Scheduler overhead time: 0.02710484666749835 Adapter cache time: 0.023573359474539757 Engine time: 0.028000194113701582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.098823445849121,
    "estimated_duration": 3600.153174142436,
    "input_throughput": 4660.593088236241,
    "output_throughput": 4116.477628352617,
    "total_throughput": 8777.070716588858,
    "itl": 170.2068629704667,
    "ttft": 2088683.1196083718,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 959,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.153601865116507,
    "arrivals": 473564,
    "finished_requests": 68056,
    "scheduler_time": 62.8168570185703
}
#Debug simulation 
Total elapsed time: 5.0989178558811545. Arrivals time: 0.2297865184955299 Scheduler time: 4.762279062066227 Scheduler overhead time: 0.03183181490749121 Adapter cache time: 0.027510231360793114 Engine time: 0.032542907167226076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.873376814182848,
    "estimated_duration": 3600.1807238634574,
    "input_throughput": 4798.557996127754,
    "output_throughput": 4227.788038281067,
    "total_throughput": 9026.346034408822,
    "itl": 202.75545766467712,
    "ttft": 2069633.1970604514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1000,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9900502949021064,
    "arrivals": 473564,
    "finished_requests": 69994,
    "scheduler_time": 64.14866363758605
}
#Debug simulation 
Total elapsed time: 4.873468012083322. Arrivals time: 0.22907181177288294 Scheduler time: 4.552355888299644 Scheduler overhead time: 0.027347862720489502 Adapter cache time: 0.02389143081381917 Engine time: 0.028008348308503628 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 8640, 540, 540, 540, 17280, 540, 8640, 17280, 8640, 540, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 540, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 540, 8640, 8640, 8640, 17280, 540, 540, 17280, 540, 17280, 540, 540, 8640, 540, 8640, 17280, 540, 540, 8640, 540, 540, 540, 540, 540, 17280, 8640, 8640, 17280, 8640, 540, 17280, 17280, 17280, 8640, 8640, 540, 540, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 540, 540, 17280, 8640, 17280, 17280, 540, 17280, 540, 8640, 540, 540, 8640, 8640, 17280, 17280, 17280, 540, 17280, 8640, 8640, 540, 540, 540, 540, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 540, 8640, 8640, 17280, 8640, 17280, 540, 540, 540, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 540, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 540, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1419660 . Total input tokens: 316436862 . Total output tokens: 283755276
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.838376620784402,
    "estimated_duration": 3600.0735621434205,
    "input_throughput": 4661.340861604169,
    "output_throughput": 4117.303367316441,
    "total_throughput": 8778.64422892061,
    "itl": 170.46920351228658,
    "ttft": 2088423.1185289377,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 960,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.1972638569772576,
    "arrivals": 473564,
    "finished_requests": 68067,
    "scheduler_time": 62.827443972543016
}
#Debug simulation 
Total elapsed time: 4.838462323881686. Arrivals time: 0.2237313031218946 Scheduler time: 4.507225915323943 Scheduler overhead time: 0.031950045842677355 Adapter cache time: 0.02790187858045101 Engine time: 0.03263954212889075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.029154004063457,
    "estimated_duration": 3600.0037148923147,
    "input_throughput": 4952.448500607535,
    "output_throughput": 4357.210781508654,
    "total_throughput": 9309.659282116188,
    "itl": 196.7051344019843,
    "ttft": 2049856.878238577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6312399675161784,
    "arrivals": 468761,
    "finished_requests": 71861,
    "scheduler_time": 66.1285967974588
}
#Debug simulation 
Total elapsed time: 5.029248295351863. Arrivals time: 0.23207232914865017 Scheduler time: 4.710378902964294 Scheduler overhead time: 0.027978059835731983 Adapter cache time: 0.016821426805108786 Engine time: 0.028875396121293306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.056076630949974,
    "estimated_duration": 3600.1588824359296,
    "input_throughput": 4952.239215602811,
    "output_throughput": 4357.023540412915,
    "total_throughput": 9309.262756015727,
    "itl": 196.70974901902562,
    "ttft": 2049932.2388373981,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7375223592738671,
    "arrivals": 468761,
    "finished_requests": 71862,
    "scheduler_time": 66.12917844389403
}
#Debug simulation 
Total elapsed time: 5.056169112212956. Arrivals time: 0.2732300809584558 Scheduler time: 4.69579130038619 Scheduler overhead time: 0.02789597027003765 Adapter cache time: 0.017066268250346184 Engine time: 0.029033534228801727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.94670684915036,
    "estimated_duration": 3600.145775958919,
    "input_throughput": 4793.57616995477,
    "output_throughput": 4225.032525508926,
    "total_throughput": 9018.608695463696,
    "itl": 165.9282836837372,
    "ttft": 2070169.965418038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6851198741048676,
    "arrivals": 468761,
    "finished_requests": 69589,
    "scheduler_time": 64.45138239892859
}
#Debug simulation 
Total elapsed time: 4.946823307313025. Arrivals time: 0.26596056343987584 Scheduler time: 4.579383648000658 Scheduler overhead time: 0.03254436841234565 Adapter cache time: 0.020253533497452736 Engine time: 0.03330316813662648 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.082010875921696,
    "estimated_duration": 3600.086008747225,
    "input_throughput": 4952.339459857562,
    "output_throughput": 4357.111736188347,
    "total_throughput": 9309.45119604591,
    "itl": 196.70680340125605,
    "ttft": 2049899.4197927308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.670104131731197,
    "arrivals": 468761,
    "finished_requests": 71862,
    "scheduler_time": 66.12910255921511
}
#Debug simulation 
Total elapsed time: 5.082101416774094. Arrivals time: 0.27576061291620135 Scheduler time: 4.718995395582169 Scheduler overhead time: 0.02801128290593624 Adapter cache time: 0.01693583559244871 Engine time: 0.029086584225296974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.982066177763045,
    "estimated_duration": 3600.04335041043,
    "input_throughput": 4791.2853599480995,
    "output_throughput": 4223.742471952859,
    "total_throughput": 9015.027831900958,
    "itl": 165.74182919374988,
    "ttft": 2070249.7900181166,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7051147261448247,
    "arrivals": 468761,
    "finished_requests": 69558,
    "scheduler_time": 64.43600066834549
}
#Debug simulation 
Total elapsed time: 4.982153072021902. Arrivals time: 0.27321780007332563 Scheduler time: 4.6067368891090155 Scheduler overhead time: 0.03275805152952671 Adapter cache time: 0.02055222075432539 Engine time: 0.033490078058093786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.062702692113817,
    "estimated_duration": 3600.134703062393,
    "input_throughput": 4952.555243233907,
    "output_throughput": 4357.223352408583,
    "total_throughput": 9309.77859564249,
    "itl": 196.7009500926369,
    "ttft": 2049894.909738467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 533,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.59369680718284,
    "arrivals": 468761,
    "finished_requests": 71865,
    "scheduler_time": 66.13183462108161
}
#Debug simulation 
Total elapsed time: 5.062823430169374. Arrivals time: 0.2791521577164531 Scheduler time: 4.696668827906251 Scheduler overhead time: 0.027939530555158854 Adapter cache time: 0.016961925197392702 Engine time: 0.028918574564158916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 8640, 270, 270, 270, 17280, 270, 8640, 17280, 8640, 270, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 270, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 270, 8640, 8640, 8640, 17280, 270, 270, 17280, 270, 17280, 270, 270, 8640, 270, 8640, 17280, 270, 270, 8640, 270, 270, 270, 270, 270, 17280, 8640, 8640, 17280, 8640, 270, 17280, 17280, 17280, 8640, 8640, 270, 270, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 270, 270, 17280, 8640, 17280, 17280, 270, 17280, 270, 8640, 270, 270, 8640, 8640, 17280, 17280, 17280, 270, 17280, 8640, 8640, 270, 270, 270, 270, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 270, 8640, 8640, 17280, 8640, 17280, 270, 270, 270, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 270, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 270, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1405350 . Total input tokens: 313239171 . Total output tokens: 280867368
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.968229961115867,
    "estimated_duration": 3600.074988124262,
    "input_throughput": 4791.815186324269,
    "output_throughput": 4224.0417352870745,
    "total_throughput": 9015.856921611343,
    "itl": 165.79147769460613,
    "ttft": 2070183.644317176,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 516,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7278761614859084,
    "arrivals": 468761,
    "finished_requests": 69566,
    "scheduler_time": 64.4397496735124
}
#Debug simulation 
Total elapsed time: 4.968323364853859. Arrivals time: 0.2719429647549987 Scheduler time: 4.594627785962075 Scheduler overhead time: 0.032579231541603804 Adapter cache time: 0.020441208500415087 Engine time: 0.03332926891744137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.139988949056715,
    "estimated_duration": 3600.201793391793,
    "input_throughput": 5020.571356077033,
    "output_throughput": 4444.0917254603555,
    "total_throughput": 9464.663081537388,
    "itl": 193.15641222227083,
    "ttft": 2039716.32517113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.071170710376465,
    "arrivals": 466310,
    "finished_requests": 73146,
    "scheduler_time": 67.41601354736433
}
#Debug simulation 
Total elapsed time: 5.140078683849424. Arrivals time: 0.2802461627870798 Scheduler time: 4.775062344502658 Scheduler overhead time: 0.028554543387144804 Adapter cache time: 0.013304349966347218 Engine time: 0.029368147253990173 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.423603359144181,
    "estimated_duration": 3600.103928025556,
    "input_throughput": 5020.432843424208,
    "output_throughput": 4443.9642076595155,
    "total_throughput": 9464.397051083723,
    "itl": 193.1605574240063,
    "ttft": 2039699.1482768743,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1404945264570465,
    "arrivals": 466310,
    "finished_requests": 73142,
    "scheduler_time": 67.41270144308137
}
#Debug simulation 
Total elapsed time: 5.423668405972421. Arrivals time: 0.2926377896219492 Scheduler time: 5.046577558387071 Scheduler overhead time: 0.02840635273605585 Adapter cache time: 0.01332188118249178 Engine time: 0.029296555556356907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.012443521991372,
    "estimated_duration": 3600.052010523434,
    "input_throughput": 4848.784392273817,
    "output_throughput": 4294.567954797985,
    "total_throughput": 9143.352347071801,
    "itl": 162.98606243696855,
    "ttft": 2064061.4588315194,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1137206136994124,
    "arrivals": 466310,
    "finished_requests": 70576,
    "scheduler_time": 65.47439890684343
}
#Debug simulation 
Total elapsed time: 5.012536338996142. Arrivals time: 0.26957312831655145 Scheduler time: 4.644292728044093 Scheduler overhead time: 0.03325693914666772 Adapter cache time: 0.016227067913860083 Engine time: 0.03370034508407116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.164226454216987,
    "estimated_duration": 3600.0108943485734,
    "input_throughput": 5020.562584511436,
    "output_throughput": 4444.079051292702,
    "total_throughput": 9464.641635804139,
    "itl": 193.1579384186975,
    "ttft": 2039660.1363307922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0963662320654861,
    "arrivals": 466310,
    "finished_requests": 73142,
    "scheduler_time": 67.41195449868545
}
#Debug simulation 
Total elapsed time: 5.1643343809992075. Arrivals time: 0.2868175678886473 Scheduler time: 4.793308940716088 Scheduler overhead time: 0.028413574676960707 Adapter cache time: 0.013168171979486942 Engine time: 0.02930435398593545 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.044792691245675,
    "estimated_duration": 3600.065528176313,
    "input_throughput": 4848.7661858873535,
    "output_throughput": 4294.551829402927,
    "total_throughput": 9143.31801529028,
    "itl": 162.9864923701751,
    "ttft": 2064068.1626825426,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1273020226322166,
    "arrivals": 466310,
    "finished_requests": 70576,
    "scheduler_time": 65.47440619962619
}
#Debug simulation 
Total elapsed time: 5.0449069030582905. Arrivals time: 0.2670880686491728 Scheduler time: 4.678502012509853 Scheduler overhead time: 0.033248609863221645 Adapter cache time: 0.016469075344502926 Engine time: 0.03406838467344642 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.138223297894001,
    "estimated_duration": 3600.0900987445793,
    "input_throughput": 5020.542682057567,
    "output_throughput": 4444.120163986796,
    "total_throughput": 9464.662846044363,
    "itl": 193.15586354419042,
    "ttft": 2039661.3386286034,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 350,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0465176032157613,
    "arrivals": 466310,
    "finished_requests": 73144,
    "scheduler_time": 67.41386011001171
}
#Debug simulation 
Total elapsed time: 5.138313286006451. Arrivals time: 0.27445785747841 Scheduler time: 4.77910225931555 Scheduler overhead time: 0.02856007358059287 Adapter cache time: 0.013347324449568987 Engine time: 0.02946091117337346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 8640, 135, 135, 135, 17280, 135, 8640, 17280, 8640, 135, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 135, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 135, 8640, 8640, 8640, 17280, 135, 135, 17280, 135, 17280, 135, 135, 8640, 135, 8640, 17280, 135, 135, 8640, 135, 135, 135, 135, 135, 17280, 8640, 8640, 17280, 8640, 135, 17280, 17280, 17280, 8640, 8640, 135, 135, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 135, 135, 17280, 8640, 17280, 17280, 135, 17280, 135, 8640, 135, 135, 8640, 8640, 17280, 17280, 17280, 135, 17280, 8640, 8640, 135, 135, 135, 135, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 135, 8640, 8640, 17280, 8640, 17280, 135, 135, 135, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 135, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 135, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1398195 . Total input tokens: 311697532 . Total output tokens: 279439003
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.0309745986014605,
    "estimated_duration": 3600.080186160738,
    "input_throughput": 4848.746443788412,
    "output_throughput": 4294.534343827448,
    "total_throughput": 9143.280787615859,
    "itl": 162.98697946717155,
    "ttft": 2064075.424454918,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 341,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1420152156427539,
    "arrivals": 466310,
    "finished_requests": 70576,
    "scheduler_time": 65.47440992897347
}
#Debug simulation 
Total elapsed time: 5.031066227704287. Arrivals time: 0.26540587842464447 Scheduler time: 4.667001984082162 Scheduler overhead time: 0.03299350570887327 Adapter cache time: 0.01644852478057146 Engine time: 0.0335976080968976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.433205566834658,
    "estimated_duration": 3600.123496620935,
    "input_throughput": 5130.461779251785,
    "output_throughput": 4483.322590224874,
    "total_throughput": 9613.78436947666,
    "itl": 190.31635847595444,
    "ttft": 2031297.6578457605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7130936443363307,
    "arrivals": 465116,
    "finished_requests": 74209,
    "scheduler_time": 68.02086142747775
}
#Debug simulation 
Total elapsed time: 5.433294974267483. Arrivals time: 0.24348099064081907 Scheduler time: 5.107720305211842 Scheduler overhead time: 0.028815746773034334 Adapter cache time: 0.010044760536402464 Engine time: 0.029562725219875574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.151222223881632,
    "estimated_duration": 3600.0561493269,
    "input_throughput": 5130.486368512149,
    "output_throughput": 4483.324795647347,
    "total_throughput": 9613.811164159497,
    "itl": 190.31727902663337,
    "ttft": 2031299.3823112645,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7604225883889041,
    "arrivals": 465116,
    "finished_requests": 74208,
    "scheduler_time": 68.01920332844719
}
#Debug simulation 
Total elapsed time: 5.151313997339457. Arrivals time: 0.2597906538285315 Scheduler time: 4.809448922518641 Scheduler overhead time: 0.028899310156702995 Adapter cache time: 0.010039090644568205 Engine time: 0.02960042003542185 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.076140208169818,
    "estimated_duration": 3600.1608641343764,
    "input_throughput": 4951.21125769081,
    "output_throughput": 4328.329646388372,
    "total_throughput": 9279.540904079182,
    "itl": 161.49193825456297,
    "ttft": 2055744.108024568,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7485646830312942,
    "arrivals": 465116,
    "finished_requests": 71554,
    "scheduler_time": 65.99970947674848
}
#Debug simulation 
Total elapsed time: 5.076239563059062. Arrivals time: 0.24696569936349988 Scheduler time: 4.732328281272203 Scheduler overhead time: 0.0336636402644217 Adapter cache time: 0.013132368680089712 Engine time: 0.03433494921773672 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.158327980898321,
    "estimated_duration": 3600.1399988172434,
    "input_throughput": 5130.582979014213,
    "output_throughput": 4483.411202148468,
    "total_throughput": 9613.994181162681,
    "itl": 190.3170426741095,
    "ttft": 2031298.594551598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7301865348243175,
    "arrivals": 465116,
    "finished_requests": 74210,
    "scheduler_time": 68.0208718580378
}
#Debug simulation 
Total elapsed time: 5.158429011236876. Arrivals time: 0.25004982063546777 Scheduler time: 4.825978011358529 Scheduler overhead time: 0.028873806819319725 Adapter cache time: 0.010100289713591337 Engine time: 0.029840086586773396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.03183167707175,
    "estimated_duration": 3600.0453284110613,
    "input_throughput": 4951.1165482649685,
    "output_throughput": 4328.266890705332,
    "total_throughput": 9279.3834389703,
    "itl": 161.48549913081132,
    "ttft": 2055762.9861423094,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7577447094395787,
    "arrivals": 465116,
    "finished_requests": 71551,
    "scheduler_time": 65.99717453844546
}
#Debug simulation 
Total elapsed time: 5.031933425925672. Arrivals time: 0.24258533539250493 Scheduler time: 4.692798892501742 Scheduler overhead time: 0.033525793347507715 Adapter cache time: 0.012643201276659966 Engine time: 0.03457998810335994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.181965090800077,
    "estimated_duration": 3600.071450827505,
    "input_throughput": 5130.535949711347,
    "output_throughput": 4483.38740507219,
    "total_throughput": 9613.923354783536,
    "itl": 190.315134213989,
    "ttft": 2031292.3228977113,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6966817187122079,
    "arrivals": 465116,
    "finished_requests": 74209,
    "scheduler_time": 68.0201019044383
}
#Debug simulation 
Total elapsed time: 5.182072295807302. Arrivals time: 0.25581571087241173 Scheduler time: 4.84373317565769 Scheduler overhead time: 0.029052674770355225 Adapter cache time: 0.010121737141162157 Engine time: 0.029872669838368893 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 8640, 66, 66, 66, 17280, 66, 8640, 17280, 8640, 66, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 66, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 66, 8640, 8640, 8640, 17280, 66, 66, 17280, 66, 17280, 66, 66, 8640, 66, 8640, 17280, 66, 66, 8640, 66, 66, 66, 66, 66, 17280, 8640, 8640, 17280, 8640, 66, 17280, 17280, 17280, 8640, 8640, 66, 66, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 66, 66, 17280, 8640, 17280, 17280, 66, 17280, 66, 8640, 66, 66, 8640, 8640, 17280, 17280, 17280, 66, 17280, 8640, 8640, 66, 66, 66, 66, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 66, 8640, 8640, 17280, 8640, 17280, 66, 66, 66, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 66, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 66, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1394538 . Total input tokens: 310862637 . Total output tokens: 278720460
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.01424410007894,
    "estimated_duration": 3600.0507050117617,
    "input_throughput": 4951.1749862816905,
    "output_throughput": 4328.413479928775,
    "total_throughput": 9279.588466210465,
    "itl": 161.48523198998885,
    "ttft": 2055722.4026973909,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 229,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7678050123527675,
    "arrivals": 465116,
    "finished_requests": 71553,
    "scheduler_time": 65.99686088785883
}
#Debug simulation 
Total elapsed time: 5.01436030305922. Arrivals time: 0.23795138206332922 Scheduler time: 4.68025884591043 Scheduler overhead time: 0.03344376478344202 Adapter cache time: 0.012797967530786991 Engine time: 0.034225855488330126 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.2048413921147585,
    "estimated_duration": 3600.0797481177615,
    "input_throughput": 5120.751008262657,
    "output_throughput": 4509.794542326044,
    "total_throughput": 9630.545550588702,
    "itl": 189.99432694140106,
    "ttft": 2027282.4801979647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.550887793907895,
    "arrivals": 464531,
    "finished_requests": 74574,
    "scheduler_time": 68.36836740531014
}
#Debug simulation 
Total elapsed time: 5.204934655223042. Arrivals time: 0.27795849833637476 Scheduler time: 4.845413648989052 Scheduler overhead time: 0.029031167272478342 Adapter cache time: 0.008885464165359735 Engine time: 0.030003598891198635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.223511994816363,
    "estimated_duration": 3600.0131964248576,
    "input_throughput": 5120.845117542278,
    "output_throughput": 4509.876801597131,
    "total_throughput": 9630.721919139409,
    "itl": 189.9980697563939,
    "ttft": 2027243.756332588,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5868318959767939,
    "arrivals": 464531,
    "finished_requests": 74572,
    "scheduler_time": 68.36683628764675
}
#Debug simulation 
Total elapsed time: 5.223614969756454. Arrivals time: 0.28687760094180703 Scheduler time: 4.856043275911361 Scheduler overhead time: 0.029052041936665773 Adapter cache time: 0.00868955859914422 Engine time: 0.029416043311357498 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.082980538718402,
    "estimated_duration": 3600.1150863285166,
    "input_throughput": 4937.040503926296,
    "output_throughput": 4352.968342459811,
    "total_throughput": 9290.008846386107,
    "itl": 161.68461916992212,
    "ttft": 2051309.4833031422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5720737553946695,
    "arrivals": 464531,
    "finished_requests": 71934,
    "scheduler_time": 66.35056143531914
}
#Debug simulation 
Total elapsed time: 5.08309827465564. Arrivals time: 0.2689836793579161 Scheduler time: 4.719059937167913 Scheduler overhead time: 0.033370227087289095 Adapter cache time: 0.011937803123146296 Engine time: 0.034100690856575966 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.170400218106806,
    "estimated_duration": 3600.1415655546543,
    "input_throughput": 5120.663080691885,
    "output_throughput": 4509.717105387956,
    "total_throughput": 9630.380186079841,
    "itl": 189.9957557431259,
    "ttft": 2027304.900147711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5635419628256939,
    "arrivals": 464531,
    "finished_requests": 74574,
    "scheduler_time": 68.36930256523497
}
#Debug simulation 
Total elapsed time: 5.170495286118239. Arrivals time: 0.2793626678176224 Scheduler time: 4.8107367544434965 Scheduler overhead time: 0.02870722534134984 Adapter cache time: 0.008683918509632349 Engine time: 0.02956386422738433 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.046448118984699,
    "estimated_duration": 3600.1004524228415,
    "input_throughput": 4937.013629172041,
    "output_throughput": 4352.97103708688,
    "total_throughput": 9289.98466625892,
    "itl": 161.6816353930954,
    "ttft": 2051290.4144223384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5791159674339013,
    "arrivals": 464531,
    "finished_requests": 71935,
    "scheduler_time": 66.3496765279229
}
#Debug simulation 
Total elapsed time: 5.04653601674363. Arrivals time: 0.26831652875989676 Scheduler time: 4.683441601227969 Scheduler overhead time: 0.03354381211102009 Adapter cache time: 0.011778184212744236 Engine time: 0.03382206289097667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.191495134029537,
    "estimated_duration": 3600.061109241974,
    "input_throughput": 5120.777520324282,
    "output_throughput": 4509.8178912353405,
    "total_throughput": 9630.595411559623,
    "itl": 189.993972864934,
    "ttft": 2027272.2434688972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 180,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5382090530823923,
    "arrivals": 464531,
    "finished_requests": 74574,
    "scheduler_time": 68.36829992054754
}
#Debug simulation 
Total elapsed time: 5.1915893498808146. Arrivals time: 0.2764402483589947 Scheduler time: 4.833999579772353 Scheduler overhead time: 0.029058131854981184 Adapter cache time: 0.008812816813588142 Engine time: 0.02970079006627202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 8640, 33, 33, 33, 17280, 33, 8640, 17280, 8640, 33, 8640, 8640, 8640, 8640, 17280, 17280, 17280, 8640, 8640, 8640, 17280, 17280, 33, 17280, 8640, 17280, 17280, 17280, 8640, 17280, 33, 8640, 8640, 8640, 17280, 33, 33, 17280, 33, 17280, 33, 33, 8640, 33, 8640, 17280, 33, 33, 8640, 33, 33, 33, 33, 33, 17280, 8640, 8640, 17280, 8640, 33, 17280, 17280, 17280, 8640, 8640, 33, 33, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 33, 33, 17280, 8640, 17280, 17280, 33, 17280, 33, 8640, 33, 33, 8640, 8640, 17280, 17280, 17280, 33, 17280, 8640, 8640, 33, 33, 33, 33, 17280, 8640, 17280, 8640, 17280, 17280, 17280, 33, 8640, 8640, 17280, 8640, 17280, 33, 33, 33, 8640, 8640, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 33, 17280, 17280, 17280, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 17280, 33, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1392789 . Total input tokens: 310479445 . Total output tokens: 278364569
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.0891544269397855,
    "estimated_duration": 3600.0894443085135,
    "input_throughput": 4936.72484390556,
    "output_throughput": 4352.719631667331,
    "total_throughput": 9289.444475572891,
    "itl": 161.67897769351512,
    "ttft": 2051301.228351308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 175,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.586786948405207,
    "arrivals": 464531,
    "finished_requests": 71931,
    "scheduler_time": 66.34916807275688
}
#Debug simulation 
Total elapsed time: 5.089246227871627. Arrivals time: 0.23119476670399308 Scheduler time: 4.762791725806892 Scheduler overhead time: 0.033437986858189106 Adapter cache time: 0.01197387371212244 Engine time: 0.034046498127281666 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.960747483652085,
    "estimated_duration": 3600.2331283674353,
    "input_throughput": 4701.573313858045,
    "output_throughput": 4125.5233954072255,
    "total_throughput": 8827.096709265272,
    "itl": 206.92055352891484,
    "ttft": 2049536.2782948455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.0582067484549365,
    "arrivals": 406769,
    "finished_requests": 68422,
    "scheduler_time": 62.61150534945149
}
#Debug simulation 
Total elapsed time: 4.9608625918626785. Arrivals time: 0.22257736325263977 Scheduler time: 4.64136829925701 Scheduler overhead time: 0.028085479978471994 Adapter cache time: 0.027649143245071173 Engine time: 0.0282210991717875 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.974791503045708,
    "estimated_duration": 3600.1800971056687,
    "input_throughput": 4701.530907747584,
    "output_throughput": 4125.332788751341,
    "total_throughput": 8826.863696498925,
    "itl": 206.93797709753747,
    "ttft": 2049492.0521391681,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.314972878943636,
    "arrivals": 406769,
    "finished_requests": 68417,
    "scheduler_time": 62.606480407660825
}
#Debug simulation 
Total elapsed time: 4.974881503731012. Arrivals time: 0.2584747765213251 Scheduler time: 4.6196165340952575 Scheduler overhead time: 0.02809536922723055 Adapter cache time: 0.02737599005922675 Engine time: 0.028335698880255222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.755623343400657,
    "estimated_duration": 3600.0843940088303,
    "input_throughput": 4526.096117945626,
    "output_throughput": 3973.5935145871786,
    "total_throughput": 8499.689632532805,
    "itl": 176.11609072424073,
    "ttft": 2075954.9214179902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2086,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.804064436145079,
    "arrivals": 406769,
    "finished_requests": 65871,
    "scheduler_time": 60.59215009812653
}
#Debug simulation 
Total elapsed time: 4.755761558189988. Arrivals time: 0.24840031703934073 Scheduler time: 4.386454748921096 Scheduler overhead time: 0.031883280258625746 Adapter cache time: 0.0417797090485692 Engine time: 0.032418408431112766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.991113463882357,
    "estimated_duration": 3600.1304283657123,
    "input_throughput": 4701.621326448387,
    "output_throughput": 4125.391369984914,
    "total_throughput": 8827.012696433301,
    "itl": 206.92517372865694,
    "ttft": 2049500.939266633,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1326,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1527605375227905,
    "arrivals": 406769,
    "finished_requests": 68420,
    "scheduler_time": 62.608194696528464
}
#Debug simulation 
Total elapsed time: 4.99123260891065. Arrivals time: 0.21976236486807466 Scheduler time: 4.674818147905171 Scheduler overhead time: 0.028248682152479887 Adapter cache time: 0.027138059958815575 Engine time: 0.028230007272213697 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.748910688795149,
    "estimated_duration": 3600.1198683244425,
    "input_throughput": 4526.056241450557,
    "output_throughput": 3973.4343641890227,
    "total_throughput": 8499.49060563958,
    "itl": 176.12560638216698,
    "ttft": 2075924.8165142532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2086,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.883249307721713,
    "arrivals": 406769,
    "finished_requests": 65871,
    "scheduler_time": 60.59206918469088
}
#Debug simulation 
Total elapsed time: 4.749000667128712. Arrivals time: 0.24874522211030126 Scheduler time: 4.379476343747228 Scheduler overhead time: 0.031902745831757784 Adapter cache time: 0.04196918569505215 Engine time: 0.032038365956395864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.006667533889413,
    "estimated_duration": 3600.0950822193245,
    "input_throughput": 4701.753596342595,
    "output_throughput": 4125.6815891772985,
    "total_throughput": 8827.435185519895,
    "itl": 206.91409756048114,
    "ttft": 2049449.5310784082,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.961816640745279,
    "arrivals": 406769,
    "finished_requests": 68422,
    "scheduler_time": 62.6106450850097
}
#Debug simulation 
Total elapsed time: 5.006756173912436. Arrivals time: 0.2639185027219355 Scheduler time: 4.645029368344694 Scheduler overhead time: 0.028515092097222805 Adapter cache time: 0.02734897146001458 Engine time: 0.028888068161904812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [53 53 54]
Adapter prompts. [1080, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 17280, 4320, 1080, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 1080, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 4320, 17280, 1080, 1080, 17280, 1080, 17280, 1080, 1080, 4320, 1080, 4320, 17280, 1080, 1080, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 4320, 4320, 17280, 4320, 1080, 17280, 17280, 17280, 4320, 4320, 1080, 1080, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 1080, 1080, 17280, 4320, 17280, 17280, 1080, 17280, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 17280, 17280, 1080, 17280, 4320, 4320, 1080, 1080, 1080, 1080, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 4320, 17280, 4320, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 1080, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1219320 . Total input tokens: 271719435 . Total output tokens: 243622496
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.740766723174602,
    "estimated_duration": 3600.190348600772,
    "input_throughput": 4525.799311231593,
    "output_throughput": 3973.255748979909,
    "total_throughput": 8499.055060211502,
    "itl": 176.1185004081447,
    "ttft": 2076043.5903078583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2084,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.967282008826498,
    "arrivals": 406769,
    "finished_requests": 65868,
    "scheduler_time": 60.592253703659935
}
#Debug simulation 
Total elapsed time: 4.740859075915068. Arrivals time: 0.21889401460066438 Scheduler time: 4.402023004833609 Scheduler overhead time: 0.03191920090466738 Adapter cache time: 0.04123410675674677 Engine time: 0.03204817697405815 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.87750057131052,
    "estimated_duration": 3600.18745115701,
    "input_throughput": 4759.13858165736,
    "output_throughput": 4201.732883419321,
    "total_throughput": 8960.871465076681,
    "itl": 204.1849605851709,
    "ttft": 2035029.6582602367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.030662358759541,
    "arrivals": 397242,
    "finished_requests": 69377,
    "scheduler_time": 63.72196886535421
}
#Debug simulation 
Total elapsed time: 4.8775954050943255. Arrivals time: 0.2564899888820946 Scheduler time: 4.522661400958896 Scheduler overhead time: 0.026921875774860382 Adapter cache time: 0.03112811129540205 Engine time: 0.027721614576876163 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.884560790844262,
    "estimated_duration": 3600.015750485182,
    "input_throughput": 4757.903350198274,
    "output_throughput": 4201.009120018918,
    "total_throughput": 8958.912470217192,
    "itl": 204.2351173486076,
    "ttft": 2035178.5004158835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.257734381901077,
    "arrivals": 397242,
    "finished_requests": 69360,
    "scheduler_time": 63.70291272065609
}
#Debug simulation 
Total elapsed time: 4.884657241869718. Arrivals time: 0.26088524842634797 Scheduler time: 4.524809579830617 Scheduler overhead time: 0.026933369692415 Adapter cache time: 0.03162667155265808 Engine time: 0.02774882735684514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.824719611089677,
    "estimated_duration": 3600.082897889029,
    "input_throughput": 4656.531662043061,
    "output_throughput": 4114.634695963772,
    "total_throughput": 8771.166358006833,
    "itl": 169.95145932478025,
    "ttft": 2052121.5274833017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.226676580924484,
    "arrivals": 397242,
    "finished_requests": 67839,
    "scheduler_time": 62.72739132515786
}
#Debug simulation 
Total elapsed time: 4.824809893965721. Arrivals time: 0.253100807312876 Scheduler time: 4.4524393836036325 Scheduler overhead time: 0.03179048094898462 Adapter cache time: 0.03997182147577405 Engine time: 0.03265140624716878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.869033845607191,
    "estimated_duration": 3600.0429464517915,
    "input_throughput": 4758.731285937844,
    "output_throughput": 4201.573488146318,
    "total_throughput": 8960.304774084161,
    "itl": 204.1892048215124,
    "ttft": 2035039.6381455373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.1164523925445415,
    "arrivals": 397242,
    "finished_requests": 69371,
    "scheduler_time": 63.717920794526854
}
#Debug simulation 
Total elapsed time: 4.869150770828128. Arrivals time: 0.2536352192983031 Scheduler time: 4.516947776079178 Scheduler overhead time: 0.027024718467146158 Adapter cache time: 0.03120149066671729 Engine time: 0.027674836106598377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.842501440085471,
    "estimated_duration": 3600.1787536265083,
    "input_throughput": 4655.186907905038,
    "output_throughput": 4113.182709353948,
    "total_throughput": 8768.369617258986,
    "itl": 169.65120053120887,
    "ttft": 2052288.7290353002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1293,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.266961550246911,
    "arrivals": 397242,
    "finished_requests": 67819,
    "scheduler_time": 62.71773453500983
}
#Debug simulation 
Total elapsed time: 4.84259525872767. Arrivals time: 0.25178926810622215 Scheduler time: 4.47188817942515 Scheduler overhead time: 0.031939512118697166 Adapter cache time: 0.03953666752204299 Engine time: 0.03244561515748501 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.889603782910854,
    "estimated_duration": 3600.0195177096357,
    "input_throughput": 4759.268919436431,
    "output_throughput": 4201.885830225679,
    "total_throughput": 8961.15474966211,
    "itl": 204.17808442331844,
    "ttft": 2035015.6394812763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1315,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.931916137796258,
    "arrivals": 397242,
    "finished_requests": 69375,
    "scheduler_time": 63.72084295979097
}
#Debug simulation 
Total elapsed time: 4.889697822742164. Arrivals time: 0.25890437280759215 Scheduler time: 4.5315334144979715 Scheduler overhead time: 0.027145602274686098 Adapter cache time: 0.031564406119287014 Engine time: 0.027895507868379354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 4320, 540, 540, 540, 17280, 540, 4320, 17280, 4320, 540, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 540, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 540, 4320, 4320, 4320, 17280, 540, 540, 17280, 540, 17280, 540, 540, 4320, 540, 4320, 17280, 540, 540, 4320, 540, 540, 540, 540, 540, 17280, 4320, 4320, 17280, 4320, 540, 17280, 17280, 17280, 4320, 4320, 540, 540, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 540, 540, 17280, 4320, 17280, 17280, 540, 17280, 540, 4320, 540, 540, 4320, 4320, 17280, 17280, 17280, 540, 17280, 4320, 4320, 540, 540, 540, 540, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 540, 4320, 4320, 17280, 4320, 17280, 540, 540, 540, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 540, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 540, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1190700 . Total input tokens: 265316916 . Total output tokens: 237958326
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.839810587931424,
    "estimated_duration": 3600.0269615410652,
    "input_throughput": 4655.33930135507,
    "output_throughput": 4113.316971843208,
    "total_throughput": 8768.65627319828,
    "itl": 169.65205944543226,
    "ttft": 2052264.1769015244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1289,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.308312861807674,
    "arrivals": 397242,
    "finished_requests": 67817,
    "scheduler_time": 62.71538701387359
}
#Debug simulation 
Total elapsed time: 4.839925639797002. Arrivals time: 0.24744699150323868 Scheduler time: 4.473192621022463 Scheduler overhead time: 0.031954361125826836 Adapter cache time: 0.03946278756484389 Engine time: 0.0327789094299078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.258218434173614,
    "estimated_duration": 3600.158620688648,
    "input_throughput": 4952.51817448795,
    "output_throughput": 4348.036753171095,
    "total_throughput": 9300.554927659046,
    "itl": 196.8217399047287,
    "ttft": 2015543.8315435676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0015589845320543,
    "arrivals": 392500,
    "finished_requests": 71967,
    "scheduler_time": 65.88287546485037
}
#Debug simulation 
Total elapsed time: 5.2582809291779995. Arrivals time: 0.49491425743326545 Scheduler time: 4.66917874570936 Scheduler overhead time: 0.027949392795562744 Adapter cache time: 0.024129195604473352 Engine time: 0.02896534977480769 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.028838787227869,
    "estimated_duration": 3600.16124973149,
    "input_throughput": 4952.51455787704,
    "output_throughput": 4348.033577986956,
    "total_throughput": 9300.548135863995,
    "itl": 196.8287168468798,
    "ttft": 2015561.8236012456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1266943312925317,
    "arrivals": 392500,
    "finished_requests": 71967,
    "scheduler_time": 65.88129984163129
}
#Debug simulation 
Total elapsed time: 5.028940394986421. Arrivals time: 0.2303401120007038 Scheduler time: 4.70425167446956 Scheduler overhead time: 0.028059526812285185 Adapter cache time: 0.024089193902909756 Engine time: 0.02892513945698738 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.981692713685334,
    "estimated_duration": 3600.077408562502,
    "input_throughput": 4826.516773965176,
    "output_throughput": 4241.44129892392,
    "total_throughput": 9067.958072889096,
    "itl": 164.93384452052592,
    "ttft": 2035498.3099245443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.08240552213044,
    "arrivals": 392500,
    "finished_requests": 70124,
    "scheduler_time": 64.62602298813789
}
#Debug simulation 
Total elapsed time: 4.981803458649665. Arrivals time: 0.22185255866497755 Scheduler time: 4.645365589298308 Scheduler overhead time: 0.033206693828105927 Adapter cache time: 0.032021111343055964 Engine time: 0.03382977889850736 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 4.982440914027393,
    "estimated_duration": 3600.0284197165465,
    "input_throughput": 4952.643679797351,
    "output_throughput": 4347.932064723099,
    "total_throughput": 9300.57574452045,
    "itl": 196.82398430590555,
    "ttft": 2015535.462296672,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0502870067441705,
    "arrivals": 392500,
    "finished_requests": 71965,
    "scheduler_time": 65.8800714871071
}
#Debug simulation 
Total elapsed time: 4.982530255801976. Arrivals time: 0.22738706739619374 Scheduler time: 4.661203999072313 Scheduler overhead time: 0.027974972501397133 Adapter cache time: 0.02398914285004139 Engine time: 0.028841375838965178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 4.968242713715881,
    "estimated_duration": 3600.1120652237846,
    "input_throughput": 4826.730858701714,
    "output_throughput": 4241.723236204529,
    "total_throughput": 9068.454094906243,
    "itl": 164.93414257695153,
    "ttft": 2035484.903042952,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1057957264035982,
    "arrivals": 392500,
    "finished_requests": 70128,
    "scheduler_time": 64.62726873318353
}
#Debug simulation 
Total elapsed time: 4.968341264873743. Arrivals time: 0.22886569518595934 Scheduler time: 4.625650805886835 Scheduler overhead time: 0.03295816062018275 Adapter cache time: 0.03156000841408968 Engine time: 0.03380655124783516 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 4.981169147882611,
    "estimated_duration": 3600.08053531249,
    "input_throughput": 4952.625594097259,
    "output_throughput": 4348.131061640612,
    "total_throughput": 9300.75665573787,
    "itl": 196.8213326542924,
    "ttft": 2015511.3804109811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 654,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9554928928659903,
    "arrivals": 392500,
    "finished_requests": 71967,
    "scheduler_time": 65.88200825346595
}
#Debug simulation 
Total elapsed time: 4.981280456762761. Arrivals time: 0.22805015183985233 Scheduler time: 4.659747032448649 Scheduler overhead time: 0.027907660230994225 Adapter cache time: 0.023819245863705873 Engine time: 0.02861058246344328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 4320, 270, 270, 270, 17280, 270, 4320, 17280, 4320, 270, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 270, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 270, 4320, 4320, 4320, 17280, 270, 270, 17280, 270, 17280, 270, 270, 4320, 270, 4320, 17280, 270, 270, 4320, 270, 270, 270, 270, 270, 17280, 4320, 4320, 17280, 4320, 270, 17280, 17280, 17280, 4320, 4320, 270, 270, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 270, 270, 17280, 4320, 17280, 17280, 270, 17280, 270, 4320, 270, 270, 4320, 4320, 17280, 17280, 17280, 270, 17280, 4320, 4320, 270, 270, 270, 270, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 270, 4320, 4320, 17280, 4320, 17280, 270, 270, 270, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 270, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 270, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1176390 . Total input tokens: 262129936 . Total output tokens: 235146256
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 4.955762259662151,
    "estimated_duration": 3600.1761701547666,
    "input_throughput": 4826.614637375648,
    "output_throughput": 4241.609098630176,
    "total_throughput": 9068.223736005824,
    "itl": 164.93677412923574,
    "ttft": 2035457.1082788147,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 639,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.133713066987693,
    "arrivals": 392500,
    "finished_requests": 70127,
    "scheduler_time": 64.62694873719451
}
#Debug simulation 
Total elapsed time: 4.955861247610301. Arrivals time: 0.2223536567762494 Scheduler time: 4.620020226109773 Scheduler overhead time: 0.03281464194878936 Adapter cache time: 0.03161619370803237 Engine time: 0.03354667965322733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.102442832197994,
    "estimated_duration": 3600.0645434875228,
    "input_throughput": 5031.248407133669,
    "output_throughput": 4450.233268451268,
    "total_throughput": 9481.481675584937,
    "itl": 193.46999505767792,
    "ttft": 1998689.6188066015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1629853426944508,
    "arrivals": 390109,
    "finished_requests": 73380,
    "scheduler_time": 67.42583718072255
}
#Debug simulation 
Total elapsed time: 5.10253964504227. Arrivals time: 0.23980185948312283 Scheduler time: 4.77225876133889 Scheduler overhead time: 0.028560295701026917 Adapter cache time: 0.018951991107314825 Engine time: 0.029594420455396175 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.140485900919884,
    "estimated_duration": 3600.0455628693035,
    "input_throughput": 5031.274933521604,
    "output_throughput": 4450.256731537271,
    "total_throughput": 9481.531665058874,
    "itl": 193.47297594322959,
    "ttft": 1998690.1075878679,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2391851323098,
    "arrivals": 390109,
    "finished_requests": 73380,
    "scheduler_time": 67.42479716066363
}
#Debug simulation 
Total elapsed time: 5.140585167799145. Arrivals time: 0.2658593845553696 Scheduler time: 4.784893677569926 Scheduler overhead time: 0.028545797802507877 Adapter cache time: 0.018823692109435797 Engine time: 0.02909740014001727 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.039514798205346,
    "estimated_duration": 3600.049709865991,
    "input_throughput": 4882.635634676922,
    "output_throughput": 4324.736949418277,
    "total_throughput": 9207.372584095197,
    "itl": 163.0766205895776,
    "ttft": 2020460.974704273,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2131201190315255,
    "arrivals": 390109,
    "finished_requests": 71215,
    "scheduler_time": 65.87700291302649
}
#Debug simulation 
Total elapsed time: 5.03961148718372. Arrivals time: 0.24672300973907113 Scheduler time: 4.683894879650325 Scheduler overhead time: 0.033160998951643705 Adapter cache time: 0.025891744066029787 Engine time: 0.03431506035849452 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.119591185823083,
    "estimated_duration": 3600.1113367825214,
    "input_throughput": 5031.183012297537,
    "output_throughput": 4450.175425496242,
    "total_throughput": 9481.358437793779,
    "itl": 193.4714881556467,
    "ttft": 1998702.911424054,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1917880753707149,
    "arrivals": 390109,
    "finished_requests": 73380,
    "scheduler_time": 67.42620940047495
}
#Debug simulation 
Total elapsed time: 5.119710850995034. Arrivals time: 0.245879128575325 Scheduler time: 4.783392458688468 Scheduler overhead time: 0.028537160716950893 Adapter cache time: 0.019149882718920708 Engine time: 0.02926339954137802 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.017109535168856,
    "estimated_duration": 3600.0163682733028,
    "input_throughput": 4882.720577304202,
    "output_throughput": 4324.712558867467,
    "total_throughput": 9207.43313617167,
    "itl": 163.07243707797625,
    "ttft": 2020473.8548121685,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.2274560506828178,
    "arrivals": 390109,
    "finished_requests": 71215,
    "scheduler_time": 65.8771461000759
}
#Debug simulation 
Total elapsed time: 5.017200949136168. Arrivals time: 0.2407458727248013 Scheduler time: 4.668004299979657 Scheduler overhead time: 0.03337736381217837 Adapter cache time: 0.025717182084918022 Engine time: 0.033782079350203276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.135063029825687,
    "estimated_duration": 3600.0336464865454,
    "input_throughput": 5031.291587420916,
    "output_throughput": 4450.271462222534,
    "total_throughput": 9481.56304964345,
    "itl": 193.46915750735653,
    "ttft": 1998676.8858715338,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 380,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1362191120628233,
    "arrivals": 390109,
    "finished_requests": 73380,
    "scheduler_time": 67.42579200131496
}
#Debug simulation 
Total elapsed time: 5.135199062991887. Arrivals time: 0.23675349447876215 Scheduler time: 4.808306562248617 Scheduler overhead time: 0.028554453514516354 Adapter cache time: 0.018692408222705126 Engine time: 0.029393815901130438 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 4320, 135, 135, 135, 17280, 135, 4320, 17280, 4320, 135, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 135, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 135, 4320, 4320, 4320, 17280, 135, 135, 17280, 135, 17280, 135, 135, 4320, 135, 4320, 17280, 135, 135, 4320, 135, 135, 135, 135, 135, 17280, 4320, 4320, 17280, 4320, 135, 17280, 17280, 17280, 4320, 4320, 135, 135, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 135, 135, 17280, 4320, 17280, 17280, 135, 17280, 135, 4320, 135, 135, 4320, 4320, 17280, 17280, 17280, 135, 17280, 4320, 4320, 135, 135, 135, 135, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 135, 4320, 4320, 17280, 4320, 17280, 135, 135, 135, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 135, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 135, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1169235 . Total input tokens: 260542665 . Total output tokens: 233746287
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.009942965116352,
    "estimated_duration": 3600.033987177628,
    "input_throughput": 4882.696680811279,
    "output_throughput": 4324.691393318174,
    "total_throughput": 9207.388074129454,
    "itl": 163.0731821614445,
    "ttft": 2020482.310302275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 371,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.244432811848822,
    "arrivals": 390109,
    "finished_requests": 71215,
    "scheduler_time": 65.87716773146245
}
#Debug simulation 
Total elapsed time: 5.010056689847261. Arrivals time: 0.21994259022176266 Scheduler time: 4.681475680787116 Scheduler overhead time: 0.033139406237751245 Adapter cache time: 0.025938313454389572 Engine time: 0.03397825779393315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.159247973933816,
    "estimated_duration": 3600.0251447002224,
    "input_throughput": 5105.678783121546,
    "output_throughput": 4497.446642514557,
    "total_throughput": 9603.125425636104,
    "itl": 190.9452265427864,
    "ttft": 1993123.3118619388,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8018477889103804,
    "arrivals": 388923,
    "finished_requests": 74081,
    "scheduler_time": 68.18242942939293
}
#Debug simulation 
Total elapsed time: 5.15933902002871. Arrivals time: 0.2334690261632204 Scheduler time: 4.837504606228322 Scheduler overhead time: 0.02890654280781746 Adapter cache time: 0.01655424851924181 Engine time: 0.029431861359626055 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.15334547823295,
    "estimated_duration": 3600.13000249964,
    "input_throughput": 5105.530074535645,
    "output_throughput": 4497.315649367753,
    "total_throughput": 9602.8457239034,
    "itl": 190.94651472207627,
    "ttft": 1993151.3966438663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8516285954439119,
    "arrivals": 388923,
    "finished_requests": 74081,
    "scheduler_time": 68.18343849402409
}
#Debug simulation 
Total elapsed time: 5.153431557118893. Arrivals time: 0.23055807733908296 Scheduler time: 4.83416226413101 Scheduler overhead time: 0.028789534233510494 Adapter cache time: 0.016703789588063955 Engine time: 0.029663320630788803 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.078862154390663,
    "estimated_duration": 3600.053504534248,
    "input_throughput": 4950.162262187141,
    "output_throughput": 4368.992288639573,
    "total_throughput": 9319.154550826714,
    "itl": 160.63370117791396,
    "ttft": 2015436.532064384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8310686480440239,
    "arrivals": 388923,
    "finished_requests": 71852,
    "scheduler_time": 66.5209954150876
}
#Debug simulation 
Total elapsed time: 5.078951918054372. Arrivals time: 0.22591917915269732 Scheduler time: 4.746410478837788 Scheduler overhead time: 0.03384218830615282 Adapter cache time: 0.022540591657161713 Engine time: 0.034415085799992085 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.20023489324376,
    "estimated_duration": 3600.155660644111,
    "input_throughput": 5105.238698682173,
    "output_throughput": 4496.880836842346,
    "total_throughput": 9602.11953552452,
    "itl": 190.96174384193418,
    "ttft": 1993186.3033035982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8213925418793251,
    "arrivals": 388923,
    "finished_requests": 74076,
    "scheduler_time": 68.17881521576453
}
#Debug simulation 
Total elapsed time: 5.200322644319385. Arrivals time: 0.24840514967218041 Scheduler time: 4.862901563756168 Scheduler overhead time: 0.029186605010181665 Adapter cache time: 0.016403116285800934 Engine time: 0.0298064723610878 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.31599256535992,
    "estimated_duration": 3600.143933572766,
    "input_throughput": 4949.824320583604,
    "output_throughput": 4368.651445663682,
    "total_throughput": 9318.475766247286,
    "itl": 160.63540061929714,
    "ttft": 2015577.213727457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8401229206658939,
    "arrivals": 388923,
    "finished_requests": 71851,
    "scheduler_time": 66.5222993861555
}
#Debug simulation 
Total elapsed time: 5.316086633130908. Arrivals time: 0.4933804329484701 Scheduler time: 4.715111450757831 Scheduler overhead time: 0.033767388202250004 Adapter cache time: 0.023813441395759583 Engine time: 0.034203181974589825 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.161500609945506,
    "estimated_duration": 3600.1889951564417,
    "input_throughput": 5105.473358406644,
    "output_throughput": 4497.320007861541,
    "total_throughput": 9602.793366268184,
    "itl": 190.94370762811096,
    "ttft": 1993139.8804761264,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 262,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7833931772643711,
    "arrivals": 388923,
    "finished_requests": 74083,
    "scheduler_time": 68.18543962519695
}
#Debug simulation 
Total elapsed time: 5.161591554991901. Arrivals time: 0.24777742428705096 Scheduler time: 4.825137326028198 Scheduler overhead time: 0.028916731011122465 Adapter cache time: 0.016592190135270357 Engine time: 0.02960787108168006 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 4320, 66, 66, 66, 17280, 66, 4320, 17280, 4320, 66, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 66, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 66, 4320, 4320, 4320, 17280, 66, 66, 17280, 66, 17280, 66, 66, 4320, 66, 4320, 17280, 66, 66, 4320, 66, 66, 66, 66, 66, 17280, 4320, 4320, 17280, 4320, 66, 17280, 17280, 17280, 4320, 4320, 66, 66, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 66, 66, 17280, 4320, 17280, 17280, 66, 17280, 66, 4320, 66, 66, 4320, 4320, 17280, 17280, 17280, 66, 17280, 4320, 4320, 66, 66, 66, 66, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 66, 4320, 4320, 17280, 4320, 17280, 66, 66, 66, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 66, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 66, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1165578 . Total input tokens: 259741186 . Total output tokens: 233022663
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.100977865047753,
    "estimated_duration": 3600.1045808300173,
    "input_throughput": 4949.878427112669,
    "output_throughput": 4368.69919939212,
    "total_throughput": 9318.577626504788,
    "itl": 160.63221110189968,
    "ttft": 2015566.5184222485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 255,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8515665152296461,
    "arrivals": 388923,
    "finished_requests": 71851,
    "scheduler_time": 66.52113659548044
}
#Debug simulation 
Total elapsed time: 5.101070510223508. Arrivals time: 0.24464085139334202 Scheduler time: 4.749463267158717 Scheduler overhead time: 0.033996574115008116 Adapter cache time: 0.02263863617554307 Engine time: 0.034442925825715065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.229024721309543,
    "estimated_duration": 3600.0140469035396,
    "input_throughput": 5129.30331921418,
    "output_throughput": 4540.243673231966,
    "total_throughput": 9669.546992446147,
    "itl": 189.74342959058583,
    "ttft": 1987832.5341346571,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5447668184200295,
    "arrivals": 388315,
    "finished_requests": 74674,
    "scheduler_time": 68.77885206535744
}
#Debug simulation 
Total elapsed time: 5.229115880094469. Arrivals time: 0.25426082452759147 Scheduler time: 4.887215217575431 Scheduler overhead time: 0.02911414112895727 Adapter cache time: 0.014935638755559921 Engine time: 0.029914972838014364 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.213536072988063,
    "estimated_duration": 3600.0105615971074,
    "input_throughput": 5129.308285086792,
    "output_throughput": 4540.2480688136475,
    "total_throughput": 9669.55635390044,
    "itl": 189.74584078517998,
    "ttft": 1987835.5105103492,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5800346047501088,
    "arrivals": 388315,
    "finished_requests": 74674,
    "scheduler_time": 68.7786377514066
}
#Debug simulation 
Total elapsed time: 5.213629617821425. Arrivals time: 0.23419737862423062 Scheduler time: 4.891629398800433 Scheduler overhead time: 0.029104412999004126 Adapter cache time: 0.015050888061523438 Engine time: 0.02990675251930952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.06978961173445,
    "estimated_duration": 3600.1401319074694,
    "input_throughput": 4964.812575373218,
    "output_throughput": 4399.156260511655,
    "total_throughput": 9363.968835884873,
    "itl": 159.7902977824701,
    "ttft": 2012609.708850297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5751172742433868,
    "arrivals": 388315,
    "finished_requests": 72268,
    "scheduler_time": 66.99876372830354
}
#Debug simulation 
Total elapsed time: 5.069878892041743. Arrivals time: 0.22474197298288345 Scheduler time: 4.739317583851516 Scheduler overhead time: 0.0339195947162807 Adapter cache time: 0.021640644874423742 Engine time: 0.034337753895670176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.247467677574605,
    "estimated_duration": 3600.085316926224,
    "input_throughput": 5129.2017756307,
    "output_throughput": 4540.153791120543,
    "total_throughput": 9669.355566751243,
    "itl": 189.74460530149378,
    "ttft": 1987871.3674786587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5563360762805685,
    "arrivals": 388315,
    "finished_requests": 74674,
    "scheduler_time": 68.78000041476994
}
#Debug simulation 
Total elapsed time: 5.247573415748775. Arrivals time: 0.2460450162179768 Scheduler time: 4.913707650266588 Scheduler overhead time: 0.029156597796827555 Adapter cache time: 0.015076788142323494 Engine time: 0.029773803427815437 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.110456156078726,
    "estimated_duration": 3600.0768254427544,
    "input_throughput": 4964.806271266672,
    "output_throughput": 4399.046678136459,
    "total_throughput": 9363.852949403132,
    "itl": 159.79313314991802,
    "ttft": 2012557.1965727417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5824109938554481,
    "arrivals": 388315,
    "finished_requests": 72266,
    "scheduler_time": 66.9971728663094
}
#Debug simulation 
Total elapsed time: 5.110574951861054. Arrivals time: 0.231065911706537 Scheduler time: 4.773389062844217 Scheduler overhead time: 0.033912829123437405 Adapter cache time: 0.02171127265319228 Engine time: 0.03453566925600171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.204422750975937,
    "estimated_duration": 3600.192029666894,
    "input_throughput": 5129.26778567103,
    "output_throughput": 4540.174764375654,
    "total_throughput": 9669.442550046684,
    "itl": 189.74284511164242,
    "ttft": 1987858.5928225701,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.532228952492588,
    "arrivals": 388315,
    "finished_requests": 74678,
    "scheduler_time": 68.78257466405442
}
#Debug simulation 
Total elapsed time: 5.20451747905463. Arrivals time: 0.23577084159478545 Scheduler time: 4.881148723885417 Scheduler overhead time: 0.029016155749559402 Adapter cache time: 0.0151604525744915 Engine time: 0.0297100399620831 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 4320, 33, 33, 33, 17280, 33, 4320, 17280, 4320, 33, 4320, 4320, 4320, 4320, 17280, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 33, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 33, 4320, 4320, 4320, 17280, 33, 33, 17280, 33, 17280, 33, 33, 4320, 33, 4320, 17280, 33, 33, 4320, 33, 33, 33, 33, 33, 17280, 4320, 4320, 17280, 4320, 33, 17280, 17280, 17280, 4320, 4320, 33, 33, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 33, 33, 17280, 4320, 17280, 17280, 33, 17280, 33, 4320, 33, 33, 4320, 4320, 17280, 17280, 17280, 33, 17280, 4320, 4320, 33, 33, 33, 33, 17280, 4320, 17280, 4320, 17280, 17280, 17280, 33, 4320, 4320, 17280, 4320, 17280, 33, 33, 33, 4320, 4320, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 33, 17280, 17280, 17280, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 17280, 33, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1163829 . Total input tokens: 259366894 . Total output tokens: 232670004
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.114453040994704,
    "estimated_duration": 3600.093636734737,
    "input_throughput": 4964.8019755984415,
    "output_throughput": 4399.138633061873,
    "total_throughput": 9363.940608660314,
    "itl": 159.7860148008131,
    "ttft": 2012538.9195534925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5898304672539243,
    "arrivals": 388315,
    "finished_requests": 72267,
    "scheduler_time": 66.99708327697314
}
#Debug simulation 
Total elapsed time: 5.114563504699618. Arrivals time: 0.238413967192173 Scheduler time: 4.770031037740409 Scheduler overhead time: 0.03393304208293557 Adapter cache time: 0.02169888000935316 Engine time: 0.0345986457541585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.17780197924003,
    "estimated_duration": 3600.0276399654167,
    "input_throughput": 5078.908505317933,
    "output_throughput": 4487.942209286817,
    "total_throughput": 9566.85071460475,
    "itl": 191.48211839414463,
    "ttft": 1958807.5135428673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.242615505356953,
    "arrivals": 339810,
    "finished_requests": 73836,
    "scheduler_time": 67.94455348477302
}
#Debug simulation 
Total elapsed time: 5.177917100023478. Arrivals time: 0.23051148792728782 Scheduler time: 4.829025391489267 Scheduler overhead time: 0.028946934267878532 Adapter cache time: 0.04637018544599414 Engine time: 0.029537816531956196 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.176636086776853,
    "estimated_duration": 3600.0146070314963,
    "input_throughput": 5092.611003352966,
    "output_throughput": 4499.019245190051,
    "total_throughput": 9591.630248543017,
    "itl": 190.98407883681145,
    "ttft": 1957660.0142423662,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1712,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.574958480251817,
    "arrivals": 339810,
    "finished_requests": 74027,
    "scheduler_time": 68.12031944319872
}
#Debug simulation 
Total elapsed time: 5.176718384027481. Arrivals time: 0.22758831921964884 Scheduler time: 4.829685973934829 Scheduler overhead time: 0.029086625669151545 Adapter cache time: 0.046649363823235035 Engine time: 0.030086512211710215 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.198945606127381,
    "estimated_duration": 3600.0942804025435,
    "input_throughput": 5048.844998017002,
    "output_throughput": 4471.438730821253,
    "total_throughput": 9520.283728838254,
    "itl": 156.55127592704756,
    "ttft": 1965007.2332512045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1703,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.558806308358851,
    "arrivals": 339810,
    "finished_requests": 73434,
    "scheduler_time": 68.04079847639665
}
#Debug simulation 
Total elapsed time: 5.199036084115505. Arrivals time: 0.2250112658366561 Scheduler time: 4.833748982753605 Scheduler overhead time: 0.03468868229538202 Adapter cache time: 0.053782185073941946 Engine time: 0.03554362105205655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.4059775630012155,
    "estimated_duration": 3600.0267573375263,
    "input_throughput": 5092.837147010414,
    "output_throughput": 4499.331558296348,
    "total_throughput": 9592.168705306762,
    "itl": 190.97005800929378,
    "ttft": 1957581.3616237664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1713,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.367521941549941,
    "arrivals": 339810,
    "finished_requests": 74033,
    "scheduler_time": 68.12453622710434
}
#Debug simulation 
Total elapsed time: 5.406039523892105. Arrivals time: 0.4799519828520715 Scheduler time: 4.80732381856069 Scheduler overhead time: 0.02890610881149769 Adapter cache time: 0.04675599792972207 Engine time: 0.02961565414443612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.249508740846068,
    "estimated_duration": 3600.1482547052697,
    "input_throughput": 5048.822913401948,
    "output_throughput": 4471.48280045425,
    "total_throughput": 9520.305713856198,
    "itl": 156.54262047884697,
    "ttft": 1965054.3493797493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1698,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.607717432994324,
    "arrivals": 339810,
    "finished_requests": 73437,
    "scheduler_time": 68.04160887136472
}
#Debug simulation 
Total elapsed time: 5.249598077032715. Arrivals time: 0.22580560389906168 Scheduler time: 4.881751981098205 Scheduler overhead time: 0.03513750620186329 Adapter cache time: 0.05402751732617617 Engine time: 0.03634554939344525 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.176916137803346,
    "estimated_duration": 3600.1379503123617,
    "input_throughput": 5078.907878631024,
    "output_throughput": 4488.0577419535,
    "total_throughput": 9566.965620584524,
    "itl": 191.47865885071582,
    "ttft": 1958786.7949003386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1715,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.127936255757086,
    "arrivals": 339810,
    "finished_requests": 73840,
    "scheduler_time": 67.9484703921626
}
#Debug simulation 
Total elapsed time: 5.177003431133926. Arrivals time: 0.22867517918348312 Scheduler time: 4.82960998499766 Scheduler overhead time: 0.0288745304569602 Adapter cache time: 0.04651554487645626 Engine time: 0.029745439998805523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [53 53 54]
Adapter prompts. [540, 17280, 540, 540, 1080, 540, 540, 540, 17280, 540, 1080, 17280, 1080, 540, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 540, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 540, 1080, 1080, 1080, 17280, 540, 540, 17280, 540, 17280, 540, 540, 1080, 540, 1080, 17280, 540, 540, 1080, 540, 540, 540, 540, 540, 17280, 1080, 1080, 17280, 1080, 540, 17280, 17280, 17280, 1080, 1080, 540, 540, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 540, 540, 17280, 1080, 17280, 17280, 540, 17280, 540, 1080, 540, 540, 1080, 1080, 17280, 17280, 17280, 540, 17280, 1080, 1080, 540, 540, 540, 540, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 540, 1080, 1080, 17280, 1080, 17280, 540, 540, 540, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 540, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 540, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1018980 . Total input tokens: 227190681 . Total output tokens: 203611993
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.433039180934429,
    "estimated_duration": 3600.112846681472,
    "input_throughput": 5048.583690023457,
    "output_throughput": 4471.2676200797505,
    "total_throughput": 9519.851310103208,
    "itl": 156.55772675441833,
    "ttft": 1965078.3538630172,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1701,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.692345987446498,
    "arrivals": 339810,
    "finished_requests": 73431,
    "scheduler_time": 68.03856050833782
}
#Debug simulation 
Total elapsed time: 5.433138869237155. Arrivals time: 0.23084585135802627 Scheduler time: 5.06292715575546 Scheduler overhead time: 0.03457711962983012 Adapter cache time: 0.05329003930091858 Engine time: 0.035264893900603056 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.40904502896592,
    "estimated_duration": 3600.141161153067,
    "input_throughput": 5253.126517389888,
    "output_throughput": 4677.782133022307,
    "total_throughput": 9930.908650412195,
    "itl": 184.45982647478272,
    "ttft": 1924540.414793417,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1051,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2165726188734,
    "arrivals": 335110,
    "finished_requests": 76813,
    "scheduler_time": 70.83399085635995
}
#Debug simulation 
Total elapsed time: 5.409140435047448. Arrivals time: 0.23534225253388286 Scheduler time: 5.058253995608538 Scheduler overhead time: 0.030006769578903913 Adapter cache time: 0.040611517149955034 Engine time: 0.03095800057053566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.4208944006823,
    "estimated_duration": 3600.1594173066946,
    "input_throughput": 5253.091268427268,
    "output_throughput": 4677.672860553048,
    "total_throughput": 9930.764128980316,
    "itl": 184.47029369329752,
    "ttft": 1924575.4369018667,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.417676338439345,
    "arrivals": 335110,
    "finished_requests": 76810,
    "scheduler_time": 70.83051852119496
}
#Debug simulation 
Total elapsed time: 5.420992976985872. Arrivals time: 0.24125319300219417 Scheduler time: 5.063045023940504 Scheduler overhead time: 0.03007882973179221 Adapter cache time: 0.04124192101880908 Engine time: 0.031254592817276716 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.443096467293799,
    "estimated_duration": 3600.0375718205455,
    "input_throughput": 5198.052972135151,
    "output_throughput": 4634.37718833665,
    "total_throughput": 9832.430160471802,
    "itl": 151.60468142844272,
    "ttft": 1934390.1502117466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1030,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.363140968028417,
    "arrivals": 335110,
    "finished_requests": 76004,
    "scheduler_time": 70.49182158013586
}
#Debug simulation 
Total elapsed time: 5.4432176114059985. Arrivals time: 0.24327808385714889 Scheduler time: 5.062121965456754 Scheduler overhead time: 0.03599446453154087 Adapter cache time: 0.04817197658121586 Engine time: 0.03679045336320996 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.415990082081407,
    "estimated_duration": 3600.027449691038,
    "input_throughput": 5253.283833050514,
    "output_throughput": 4677.844331838435,
    "total_throughput": 9931.128164888949,
    "itl": 184.46514784782892,
    "ttft": 1924519.1187716702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1049,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2869258365383973,
    "arrivals": 335110,
    "finished_requests": 76810,
    "scheduler_time": 70.83033997848001
}
#Debug simulation 
Total elapsed time: 5.416089219972491. Arrivals time: 0.2485646135173738 Scheduler time: 5.051039850804955 Scheduler overhead time: 0.02996984962373972 Adapter cache time: 0.0414168038405478 Engine time: 0.030970086343586445 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.522942184936255,
    "estimated_duration": 3600.0505227766357,
    "input_throughput": 5198.213158843798,
    "output_throughput": 4634.424682221374,
    "total_throughput": 9832.63784106517,
    "itl": 151.6093585295218,
    "ttft": 1934374.7959244922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1031,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.406008423306046,
    "arrivals": 335110,
    "finished_requests": 76005,
    "scheduler_time": 70.49120928892295
}
#Debug simulation 
Total elapsed time: 5.52304724091664. Arrivals time: 0.25587270967662334 Scheduler time: 5.125517059583217 Scheduler overhead time: 0.03612185921519995 Adapter cache time: 0.05116880824789405 Engine time: 0.037278573494404554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.418161130044609,
    "estimated_duration": 3600.0153073151873,
    "input_throughput": 5253.3101627571,
    "output_throughput": 4677.945664780911,
    "total_throughput": 9931.255827538012,
    "itl": 184.45604476111058,
    "ttft": 1924525.3042612514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1053,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.148522960531916,
    "arrivals": 335110,
    "finished_requests": 76813,
    "scheduler_time": 70.83238868583636
}
#Debug simulation 
Total elapsed time: 5.418276721145958. Arrivals time: 0.24868038017302752 Scheduler time: 5.053330891765654 Scheduler overhead time: 0.02996948827058077 Adapter cache time: 0.041474960278719664 Engine time: 0.030752602498978376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 1080, 270, 270, 270, 17280, 270, 1080, 17280, 1080, 270, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 270, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 270, 1080, 1080, 1080, 17280, 270, 270, 17280, 270, 17280, 270, 270, 1080, 270, 1080, 17280, 270, 270, 1080, 270, 270, 270, 270, 270, 17280, 1080, 1080, 17280, 1080, 270, 17280, 17280, 17280, 1080, 1080, 270, 270, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 270, 270, 17280, 1080, 17280, 17280, 270, 17280, 270, 1080, 270, 270, 1080, 1080, 17280, 17280, 17280, 270, 17280, 1080, 1080, 270, 270, 270, 270, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 270, 1080, 1080, 17280, 1080, 17280, 270, 270, 270, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 270, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 270, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1004670 . Total input tokens: 223978595 . Total output tokens: 200765999
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.4229268240742385,
    "estimated_duration": 3600.0282270776943,
    "input_throughput": 5198.050909503644,
    "output_throughput": 4634.388106879678,
    "total_throughput": 9832.439016383323,
    "itl": 151.61039005598164,
    "ttft": 1934353.8717163028,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1030,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4487392827123844,
    "arrivals": 335110,
    "finished_requests": 76002,
    "scheduler_time": 70.49055865496182
}
#Debug simulation 
Total elapsed time: 5.423017076216638. Arrivals time: 0.23299538111314178 Scheduler time: 5.052653639111668 Scheduler overhead time: 0.03595246374607086 Adapter cache time: 0.04805769119411707 Engine time: 0.036568230483680964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.5061808419413865,
    "estimated_duration": 3600.1531491130504,
    "input_throughput": 5456.799248898594,
    "output_throughput": 4792.641669771974,
    "total_throughput": 10249.440918670567,
    "itl": 178.9592756709786,
    "ttft": 1906509.0432051085,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7230545998341642,
    "arrivals": 332795,
    "finished_requests": 78937,
    "scheduler_time": 72.49637150322553
}
#Debug simulation 
Total elapsed time: 5.506284588947892. Arrivals time: 0.24661815306171775 Scheduler time: 5.146592579782009 Scheduler overhead time: 0.030630096327513456 Adapter cache time: 0.036254010163247585 Engine time: 0.031803368125110865 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.746117124799639,
    "estimated_duration": 3600.0592844246653,
    "input_throughput": 5456.710417239541,
    "output_throughput": 4792.72662943311,
    "total_throughput": 10249.43704667265,
    "itl": 178.9718478254868,
    "ttft": 1906459.0134666304,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 564,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8355256575555579,
    "arrivals": 332795,
    "finished_requests": 78931,
    "scheduler_time": 72.49227691610828
}
#Debug simulation 
Total elapsed time: 5.746183373965323. Arrivals time: 0.48808164754882455 Scheduler time: 5.145045976154506 Scheduler overhead time: 0.030538895167410374 Adapter cache time: 0.03599342051893473 Engine time: 0.032047797460108995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.510380621999502,
    "estimated_duration": 3600.044016495975,
    "input_throughput": 5374.5690084180205,
    "output_throughput": 4731.776589937437,
    "total_throughput": 10106.345598355458,
    "itl": 148.4267076164468,
    "ttft": 1919486.2219008915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8193553681485466,
    "arrivals": 332795,
    "finished_requests": 77778,
    "scheduler_time": 71.92498551288608
}
#Debug simulation 
Total elapsed time: 5.510467481333762. Arrivals time: 0.23982963990420103 Scheduler time: 5.136414702516049 Scheduler overhead time: 0.036498415283858776 Adapter cache time: 0.04299927456304431 Engine time: 0.037510641384869814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.5113529451191425,
    "estimated_duration": 3600.0031994120363,
    "input_throughput": 5456.795428184174,
    "output_throughput": 4792.801296070513,
    "total_throughput": 10249.596724254687,
    "itl": 178.96569072279254,
    "ttft": 1906447.0649790238,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 562,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7609015434677617,
    "arrivals": 332795,
    "finished_requests": 78931,
    "scheduler_time": 72.49256372791683
}
#Debug simulation 
Total elapsed time: 5.5114660412073135. Arrivals time: 0.23844568198546767 Scheduler time: 5.159435268491507 Scheduler overhead time: 0.03098512813448906 Adapter cache time: 0.03605107357725501 Engine time: 0.032017657533288 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.718663301784545,
    "estimated_duration": 3600.0828363688674,
    "input_throughput": 5374.637162381525,
    "output_throughput": 4732.062781417204,
    "total_throughput": 10106.699943798729,
    "itl": 148.42097503779968,
    "ttft": 1919492.57964265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 558,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8396017277613328,
    "arrivals": 332795,
    "finished_requests": 77781,
    "scheduler_time": 71.92598514750226
}
#Debug simulation 
Total elapsed time: 5.718727294821292. Arrivals time: 0.4870568956248462 Scheduler time: 5.0982119627296925 Scheduler overhead time: 0.036397406831383705 Adapter cache time: 0.0426160441711545 Engine time: 0.03727655950933695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.476954984012991,
    "estimated_duration": 3600.11649771565,
    "input_throughput": 5456.854802466911,
    "output_throughput": 4792.690461808162,
    "total_throughput": 10249.545264275073,
    "itl": 178.9582017692157,
    "ttft": 1906495.0585296014,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 563,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.683398316029902,
    "arrivals": 332795,
    "finished_requests": 78937,
    "scheduler_time": 72.49619486694075
}
#Debug simulation 
Total elapsed time: 5.477041510399431. Arrivals time: 0.23828883236274123 Scheduler time: 5.125511224381626 Scheduler overhead time: 0.030698906164616346 Adapter cache time: 0.03625218151137233 Engine time: 0.031834032852202654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 1080, 135, 135, 135, 17280, 135, 1080, 17280, 1080, 135, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 135, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 135, 1080, 1080, 1080, 17280, 135, 135, 17280, 135, 17280, 135, 135, 1080, 135, 1080, 17280, 135, 135, 1080, 135, 135, 135, 135, 135, 17280, 1080, 1080, 17280, 1080, 135, 17280, 17280, 17280, 1080, 1080, 135, 135, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 135, 135, 17280, 1080, 17280, 17280, 135, 17280, 135, 1080, 135, 135, 1080, 1080, 17280, 17280, 17280, 135, 17280, 1080, 1080, 135, 135, 135, 135, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 135, 1080, 1080, 17280, 1080, 17280, 135, 135, 135, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 135, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 135, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 997515 . Total input tokens: 222378898 . Total output tokens: 199330563
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.5065841088071465,
    "estimated_duration": 3600.0001465011615,
    "input_throughput": 5374.6475590577475,
    "output_throughput": 4732.0381407641435,
    "total_throughput": 10106.68569982189,
    "itl": 148.42310883427533,
    "ttft": 1919457.9743895775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 557,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8614574586227508,
    "arrivals": 332795,
    "finished_requests": 77779,
    "scheduler_time": 71.92433093422665
}
#Debug simulation 
Total elapsed time: 5.506669037044048. Arrivals time: 0.23289216263219714 Scheduler time: 5.139902575407177 Scheduler overhead time: 0.03644534572958946 Adapter cache time: 0.04256965685635805 Engine time: 0.037681118585169315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.585477862972766,
    "estimated_duration": 3600.0087763505785,
    "input_throughput": 5554.022848874543,
    "output_throughput": 4880.394491096385,
    "total_throughput": 10434.417339970929,
    "itl": 175.5352946812664,
    "ttft": 1884472.1929340484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9365092496434214,
    "arrivals": 331579,
    "finished_requests": 80611,
    "scheduler_time": 73.8638179174661
}
#Debug simulation 
Total elapsed time: 5.585566194262356. Arrivals time: 0.24255092721432447 Scheduler time: 5.233587801456451 Scheduler overhead time: 0.031087514013051987 Adapter cache time: 0.031690677627921104 Engine time: 0.03207329008728266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.6504553919658065,
    "estimated_duration": 3600.006102577499,
    "input_throughput": 5554.026696144904,
    "output_throughput": 4880.351449243627,
    "total_throughput": 10434.378145388531,
    "itl": 175.53798653222128,
    "ttft": 1884492.0092003064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9942228820174984,
    "arrivals": 331579,
    "finished_requests": 80610,
    "scheduler_time": 73.86235054046799
}
#Debug simulation 
Total elapsed time: 5.650562078226358. Arrivals time: 0.24970260495319963 Scheduler time: 5.2900111195631325 Scheduler overhead time: 0.03157339198514819 Adapter cache time: 0.03202668111771345 Engine time: 0.032469692174345255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.591230121906847,
    "estimated_duration": 3600.0195081593965,
    "input_throughput": 5459.850691211787,
    "output_throughput": 4805.040906248915,
    "total_throughput": 10264.891597460703,
    "itl": 146.19543658010699,
    "ttft": 1899004.376512399,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9903184964694134,
    "arrivals": 331579,
    "finished_requests": 79240,
    "scheduler_time": 73.05143234163084
}
#Debug simulation 
Total elapsed time: 5.591343901120126. Arrivals time: 0.242537391372025 Scheduler time: 5.218109410721809 Scheduler overhead time: 0.036984740756452084 Adapter cache time: 0.03834011312574148 Engine time: 0.0377506441436708 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.856433346867561,
    "estimated_duration": 3600.0522171595076,
    "input_throughput": 5553.955830056257,
    "output_throughput": 4880.3356007604125,
    "total_throughput": 10434.29143081667,
    "itl": 175.5366065778013,
    "ttft": 1884491.9585178155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.957040708039425,
    "arrivals": 331579,
    "finished_requests": 80611,
    "scheduler_time": 73.86404977113682
}
#Debug simulation 
Total elapsed time: 5.856501055881381. Arrivals time: 0.2505775815807283 Scheduler time: 5.495449300389737 Scheduler overhead time: 0.03152370918542147 Adapter cache time: 0.031790207140147686 Engine time: 0.032605110201984644 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.627325251698494,
    "estimated_duration": 3600.0224442803983,
    "input_throughput": 5459.814849551276,
    "output_throughput": 4804.949210103508,
    "total_throughput": 10264.764059654784,
    "itl": 146.20044412756116,
    "ttft": 1898977.1476597628,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.001636337246751,
    "arrivals": 331579,
    "finished_requests": 79239,
    "scheduler_time": 73.05121436961477
}
#Debug simulation 
Total elapsed time: 5.627417951822281. Arrivals time: 0.24575100699439645 Scheduler time: 5.250228697899729 Scheduler overhead time: 0.03712280746549368 Adapter cache time: 0.038588861003518105 Engine time: 0.03815224440768361 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.6175579321570694,
    "estimated_duration": 3600.119845744851,
    "input_throughput": 5554.081490824103,
    "output_throughput": 4880.323920540175,
    "total_throughput": 10434.405411364278,
    "itl": 175.53270875062248,
    "ttft": 1884454.2053335921,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 306,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.914955390240067,
    "arrivals": 331579,
    "finished_requests": 80614,
    "scheduler_time": 73.86664562450733
}
#Debug simulation 
Total elapsed time: 5.617661020252854. Arrivals time: 0.26798224914819 Scheduler time: 5.239346749149263 Scheduler overhead time: 0.03138125315308571 Adapter cache time: 0.03188046161085367 Engine time: 0.0323513844050467 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 1080, 66, 66, 66, 17280, 66, 1080, 17280, 1080, 66, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 66, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 66, 1080, 1080, 1080, 17280, 66, 66, 17280, 66, 17280, 66, 66, 1080, 66, 1080, 17280, 66, 66, 1080, 66, 66, 66, 66, 66, 17280, 1080, 1080, 17280, 1080, 66, 17280, 17280, 17280, 1080, 1080, 66, 66, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 66, 66, 17280, 1080, 17280, 17280, 66, 17280, 66, 1080, 66, 66, 1080, 1080, 17280, 17280, 17280, 66, 17280, 1080, 1080, 66, 66, 66, 66, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 66, 1080, 1080, 17280, 1080, 17280, 66, 66, 66, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 66, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 66, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 993858 . Total input tokens: 221564047 . Total output tokens: 198603084
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.620718611869961,
    "estimated_duration": 3600.1525358391646,
    "input_throughput": 5459.722832387821,
    "output_throughput": 4804.821692358644,
    "total_throughput": 10264.544524746465,
    "itl": 146.19858245533268,
    "ttft": 1899015.9611923925,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0145889772474816,
    "arrivals": 331579,
    "finished_requests": 79241,
    "scheduler_time": 73.05362738533597
}
#Debug simulation 
Total elapsed time: 5.620808430016041. Arrivals time: 0.2669222829863429 Scheduler time: 5.2221258115023375 Scheduler overhead time: 0.037379291374236345 Adapter cache time: 0.038648671470582485 Engine time: 0.0382612245157361 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.666807429865003,
    "estimated_duration": 3600.00252987236,
    "input_throughput": 5583.979687012259,
    "output_throughput": 4918.4823769076065,
    "total_throughput": 10502.462063919866,
    "itl": 174.2884318698065,
    "ttft": 1882855.5059206511,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6090370610426172,
    "arrivals": 330904,
    "finished_requests": 80812,
    "scheduler_time": 74.43053138561459
}
#Debug simulation 
Total elapsed time: 5.6669011930935085. Arrivals time: 0.2413288690149784 Scheduler time: 5.316040811128914 Scheduler overhead time: 0.03140046587213874 Adapter cache time: 0.030810100957751274 Engine time: 0.032512225210666656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.662111448124051,
    "estimated_duration": 3600.1853529444365,
    "input_throughput": 5583.941944421955,
    "output_throughput": 4918.38954500443,
    "total_throughput": 10502.331489426386,
    "itl": 174.2902583457483,
    "ttft": 1882886.6943658616,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6497717813565412,
    "arrivals": 330904,
    "finished_requests": 80816,
    "scheduler_time": 74.43443011731948
}
#Debug simulation 
Total elapsed time: 5.662199502810836. Arrivals time: 0.2725749481469393 Scheduler time: 5.2800389882177114 Scheduler overhead time: 0.031298542860895395 Adapter cache time: 0.03084414452314377 Engine time: 0.03261970728635788 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.628931088373065,
    "estimated_duration": 3600.069282550782,
    "input_throughput": 5478.466510518271,
    "output_throughput": 4833.935025736417,
    "total_throughput": 10312.401536254689,
    "itl": 144.40812087625432,
    "ttft": 1899539.317474618,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6447643774747875,
    "arrivals": 330904,
    "finished_requests": 79310,
    "scheduler_time": 73.48156647191782
}
#Debug simulation 
Total elapsed time: 5.629045071080327. Arrivals time: 0.26436225697398186 Scheduler time: 5.233715430833399 Scheduler overhead time: 0.037364810705184937 Adapter cache time: 0.03773728059604764 Engine time: 0.038248022086918354 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.617151541169733,
    "estimated_duration": 3600.0251976228974,
    "input_throughput": 5583.944527186534,
    "output_throughput": 4918.451407421166,
    "total_throughput": 10502.3959346077,
    "itl": 174.28629339311132,
    "ttft": 1882855.4945816514,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6244388716132393,
    "arrivals": 330904,
    "finished_requests": 80812,
    "scheduler_time": 74.43190771200065
}
#Debug simulation 
Total elapsed time: 5.617240610066801. Arrivals time: 0.26710043800994754 Scheduler time: 5.240977622568607 Scheduler overhead time: 0.03134326357394457 Adapter cache time: 0.030662203207612038 Engine time: 0.03236100543290377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.624695582780987,
    "estimated_duration": 3600.0054738437298,
    "input_throughput": 5478.7663916913725,
    "output_throughput": 4834.133482974651,
    "total_throughput": 10312.899874666024,
    "itl": 144.41346370934863,
    "ttft": 1899482.636470942,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6525611122325085,
    "arrivals": 330904,
    "finished_requests": 79310,
    "scheduler_time": 73.47999465967922
}
#Debug simulation 
Total elapsed time: 5.624783954583108. Arrivals time: 0.24143687775358558 Scheduler time: 5.25252175796777 Scheduler overhead time: 0.03721755836158991 Adapter cache time: 0.0376440011896193 Engine time: 0.03845593612641096 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.6055012522265315,
    "estimated_duration": 3600.1782632228173,
    "input_throughput": 5583.952940709091,
    "output_throughput": 4918.399230639457,
    "total_throughput": 10502.352171348548,
    "itl": 174.28771994902286,
    "ttft": 1882877.957706126,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 199,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5950200086855337,
    "arrivals": 330904,
    "finished_requests": 80816,
    "scheduler_time": 74.43452327631121
}
#Debug simulation 
Total elapsed time: 5.6056169448420405. Arrivals time: 0.26197746861726046 Scheduler time: 5.234913987573236 Scheduler overhead time: 0.03127453802153468 Adapter cache time: 0.030631140805780888 Engine time: 0.032181310933083296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 1080, 33, 33, 33, 17280, 33, 1080, 17280, 1080, 33, 1080, 1080, 1080, 1080, 17280, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 33, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 33, 1080, 1080, 1080, 17280, 33, 33, 17280, 33, 17280, 33, 33, 1080, 33, 1080, 17280, 33, 33, 1080, 33, 33, 33, 33, 33, 17280, 1080, 1080, 17280, 1080, 33, 17280, 17280, 17280, 1080, 1080, 33, 33, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 33, 33, 17280, 1080, 17280, 17280, 33, 17280, 33, 1080, 33, 33, 1080, 1080, 17280, 17280, 17280, 33, 17280, 1080, 1080, 33, 33, 33, 33, 17280, 1080, 17280, 1080, 17280, 17280, 17280, 33, 1080, 1080, 17280, 1080, 17280, 33, 33, 33, 1080, 1080, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 33, 17280, 17280, 17280, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 17280, 33, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 992109 . Total input tokens: 221179572 . Total output tokens: 198253330
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.590165188070387,
    "estimated_duration": 3600.0795252164144,
    "input_throughput": 5478.69452934219,
    "output_throughput": 4834.070991516177,
    "total_throughput": 10312.765520858367,
    "itl": 144.4151580794153,
    "ttft": 1899543.102225285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.661615384854378,
    "arrivals": 330904,
    "finished_requests": 79312,
    "scheduler_time": 73.48121259027008
}
#Debug simulation 
Total elapsed time: 5.590257075149566. Arrivals time: 0.2639190084300935 Scheduler time: 5.196058151777834 Scheduler overhead time: 0.03728816378861666 Adapter cache time: 0.03732110746204853 Engine time: 0.038086671847850084 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.684320368804038,
    "estimated_duration": 3600.1167172905425,
    "input_throughput": 5621.8899522880665,
    "output_throughput": 4967.926154755445,
    "total_throughput": 10589.816107043513,
    "itl": 173.16557133754418,
    "ttft": 1873461.687244845,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1050,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.213512131129467,
    "arrivals": 325545,
    "finished_requests": 81624,
    "scheduler_time": 75.12194381571862
}
#Debug simulation 
Total elapsed time: 5.68440683465451. Arrivals time: 0.23759094718843699 Scheduler time: 5.329080344643444 Scheduler overhead time: 0.0316510503180325 Adapter cache time: 0.038402602542191744 Engine time: 0.03273802623152733 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.7127442210912704,
    "estimated_duration": 3600.145294076438,
    "input_throughput": 5621.731165489536,
    "output_throughput": 4967.88560990235,
    "total_throughput": 10589.616775391885,
    "itl": 173.1743396776021,
    "ttft": 1873435.2143494894,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1051,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.417936104570982,
    "arrivals": 325545,
    "finished_requests": 81622,
    "scheduler_time": 75.11966938709077
}
#Debug simulation 
Total elapsed time: 5.712842153850943. Arrivals time: 0.2471537790261209 Scheduler time: 5.347329031676054 Scheduler overhead time: 0.03183764033019543 Adapter cache time: 0.03863817499950528 Engine time: 0.03286163043230772 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.698048552963883,
    "estimated_duration": 3600.0259714873305,
    "input_throughput": 5542.801123669678,
    "output_throughput": 4907.638761478356,
    "total_throughput": 10450.439885148035,
    "itl": 142.6305226306358,
    "ttft": 1885739.146881398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1033,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.3699107921123126,
    "arrivals": 325545,
    "finished_requests": 80489,
    "scheduler_time": 74.55743067589083
}
#Debug simulation 
Total elapsed time: 5.6981485690921545. Arrivals time: 0.24701463244855404 Scheduler time: 5.3134857984259725 Scheduler overhead time: 0.037802306935191154 Adapter cache time: 0.04321434814482927 Engine time: 0.03883314039558172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.7108109928667545,
    "estimated_duration": 3600.015337913364,
    "input_throughput": 5621.8185480650445,
    "output_throughput": 4968.014944726996,
    "total_throughput": 10589.83349279204,
    "itl": 173.16998000992555,
    "ttft": 1873415.8477627714,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1050,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2821525757829297,
    "arrivals": 325545,
    "finished_requests": 81621,
    "scheduler_time": 75.11896130141861
}
#Debug simulation 
Total elapsed time: 5.710922006051987. Arrivals time: 0.2583290860056877 Scheduler time: 5.334579945076257 Scheduler overhead time: 0.03172703366726637 Adapter cache time: 0.0386434318497777 Engine time: 0.032669537235051394 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_160_slots_128_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.714615785051137,
    "estimated_duration": 3600.105243016173,
    "input_throughput": 5542.734073874506,
    "output_throughput": 4907.312649893172,
    "total_throughput": 10450.046723767678,
    "itl": 142.63805638501995,
    "ttft": 1885811.6316612698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1034,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.412606767546387,
    "arrivals": 325545,
    "finished_requests": 80488,
    "scheduler_time": 74.55816339316695
}
#Debug simulation 
Total elapsed time: 5.714735276997089. Arrivals time: 0.2537608854472637 Scheduler time: 5.322055580560118 Scheduler overhead time: 0.03830937389284372 Adapter cache time: 0.04371291724964976 Engine time: 0.03894112631678581 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_160_slots_128_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_160_slots_128_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.693509514909238,
    "estimated_duration": 3600.019336238909,
    "input_throughput": 5622.042025236734,
    "output_throughput": 4968.060537887368,
    "total_throughput": 10590.102563124103,
    "itl": 173.1623349242761,
    "ttft": 1873414.6203632602,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1051,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.142542859942112,
    "arrivals": 325545,
    "finished_requests": 81624,
    "scheduler_time": 75.12152388425062
}
#Debug simulation 
Total elapsed time: 5.693608858156949. Arrivals time: 0.26025624666363 Scheduler time: 5.3157580168917775 Scheduler overhead time: 0.03166597010567784 Adapter cache time: 0.03832729114219546 Engine time: 0.032696743961423635 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_160_slots_128_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_160_slots_128_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [53 53 54]
Adapter prompts. [270, 17280, 270, 270, 540, 270, 270, 270, 17280, 270, 540, 17280, 540, 270, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 270, 17280, 540, 17280, 17280, 17280, 540, 17280, 270, 540, 540, 540, 17280, 270, 270, 17280, 270, 17280, 270, 270, 540, 270, 540, 17280, 270, 270, 540, 270, 270, 270, 270, 270, 17280, 540, 540, 17280, 540, 270, 17280, 17280, 17280, 540, 540, 270, 270, 270, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 270, 270, 17280, 540, 17280, 17280, 270, 17280, 270, 540, 270, 270, 540, 540, 17280, 17280, 17280, 270, 17280, 540, 540, 270, 270, 270, 270, 17280, 540, 17280, 540, 17280, 17280, 17280, 270, 540, 540, 17280, 540, 17280, 270, 270, 270, 540, 540, 540, 17280, 17280, 540, 17280, 270, 540, 540, 270, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 270, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 976050 . Total input tokens: 217550544 . Total output tokens: 195044193
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.777948284056038,
    "estimated_duration": 3600.0006830731613,
    "input_throughput": 5542.585337224642,
    "output_throughput": 4907.164624457652,
    "total_throughput": 10449.749961682293,
    "itl": 142.6328324122906,
    "ttft": 1885780.681730991,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1035,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.4607158679888093,
    "arrivals": 325545,
    "finished_requests": 80484,
    "scheduler_time": 74.55503840704735
}
#Debug simulation 
Total elapsed time: 5.778054881840944. Arrivals time: 0.2544792168773711 Scheduler time: 5.385601799469441 Scheduler overhead time: 0.037935114465653896 Adapter cache time: 0.04337815148755908 Engine time: 0.03872978547587991 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.856843352783471,
    "estimated_duration": 3600.0622482846384,
    "input_throughput": 5771.42214968646,
    "output_throughput": 5096.066605165396,
    "total_throughput": 10867.488754851856,
    "itl": 168.64584333753936,
    "ttft": 1847736.1104448594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.860776548311143,
    "arrivals": 323217,
    "finished_requests": 84081,
    "scheduler_time": 77.06573461910318
}
#Debug simulation 
Total elapsed time: 5.8569590649567544. Arrivals time: 0.24688819656148553 Scheduler time: 5.494410069659352 Scheduler overhead time: 0.03260535839945078 Adapter cache time: 0.03405704442411661 Engine time: 0.033658127300441265 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.81505202781409,
    "estimated_duration": 3600.1131872592496,
    "input_throughput": 5771.340488274427,
    "output_throughput": 5095.994499541513,
    "total_throughput": 10867.334987815939,
    "itl": 168.653853312182,
    "ttft": 1847789.4151547463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 606,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9745914154499815,
    "arrivals": 323217,
    "finished_requests": 84081,
    "scheduler_time": 77.06405256448647
}
#Debug simulation 
Total elapsed time: 5.815138325095177. Arrivals time: 0.2477314448915422 Scheduler time: 5.452425747178495 Scheduler overhead time: 0.03242036886513233 Adapter cache time: 0.03385416464880109 Engine time: 0.03348425077274442 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.824476192705333,
    "estimated_duration": 3600.004987791295,
    "input_throughput": 5677.496022731878,
    "output_throughput": 5019.272490254546,
    "total_throughput": 10696.768512986424,
    "itl": 140.13929193315602,
    "ttft": 1861122.0864114703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.987727481722844,
    "arrivals": 323217,
    "finished_requests": 82703,
    "scheduler_time": 76.2422378928638
}
#Debug simulation 
Total elapsed time: 5.824565866030753. Arrivals time: 0.24362200032919645 Scheduler time: 5.445915282703936 Scheduler overhead time: 0.038560766726732254 Adapter cache time: 0.03866115119308233 Engine time: 0.03967134561389685 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.811604817863554,
    "estimated_duration": 3600.1692706112676,
    "input_throughput": 5771.2505824683685,
    "output_throughput": 5095.915114259345,
    "total_throughput": 10867.165696727714,
    "itl": 168.64934778080044,
    "ttft": 1847786.2312340839,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8946366161014763,
    "arrivals": 323217,
    "finished_requests": 84081,
    "scheduler_time": 77.06680457636577
}
#Debug simulation 
Total elapsed time: 5.811716258060187. Arrivals time: 0.26893532695248723 Scheduler time: 5.42696802970022 Scheduler overhead time: 0.032580295111984015 Adapter cache time: 0.03472469933331013 Engine time: 0.03334429021924734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.835148585960269,
    "estimated_duration": 3600.081059799715,
    "input_throughput": 5677.376331392131,
    "output_throughput": 5019.166985369286,
    "total_throughput": 10696.543316761416,
    "itl": 140.1425934648822,
    "ttft": 1861198.229372002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0133812541514664,
    "arrivals": 323217,
    "finished_requests": 82704,
    "scheduler_time": 76.24292410904312
}
#Debug simulation 
Total elapsed time: 5.835234604310244. Arrivals time: 0.24445646349340677 Scheduler time: 5.45632903277874 Scheduler overhead time: 0.03880761656910181 Adapter cache time: 0.03792924387380481 Engine time: 0.03952119126915932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.867219322361052,
    "estimated_duration": 3600.027018847797,
    "input_throughput": 5771.478628138162,
    "output_throughput": 5096.116474667949,
    "total_throughput": 10867.59510280611,
    "itl": 168.64528261346433,
    "ttft": 1847722.2094378462,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 608,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.817950579300495,
    "arrivals": 323217,
    "finished_requests": 84081,
    "scheduler_time": 77.06561722348859
}
#Debug simulation 
Total elapsed time: 5.867308820132166. Arrivals time: 0.2540344591252506 Scheduler time: 5.49667417537421 Scheduler overhead time: 0.032735174521803856 Adapter cache time: 0.03453213721513748 Engine time: 0.03384660370647907 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 540, 135, 135, 135, 17280, 135, 540, 17280, 540, 135, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 135, 17280, 540, 17280, 17280, 17280, 540, 17280, 135, 540, 540, 540, 17280, 135, 135, 17280, 135, 17280, 135, 135, 540, 135, 540, 17280, 135, 135, 540, 135, 135, 135, 135, 135, 17280, 540, 540, 17280, 540, 135, 17280, 17280, 17280, 540, 540, 135, 135, 135, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 135, 135, 17280, 540, 17280, 17280, 135, 17280, 135, 540, 135, 135, 540, 540, 17280, 17280, 17280, 135, 17280, 540, 540, 135, 135, 135, 135, 17280, 540, 17280, 540, 17280, 17280, 17280, 135, 540, 540, 17280, 540, 17280, 135, 135, 135, 540, 540, 540, 17280, 17280, 540, 17280, 135, 540, 540, 135, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 135, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 968895 . Total input tokens: 215948582 . Total output tokens: 193639611
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.78310589119792,
    "estimated_duration": 3600.0051759393064,
    "input_throughput": 5677.462392722012,
    "output_throughput": 5019.185839166314,
    "total_throughput": 10696.648231888325,
    "itl": 140.1451791003636,
    "ttft": 1861143.5699970333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 609,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0377774887159426,
    "arrivals": 323217,
    "finished_requests": 82703,
    "scheduler_time": 76.24170325904375
}
#Debug simulation 
Total elapsed time: 5.783225095830858. Arrivals time: 0.245223558973521 Scheduler time: 5.403878259472549 Scheduler overhead time: 0.03829074837267399 Adapter cache time: 0.038224592339247465 Engine time: 0.03934924164786935 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.9211773900315166,
    "estimated_duration": 3600.0454746398204,
    "input_throughput": 5886.025926414173,
    "output_throughput": 5201.961511853858,
    "total_throughput": 11087.98743826803,
    "itl": 165.46658797670082,
    "ttft": 1834544.6293053487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9854770535463454,
    "arrivals": 321998,
    "finished_requests": 85534,
    "scheduler_time": 78.60973789583623
}
#Debug simulation 
Total elapsed time: 5.921264125965536. Arrivals time: 0.2440563952550292 Scheduler time: 5.563857778906822 Scheduler overhead time: 0.03296418581157923 Adapter cache time: 0.03059886209666729 Engine time: 0.03410793747752905 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.201505265198648,
    "estimated_duration": 3600.1749694620644,
    "input_throughput": 5885.935316962179,
    "output_throughput": 5201.984114343846,
    "total_throughput": 11087.919431306025,
    "itl": 165.46860206231247,
    "ttft": 1834580.948894515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.046149639920337,
    "arrivals": 321998,
    "finished_requests": 85536,
    "scheduler_time": 78.61203503630094
}
#Debug simulation 
Total elapsed time: 6.201568718999624. Arrivals time: 0.25708633242174983 Scheduler time: 5.830136239994317 Scheduler overhead time: 0.03325438406318426 Adapter cache time: 0.03108046529814601 Engine time: 0.034397478215396404 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.914564358070493,
    "estimated_duration": 3600.075664736799,
    "input_throughput": 5771.807854910501,
    "output_throughput": 5108.395131840802,
    "total_throughput": 10880.202986751303,
    "itl": 137.195504902706,
    "ttft": 1852034.9406285605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0322449739649946,
    "arrivals": 321998,
    "finished_requests": 83884,
    "scheduler_time": 77.60857619415995
}
#Debug simulation 
Total elapsed time: 5.914679276756942. Arrivals time: 0.24702697806060314 Scheduler time: 5.535165768116713 Scheduler overhead time: 0.03932627011090517 Adapter cache time: 0.03410845762118697 Engine time: 0.040386687964200974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.978649944067001,
    "estimated_duration": 3600.0884927864745,
    "input_throughput": 5886.055312934449,
    "output_throughput": 5202.030738279068,
    "total_throughput": 11088.086051213517,
    "itl": 165.46762754200608,
    "ttft": 1834564.598775537,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0052901080763,
    "arrivals": 321998,
    "finished_requests": 85535,
    "scheduler_time": 78.61048601794667
}
#Debug simulation 
Total elapsed time: 5.978744960855693. Arrivals time: 0.2529196972027421 Scheduler time: 5.610938917845488 Scheduler overhead time: 0.03325306065380573 Adapter cache time: 0.031145012006163597 Engine time: 0.0348025974817574 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.8571856850758195,
    "estimated_duration": 3600.0486720637537,
    "input_throughput": 5771.851131137158,
    "output_throughput": 5108.433433889507,
    "total_throughput": 10880.284565026664,
    "itl": 137.19390043888458,
    "ttft": 1852001.3833600676,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0445688450336508,
    "arrivals": 321998,
    "finished_requests": 83884,
    "scheduler_time": 77.60793029668697
}
#Debug simulation 
Total elapsed time: 5.857275332324207. Arrivals time: 0.2490517133846879 Scheduler time: 5.476313503924757 Scheduler overhead time: 0.03897667769342661 Adapter cache time: 0.034243364818394184 Engine time: 0.04023757064715028 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 5.960791959892958,
    "estimated_duration": 3600.016635184558,
    "input_throughput": 5886.073078913337,
    "output_throughput": 5202.003184365822,
    "total_throughput": 11088.076263279158,
    "itl": 165.46566848554568,
    "ttft": 1834531.3288718103,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 322,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9627961949585019,
    "arrivals": 321998,
    "finished_requests": 85534,
    "scheduler_time": 78.60964194921682
}
#Debug simulation 
Total elapsed time: 5.9608891271054745. Arrivals time: 0.25074872793629766 Scheduler time: 5.595872656442225 Scheduler overhead time: 0.033116066828370094 Adapter cache time: 0.03082066122442484 Engine time: 0.03467827057465911 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_160_slots_128_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 540, 66, 66, 66, 17280, 66, 540, 17280, 540, 66, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 66, 17280, 540, 17280, 17280, 17280, 540, 17280, 66, 540, 540, 540, 17280, 66, 66, 17280, 66, 17280, 66, 66, 540, 66, 540, 17280, 66, 66, 540, 66, 66, 66, 66, 66, 17280, 540, 540, 17280, 540, 66, 17280, 17280, 17280, 540, 540, 66, 66, 66, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 66, 66, 17280, 540, 17280, 17280, 66, 17280, 66, 540, 66, 66, 540, 540, 17280, 17280, 17280, 66, 17280, 540, 540, 66, 66, 66, 66, 17280, 540, 17280, 540, 17280, 17280, 17280, 66, 540, 540, 17280, 540, 17280, 66, 66, 66, 540, 540, 540, 17280, 17280, 540, 17280, 66, 540, 540, 66, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 66, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 965238 . Total input tokens: 215125306 . Total output tokens: 192903108
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.867582336999476,
    "estimated_duration": 3600.11369885914,
    "input_throughput": 5771.746877490217,
    "output_throughput": 5108.341163177124,
    "total_throughput": 10880.088040667342,
    "itl": 137.19662294568744,
    "ttft": 1851978.889945048,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.057395731247966,
    "arrivals": 321998,
    "finished_requests": 83884,
    "scheduler_time": 77.60894369155862
}
#Debug simulation 
Total elapsed time: 5.86767395073548. Arrivals time: 0.24637632444500923 Scheduler time: 5.488407396245748 Scheduler overhead time: 0.039058750960975885 Adapter cache time: 0.03459048643708229 Engine time: 0.040847230702638626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.084816494956613,
    "estimated_duration": 3600.137751760597,
    "input_throughput": 5970.650425664432,
    "output_throughput": 5250.23354752369,
    "total_throughput": 11220.883973188122,
    "itl": 163.3951939008341,
    "ttft": 1828438.8048537967,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6029160855547517,
    "arrivals": 321377,
    "finished_requests": 86245,
    "scheduler_time": 79.39883881239345
}
#Debug simulation 
Total elapsed time: 6.084934276062995. Arrivals time: 0.25720582297071815 Scheduler time: 5.713887949939817 Scheduler overhead time: 0.033684761729091406 Adapter cache time: 0.0293138618580997 Engine time: 0.034904876723885536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.003801847342402,
    "estimated_duration": 3600.0528560682633,
    "input_throughput": 5970.551227815759,
    "output_throughput": 5250.239025835459,
    "total_throughput": 11220.790253651217,
    "itl": 163.39603873367034,
    "ttft": 1828472.5954333362,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6401143229007735,
    "arrivals": 321377,
    "finished_requests": 86242,
    "scheduler_time": 79.39636254872988
}
#Debug simulation 
Total elapsed time: 6.003891044296324. Arrivals time: 0.2738420688547194 Scheduler time: 5.617303677368909 Scheduler overhead time: 0.033492565620690584 Adapter cache time: 0.02903732657432556 Engine time: 0.03451567282900214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.96319625293836,
    "estimated_duration": 3600.1261600550174,
    "input_throughput": 5842.851907078318,
    "output_throughput": 5148.487074052078,
    "total_throughput": 10991.338981130397,
    "itl": 135.90931411280255,
    "ttft": 1847459.9925570164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6288805796392287,
    "arrivals": 321377,
    "finished_requests": 84454,
    "scheduler_time": 78.21232866486537
}
#Debug simulation 
Total elapsed time: 5.963287911377847. Arrivals time: 0.27951597794890404 Scheduler time: 5.552247735671699 Scheduler overhead time: 0.039572136010974646 Adapter cache time: 0.0325182075612247 Engine time: 0.040809149853885174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 5.96829618839547,
    "estimated_duration": 3600.003186132481,
    "input_throughput": 5970.494993669992,
    "output_throughput": 5250.225353357352,
    "total_throughput": 11220.720347027345,
    "itl": 163.3957996264114,
    "ttft": 1828460.4809828002,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6168243897496735,
    "arrivals": 321377,
    "finished_requests": 86241,
    "scheduler_time": 79.3950181273515
}
#Debug simulation 
Total elapsed time: 5.968387242406607. Arrivals time: 0.24837444862350821 Scheduler time: 5.607427912764251 Scheduler overhead time: 0.033307593781501055 Adapter cache time: 0.02894572541117668 Engine time: 0.03462680708616972 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 5.957147994078696,
    "estimated_duration": 3600.100540605001,
    "input_throughput": 5842.752657253603,
    "output_throughput": 5148.5231567686,
    "total_throughput": 10991.275814022201,
    "itl": 135.91789333568903,
    "ttft": 1847416.7832449598,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6359227916784603,
    "arrivals": 321377,
    "finished_requests": 84453,
    "scheduler_time": 78.21161122209827
}
#Debug simulation 
Total elapsed time: 5.957239683251828. Arrivals time: 0.24683656450361013 Scheduler time: 5.578646685462445 Scheduler overhead time: 0.03959527937695384 Adapter cache time: 0.03254821849986911 Engine time: 0.04091495322063565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.080330573022366,
    "estimated_duration": 3600.0990600401797,
    "input_throughput": 5970.714594658986,
    "output_throughput": 5250.289973906731,
    "total_throughput": 11221.004568565717,
    "itl": 163.3947573923833,
    "ttft": 1828432.0079090083,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 197,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5890399080957294,
    "arrivals": 321377,
    "finished_requests": 86245,
    "scheduler_time": 79.39822198124847
}
#Debug simulation 
Total elapsed time: 6.0804381663911045. Arrivals time: 0.2817174638621509 Scheduler time: 5.684845620766282 Scheduler overhead time: 0.033828206826001406 Adapter cache time: 0.029466948937624693 Engine time: 0.03468654304742813 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [53 53 54]
Adapter prompts. [33, 17280, 33, 33, 540, 33, 33, 33, 17280, 33, 540, 17280, 540, 33, 540, 540, 540, 540, 17280, 17280, 17280, 540, 540, 540, 17280, 17280, 33, 17280, 540, 17280, 17280, 17280, 540, 17280, 33, 540, 540, 540, 17280, 33, 33, 17280, 33, 17280, 33, 33, 540, 33, 540, 17280, 33, 33, 540, 33, 33, 33, 33, 33, 17280, 540, 540, 17280, 540, 33, 17280, 17280, 17280, 540, 540, 33, 33, 33, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 33, 33, 17280, 540, 17280, 17280, 33, 17280, 33, 540, 33, 33, 540, 540, 17280, 17280, 17280, 33, 17280, 540, 540, 33, 33, 33, 33, 17280, 540, 17280, 540, 17280, 17280, 17280, 33, 540, 540, 17280, 540, 17280, 33, 33, 33, 540, 540, 540, 17280, 17280, 540, 17280, 33, 540, 540, 33, 17280, 17280, 17280, 17280, 17280, 17280, 540, 540, 540, 540, 540, 17280, 33, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 963489 . Total input tokens: 214745861 . Total output tokens: 192549450
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 5.948839423246682,
    "estimated_duration": 3600.090463328144,
    "input_throughput": 5842.909841924904,
    "output_throughput": 5148.538123918399,
    "total_throughput": 10991.447965843303,
    "itl": 135.91171506295183,
    "ttft": 1847426.287268108,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 193,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6443482953682556,
    "arrivals": 321377,
    "finished_requests": 84454,
    "scheduler_time": 78.2107488767874
}
#Debug simulation 
Total elapsed time: 5.948949167039245. Arrivals time: 0.24600408179685473 Scheduler time: 5.5712008201517165 Scheduler overhead time: 0.03975431760773063 Adapter cache time: 0.032217026222497225 Engine time: 0.040943476371467113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.1024962132796645,
    "estimated_duration": 3600.106378422768,
    "input_throughput": 6062.8952885449435,
    "output_throughput": 5353.540694106871,
    "total_throughput": 11416.435982651814,
    "itl": 160.7372318557316,
    "ttft": 1811068.958973496,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.87913947477474,
    "arrivals": 318402,
    "finished_requests": 87795,
    "scheduler_time": 80.90283088778396
}
#Debug simulation 
Total elapsed time: 6.102596988435835. Arrivals time: 0.26933737751096487 Scheduler time: 5.719799540005624 Scheduler overhead time: 0.03384675085544586 Adapter cache time: 0.028561352752149105 Engine time: 0.035037425346672535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.150844108778983,
    "estimated_duration": 3600.0763675719163,
    "input_throughput": 6062.827499051375,
    "output_throughput": 5353.207830143347,
    "total_throughput": 11416.035329194721,
    "itl": 160.74039731555965,
    "ttft": 1811119.8579229529,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.997694627172318,
    "arrivals": 318402,
    "finished_requests": 87792,
    "scheduler_time": 80.89978575927691
}
#Debug simulation 
Total elapsed time: 6.150957963895053. Arrivals time: 0.26252863835543394 Scheduler time: 5.7746351095847785 Scheduler overhead time: 0.03389860736206174 Adapter cache time: 0.028459087014198303 Engine time: 0.035302658565342426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.055266911163926,
    "estimated_duration": 3600.142099405992,
    "input_throughput": 5932.177511416497,
    "output_throughput": 5244.952693149402,
    "total_throughput": 11177.130204565898,
    "itl": 133.67933285051768,
    "ttft": 1830129.6299401415,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9525708283111574,
    "arrivals": 318402,
    "finished_requests": 85837,
    "scheduler_time": 79.64880083824265
}
#Debug simulation 
Total elapsed time: 6.055363568011671. Arrivals time: 0.26004974357783794 Scheduler time: 5.6642416580580175 Scheduler overhead time: 0.04008703259751201 Adapter cache time: 0.030514617450535297 Engine time: 0.04146293690428138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 6.133462212048471,
    "estimated_duration": 3600.1382590403855,
    "input_throughput": 6062.841877027799,
    "output_throughput": 5353.541062375014,
    "total_throughput": 11416.382939402813,
    "itl": 160.73784223511404,
    "ttft": 1811087.8364923801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9200615166686352,
    "arrivals": 318402,
    "finished_requests": 87796,
    "scheduler_time": 80.90312501979305
}
#Debug simulation 
Total elapsed time: 6.133562959730625. Arrivals time: 0.293639135081321 Scheduler time: 5.726517097558826 Scheduler overhead time: 0.033825584687292576 Adapter cache time: 0.02839825674891472 Engine time: 0.03512683091685176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 6.046063685789704,
    "estimated_duration": 3600.046712078717,
    "input_throughput": 5932.100527570333,
    "output_throughput": 5244.95249926833,
    "total_throughput": 11177.053026838663,
    "itl": 133.6821822511241,
    "ttft": 1830083.646889156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.975835278797899,
    "arrivals": 318402,
    "finished_requests": 85832,
    "scheduler_time": 79.64579358785554
}
#Debug simulation 
Total elapsed time: 6.046155938878655. Arrivals time: 0.2703280304558575 Scheduler time: 5.644758372567594 Scheduler overhead time: 0.03997305128723383 Adapter cache time: 0.030522314831614494 Engine time: 0.04163083899766207 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.082252255175263,
    "estimated_duration": 3600.1792475114453,
    "input_throughput": 6063.0064503283065,
    "output_throughput": 5353.76537524434,
    "total_throughput": 11416.771825572647,
    "itl": 160.73433149348617,
    "ttft": 1811055.3354532467,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 614,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8358908810699075,
    "arrivals": 318402,
    "finished_requests": 87798,
    "scheduler_time": 80.90559104241379
}
#Debug simulation 
Total elapsed time: 6.082346196752042. Arrivals time: 0.2680928073823452 Scheduler time: 5.701053643133491 Scheduler overhead time: 0.03382173692807555 Adapter cache time: 0.02819645218551159 Engine time: 0.03519802587106824 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_160_slots_128_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [53 53 54]
Adapter prompts. [135, 17280, 135, 135, 270, 135, 135, 135, 17280, 135, 270, 17280, 270, 135, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 135, 17280, 270, 17280, 17280, 17280, 270, 17280, 135, 270, 270, 270, 17280, 135, 135, 17280, 135, 17280, 135, 135, 270, 135, 270, 17280, 135, 135, 270, 135, 135, 135, 135, 135, 17280, 270, 270, 17280, 270, 135, 17280, 17280, 17280, 270, 270, 135, 135, 135, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 135, 135, 17280, 270, 17280, 17280, 135, 17280, 135, 270, 135, 135, 270, 270, 17280, 17280, 17280, 135, 17280, 270, 270, 135, 135, 135, 135, 17280, 270, 17280, 270, 17280, 17280, 17280, 135, 270, 270, 17280, 270, 17280, 135, 135, 135, 270, 270, 270, 17280, 17280, 270, 17280, 135, 270, 270, 135, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 135, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 954585 . Total input tokens: 212750706 . Total output tokens: 190774017
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.035359601024538,
    "estimated_duration": 3600.0751104505966,
    "input_throughput": 5932.28761755666,
    "output_throughput": 5245.049733875302,
    "total_throughput": 11177.337351431963,
    "itl": 133.68274173799486,
    "ttft": 1830120.0234643747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.0008602822944477,
    "arrivals": 318402,
    "finished_requests": 85836,
    "scheduler_time": 79.64664508390253
}
#Debug simulation 
Total elapsed time: 6.035464456770569. Arrivals time: 0.26982859475538135 Scheduler time: 5.634573847986758 Scheduler overhead time: 0.04002307401970029 Adapter cache time: 0.03056036029011011 Engine time: 0.04156483802944422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 601664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_160_slots_128_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 6.44858931330964,
    "estimated_duration": 3600.0346720360412,
    "input_throughput": 6177.316894401666,
    "output_throughput": 5451.023611642466,
    "total_throughput": 11628.340506044133,
    "itl": 157.83363826628963,
    "ttft": 1787371.1717982525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1109570510475921,
    "arrivals": 317160,
    "finished_requests": 89717,
    "scheduler_time": 82.30606281440592
}
#Debug simulation 
Total elapsed time: 6.448652601335198. Arrivals time: 0.26362803066149354 Scheduler time: 6.073311364743859 Scheduler overhead time: 0.03445690078660846 Adapter cache time: 0.025340937077999115 Engine time: 0.03566356189548969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 526992,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_160_slots_128_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.274870730005205,
    "estimated_duration": 3600.00065979991,
    "input_throughput": 6177.228590073648,
    "output_throughput": 5450.937334297788,
    "total_throughput": 11628.165924371437,
    "itl": 157.8371513979103,
    "ttft": 1787313.4866672305,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 363,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1826339428382981,
    "arrivals": 317160,
    "finished_requests": 89715,
    "scheduler_time": 82.30346890558529
}
#Debug simulation 
Total elapsed time: 6.274957084096968. Arrivals time: 0.2912785937078297 Scheduler time: 5.87122435728088 Scheduler overhead time: 0.0346739855594933 Adapter cache time: 0.02540851477533579 Engine time: 0.03600000962615013 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 128,
    "served_adapters": 160,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 346768,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_160_slots_128_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [53 53 54]
Adapter prompts. [66, 17280, 66, 66, 270, 66, 66, 66, 17280, 66, 270, 17280, 270, 66, 270, 270, 270, 270, 17280, 17280, 17280, 270, 270, 270, 17280, 17280, 66, 17280, 270, 17280, 17280, 17280, 270, 17280, 66, 270, 270, 270, 17280, 66, 66, 17280, 66, 17280, 66, 66, 270, 66, 270, 17280, 66, 66, 270, 66, 66, 66, 66, 66, 17280, 270, 270, 17280, 270, 66, 17280, 17280, 17280, 270, 270, 66, 66, 66, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 66, 66, 17280, 270, 17280, 17280, 66, 17280, 66, 270, 66, 66, 270, 270, 17280, 17280, 17280, 66, 17280, 270, 270, 66, 66, 66, 66, 17280, 270, 17280, 270, 17280, 17280, 17280, 66, 270, 270, 17280, 270, 17280, 66, 66, 66, 270, 270, 270, 17280, 17280, 270, 17280, 66, 270, 270, 66, 17280, 17280, 17280, 17280, 17280, 17280, 270, 270, 270, 270, 270, 17280, 66, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 950928 . Total input tokens: 211961236 . Total output tokens: 190027670
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 6.1078411638736725,
    "estimated_duration": 3600.10391252658,
    "input_throughput": 6023.019759110885,
    "output_throughput": 5322.13637871169,
    "total_throughput": 11345.156137822576,
    "itl": 132.00994905255575,
    "ttft": 1807980.839539328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1678211665898626,
    "arrivals": 317160,
    "finished_requests": 87559,
    "scheduler_time": 80.8244552616772
}
#Debug simulation 
Total elapsed time: 6.10795674007386. Arrivals time: 0.2809195648878813 Scheduler time: 5.698420597240329 Scheduler overhead time: 0.04055973934009671 Adapter cache time: 0.027051142416894436 Engine time: 0.041882709600031376 
