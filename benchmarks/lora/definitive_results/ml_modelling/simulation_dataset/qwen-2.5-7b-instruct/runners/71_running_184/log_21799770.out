INFO 06-01 00:47:08 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:09 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_160_slots_16_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_160_slots_16_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 115.3209079420194,
    "estimated_duration": 3600.068011802615,
    "input_throughput": 7355.002714724204,
    "output_throughput": 6485.411643184291,
    "total_throughput": 13840.414357908496,
    "itl": 99.16145452938696,
    "ttft": 1567503.557087852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1319734353944721,
    "arrivals": 236027,
    "finished_requests": 107067,
    "scheduler_time": 277.57968714402426
}
#Debug simulation 
Total elapsed time: 115.3211295939982. Arrivals time: 0.6100969216786325 Scheduler time: 114.46565939486027 Scheduler overhead time: 0.09562173299491405 Adapter cache time: 0.019690307322889566 Engine time: 0.09344220906496048 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_160_slots_16_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_160_slots_16_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 115.53497743513435,
    "estimated_duration": 3600.033322984666,
    "input_throughput": 7363.79322678645,
    "output_throughput": 6495.466542130005,
    "total_throughput": 13859.259768916456,
    "itl": 98.96011359902386,
    "ttft": 1566957.6637096044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1087350285635351,
    "arrivals": 236027,
    "finished_requests": 107140,
    "scheduler_time": 277.25237469677757
}
#Debug simulation 
Total elapsed time: 115.53518734220415. Arrivals time: 0.602619425393641 Scheduler time: 114.68721142085269 Scheduler overhead time: 0.09719508793205023 Adapter cache time: 0.019351447466760874 Engine time: 0.09311223588883877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_160_slots_16_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_160_slots_16_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 115.92736262688413,
    "estimated_duration": 3600.0826433744955,
    "input_throughput": 7354.972822285179,
    "output_throughput": 6485.3852849653185,
    "total_throughput": 13840.358107250497,
    "itl": 99.16178731067433,
    "ttft": 1567509.4705016299,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1464351208321801,
    "arrivals": 236027,
    "finished_requests": 107067,
    "scheduler_time": 277.57995706906274
}
#Debug simulation 
Total elapsed time: 115.9275309862569. Arrivals time: 0.622079633641988 Scheduler time: 115.05800032336265 Scheduler overhead time: 0.09750597132369876 Adapter cache time: 0.02001895708963275 Engine time: 0.09377709636464715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_160_slots_16_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_160_slots_16_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 115.08102089213207,
    "estimated_duration": 3600.1107676476286,
    "input_throughput": 7363.747037517478,
    "output_throughput": 6495.35097923653,
    "total_throughput": 13859.09801675401,
    "itl": 98.95926471314209,
    "ttft": 1566988.7171260647,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 354,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0584778043953695,
    "arrivals": 236027,
    "finished_requests": 107141,
    "scheduler_time": 277.26001195273136
}
#Debug simulation 
Total elapsed time: 115.0811936170794. Arrivals time: 0.611456052865833 Scheduler time: 114.22451653005555 Scheduler overhead time: 0.0963193578645587 Adapter cache time: 0.01956325862556696 Engine time: 0.09319123951718211 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_160_slots_16_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_160_slots_16_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 4320, 270, 270, 270, 8640, 270, 4320, 8640, 4320, 270, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 270, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 270, 4320, 4320, 4320, 8640, 270, 270, 8640, 270, 8640, 270, 270, 4320, 270, 4320, 8640, 270, 270, 4320, 270, 270, 270, 270, 270, 8640, 4320, 4320, 8640, 4320, 270, 8640, 8640, 8640, 4320, 4320, 270, 270, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 270, 270, 8640, 4320, 8640, 8640, 270, 8640, 270, 4320, 270, 270, 4320, 4320, 8640, 8640, 8640, 270, 8640, 4320, 4320, 270, 270, 270, 270, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 270, 4320, 4320, 8640, 4320, 8640, 270, 270, 270, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 270, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 270, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 709830 . Total input tokens: 158254628 . Total output tokens: 141942651
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 116.12175747845322,
    "estimated_duration": 3600.098307603251,
    "input_throughput": 7354.940820387748,
    "output_throughput": 6485.3570666918185,
    "total_throughput": 13840.297887079567,
    "itl": 99.1621118297596,
    "ttft": 1567515.845055261,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 346,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1613998214155474,
    "arrivals": 236027,
    "finished_requests": 107067,
    "scheduler_time": 277.5803736992424
}
#Debug simulation 
Total elapsed time: 116.12192698940635. Arrivals time: 0.6175179518759251 Scheduler time: 115.2576182349585 Scheduler overhead time: 0.0969455479644239 Adapter cache time: 0.019896541256457567 Engine time: 0.0941359824500978 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 120.43054417287931,
    "estimated_duration": 3600.0169501254236,
    "input_throughput": 7333.481860156299,
    "output_throughput": 6508.597966235595,
    "total_throughput": 13842.079826391895,
    "itl": 100.16106865925018,
    "ttft": 1565250.8574135064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9487512006191524,
    "arrivals": 233648,
    "finished_requests": 106359,
    "scheduler_time": 276.0719718482745
}
#Debug simulation 
Total elapsed time: 120.4307151180692. Arrivals time: 0.6193325882777572 Scheduler time: 119.56472963653505 Scheduler overhead time: 0.09748617187142372 Adapter cache time: 0.019700300879776478 Engine time: 0.09369517909362912 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 118.79049296025187,
    "estimated_duration": 3600.0567196706425,
    "input_throughput": 7445.150198203576,
    "output_throughput": 6597.443554215145,
    "total_throughput": 14042.593752418721,
    "itl": 101.19857375381615,
    "ttft": 1549055.36363663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0560858106287252,
    "arrivals": 233648,
    "finished_requests": 107879,
    "scheduler_time": 270.6790681092355
}
#Debug simulation 
Total elapsed time: 118.79067707341164. Arrivals time: 0.622553052380681 Scheduler time: 117.92213512212038 Scheduler overhead time: 0.09701791685074568 Adapter cache time: 0.019455738365650177 Engine time: 0.09383659576997161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 119.53950140904635,
    "estimated_duration": 3600.0582023032034,
    "input_throughput": 7445.147132024786,
    "output_throughput": 6597.44083715223,
    "total_throughput": 14042.587969177015,
    "itl": 101.19861717561575,
    "ttft": 1549055.997354021,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0575882844440696,
    "arrivals": 233648,
    "finished_requests": 107879,
    "scheduler_time": 270.6790482679665
}
#Debug simulation 
Total elapsed time: 119.53968239203095. Arrivals time: 0.6162770027294755 Scheduler time: 118.67831986164674 Scheduler overhead time: 0.09623229829594493 Adapter cache time: 0.019514021463692188 Engine time: 0.09286886779591441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 120.51716224988922,
    "estimated_duration": 3600.040080098928,
    "input_throughput": 7333.434743113893,
    "output_throughput": 6508.55614900713,
    "total_throughput": 13841.990892121024,
    "itl": 100.16148718077648,
    "ttft": 1565260.3158275331,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9706352904927952,
    "arrivals": 233648,
    "finished_requests": 106359,
    "scheduler_time": 276.07261750039083
}
#Debug simulation 
Total elapsed time: 120.51733237085864. Arrivals time: 0.6075598867610097 Scheduler time: 119.66291426867247 Scheduler overhead time: 0.09739577677100897 Adapter cache time: 0.019363311119377613 Engine time: 0.09374358039349318 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 120.26843415712938,
    "estimated_duration": 3600.071842597513,
    "input_throughput": 7445.118923143825,
    "output_throughput": 6597.415840141437,
    "total_throughput": 14042.53476328526,
    "itl": 101.19854827590918,
    "ttft": 1549062.1553923006,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0711696933768742,
    "arrivals": 233648,
    "finished_requests": 107879,
    "scheduler_time": 270.6794072690947
}
#Debug simulation 
Total elapsed time: 120.26861589215696. Arrivals time: 0.6243441859260201 Scheduler time: 119.39731890149415 Scheduler overhead time: 0.09723477391526103 Adapter cache time: 0.019680031575262547 Engine time: 0.09452548064291477 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 125.44215473299846,
    "estimated_duration": 3600.1154639616016,
    "input_throughput": 7333.532844790335,
    "output_throughput": 6508.46541855523,
    "total_throughput": 13841.998263345564,
    "itl": 100.16029891811905,
    "ttft": 1565285.0127299454,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9269155914196757,
    "arrivals": 233648,
    "finished_requests": 106362,
    "scheduler_time": 276.0799866507001
}
#Debug simulation 
Total elapsed time: 125.44232923584059. Arrivals time: 0.6699261697940528 Scheduler time: 124.51059023477137 Scheduler overhead time: 0.10393346473574638 Adapter cache time: 0.020804360508918762 Engine time: 0.09943538997322321 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 4320, 135, 135, 135, 8640, 135, 4320, 8640, 4320, 135, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 135, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 135, 4320, 4320, 4320, 8640, 135, 135, 8640, 135, 8640, 135, 135, 4320, 135, 4320, 8640, 135, 135, 4320, 135, 135, 135, 135, 135, 8640, 4320, 4320, 8640, 4320, 135, 8640, 8640, 8640, 4320, 4320, 135, 135, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 135, 135, 8640, 4320, 8640, 8640, 135, 8640, 135, 4320, 135, 135, 4320, 4320, 8640, 8640, 8640, 135, 8640, 4320, 4320, 135, 135, 135, 135, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 135, 4320, 4320, 8640, 4320, 8640, 135, 135, 135, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 135, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 135, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 702675 . Total input tokens: 156663449 . Total output tokens: 140515378
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 123.98861247999594,
    "estimated_duration": 3600.060203601396,
    "input_throughput": 7445.142993216361,
    "output_throughput": 6597.437169589557,
    "total_throughput": 14042.580162805918,
    "itl": 101.19874252743278,
    "ttft": 1549049.7898411632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 323,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0853798712417528,
    "arrivals": 233648,
    "finished_requests": 107879,
    "scheduler_time": 270.67485762991686
}
#Debug simulation 
Total elapsed time: 123.98878581821918. Arrivals time: 0.7247995072975755 Scheduler time: 123.00473492732272 Scheduler overhead time: 0.10228085471317172 Adapter cache time: 0.02087141014635563 Engine time: 0.0991301042959094 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 125.79404774680734,
    "estimated_duration": 3600.0735306071674,
    "input_throughput": 7388.145484771296,
    "output_throughput": 6545.242978974916,
    "total_throughput": 13933.38846374621,
    "itl": 101.62770687053072,
    "ttft": 1557766.5234433769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9824165658024127,
    "arrivals": 232444,
    "finished_requests": 107421,
    "scheduler_time": 273.4714133570379
}
#Debug simulation 
Total elapsed time: 125.79423022596166. Arrivals time: 0.7405131692066789 Scheduler time: 124.78916149958968 Scheduler overhead time: 0.10456030303612351 Adapter cache time: 0.02106289705261588 Engine time: 0.10092903254553676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 122.02026712941006,
    "estimated_duration": 3600.054442443619,
    "input_throughput": 7425.277152713127,
    "output_throughput": 6589.763676989598,
    "total_throughput": 14015.040829702726,
    "itl": 101.89030759835262,
    "ttft": 1554386.0439652738,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1241886059613957,
    "arrivals": 232444,
    "finished_requests": 107931,
    "scheduler_time": 272.1595247593764
}
#Debug simulation 
Total elapsed time: 122.02043496724218. Arrivals time: 0.7319281906820834 Scheduler time: 121.02658215351403 Scheduler overhead time: 0.10341326193884015 Adapter cache time: 0.021114372182637453 Engine time: 0.10078243119642138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 122.89055738504976,
    "estimated_duration": 3600.0562025650156,
    "input_throughput": 7425.273522383916,
    "output_throughput": 6589.76045515544,
    "total_throughput": 14015.033977539357,
    "itl": 101.89033554436034,
    "ttft": 1554386.7601343493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1258863976970377,
    "arrivals": 232444,
    "finished_requests": 107931,
    "scheduler_time": 272.15958708901957
}
#Debug simulation 
Total elapsed time: 122.89073590422049. Arrivals time: 0.7307304856367409 Scheduler time: 121.89619060000405 Scheduler overhead time: 0.10408613597974181 Adapter cache time: 0.020996135659515858 Engine time: 0.10132748587056994 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 122.78738025389612,
    "estimated_duration": 3600.004846769298,
    "input_throughput": 7425.379447471907,
    "output_throughput": 6589.85446124881,
    "total_throughput": 14015.233908720718,
    "itl": 101.8890224484768,
    "ttft": 1554366.0920850893,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0755657630669908,
    "arrivals": 232444,
    "finished_requests": 107931,
    "scheduler_time": 272.15865196651504
}
#Debug simulation 
Total elapsed time: 122.78755785292014. Arrivals time: 0.7307309415191412 Scheduler time: 121.79605962894857 Scheduler overhead time: 0.10246108332648873 Adapter cache time: 0.020826647523790598 Engine time: 0.09967865655198693 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 122.88244878966361,
    "estimated_duration": 3600.0714576696128,
    "input_throughput": 7425.24205819617,
    "output_throughput": 6589.7325314083155,
    "total_throughput": 14014.974589604484,
    "itl": 101.89074680066028,
    "ttft": 1554392.8697368482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.140851098280405,
    "arrivals": 232444,
    "finished_requests": 107931,
    "scheduler_time": 272.1598774930485
}
#Debug simulation 
Total elapsed time: 122.88261966500431. Arrivals time: 0.7381518012844026 Scheduler time: 121.88202115427703 Scheduler overhead time: 0.10352457035332918 Adapter cache time: 0.02105439268052578 Engine time: 0.10031907306984067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 126.38507024990395,
    "estimated_duration": 3600.0503352196442,
    "input_throughput": 7388.1930871328295,
    "output_throughput": 6545.285150453977,
    "total_throughput": 13933.478237586807,
    "itl": 101.62803349012317,
    "ttft": 1557756.1485439767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9598061446635997,
    "arrivals": 232444,
    "finished_requests": 107421,
    "scheduler_time": 273.47082839060647
}
#Debug simulation 
Total elapsed time: 126.38524975581095. Arrivals time: 0.7450912408530712 Scheduler time: 125.37532560154796 Scheduler overhead time: 0.10420192033052444 Adapter cache time: 0.02103484934195876 Engine time: 0.1018485096283257 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 4320, 66, 66, 66, 8640, 66, 4320, 8640, 4320, 66, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 66, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 66, 4320, 4320, 4320, 8640, 66, 66, 8640, 66, 8640, 66, 66, 4320, 66, 4320, 8640, 66, 66, 4320, 66, 66, 66, 66, 66, 8640, 4320, 4320, 8640, 4320, 66, 8640, 8640, 8640, 4320, 4320, 66, 66, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 66, 66, 8640, 4320, 8640, 8640, 66, 8640, 66, 4320, 66, 66, 4320, 4320, 8640, 8640, 8640, 66, 8640, 4320, 4320, 66, 66, 66, 66, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 66, 4320, 4320, 8640, 4320, 8640, 66, 66, 66, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 66, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 66, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 699018 . Total input tokens: 155862900 . Total output tokens: 139789936
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 122.58813144220039,
    "estimated_duration": 3600.0866254164525,
    "input_throughput": 7425.210774451227,
    "output_throughput": 6589.704767800053,
    "total_throughput": 14014.91554225128,
    "itl": 101.89110269341377,
    "ttft": 1554399.1892614355,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1553127837181127,
    "arrivals": 232444,
    "finished_requests": 107931,
    "scheduler_time": 272.16029412322797
}
#Debug simulation 
Total elapsed time: 122.58830302627757. Arrivals time: 0.7326370226219296 Scheduler time: 121.5932736126706 Scheduler overhead time: 0.10355902789160609 Adapter cache time: 0.0207206760533154 Engine time: 0.10138007625937462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 123.92545624123886,
    "estimated_duration": 3600.0186785922174,
    "input_throughput": 7497.867763995982,
    "output_throughput": 6598.967150162096,
    "total_throughput": 14096.834914158078,
    "itl": 101.97617343123169,
    "ttft": 1541974.2520039973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9701746148266817,
    "arrivals": 231825,
    "finished_requests": 108550,
    "scheduler_time": 270.486621253469
}
#Debug simulation 
Total elapsed time: 123.92563160322607. Arrivals time: 0.7286493987776339 Scheduler time: 122.93604536727071 Scheduler overhead time: 0.10283796396106482 Adapter cache time: 0.02043964760378003 Engine time: 0.10061650164425373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 129.26399045111611,
    "estimated_duration": 3600.0402019562557,
    "input_throughput": 7360.200584871682,
    "output_throughput": 6475.207967770148,
    "total_throughput": 13835.40855264183,
    "itl": 99.38454168994537,
    "ttft": 1565060.0121386973,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9439305053884213,
    "arrivals": 231825,
    "finished_requests": 106529,
    "scheduler_time": 277.30223994031144
}
#Debug simulation 
Total elapsed time: 129.2641795570962. Arrivals time: 0.738324882928282 Scheduler time: 128.25952737312764 Scheduler overhead time: 0.1057355459779501 Adapter cache time: 0.020991819445043802 Engine time: 0.10136012081056833 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 129.46496339002624,
    "estimated_duration": 3600.042386637745,
    "input_throughput": 7360.196118342611,
    "output_throughput": 6475.204038297807,
    "total_throughput": 13835.400156640417,
    "itl": 99.38457636284987,
    "ttft": 1565060.8152008245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9460229951143313,
    "arrivals": 231825,
    "finished_requests": 106529,
    "scheduler_time": 277.30233213206674
}
#Debug simulation 
Total elapsed time: 129.46514146402478. Arrivals time: 0.7389851016923785 Scheduler time: 128.45836869254708 Scheduler overhead time: 0.10524813272058964 Adapter cache time: 0.020909431856125593 Engine time: 0.10382667277008295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 123.8070203056559,
    "estimated_duration": 3600.0412161807426,
    "input_throughput": 7497.8208245727,
    "output_throughput": 6598.925838188874,
    "total_throughput": 14096.746662761574,
    "itl": 101.97679070928402,
    "ttft": 1541983.8593071073,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9919742378755507,
    "arrivals": 231825,
    "finished_requests": 108550,
    "scheduler_time": 270.4872591803317
}
#Debug simulation 
Total elapsed time: 123.8071992509067. Arrivals time: 0.7372427703812718 Scheduler time: 122.80849262792617 Scheduler overhead time: 0.10295749828219414 Adapter cache time: 0.020683278795331717 Engine time: 0.10123049840331078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 127.05904136784375,
    "estimated_duration": 3600.0292972672887,
    "input_throughput": 7526.113473733865,
    "output_throughput": 6631.337977755218,
    "total_throughput": 14157.451451489083,
    "itl": 102.54434092394656,
    "ttft": 1540672.3502805422,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0334782484546359,
    "arrivals": 231825,
    "finished_requests": 108960,
    "scheduler_time": 268.7056859075687
}
#Debug simulation 
Total elapsed time: 127.059223765973. Arrivals time: 0.7287223418243229 Scheduler time: 126.06518246466294 Scheduler overhead time: 0.10388602642342448 Adapter cache time: 0.02150815073400736 Engine time: 0.10302125522866845 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 123.3982855528593,
    "estimated_duration": 3600.123677089085,
    "input_throughput": 7498.122681670314,
    "output_throughput": 6599.416889813725,
    "total_throughput": 14097.539571484038,
    "itl": 101.97737671547738,
    "ttft": 1541897.3636503038,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 317,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.947845943483991,
    "arrivals": 231825,
    "finished_requests": 108557,
    "scheduler_time": 270.49491119384567
}
#Debug simulation 
Total elapsed time: 123.39845935208723. Arrivals time: 0.7305089440196753 Scheduler time: 122.40867193695158 Scheduler overhead time: 0.1023956909775734 Adapter cache time: 0.02062149764969945 Engine time: 0.09972826577723026 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 4320, 33, 33, 33, 8640, 33, 4320, 8640, 4320, 33, 4320, 4320, 4320, 4320, 8640, 8640, 8640, 4320, 4320, 4320, 8640, 8640, 33, 8640, 4320, 8640, 8640, 8640, 4320, 8640, 33, 4320, 4320, 4320, 8640, 33, 33, 8640, 33, 8640, 33, 33, 4320, 33, 4320, 8640, 33, 33, 4320, 33, 33, 33, 33, 33, 8640, 4320, 4320, 8640, 4320, 33, 8640, 8640, 8640, 4320, 4320, 33, 33, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 33, 33, 8640, 4320, 8640, 8640, 33, 8640, 33, 4320, 33, 33, 4320, 4320, 8640, 8640, 8640, 33, 8640, 4320, 4320, 33, 33, 33, 33, 8640, 4320, 8640, 4320, 8640, 8640, 8640, 33, 4320, 4320, 8640, 4320, 8640, 33, 33, 33, 4320, 4320, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 33, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 8640, 33, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 697269 . Total input tokens: 155478076 . Total output tokens: 139435224
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 126.39978225436062,
    "estimated_duration": 3600.000003190749,
    "input_throughput": 7472.529993376954,
    "output_throughput": 6579.560271946188,
    "total_throughput": 14052.09026532314,
    "itl": 101.03626614251243,
    "ttft": 1547463.3365474883,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0397951095551303,
    "arrivals": 231825,
    "finished_requests": 108166,
    "scheduler_time": 271.4475065129477
}
#Debug simulation 
Total elapsed time: 126.39996242523193. Arrivals time: 0.7318427111022174 Scheduler time: 125.40738058416173 Scheduler overhead time: 0.10267428494989872 Adapter cache time: 0.020772508345544338 Engine time: 0.10015769070014358 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 129.61180140264332,
    "estimated_duration": 3600.068073537376,
    "input_throughput": 7133.013175155837,
    "output_throughput": 6319.9729936339445,
    "total_throughput": 13452.986168789783,
    "itl": 95.79024740996705,
    "ttft": 1445617.0168578636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8446946173254389,
    "arrivals": 183717,
    "finished_requests": 103649,
    "scheduler_time": 275.4422016167625
}
#Debug simulation 
Total elapsed time: 129.61197655368596. Arrivals time: 0.6984029212035239 Scheduler time: 128.63734450284392 Scheduler overhead time: 0.10891083581373096 Adapter cache time: 0.021066876128315926 Engine time: 0.10635186592116952 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 128.68541292008013,
    "estimated_duration": 3600.011096531547,
    "input_throughput": 7132.943569185184,
    "output_throughput": 6319.741909106816,
    "total_throughput": 13452.685478292,
    "itl": 95.79108833957253,
    "ttft": 1445675.4620062069,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8996182293491485,
    "arrivals": 183717,
    "finished_requests": 103646,
    "scheduler_time": 275.43155227768705
}
#Debug simulation 
Total elapsed time: 128.6855857730843. Arrivals time: 0.6944468142464757 Scheduler time: 127.718537166249 Scheduler overhead time: 0.1076621594838798 Adapter cache time: 0.02080522198230028 Engine time: 0.10547045059502125 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 129.20545739866793,
    "estimated_duration": 3600.0128347207683,
    "input_throughput": 7132.940125195899,
    "output_throughput": 6319.738857754564,
    "total_throughput": 13452.678982950463,
    "itl": 95.79113541580833,
    "ttft": 1445676.2371366487,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.901390246264641,
    "arrivals": 183717,
    "finished_requests": 103646,
    "scheduler_time": 275.43157191853925
}
#Debug simulation 
Total elapsed time: 129.2056315629743. Arrivals time: 0.7000803868286312 Scheduler time: 128.23375999648124 Scheduler overhead time: 0.10758689604699612 Adapter cache time: 0.02114424342289567 Engine time: 0.10478193033486605 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 129.82653594203293,
    "estimated_duration": 3600.0610525944776,
    "input_throughput": 7133.027086163864,
    "output_throughput": 6319.985319027559,
    "total_throughput": 13453.012405191423,
    "itl": 95.79107347380408,
    "ttft": 1445605.8330744747,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.863661841326396,
    "arrivals": 183717,
    "finished_requests": 103649,
    "scheduler_time": 275.4378427014926
}
#Debug simulation 
Total elapsed time: 129.8267056918703. Arrivals time: 0.7058359570801258 Scheduler time: 128.84773946972564 Scheduler overhead time: 0.10875178594142199 Adapter cache time: 0.02115724142640829 Engine time: 0.10424649808555841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 129.21867041103542,
    "estimated_duration": 3600.024514494202,
    "input_throughput": 7132.916983374435,
    "output_throughput": 6319.718354250291,
    "total_throughput": 13452.635337624726,
    "itl": 95.79134131309284,
    "ttft": 1445681.4429298465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9124565794691489,
    "arrivals": 183717,
    "finished_requests": 103646,
    "scheduler_time": 275.4318852430319
}
#Debug simulation 
Total elapsed time: 129.21885273698717. Arrivals time: 0.6964758327230811 Scheduler time: 128.2491007782519 Scheduler overhead time: 0.10858976608142257 Adapter cache time: 0.020734401419758797 Engine time: 0.10469786683097482 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 129.14191622566432,
    "estimated_duration": 3600.0480895257288,
    "input_throughput": 7133.052770798682,
    "output_throughput": 6320.008076058061,
    "total_throughput": 13453.060846856743,
    "itl": 95.78900238468937,
    "ttft": 1445608.7582710322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8252538813930016,
    "arrivals": 183717,
    "finished_requests": 103649,
    "scheduler_time": 275.4416690911053
}
#Debug simulation 
Total elapsed time: 129.1420945818536. Arrivals time: 0.7075804388150573 Scheduler time: 128.16109775705263 Scheduler overhead time: 0.10830530477687716 Adapter cache time: 0.02081540832296014 Engine time: 0.10539537388831377 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [53 53 54]
Adapter prompts. [540, 8640, 540, 540, 1080, 540, 540, 540, 8640, 540, 1080, 8640, 1080, 540, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 540, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 540, 1080, 1080, 1080, 8640, 540, 540, 8640, 540, 8640, 540, 540, 1080, 540, 1080, 8640, 540, 540, 1080, 540, 540, 540, 540, 540, 8640, 1080, 1080, 8640, 1080, 540, 8640, 8640, 8640, 1080, 1080, 540, 540, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 540, 540, 8640, 1080, 8640, 8640, 540, 8640, 540, 1080, 540, 540, 1080, 1080, 8640, 8640, 8640, 540, 8640, 1080, 1080, 540, 540, 540, 540, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 540, 1080, 1080, 8640, 1080, 8640, 540, 540, 540, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 540, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 540, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 552420 . Total input tokens: 123244485 . Total output tokens: 110517912
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 129.18128773989156,
    "estimated_duration": 3600.0364279725754,
    "input_throughput": 7132.893378654338,
    "output_throughput": 6319.697440620819,
    "total_throughput": 13452.590819275158,
    "itl": 95.79151884759473,
    "ttft": 1445686.2506496052,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 276,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.924277435392146,
    "arrivals": 183717,
    "finished_requests": 103646,
    "scheduler_time": 275.43217794265087
}
#Debug simulation 
Total elapsed time: 129.18146048206836. Arrivals time: 0.6999890357255936 Scheduler time: 128.2079755156301 Scheduler overhead time: 0.10911242943257093 Adapter cache time: 0.02086269063875079 Engine time: 0.10480778012424707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 126.55156097514555,
    "estimated_duration": 3600.0328642115564,
    "input_throughput": 7094.404124444996,
    "output_throughput": 6350.433138339407,
    "total_throughput": 13444.837262784404,
    "itl": 96.80304853912321,
    "ttft": 1408624.5237215897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9487512006191524,
    "arrivals": 179046,
    "finished_requests": 103627,
    "scheduler_time": 272.81772365447324
}
#Debug simulation 
Total elapsed time: 126.55173499090597. Arrivals time: 0.687553137075156 Scheduler time: 125.5913750869222 Scheduler overhead time: 0.10790900979191065 Adapter cache time: 0.02179765049368143 Engine time: 0.10406206361949444 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 125.81237012473866,
    "estimated_duration": 3600.050424038164,
    "input_throughput": 7098.0536354090555,
    "output_throughput": 6353.44017608058,
    "total_throughput": 13451.493811489636,
    "itl": 96.83155729034426,
    "ttft": 1411225.9118094922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0178835182450765,
    "arrivals": 179046,
    "finished_requests": 103678,
    "scheduler_time": 272.7170929933992
}
#Debug simulation 
Total elapsed time: 125.81254465598613. Arrivals time: 0.6878503230400383 Scheduler time: 124.84860555641353 Scheduler overhead time: 0.10962804267182946 Adapter cache time: 0.02130518713966012 Engine time: 0.10608097957447171 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 126.55320146633312,
    "estimated_duration": 3600.0521262553984,
    "input_throughput": 7098.050279227309,
    "output_throughput": 6353.437171975365,
    "total_throughput": 13451.487451202674,
    "itl": 96.83157592563211,
    "ttft": 1411226.5416946074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0197253596782738,
    "arrivals": 179046,
    "finished_requests": 103678,
    "scheduler_time": 272.71715344634896
}
#Debug simulation 
Total elapsed time: 126.55337106529623. Arrivals time: 0.7004892234690487 Scheduler time: 125.5776945031248 Scheduler overhead time: 0.10921859554946423 Adapter cache time: 0.02161215664818883 Engine time: 0.10531462961807847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 126.92625222913921,
    "estimated_duration": 3600.009473347661,
    "input_throughput": 7094.398553415572,
    "output_throughput": 6350.4294000484715,
    "total_throughput": 13444.827953464044,
    "itl": 96.80398489626188,
    "ttft": 1408650.7246094292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 310,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.969409504537474,
    "arrivals": 179046,
    "finished_requests": 103626,
    "scheduler_time": 272.8124947240865
}
#Debug simulation 
Total elapsed time: 126.92642670916393. Arrivals time: 0.6950824977830052 Scheduler time: 125.95625090552494 Scheduler overhead time: 0.10841081663966179 Adapter cache time: 0.021792857442051172 Engine time: 0.10544101940467954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 126.46046777814627,
    "estimated_duration": 3600.005654256469,
    "input_throughput": 7098.141907023668,
    "output_throughput": 6353.519187659176,
    "total_throughput": 13451.661094682844,
    "itl": 96.83195001749264,
    "ttft": 1411164.956642746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 312,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.032803753465419,
    "arrivals": 179046,
    "finished_requests": 103678,
    "scheduler_time": 272.7113298562386
}
#Debug simulation 
Total elapsed time: 126.46063663437963. Arrivals time: 0.687917566858232 Scheduler time: 125.49777300888672 Scheduler overhead time: 0.10832263668999076 Adapter cache time: 0.021694849245250225 Engine time: 0.10554699320346117 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 129.88038553483784,
    "estimated_duration": 3600.0243383614397,
    "input_throughput": 7085.059044797818,
    "output_throughput": 6327.833053031118,
    "total_throughput": 13412.892097828937,
    "itl": 96.98079389976,
    "ttft": 1414969.1410035067,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9089752896502626,
    "arrivals": 179046,
    "finished_requests": 103500,
    "scheduler_time": 273.0135232162814
}
#Debug simulation 
Total elapsed time: 129.88055560085922. Arrivals time: 0.69222671398893 Scheduler time: 128.91227977676317 Scheduler overhead time: 0.11016586096957326 Adapter cache time: 0.021167932078242302 Engine time: 0.10546712996438146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 1080, 270, 270, 270, 8640, 270, 1080, 8640, 1080, 270, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 270, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 270, 1080, 1080, 1080, 8640, 270, 270, 8640, 270, 8640, 270, 270, 1080, 270, 1080, 8640, 270, 270, 1080, 270, 270, 270, 270, 270, 8640, 1080, 1080, 8640, 1080, 270, 8640, 8640, 8640, 1080, 1080, 270, 270, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 270, 270, 8640, 1080, 8640, 8640, 270, 8640, 270, 1080, 270, 270, 1080, 1080, 8640, 8640, 8640, 270, 8640, 1080, 1080, 270, 270, 270, 270, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 270, 1080, 1080, 8640, 1080, 8640, 270, 270, 270, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 270, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 270, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 538110 . Total input tokens: 120059615 . Total output tokens: 107649944
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 126.88736013276502,
    "estimated_duration": 3600.062493790295,
    "input_throughput": 7076.54882212275,
    "output_throughput": 6312.580695251614,
    "total_throughput": 13389.129517374364,
    "itl": 96.62773694275758,
    "ttft": 1421453.7910429565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 297,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9946733491495305,
    "arrivals": 179046,
    "finished_requests": 103313,
    "scheduler_time": 274.064187589154
}
#Debug simulation 
Total elapsed time: 126.88754606293514. Arrivals time: 0.6876494395546615 Scheduler time: 125.92425251426175 Scheduler overhead time: 0.10937130311504006 Adapter cache time: 0.021329875104129314 Engine time: 0.10536002879962325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 128.7670850022696,
    "estimated_duration": 3600.0285239698824,
    "input_throughput": 7154.694977693316,
    "output_throughput": 6328.786521634407,
    "total_throughput": 13483.481499327723,
    "itl": 98.4635287046547,
    "ttft": 1418364.7656627027,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 283,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8661180315329682,
    "arrivals": 176720,
    "finished_requests": 103704,
    "scheduler_time": 271.9906190493393
}
#Debug simulation 
Total elapsed time: 128.7672633221373. Arrivals time: 0.6848842757754028 Scheduler time: 127.8054403536953 Scheduler overhead time: 0.10831552650779486 Adapter cache time: 0.021570955868810415 Engine time: 0.10773844784125686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 128.7768375617452,
    "estimated_duration": 3600.065786702179,
    "input_throughput": 7162.433835305111,
    "output_throughput": 6345.474320047423,
    "total_throughput": 13507.908155352534,
    "itl": 98.2341852883597,
    "ttft": 1417213.3104438565,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9633753053657763,
    "arrivals": 176720,
    "finished_requests": 103865,
    "scheduler_time": 272.05018321759485
}
#Debug simulation 
Total elapsed time: 128.77701855171472. Arrivals time: 0.6960307904519141 Scheduler time: 127.80284890392795 Scheduler overhead time: 0.11040592240169644 Adapter cache time: 0.021847808733582497 Engine time: 0.10705746989697218 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 128.56146538397297,
    "estimated_duration": 3600.067398184489,
    "input_throughput": 7162.4306292164065,
    "output_throughput": 6345.471479650707,
    "total_throughput": 13507.902108867114,
    "itl": 98.23421694853667,
    "ttft": 1417213.9264055123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9649503117986074,
    "arrivals": 176720,
    "finished_requests": 103865,
    "scheduler_time": 272.0502196934632
}
#Debug simulation 
Total elapsed time: 128.56163920881227. Arrivals time: 0.6947590750642121 Scheduler time: 127.59096249984577 Scheduler overhead time: 0.11008248338475823 Adapter cache time: 0.021381972823292017 Engine time: 0.10558563750237226 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 128.71452524326742,
    "estimated_duration": 3600.022933502179,
    "input_throughput": 7162.519093986875,
    "output_throughput": 6345.549853977388,
    "total_throughput": 13508.068947964262,
    "itl": 98.23345222736849,
    "ttft": 1417195.9555356945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9216985828848586,
    "arrivals": 176720,
    "finished_requests": 103865,
    "scheduler_time": 272.0491067786443
}
#Debug simulation 
Total elapsed time: 128.714701439254. Arrivals time: 0.6993855880573392 Scheduler time: 127.73920419160277 Scheduler overhead time: 0.10884040733799338 Adapter cache time: 0.021675550378859043 Engine time: 0.10632112668827176 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 127.94934580195695,
    "estimated_duration": 3600.0795098627527,
    "input_throughput": 7162.406532788777,
    "output_throughput": 6345.4501317030345,
    "total_throughput": 13507.856664491812,
    "itl": 98.23450250132494,
    "ttft": 1417218.6967867336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9777771980129234,
    "arrivals": 176720,
    "finished_requests": 103865,
    "scheduler_time": 272.05050568797566
}
#Debug simulation 
Total elapsed time: 127.94951902097091. Arrivals time: 0.6991994418203831 Scheduler time: 126.97111635049805 Scheduler overhead time: 0.11068554222583771 Adapter cache time: 0.021414478309452534 Engine time: 0.10755764646455646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 128.7441875091754,
    "estimated_duration": 3600.0117592142146,
    "input_throughput": 7141.57972795393,
    "output_throughput": 6341.520396862002,
    "total_throughput": 13483.100124815932,
    "itl": 98.71986601904548,
    "ttft": 1419026.8282218927,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 281,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8402041328675125,
    "arrivals": 176720,
    "finished_requests": 103536,
    "scheduler_time": 272.6054303048384
}
#Debug simulation 
Total elapsed time: 128.74436079524457. Arrivals time: 0.7002594778314233 Scheduler time: 127.76712537044659 Scheduler overhead time: 0.11001827195286751 Adapter cache time: 0.021800612565129995 Engine time: 0.1062375851906836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 1080, 135, 135, 135, 8640, 135, 1080, 8640, 1080, 135, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 135, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 135, 1080, 1080, 1080, 8640, 135, 135, 8640, 135, 8640, 135, 135, 1080, 135, 1080, 8640, 135, 135, 1080, 135, 135, 135, 135, 135, 8640, 1080, 1080, 8640, 1080, 135, 8640, 8640, 8640, 1080, 1080, 135, 135, 135, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 135, 135, 8640, 1080, 8640, 8640, 135, 8640, 135, 1080, 135, 135, 1080, 1080, 8640, 8640, 8640, 135, 8640, 1080, 1080, 135, 135, 135, 135, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 135, 1080, 1080, 8640, 1080, 8640, 135, 135, 135, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 135, 1080, 1080, 135, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 135, 8640, 135, 8640, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 8640, 1080, 135, 135]
Prompts retrieved: 530955 . Total input tokens: 118470383 . Total output tokens: 106229005
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 127.85074754012749,
    "estimated_duration": 3600.0106623795728,
    "input_throughput": 7162.437953157743,
    "output_throughput": 6345.298984448257,
    "total_throughput": 13507.736937606001,
    "itl": 98.23564795492716,
    "ttft": 1417171.1810268615,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9899753152951652,
    "arrivals": 176720,
    "finished_requests": 103862,
    "scheduler_time": 272.04378006989253
}
#Debug simulation 
Total elapsed time: 127.85092758201063. Arrivals time: 0.703660084400326 Scheduler time: 126.8685984746553 Scheduler overhead time: 0.11094137001782656 Adapter cache time: 0.021642765030264854 Engine time: 0.10660604434087873 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 130.8217518897727,
    "estimated_duration": 3600.0661869229207,
    "input_throughput": 7221.759448323345,
    "output_throughput": 6384.770392137275,
    "total_throughput": 13606.52984046062,
    "itl": 99.98764586447706,
    "ttft": 1406278.3835239848,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8538760805572372,
    "arrivals": 175562,
    "finished_requests": 105028,
    "scheduler_time": 268.0953258064916
}
#Debug simulation 
Total elapsed time: 130.82192680379376. Arrivals time: 0.7064570104703307 Scheduler time: 129.83725327951834 Scheduler overhead time: 0.11085930606350303 Adapter cache time: 0.021457151044160128 Engine time: 0.1069545135833323 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 131.22528607817367,
    "estimated_duration": 3600.000448714708,
    "input_throughput": 7221.089099941976,
    "output_throughput": 6384.640593088295,
    "total_throughput": 13605.729693030271,
    "itl": 99.9883742003,
    "ttft": 1406327.1077892429,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9122657380998183,
    "arrivals": 175562,
    "finished_requests": 105022,
    "scheduler_time": 268.0882565145457
}
#Debug simulation 
Total elapsed time: 131.225461371243. Arrivals time: 0.7046374082565308 Scheduler time: 130.24148059263825 Scheduler overhead time: 0.1102321813814342 Adapter cache time: 0.021420015953481197 Engine time: 0.10850928351283073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 131.08257295098156,
    "estimated_duration": 3600.0017780881653,
    "input_throughput": 7221.086433408797,
    "output_throughput": 6384.638235430642,
    "total_throughput": 13605.72466883944,
    "itl": 99.98839843919694,
    "ttft": 1406327.6531454206,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9135560302622663,
    "arrivals": 175562,
    "finished_requests": 105022,
    "scheduler_time": 268.08829559583324
}
#Debug simulation 
Total elapsed time: 131.082751119975. Arrivals time: 0.7101230775006115 Scheduler time: 130.0964625631459 Scheduler overhead time: 0.10922420723363757 Adapter cache time: 0.021420846227556467 Engine time: 0.1065633618272841 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 131.15551402419806,
    "estimated_duration": 3600.086231345861,
    "input_throughput": 7221.719239286268,
    "output_throughput": 6384.734843255972,
    "total_throughput": 13606.454082542241,
    "itl": 99.98815309412767,
    "ttft": 1406287.7458281838,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8726319922111023,
    "arrivals": 175562,
    "finished_requests": 105028,
    "scheduler_time": 268.0962141634243
}
#Debug simulation 
Total elapsed time: 131.15568979922682. Arrivals time: 0.7169264643453062 Scheduler time: 130.15909171150997 Scheduler overhead time: 0.11077717831358314 Adapter cache time: 0.02133274171501398 Engine time: 0.10743591049686074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 130.8284710701555,
    "estimated_duration": 3600.1138772850277,
    "input_throughput": 7221.663782370856,
    "output_throughput": 6384.685813698273,
    "total_throughput": 13606.349596069129,
    "itl": 99.98881244389766,
    "ttft": 1406289.2035694136,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.925754147544508,
    "arrivals": 175562,
    "finished_requests": 105028,
    "scheduler_time": 268.09235971545206
}
#Debug simulation 
Total elapsed time: 130.82864648010582. Arrivals time: 0.7162583381868899 Scheduler time: 129.83384225144982 Scheduler overhead time: 0.1115065854974091 Adapter cache time: 0.02087473589926958 Engine time: 0.10671638930216432 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 131.35768993897364,
    "estimated_duration": 3600.04544805714,
    "input_throughput": 7221.8010508814405,
    "output_throughput": 6384.807173032992,
    "total_throughput": 13606.608223914433,
    "itl": 99.9870927042477,
    "ttft": 1406269.472727271,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8342240322777081,
    "arrivals": 175562,
    "finished_requests": 105028,
    "scheduler_time": 268.09473918184653
}
#Debug simulation 
Total elapsed time: 131.35786065086722. Arrivals time: 0.7092477013356984 Scheduler time: 130.3721582167782 Scheduler overhead time: 0.10955830989405513 Adapter cache time: 0.02145324368029833 Engine time: 0.10596483759582043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 1080, 66, 66, 66, 8640, 66, 1080, 8640, 1080, 66, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 66, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 66, 1080, 1080, 1080, 8640, 66, 66, 8640, 66, 8640, 66, 66, 1080, 66, 1080, 8640, 66, 66, 1080, 66, 66, 66, 66, 66, 8640, 1080, 1080, 8640, 1080, 66, 8640, 8640, 8640, 1080, 1080, 66, 66, 66, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 66, 66, 8640, 1080, 8640, 8640, 66, 8640, 66, 1080, 66, 66, 1080, 1080, 8640, 8640, 8640, 66, 8640, 1080, 1080, 66, 66, 66, 66, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 66, 1080, 1080, 8640, 1080, 8640, 66, 66, 66, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 66, 1080, 1080, 66, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 66, 8640, 66, 8640, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 8640, 1080, 66, 66]
Prompts retrieved: 527298 . Total input tokens: 117664903 . Total output tokens: 105498308
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 132.20016562379897,
    "estimated_duration": 3600.125933364403,
    "input_throughput": 7221.639598508015,
    "output_throughput": 6384.664432702058,
    "total_throughput": 13606.304031210073,
    "itl": 99.98920005127626,
    "ttft": 1406295.15758204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 279,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9375750034675051,
    "arrivals": 175562,
    "finished_requests": 105028,
    "scheduler_time": 268.0928065172272
}
#Debug simulation 
Total elapsed time: 132.20034131314605. Arrivals time: 0.7200048053637147 Scheduler time: 131.20556203695014 Scheduler overhead time: 0.10911157028749585 Adapter cache time: 0.021195174660533667 Engine time: 0.10526823392137885 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 129.81100250501186,
    "estimated_duration": 3600.062216113403,
    "input_throughput": 7209.029300615419,
    "output_throughput": 6373.015415486165,
    "total_throughput": 13582.044716101584,
    "itl": 99.10734254218093,
    "ttft": 1403338.8573826368,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8630575437890354,
    "arrivals": 174997,
    "finished_requests": 104471,
    "scheduler_time": 268.82487627841505
}
#Debug simulation 
Total elapsed time: 129.81118565099314. Arrivals time: 0.6995602329261601 Scheduler time: 128.83490972733125 Scheduler overhead time: 0.1102428175508976 Adapter cache time: 0.020829597022384405 Engine time: 0.10648536682128906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 130.06169184716418,
    "estimated_duration": 3600.015935239735,
    "input_throughput": 7208.9372566268285,
    "output_throughput": 6372.880124063173,
    "total_throughput": 13581.817380690001,
    "itl": 99.10724053927993,
    "ttft": 1403284.0007884398,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9200101030292034,
    "arrivals": 174997,
    "finished_requests": 104469,
    "scheduler_time": 268.81848219293835
}
#Debug simulation 
Total elapsed time: 130.06187442410737. Arrivals time: 0.7077687010169029 Scheduler time: 129.0765116387047 Scheduler overhead time: 0.11052050767466426 Adapter cache time: 0.021279604639858007 Engine time: 0.10655289283022285 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 130.38508319016546,
    "estimated_duration": 3600.017624231478,
    "input_throughput": 7208.933874466858,
    "output_throughput": 6372.877134149502,
    "total_throughput": 13581.81100861636,
    "itl": 99.10728706537121,
    "ttft": 1403284.6221358767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9216748443245935,
    "arrivals": 174997,
    "finished_requests": 104469,
    "scheduler_time": 268.81850644337834
}
#Debug simulation 
Total elapsed time: 130.38525932189077. Arrivals time: 0.6930858474224806 Scheduler time: 129.413739413023 Scheduler overhead time: 0.11032234691083431 Adapter cache time: 0.021534665022045374 Engine time: 0.10693418979644775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 130.17641480034217,
    "estimated_duration": 3600.0818987417633,
    "input_throughput": 7208.9898868885775,
    "output_throughput": 6372.980572474953,
    "total_throughput": 13581.97045936353,
    "itl": 99.10676311526926,
    "ttft": 1403348.1178619198,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8811935477773682,
    "arrivals": 174997,
    "finished_requests": 104471,
    "scheduler_time": 268.8258333557699
}
#Debug simulation 
Total elapsed time: 130.17659368924797. Arrivals time: 0.7098456253297627 Scheduler time: 129.19036044972017 Scheduler overhead time: 0.10923382686451077 Adapter cache time: 0.021176823414862156 Engine time: 0.10627360315993428 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 129.3259607842192,
    "estimated_duration": 3600.0293299998516,
    "input_throughput": 7208.910434071678,
    "output_throughput": 6372.856412256215,
    "total_throughput": 13581.766846327893,
    "itl": 99.10757757350473,
    "ttft": 1403289.0863390323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9336214540340054,
    "arrivals": 174997,
    "finished_requests": 104469,
    "scheduler_time": 268.8186657563732
}
#Debug simulation 
Total elapsed time: 129.32613321300596. Arrivals time: 0.7022914923727512 Scheduler time: 128.34678294556215 Scheduler overhead time: 0.11097826808691025 Adapter cache time: 0.02095266431570053 Engine time: 0.105967425275594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 126.81866296194494,
    "estimated_duration": 3600.0417988735676,
    "input_throughput": 7209.070185829656,
    "output_throughput": 6373.051559339898,
    "total_throughput": 13582.121745169554,
    "itl": 99.10715013517401,
    "ttft": 1403330.2870202365,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 282,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8431941831624147,
    "arrivals": 174997,
    "finished_requests": 104471,
    "scheduler_time": 268.824322399158
}
#Debug simulation 
Total elapsed time: 126.818836085964. Arrivals time: 0.6372132748365402 Scheduler time: 125.91306707821786 Scheduler overhead time: 0.10601666662842035 Adapter cache time: 0.02052498096600175 Engine time: 0.1029376513324678 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 1080, 33, 33, 33, 8640, 33, 1080, 8640, 1080, 33, 1080, 1080, 1080, 1080, 8640, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 33, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 33, 1080, 1080, 1080, 8640, 33, 33, 8640, 33, 8640, 33, 33, 1080, 33, 1080, 8640, 33, 33, 1080, 33, 33, 33, 33, 33, 8640, 1080, 1080, 8640, 1080, 33, 8640, 8640, 8640, 1080, 1080, 33, 33, 33, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 33, 33, 8640, 1080, 8640, 8640, 33, 8640, 33, 1080, 33, 33, 1080, 1080, 8640, 8640, 8640, 33, 8640, 1080, 1080, 33, 33, 33, 33, 8640, 1080, 8640, 1080, 8640, 8640, 8640, 33, 1080, 1080, 8640, 1080, 8640, 33, 33, 33, 1080, 1080, 1080, 8640, 8640, 1080, 8640, 33, 1080, 1080, 33, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 8640, 33, 8640, 33, 8640, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 8640, 1080, 33, 33]
Prompts retrieved: 525549 . Total input tokens: 117286699 . Total output tokens: 105141760
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 127.83166943397373,
    "estimated_duration": 3600.029503230595,
    "input_throughput": 7200.7798760363485,
    "output_throughput": 6377.593288998487,
    "total_throughput": 13578.373165034836,
    "itl": 99.48887753986749,
    "ttft": 1388341.1058453294,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 260,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8718771235644855,
    "arrivals": 174997,
    "finished_requests": 104355,
    "scheduler_time": 269.52132827922065
}
#Debug simulation 
Total elapsed time: 127.83183956192806. Arrivals time: 0.6024576001800597 Scheduler time: 126.96462571294978 Scheduler overhead time: 0.10458370810374618 Adapter cache time: 0.02041577734053135 Engine time: 0.10130644729360938 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-8/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 122.02207540068775,
    "estimated_duration": 3600.0511909751135,
    "input_throughput": 7054.0663598512565,
    "output_throughput": 6254.94911195991,
    "total_throughput": 13309.015471811166,
    "itl": 97.07875541210667,
    "ttft": 1400984.3444942837,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 327,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0007794922660092,
    "arrivals": 169522,
    "finished_requests": 102297,
    "scheduler_time": 273.7264360441537
}
#Debug simulation 
Total elapsed time: 122.02224819594994. Arrivals time: 0.5858296020887792 Scheduler time: 121.17029779916629 Scheduler overhead time: 0.1046030237339437 Adapter cache time: 0.02102983184158802 Engine time: 0.10151197900995612 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-16/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 124.10974409105256,
    "estimated_duration": 3600.018778704466,
    "input_throughput": 7037.8310662917565,
    "output_throughput": 6261.196506344779,
    "total_throughput": 13299.027572636536,
    "itl": 97.46960650691003,
    "ttft": 1404066.610335794,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.945973481980623,
    "arrivals": 169522,
    "finished_requests": 102376,
    "scheduler_time": 274.1555867678429
}
#Debug simulation 
Total elapsed time: 124.1099187922664. Arrivals time: 0.594569452572614 Scheduler time: 123.24547889316455 Scheduler overhead time: 0.10717166820541024 Adapter cache time: 0.020629805512726307 Engine time: 0.1022352953441441 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-8-32/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 124.04762940108776,
    "estimated_duration": 3600.020548525949,
    "input_throughput": 7037.827606393557,
    "output_throughput": 6261.193428251214,
    "total_throughput": 13299.021034644771,
    "itl": 97.46965317570299,
    "ttft": 1404067.2726377265,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9477092325873722,
    "arrivals": 169522,
    "finished_requests": 102376,
    "scheduler_time": 274.1556208387089
}
#Debug simulation 
Total elapsed time: 124.0478045521304. Arrivals time: 0.5941447340883315 Scheduler time: 123.17183354264125 Scheduler overhead time: 0.10603465791791677 Adapter cache time: 0.020752036478370428 Engine time: 0.1156417764723301 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-16/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 123.86701421206817,
    "estimated_duration": 3600.057538037806,
    "input_throughput": 7038.385284755943,
    "output_throughput": 6261.251594408069,
    "total_throughput": 13299.636879164012,
    "itl": 97.46906505113986,
    "ttft": 1404058.0496411482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9063397360919072,
    "arrivals": 169522,
    "finished_requests": 102380,
    "scheduler_time": 274.1618467623372
}
#Debug simulation 
Total elapsed time: 123.86718988372013. Arrivals time: 0.5935790711082518 Scheduler time: 123.00871135573834 Scheduler overhead time: 0.10428362572565675 Adapter cache time: 0.020290684886276722 Engine time: 0.10133973136544228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_8-16-32/adapters_160_slots_16_rate_0.8-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 124.66494427761063,
    "estimated_duration": 3600.0332355940177,
    "input_throughput": 7037.802804012008,
    "output_throughput": 6261.1713628473635,
    "total_throughput": 13298.974166859372,
    "itl": 97.46998955133442,
    "ttft": 1404072.1735231201,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9599073498696138,
    "arrivals": 169522,
    "finished_requests": 102376,
    "scheduler_time": 274.15590971234457
}
#Debug simulation 
Total elapsed time: 124.66511237295344. Arrivals time: 0.6052530179731548 Scheduler time: 123.79056634148583 Scheduler overhead time: 0.1067634797655046 Adapter cache time: 0.021143397316336632 Engine time: 0.10218506958335638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_160_slots_16_rate_0.8-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-16/adapters_160_slots_16_rate_0.8-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 121.45184128638357,
    "estimated_duration": 3600.027840840282,
    "input_throughput": 7054.112113219812,
    "output_throughput": 6254.989682175359,
    "total_throughput": 13309.101795395172,
    "itl": 97.07850242220486,
    "ttft": 1400976.061324636,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 327,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9777464464330128,
    "arrivals": 169522,
    "finished_requests": 102297,
    "scheduler_time": 273.72590166021064
}
#Debug simulation 
Total elapsed time: 121.45201422832906. Arrivals time: 0.591992424800992 Scheduler time: 120.59282000083476 Scheduler overhead time: 0.10466476017609239 Adapter cache time: 0.02117493748664856 Engine time: 0.10177864832803607 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_160_slots_16_rate_0.8-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.025_size_16-16-32/adapters_160_slots_16_rate_0.8-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.8  ]. Counts: [53 53 54]
Adapter prompts. [270, 8640, 270, 270, 540, 270, 270, 270, 8640, 270, 540, 8640, 540, 270, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 270, 8640, 540, 8640, 8640, 8640, 540, 8640, 270, 540, 540, 540, 8640, 270, 270, 8640, 270, 8640, 270, 270, 540, 270, 540, 8640, 270, 270, 540, 270, 270, 270, 270, 270, 8640, 540, 540, 8640, 540, 270, 8640, 8640, 8640, 540, 540, 270, 270, 270, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 270, 270, 8640, 540, 8640, 8640, 270, 8640, 270, 540, 270, 270, 540, 540, 8640, 8640, 8640, 270, 8640, 540, 540, 270, 270, 270, 270, 8640, 540, 8640, 540, 8640, 8640, 8640, 270, 540, 540, 8640, 540, 8640, 270, 270, 270, 540, 540, 540, 8640, 8640, 540, 8640, 270, 540, 540, 270, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 270, 8640, 270, 8640, 540, 270, 540, 270, 270, 540, 540, 540, 8640, 540, 270, 270]
Prompts retrieved: 509490 . Total input tokens: 113701235 . Total output tokens: 101957884
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 124.58398114610463,
    "estimated_duration": 3600.046418551835,
    "input_throughput": 7037.777032383894,
    "output_throughput": 6261.1484351546715,
    "total_throughput": 13298.925467538565,
    "itl": 97.47035146270751,
    "ttft": 1404077.6341174275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 290,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9719797133654408,
    "arrivals": 169522,
    "finished_requests": 102376,
    "scheduler_time": 274.1563241198759
}
#Debug simulation 
Total elapsed time: 124.58415140397847. Arrivals time: 0.5936132059432566 Scheduler time: 123.72124806698412 Scheduler overhead time: 0.10622210055589676 Adapter cache time: 0.021137420553714037 Engine time: 0.10265199141576886 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 124.62981456425041,
    "estimated_duration": 3600.1101982557993,
    "input_throughput": 7154.238226507185,
    "output_throughput": 6281.707712990718,
    "total_throughput": 13435.945939497904,
    "itl": 97.73565885162299,
    "ttft": 1388706.971162359,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9824165658024127,
    "arrivals": 167239,
    "finished_requests": 103493,
    "scheduler_time": 271.021429519593
}
#Debug simulation 
Total elapsed time: 124.62998240813613. Arrivals time: 0.5927408374845982 Scheduler time: 123.76724954508245 Scheduler overhead time: 0.10614489577710629 Adapter cache time: 0.021266696508973837 Engine time: 0.1032523475587368 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 124.94651573710144,
    "estimated_duration": 3600.0389163085147,
    "input_throughput": 7154.470992888772,
    "output_throughput": 6281.868481352259,
    "total_throughput": 13436.339474241031,
    "itl": 97.74047322512216,
    "ttft": 1388609.6917318366,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0458898737886977,
    "arrivals": 167239,
    "finished_requests": 103489,
    "scheduler_time": 271.00606051559447
}
#Debug simulation 
Total elapsed time: 124.94668232602999. Arrivals time: 0.5956943947821856 Scheduler time: 124.08395287301391 Scheduler overhead time: 0.10475642699748278 Adapter cache time: 0.021190593019127846 Engine time: 0.1023154747672379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 124.42359185684472,
    "estimated_duration": 3600.040506290603,
    "input_throughput": 7154.467833068568,
    "output_throughput": 6281.865706922818,
    "total_throughput": 13436.333539991387,
    "itl": 97.74050457312794,
    "ttft": 1388610.369456285,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0474459854140934,
    "arrivals": 167239,
    "finished_requests": 103489,
    "scheduler_time": 271.0060943860437
}
#Debug simulation 
Total elapsed time: 124.42375890817493. Arrivals time: 0.5959766288287938 Scheduler time: 123.56069360300899 Scheduler overhead time: 0.10565178841352463 Adapter cache time: 0.021418833173811436 Engine time: 0.10126746166497469 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 125.04394330689684,
    "estimated_duration": 3600.01899174807,
    "input_throughput": 7154.292813187019,
    "output_throughput": 6281.6888054857645,
    "total_throughput": 13435.981618672784,
    "itl": 97.7363008309047,
    "ttft": 1388678.3450353171,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 321,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0043430343736,
    "arrivals": 167239,
    "finished_requests": 103492,
    "scheduler_time": 271.01352844695117
}
#Debug simulation 
Total elapsed time: 125.04411560576409. Arrivals time: 0.5903993607498705 Scheduler time: 124.18649660190567 Scheduler overhead time: 0.10607939166948199 Adapter cache time: 0.020782550796866417 Engine time: 0.10137892002239823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 125.53537269402295,
    "estimated_duration": 3600.005811841305,
    "input_throughput": 7139.203752244653,
    "output_throughput": 6278.857363410397,
    "total_throughput": 13418.06111565505,
    "itl": 97.4465065952043,
    "ttft": 1373981.658005988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0251879545487517,
    "arrivals": 167239,
    "finished_requests": 103151,
    "scheduler_time": 272.2061480772588
}
#Debug simulation 
Total elapsed time: 125.5355510096997. Arrivals time: 0.5930670970119536 Scheduler time: 124.67333352239802 Scheduler overhead time: 0.10656774928793311 Adapter cache time: 0.02134018950164318 Engine time: 0.1022661766037345 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 126.102096718736,
    "estimated_duration": 3600.0218230112255,
    "input_throughput": 7139.2117224729,
    "output_throughput": 6278.990548199662,
    "total_throughput": 13418.202270672562,
    "itl": 97.44327930113795,
    "ttft": 1373935.7740878835,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9239255411247735,
    "arrivals": 167239,
    "finished_requests": 103153,
    "scheduler_time": 272.2198766095472
}
#Debug simulation 
Total elapsed time: 126.10227297386155. Arrivals time: 0.5903361067175865 Scheduler time: 125.24467448191717 Scheduler overhead time: 0.10623480519279838 Adapter cache time: 0.0209427191875875 Engine time: 0.10152063611894846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 540, 135, 135, 135, 8640, 135, 540, 8640, 540, 135, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 135, 8640, 540, 8640, 8640, 8640, 540, 8640, 135, 540, 540, 540, 8640, 135, 135, 8640, 135, 8640, 135, 135, 540, 135, 540, 8640, 135, 135, 540, 135, 135, 135, 135, 135, 8640, 540, 540, 8640, 540, 135, 8640, 8640, 8640, 540, 540, 135, 135, 135, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 135, 135, 8640, 540, 8640, 8640, 135, 8640, 135, 540, 135, 135, 540, 540, 8640, 8640, 8640, 135, 8640, 540, 540, 135, 135, 135, 135, 8640, 540, 8640, 540, 8640, 8640, 8640, 135, 540, 540, 8640, 540, 8640, 135, 135, 135, 540, 540, 540, 8640, 8640, 540, 8640, 135, 540, 540, 135, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 135, 8640, 135, 8640, 540, 135, 540, 135, 135, 540, 540, 540, 8640, 540, 135, 135]
Prompts retrieved: 502335 . Total input tokens: 112121278 . Total output tokens: 100538812
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 124.99357643397525,
    "estimated_duration": 3600.019253009485,
    "input_throughput": 7139.177097043232,
    "output_throughput": 6278.833920430827,
    "total_throughput": 13418.011017474058,
    "itl": 97.44668695161555,
    "ttft": 1373986.9370366111,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 309,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0381405945494822,
    "arrivals": 167239,
    "finished_requests": 103151,
    "scheduler_time": 272.20645292936405
}
#Debug simulation 
Total elapsed time: 124.99374551186338. Arrivals time: 0.5843829093500972 Scheduler time: 124.14035369548947 Scheduler overhead time: 0.10623917728662491 Adapter cache time: 0.020994334015995264 Engine time: 0.10233967378735542 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 127.65005786297843,
    "estimated_duration": 3600.0757949210843,
    "input_throughput": 7134.923946945139,
    "output_throughput": 6367.8859296079145,
    "total_throughput": 13502.809876553052,
    "itl": 98.76141204631051,
    "ttft": 1353557.7274569622,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9028438844601612,
    "arrivals": 166072,
    "finished_requests": 104145,
    "scheduler_time": 265.54155697005496
}
#Debug simulation 
Total elapsed time: 127.65022888081148. Arrivals time: 0.5904948143288493 Scheduler time: 126.79428134579211 Scheduler overhead time: 0.10408247681334615 Adapter cache time: 0.02064564125612378 Engine time: 0.10225667571648955 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 128.0217987713404,
    "estimated_duration": 3600.0549262984873,
    "input_throughput": 7157.366631206675,
    "output_throughput": 6374.881903148268,
    "total_throughput": 13532.248534354943,
    "itl": 98.53349290167729,
    "ttft": 1360717.8773035887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.960385255070874,
    "arrivals": 166072,
    "finished_requests": 104455,
    "scheduler_time": 264.36332472685825
}
#Debug simulation 
Total elapsed time: 128.02197062410414. Arrivals time: 0.6000726358033717 Scheduler time: 127.15600768616423 Scheduler overhead time: 0.10528801986947656 Adapter cache time: 0.02081061201170087 Engine time: 0.10102248843759298 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 128.28804663894698,
    "estimated_duration": 3600.0565003455936,
    "input_throughput": 7157.3635018023915,
    "output_throughput": 6374.879115868565,
    "total_throughput": 13532.242617670958,
    "itl": 98.53352027301382,
    "ttft": 1360718.5397988434,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9619067929498901,
    "arrivals": 166072,
    "finished_requests": 104455,
    "scheduler_time": 264.3633772360755
}
#Debug simulation 
Total elapsed time: 128.28822823101655. Arrivals time: 0.6046486664563417 Scheduler time: 127.41672617895529 Scheduler overhead time: 0.10515161463990808 Adapter cache time: 0.021049936767667532 Engine time: 0.10140133695676923 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 127.2410563393496,
    "estimated_duration": 3600.019905027968,
    "input_throughput": 7134.698884338657,
    "output_throughput": 6367.5909035902705,
    "total_throughput": 13502.289787928928,
    "itl": 98.76135728976166,
    "ttft": 1353651.6168886323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9249673454323818,
    "arrivals": 166072,
    "finished_requests": 104140,
    "scheduler_time": 265.535262595211
}
#Debug simulation 
Total elapsed time: 127.24123044032604. Arrivals time: 0.5862845927476883 Scheduler time: 126.39155156817287 Scheduler overhead time: 0.10399970877915621 Adapter cache time: 0.020556598901748657 Engine time: 0.1005326034501195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 127.84609556896612,
    "estimated_duration": 3600.0694813529503,
    "input_throughput": 7157.337694025971,
    "output_throughput": 6374.856129547571,
    "total_throughput": 13532.193823573542,
    "itl": 98.53379150776298,
    "ttft": 1360724.1630487654,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9741049102321317,
    "arrivals": 166072,
    "finished_requests": 104455,
    "scheduler_time": 264.3636599332596
}
#Debug simulation 
Total elapsed time: 127.84627172304317. Arrivals time: 0.5818695439957082 Scheduler time: 126.99864281294867 Scheduler overhead time: 0.10408621234819293 Adapter cache time: 0.020687140990048647 Engine time: 0.10225458163768053 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 127.38396023819223,
    "estimated_duration": 3600.0545455988135,
    "input_throughput": 7134.96606083436,
    "output_throughput": 6367.923516055172,
    "total_throughput": 13502.889576889533,
    "itl": 98.76081255838635,
    "ttft": 1353548.785430013,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 295,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.882064836996143,
    "arrivals": 166072,
    "finished_requests": 104145,
    "scheduler_time": 265.5409866566154
}
#Debug simulation 
Total elapsed time: 127.38413547212258. Arrivals time: 0.5710641192272305 Scheduler time: 126.54805291304365 Scheduler overhead time: 0.10350138414651155 Adapter cache time: 0.020364738535135984 Engine time: 0.10212521580979228 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 540, 66, 66, 66, 8640, 66, 540, 8640, 540, 66, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 66, 8640, 540, 8640, 8640, 8640, 540, 8640, 66, 540, 540, 540, 8640, 66, 66, 8640, 66, 8640, 66, 66, 540, 66, 540, 8640, 66, 66, 540, 66, 66, 66, 66, 66, 8640, 540, 540, 8640, 540, 66, 8640, 8640, 8640, 540, 540, 66, 66, 66, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 66, 66, 8640, 540, 8640, 8640, 66, 8640, 66, 540, 66, 66, 540, 540, 8640, 8640, 8640, 66, 8640, 540, 540, 66, 66, 66, 66, 8640, 540, 8640, 540, 8640, 8640, 8640, 66, 540, 540, 8640, 540, 8640, 66, 66, 66, 540, 540, 540, 8640, 8640, 540, 8640, 66, 540, 540, 66, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 66, 8640, 66, 8640, 540, 66, 540, 66, 66, 540, 540, 540, 8640, 540, 66, 66]
Prompts retrieved: 498678 . Total input tokens: 111319766 . Total output tokens: 99798210
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 126.76762057095766,
    "estimated_duration": 3600.0718410068407,
    "input_throughput": 7188.983482274575,
    "output_throughput": 6401.662805027398,
    "total_throughput": 13590.646287301974,
    "itl": 99.22599403697414,
    "ttft": 1358714.5733501604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 302,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0122059497982319,
    "arrivals": 166072,
    "finished_requests": 104808,
    "scheduler_time": 263.3913145132634
}
#Debug simulation 
Total elapsed time: 126.7677929373458. Arrivals time: 0.5783756719902158 Scheduler time: 125.92563769919798 Scheduler overhead time: 0.10401849541813135 Adapter cache time: 0.020621702075004578 Engine time: 0.10043019568547606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 128.42896960070357,
    "estimated_duration": 3600.0519971056196,
    "input_throughput": 7206.8725731904515,
    "output_throughput": 6381.856989418909,
    "total_throughput": 13588.72956260936,
    "itl": 99.6336829846387,
    "ttft": 1355669.1788570185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8110292521421787,
    "arrivals": 165445,
    "finished_requests": 104248,
    "scheduler_time": 265.65500130759074
}
#Debug simulation 
Total elapsed time: 128.42914337664843. Arrivals time: 0.5790876471437514 Scheduler time: 127.58410569885746 Scheduler overhead time: 0.10525515908375382 Adapter cache time: 0.020777068100869656 Engine time: 0.10127700632438064 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 127.24556662375107,
    "estimated_duration": 3600.081003088377,
    "input_throughput": 7213.506578802524,
    "output_throughput": 6352.862332925277,
    "total_throughput": 13566.3689117278,
    "itl": 99.47690626523375,
    "ttft": 1378865.9885473156,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8858937638299585,
    "arrivals": 165445,
    "finished_requests": 104237,
    "scheduler_time": 265.4023678595973
}
#Debug simulation 
Total elapsed time: 127.2457375638187. Arrivals time: 0.5883353650569916 Scheduler time: 126.39370396360755 Scheduler overhead time: 0.10451096436008811 Adapter cache time: 0.02029992314055562 Engine time: 0.10095178615301847 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 127.10922006890178,
    "estimated_duration": 3600.0823091511606,
    "input_throughput": 7213.503961836669,
    "output_throughput": 6352.860028189899,
    "total_throughput": 13566.363990026568,
    "itl": 99.47694008632325,
    "ttft": 1378866.517135372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8871843945048794,
    "arrivals": 165445,
    "finished_requests": 104237,
    "scheduler_time": 265.40238329169546
}
#Debug simulation 
Total elapsed time: 127.10939595196396. Arrivals time: 0.5792381591163576 Scheduler time: 126.26691750017926 Scheduler overhead time: 0.10346332844346762 Adapter cache time: 0.020216153468936682 Engine time: 0.10144509887322783 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 126.51998203992844,
    "estimated_duration": 3600.043020549286,
    "input_throughput": 7213.582685475152,
    "output_throughput": 6352.929359302608,
    "total_throughput": 13566.512044777759,
    "itl": 99.47584677391261,
    "ttft": 1378850.2311702215,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8483029945334445,
    "arrivals": 165445,
    "finished_requests": 104237,
    "scheduler_time": 265.4015702189065
}
#Debug simulation 
Total elapsed time: 126.52024973975495. Arrivals time: 0.5817456752993166 Scheduler time: 125.67697347747162 Scheduler overhead time: 0.10383455455303192 Adapter cache time: 0.01981158973649144 Engine time: 0.09959114110097289 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 127.13451889390126,
    "estimated_duration": 3600.095478897953,
    "input_throughput": 7213.477573641906,
    "output_throughput": 6352.836788373492,
    "total_throughput": 13566.314362015397,
    "itl": 99.47719837668153,
    "ttft": 1378872.1371008372,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 271,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8987537428550469,
    "arrivals": 165445,
    "finished_requests": 104237,
    "scheduler_time": 265.40268318860416
}
#Debug simulation 
Total elapsed time: 127.13469278113917. Arrivals time: 0.5724875531159341 Scheduler time: 126.29980647796765 Scheduler overhead time: 0.10501654073596 Adapter cache time: 0.019401407334953547 Engine time: 0.09983390895649791 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 128.27055694395676,
    "estimated_duration": 3600.0329773490057,
    "input_throughput": 7206.910648664524,
    "output_throughput": 6381.890706156352,
    "total_throughput": 13588.801354820876,
    "itl": 99.63323251664467,
    "ttft": 1355661.2007262104,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 265,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7923633281490776,
    "arrivals": 165445,
    "finished_requests": 104248,
    "scheduler_time": 265.6545474363463
}
#Debug simulation 
Total elapsed time: 128.27073253365234. Arrivals time: 0.5714351702481508 Scheduler time: 127.43483441742137 Scheduler overhead time: 0.10374195594340563 Adapter cache time: 0.020091596990823746 Engine time: 0.1010858858935535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.05-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 540, 33, 33, 33, 8640, 33, 540, 8640, 540, 33, 540, 540, 540, 540, 8640, 8640, 8640, 540, 540, 540, 8640, 8640, 33, 8640, 540, 8640, 8640, 8640, 540, 8640, 33, 540, 540, 540, 8640, 33, 33, 8640, 33, 8640, 33, 33, 540, 33, 540, 8640, 33, 33, 540, 33, 33, 33, 33, 33, 8640, 540, 540, 8640, 540, 33, 8640, 8640, 8640, 540, 540, 33, 33, 33, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 33, 33, 8640, 540, 8640, 8640, 33, 8640, 33, 540, 33, 33, 540, 540, 8640, 8640, 8640, 33, 8640, 540, 540, 33, 33, 33, 33, 8640, 540, 8640, 540, 8640, 8640, 8640, 33, 540, 540, 8640, 540, 8640, 33, 33, 33, 540, 540, 540, 8640, 8640, 540, 8640, 33, 540, 540, 33, 8640, 8640, 8640, 8640, 8640, 8640, 540, 540, 540, 540, 540, 8640, 33, 8640, 33, 8640, 540, 33, 540, 33, 33, 540, 540, 540, 8640, 540, 33, 33]
Prompts retrieved: 496929 . Total input tokens: 110918909 . Total output tokens: 99458983
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 125.3118281038478,
    "estimated_duration": 3600.089183752143,
    "input_throughput": 7149.660935114238,
    "output_throughput": 6321.17534829569,
    "total_throughput": 13470.836283409928,
    "itl": 98.37956658274503,
    "ttft": 1372672.9007918555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 296,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9948708392679748,
    "arrivals": 165445,
    "finished_requests": 103394,
    "scheduler_time": 268.3255453500004
}
#Debug simulation 
Total elapsed time: 125.31198712484911. Arrivals time: 0.5656377361156046 Scheduler time: 124.48330276366323 Scheduler overhead time: 0.10413802089169621 Adapter cache time: 0.02013543201610446 Engine time: 0.09996443754062057 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 129.74249014211819,
    "estimated_duration": 3600.1091607150315,
    "input_throughput": 7105.85675544324,
    "output_throughput": 6281.096764162789,
    "total_throughput": 13386.95351960603,
    "itl": 99.62128174759721,
    "ttft": 1371575.7439890455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 268,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8202107153739769,
    "arrivals": 162486,
    "finished_requests": 102952,
    "scheduler_time": 269.35471588721674
}
#Debug simulation 
Total elapsed time: 129.74264942808077. Arrivals time: 0.582165040075779 Scheduler time: 128.8979969616048 Scheduler overhead time: 0.10505818529054523 Adapter cache time: 0.018901127390563488 Engine time: 0.10060420352965593 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 127.78219446120784,
    "estimated_duration": 3600.003111324255,
    "input_throughput": 7173.166300542696,
    "output_throughput": 6354.150341716089,
    "total_throughput": 13527.316642258786,
    "itl": 100.38488881020497,
    "ttft": 1354156.0320924635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8003891451121343,
    "arrivals": 162486,
    "finished_requests": 103963,
    "scheduler_time": 264.8320726967734
}
#Debug simulation 
Total elapsed time: 127.78235335834324. Arrivals time: 0.5720311626791954 Scheduler time: 126.94801668077707 Scheduler overhead time: 0.10456303367391229 Adapter cache time: 0.018711354583501816 Engine time: 0.1002513780258596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 125.460381987039,
    "estimated_duration": 3600.0041714624144,
    "input_throughput": 7173.164188170888,
    "output_throughput": 6354.148470530134,
    "total_throughput": 13527.312658701021,
    "itl": 100.38490115052905,
    "ttft": 1354156.4627759282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8016452020406761,
    "arrivals": 162486,
    "finished_requests": 103963,
    "scheduler_time": 264.83208257173294
}
#Debug simulation 
Total elapsed time: 125.46053603338078. Arrivals time: 0.5000428878702223 Scheduler time: 124.71423142356798 Scheduler overhead time: 0.09975350089371204 Adapter cache time: 0.016994531266391277 Engine time: 0.09323769109323621 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 128.60910352971405,
    "estimated_duration": 3600.0618579652037,
    "input_throughput": 7152.131828799501,
    "output_throughput": 6338.08609413637,
    "total_throughput": 13490.21792293587,
    "itl": 100.00358054365809,
    "ttft": 1352632.6240810493,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7421467360039263,
    "arrivals": 162486,
    "finished_requests": 103647,
    "scheduler_time": 266.1143927713584
}
#Debug simulation 
Total elapsed time: 128.60926740290597. Arrivals time: 0.5141336764208972 Scheduler time: 127.84762256918475 Scheduler overhead time: 0.09964655712246895 Adapter cache time: 0.01726539619266987 Engine time: 0.09406113857403398 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 125.55734846601263,
    "estimated_duration": 3600.0140315982376,
    "input_throughput": 7173.144541477137,
    "output_throughput": 6354.131067051589,
    "total_throughput": 13527.275608528726,
    "itl": 100.38523259223392,
    "ttft": 1354160.997161312,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 245,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8119570125266945,
    "arrivals": 162486,
    "finished_requests": 103963,
    "scheduler_time": 264.8324312057181
}
#Debug simulation 
Total elapsed time: 125.55750983115286. Arrivals time: 0.5141853694804013 Scheduler time: 124.79526399914175 Scheduler overhead time: 0.09971665916964412 Adapter cache time: 0.017500951420515776 Engine time: 0.09433767013251781 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 129.6962322210893,
    "estimated_duration": 3600.076308558368,
    "input_throughput": 7159.506852320409,
    "output_throughput": 6331.295518879542,
    "total_throughput": 13490.802371199952,
    "itl": 100.14288371150687,
    "ttft": 1367095.2982194177,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 242,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7235921713663275,
    "arrivals": 162486,
    "finished_requests": 103794,
    "scheduler_time": 265.78997707843394
}
#Debug simulation 
Total elapsed time: 129.69640004215762. Arrivals time: 0.5139079615473747 Scheduler time: 128.93536481866613 Scheduler overhead time: 0.09925376810133457 Adapter cache time: 0.018166138790547848 Engine time: 0.09352367138490081 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  0.8   ]. Counts: [53 53 54]
Adapter prompts. [135, 8640, 135, 135, 270, 135, 135, 135, 8640, 135, 270, 8640, 270, 135, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 135, 8640, 270, 8640, 8640, 8640, 270, 8640, 135, 270, 270, 270, 8640, 135, 135, 8640, 135, 8640, 135, 135, 270, 135, 270, 8640, 135, 135, 270, 135, 135, 135, 135, 135, 8640, 270, 270, 8640, 270, 135, 8640, 8640, 8640, 270, 270, 135, 135, 135, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 135, 135, 8640, 270, 8640, 8640, 135, 8640, 135, 270, 135, 135, 270, 270, 8640, 8640, 8640, 135, 8640, 270, 270, 135, 135, 135, 135, 8640, 270, 8640, 270, 8640, 8640, 8640, 135, 270, 270, 8640, 270, 8640, 135, 135, 135, 270, 270, 270, 8640, 8640, 270, 8640, 135, 270, 270, 135, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 135, 8640, 135, 8640, 270, 135, 270, 135, 135, 270, 270, 270, 8640, 270, 135, 135]
Prompts retrieved: 488025 . Total input tokens: 108923580 . Total output tokens: 97705388
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 127.70126203866675,
    "estimated_duration": 3600.035108913856,
    "input_throughput": 7111.853975147619,
    "output_throughput": 6317.827829979711,
    "total_throughput": 13429.68180512733,
    "itl": 99.24062492456503,
    "ttft": 1352399.5012356774,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 252,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8452139663696326,
    "arrivals": 162486,
    "finished_requests": 103012,
    "scheduler_time": 268.7695661929069
}
#Debug simulation 
Total elapsed time: 127.70142165990546. Arrivals time: 0.5069131618365645 Scheduler time: 126.9458277230151 Scheduler overhead time: 0.10044362582266331 Adapter cache time: 0.017632065806537867 Engine time: 0.09331703698262572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 123.64957976015285,
    "estimated_duration": 3600.0540403683262,
    "input_throughput": 7236.3905396638565,
    "output_throughput": 6356.236807394933,
    "total_throughput": 13592.62734705879,
    "itl": 99.43852100596769,
    "ttft": 1352694.9217686476,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.759000960495322,
    "arrivals": 161281,
    "finished_requests": 104689,
    "scheduler_time": 263.8044761113329
}
#Debug simulation 
Total elapsed time: 123.64974434720352. Arrivals time: 0.5158399813808501 Scheduler time: 122.88920017937198 Scheduler overhead time: 0.09889855980873108 Adapter cache time: 0.01728937542065978 Engine time: 0.09218570170924067 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 123.8580609341152,
    "estimated_duration": 3600.106873468999,
    "input_throughput": 7236.284342552681,
    "output_throughput": 6356.143526914396,
    "total_throughput": 13592.427869467076,
    "itl": 99.43923532972828,
    "ttft": 1352717.9712657183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8109936772706026,
    "arrivals": 161281,
    "finished_requests": 104689,
    "scheduler_time": 263.80584377369684
}
#Debug simulation 
Total elapsed time: 123.85822530928999. Arrivals time: 0.5086548295803368 Scheduler time: 123.10410609841347 Scheduler overhead time: 0.09910525334998965 Adapter cache time: 0.017244895920157433 Engine time: 0.09274931158870459 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 124.49488830007613,
    "estimated_duration": 3600.108059454522,
    "input_throughput": 7236.28195869966,
    "output_throughput": 6356.1414330066345,
    "total_throughput": 13592.423391706294,
    "itl": 99.43924787136312,
    "ttft": 1352718.611687297,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8121247485652605,
    "arrivals": 161281,
    "finished_requests": 104689,
    "scheduler_time": 263.8058986879163
}
#Debug simulation 
Total elapsed time: 124.49503714079037. Arrivals time: 0.5160140725784004 Scheduler time: 123.73152197198942 Scheduler overhead time: 0.09964455012232065 Adapter cache time: 0.017898979131132364 Engine time: 0.09376646718010306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 123.71433326415718,
    "estimated_duration": 3600.0697662726657,
    "input_throughput": 7236.358929502727,
    "output_throughput": 6356.209041940794,
    "total_throughput": 13592.567971443521,
    "itl": 99.4389551250204,
    "ttft": 1352701.109209993,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.775854479884731,
    "arrivals": 161281,
    "finished_requests": 104689,
    "scheduler_time": 263.8046489977948
}
#Debug simulation 
Total elapsed time: 123.71447785524651. Arrivals time: 0.508799129165709 Scheduler time: 122.96047123475 Scheduler overhead time: 0.09921484254300594 Adapter cache time: 0.017613775562494993 Engine time: 0.09246151614934206 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 123.91529197711498,
    "estimated_duration": 3600.119033119318,
    "input_throughput": 7236.259901503258,
    "output_throughput": 6356.122058601278,
    "total_throughput": 13592.381960104536,
    "itl": 99.43959116906593,
    "ttft": 1352723.4019682663,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8229395741969386,
    "arrivals": 161281,
    "finished_requests": 104689,
    "scheduler_time": 263.80627645528745
}
#Debug simulation 
Total elapsed time: 123.91544415801764. Arrivals time: 0.5267268484458327 Scheduler time: 123.14188033668324 Scheduler overhead time: 0.09989992203190923 Adapter cache time: 0.017353917006403208 Engine time: 0.09234332852065563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 123.9545184019953,
    "estimated_duration": 3600.035482116242,
    "input_throughput": 7236.427843396134,
    "output_throughput": 6356.26957391781,
    "total_throughput": 13592.697417313944,
    "itl": 99.43814957092053,
    "ttft": 1352686.4144894946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7415324731357406,
    "arrivals": 161281,
    "finished_requests": 104689,
    "scheduler_time": 263.8038865394663
}
#Debug simulation 
Total elapsed time: 123.95466904295608. Arrivals time: 0.5156334284693003 Scheduler time: 123.19098082836717 Scheduler overhead time: 0.10017927456647158 Adapter cache time: 0.017389181070029736 Engine time: 0.09354170178994536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 270, 66, 66, 66, 8640, 66, 270, 8640, 270, 66, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 66, 8640, 270, 8640, 8640, 8640, 270, 8640, 66, 270, 270, 270, 8640, 66, 66, 8640, 66, 8640, 66, 66, 270, 66, 270, 8640, 66, 66, 270, 66, 66, 66, 66, 66, 8640, 270, 270, 8640, 270, 66, 8640, 8640, 8640, 270, 270, 66, 66, 66, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 66, 66, 8640, 270, 8640, 8640, 66, 8640, 66, 270, 66, 66, 270, 270, 8640, 8640, 8640, 66, 8640, 270, 270, 66, 66, 66, 66, 8640, 270, 8640, 270, 8640, 8640, 8640, 66, 270, 270, 8640, 270, 8640, 66, 66, 66, 270, 270, 270, 8640, 8640, 270, 8640, 66, 270, 270, 66, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 66, 8640, 66, 8640, 270, 66, 270, 66, 66, 270, 270, 270, 8640, 270, 66, 66]
Prompts retrieved: 484368 . Total input tokens: 108100252 . Total output tokens: 96978099
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 123.81112276110798,
    "estimated_duration": 3600.00288775773,
    "input_throughput": 7236.319195350919,
    "output_throughput": 6356.069345892115,
    "total_throughput": 13592.388541243034,
    "itl": 99.43966080082224,
    "ttft": 1352740.6539490386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 248,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8335028922557871,
    "arrivals": 161281,
    "finished_requests": 104685,
    "scheduler_time": 263.79770465621067
}
#Debug simulation 
Total elapsed time: 123.81127545703202. Arrivals time: 0.5101152779534459 Scheduler time: 123.05667003430426 Scheduler overhead time: 0.09883212158456445 Adapter cache time: 0.0174106378108263 Engine time: 0.0922604058869183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 122.03817417100072,
    "estimated_duration": 3600.1102365606166,
    "input_throughput": 7173.242012908896,
    "output_throughput": 6370.565203000788,
    "total_throughput": 13543.807215909685,
    "itl": 101.30028084321903,
    "ttft": 1346670.947420809,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.783484862446784,
    "arrivals": 160680,
    "finished_requests": 104527,
    "scheduler_time": 262.07675277617994
}
#Debug simulation 
Total elapsed time: 122.03832439659163. Arrivals time: 0.5135432230308652 Scheduler time: 121.28153420984745 Scheduler overhead time: 0.09844849072396755 Adapter cache time: 0.01717091863974929 Engine time: 0.09183815866708755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 118.65192350791767,
    "estimated_duration": 3600.0057805369042,
    "input_throughput": 7185.572350981624,
    "output_throughput": 6383.610860917177,
    "total_throughput": 13569.1832118988,
    "itl": 101.61946828055451,
    "ttft": 1355164.678966882,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8897010047617414,
    "arrivals": 160680,
    "finished_requests": 104631,
    "scheduler_time": 261.24302253319604
}
#Debug simulation 
Total elapsed time: 118.65207498893142. Arrivals time: 0.509065929800272 Scheduler time: 117.89973594620824 Scheduler overhead time: 0.09772966988384724 Adapter cache time: 0.017992079723626375 Engine time: 0.09128955006599426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 119.27341288980097,
    "estimated_duration": 3600.0070007796803,
    "input_throughput": 7185.569915391151,
    "output_throughput": 6383.608697156096,
    "total_throughput": 13569.178612547248,
    "itl": 101.61944560148424,
    "ttft": 1355165.2304858605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.890902408342813,
    "arrivals": 160680,
    "finished_requests": 104631,
    "scheduler_time": 261.24304137238096
}
#Debug simulation 
Total elapsed time: 119.27355895983055. Arrivals time: 0.5177130666561425 Scheduler time: 118.51176558667794 Scheduler overhead time: 0.09837913699448109 Adapter cache time: 0.017596200108528137 Engine time: 0.09143045963719487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 122.07331809680909,
    "estimated_duration": 3600.0067943974263,
    "input_throughput": 7172.813129182482,
    "output_throughput": 6370.652976458837,
    "total_throughput": 13543.466105641319,
    "itl": 101.30031689600861,
    "ttft": 1346735.6571678182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8022264541545907,
    "arrivals": 160680,
    "finished_requests": 104521,
    "scheduler_time": 262.06840424286725
}
#Debug simulation 
Total elapsed time: 122.07346530584618. Arrivals time: 0.5171409659087658 Scheduler time: 121.31022997619584 Scheduler overhead time: 0.09986226400360465 Adapter cache time: 0.017660331446677446 Engine time: 0.09226945322006941 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 118.54267216101289,
    "estimated_duration": 3600.019821993666,
    "input_throughput": 7185.544324496087,
    "output_throughput": 6383.585962388746,
    "total_throughput": 13569.130286884832,
    "itl": 101.61967061749148,
    "ttft": 1355170.4249056994,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9031005256250549,
    "arrivals": 160680,
    "finished_requests": 104631,
    "scheduler_time": 261.24336435335374
}
#Debug simulation 
Total elapsed time: 118.5428725508973. Arrivals time: 0.5090206987224519 Scheduler time: 117.79176950315014 Scheduler overhead time: 0.09726383443921804 Adapter cache time: 0.017445446457713842 Engine time: 0.09123747516423464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 122.17510867491364,
    "estimated_duration": 3600.0922725223186,
    "input_throughput": 7173.277806545416,
    "output_throughput": 6370.596991374147,
    "total_throughput": 13543.874797919563,
    "itl": 101.29983808655913,
    "ttft": 1346663.3879126117,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 256,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.765452875494958,
    "arrivals": 160680,
    "finished_requests": 104527,
    "scheduler_time": 262.07632053189405
}
#Debug simulation 
Total elapsed time: 122.17525856290013. Arrivals time: 0.5127453072927892 Scheduler time: 121.41700877482072 Scheduler overhead time: 0.09919207450002432 Adapter cache time: 0.017331481911242008 Engine time: 0.09227912640199065 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.025-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 270, 33, 33, 33, 8640, 33, 270, 8640, 270, 33, 270, 270, 270, 270, 8640, 8640, 8640, 270, 270, 270, 8640, 8640, 33, 8640, 270, 8640, 8640, 8640, 270, 8640, 33, 270, 270, 270, 8640, 33, 33, 8640, 33, 8640, 33, 33, 270, 33, 270, 8640, 33, 33, 270, 33, 33, 33, 33, 33, 8640, 270, 270, 8640, 270, 33, 8640, 8640, 8640, 270, 270, 33, 33, 33, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 33, 33, 8640, 270, 8640, 8640, 33, 8640, 33, 270, 33, 33, 270, 270, 8640, 8640, 8640, 33, 8640, 270, 270, 33, 33, 33, 33, 8640, 270, 8640, 270, 8640, 8640, 8640, 33, 270, 270, 8640, 270, 8640, 33, 33, 33, 270, 270, 270, 8640, 8640, 270, 8640, 33, 270, 270, 33, 8640, 8640, 8640, 8640, 8640, 8640, 270, 270, 270, 270, 270, 8640, 33, 8640, 33, 8640, 270, 33, 270, 33, 33, 270, 270, 270, 8640, 270, 33, 33]
Prompts retrieved: 482619 . Total input tokens: 107713687 . Total output tokens: 96635109
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 118.85311582218856,
    "estimated_duration": 3600.031647709516,
    "input_throughput": 7185.52072075764,
    "output_throughput": 6383.564992997062,
    "total_throughput": 13569.085713754703,
    "itl": 101.6199954314344,
    "ttft": 1355175.4306162945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 272,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9144183664023924,
    "arrivals": 160680,
    "finished_requests": 104631,
    "scheduler_time": 261.243572112695
}
#Debug simulation 
Total elapsed time: 118.85326002445072. Arrivals time: 0.5143647156655788 Scheduler time: 118.09550693677738 Scheduler overhead time: 0.09832282969728112 Adapter cache time: 0.01749894255772233 Engine time: 0.09170302469283342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 119.01982957636937,
    "estimated_duration": 3600.0845340037836,
    "input_throughput": 7108.009758743378,
    "output_throughput": 6315.359760376145,
    "total_throughput": 13423.369519119522,
    "itl": 100.77904133594667,
    "ttft": 1369095.2734607477,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 263,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8049082766543132,
    "arrivals": 158932,
    "finished_requests": 103455,
    "scheduler_time": 264.7756240578524
}
#Debug simulation 
Total elapsed time: 119.01997838914394. Arrivals time: 0.5144405523315072 Scheduler time: 118.26057889126241 Scheduler overhead time: 0.09894908452406526 Adapter cache time: 0.017410882748663425 Engine time: 0.0923226629383862 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 121.69209822406992,
    "estimated_duration": 3600.004713391446,
    "input_throughput": 7064.411028518191,
    "output_throughput": 6285.893992255841,
    "total_throughput": 13350.305020774033,
    "itl": 100.29258140041038,
    "ttft": 1369608.6409969386,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.863737625810322,
    "arrivals": 158932,
    "finished_requests": 102879,
    "scheduler_time": 266.36589768559196
}
#Debug simulation 
Total elapsed time: 121.69225126784295. Arrivals time: 0.514007932972163 Scheduler time: 120.92835619905964 Scheduler overhead time: 0.10089377500116825 Adapter cache time: 0.01771671650931239 Engine time: 0.09389907913282514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 121.72011140128598,
    "estimated_duration": 3600.006057965014,
    "input_throughput": 7064.40839001698,
    "output_throughput": 6285.891644524538,
    "total_throughput": 13350.300034541518,
    "itl": 100.29262073747809,
    "ttft": 1369609.2103494843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8648680200800343,
    "arrivals": 158932,
    "finished_requests": 102879,
    "scheduler_time": 266.36591178771386
}
#Debug simulation 
Total elapsed time: 121.72024984098971. Arrivals time: 0.5089746783487499 Scheduler time: 120.96370563143864 Scheduler overhead time: 0.10033094557002187 Adapter cache time: 0.01780448667705059 Engine time: 0.09262447198852897 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 122.30101436795667,
    "estimated_duration": 3600.119710491534,
    "input_throughput": 7064.352311920103,
    "output_throughput": 6285.783757149098,
    "total_throughput": 13350.136069069202,
    "itl": 100.29152161908247,
    "ttft": 1369646.6369020122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8257382611953678,
    "arrivals": 158932,
    "finished_requests": 102881,
    "scheduler_time": 266.37865742629447
}
#Debug simulation 
Total elapsed time: 122.30115503398702. Arrivals time: 0.5194649612531066 Scheduler time: 121.53258207812905 Scheduler overhead time: 0.10011034272611141 Adapter cache time: 0.018050207756459713 Engine time: 0.09346862323582172 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 121.62107854895294,
    "estimated_duration": 3600.018207078362,
    "input_throughput": 7064.3845494991465,
    "output_throughput": 6285.87043129569,
    "total_throughput": 13350.254980794836,
    "itl": 100.29310659715253,
    "ttft": 1369614.185032541,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8765631222166166,
    "arrivals": 158932,
    "finished_requests": 102879,
    "scheduler_time": 266.3662657603582
}
#Debug simulation 
Total elapsed time: 121.62122469581664. Arrivals time: 0.5101270140148699 Scheduler time: 120.86218093149364 Scheduler overhead time: 0.10085187247022986 Adapter cache time: 0.01809449214488268 Engine time: 0.09318029135465622 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 119.37485315604135,
    "estimated_duration": 3600.0649710962484,
    "input_throughput": 7108.048383973419,
    "output_throughput": 6315.394078312081,
    "total_throughput": 13423.442462285499,
    "itl": 100.77860254199835,
    "ttft": 1369087.104497678,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 263,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7863832275592733,
    "arrivals": 158932,
    "finished_requests": 103455,
    "scheduler_time": 264.77517989762174
}
#Debug simulation 
Total elapsed time: 119.37500525917858. Arrivals time: 0.5006179814226925 Scheduler time: 118.6302190986462 Scheduler overhead time: 0.09860431449487805 Adapter cache time: 0.01763444021344185 Engine time: 0.09163910010829568 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.8-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  0.8    ]. Counts: [53 53 54]
Adapter prompts. [66, 8640, 66, 66, 135, 66, 66, 66, 8640, 66, 135, 8640, 135, 66, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 66, 8640, 135, 8640, 8640, 8640, 135, 8640, 66, 135, 135, 135, 8640, 66, 66, 8640, 66, 8640, 66, 66, 135, 66, 135, 8640, 66, 66, 135, 66, 66, 66, 66, 66, 8640, 135, 135, 8640, 135, 66, 8640, 8640, 8640, 135, 135, 66, 66, 66, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 66, 66, 8640, 135, 8640, 8640, 66, 8640, 66, 135, 66, 66, 135, 135, 8640, 8640, 8640, 66, 8640, 135, 135, 66, 66, 66, 66, 8640, 135, 8640, 135, 8640, 8640, 8640, 66, 135, 135, 8640, 135, 8640, 66, 66, 66, 135, 135, 135, 8640, 8640, 135, 8640, 66, 135, 135, 66, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 66, 8640, 66, 8640, 135, 66, 135, 66, 66, 135, 135, 135, 8640, 135, 66, 66]
Prompts retrieved: 477213 . Total input tokens: 106497215 . Total output tokens: 95530772
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 121.58710453473032,
    "estimated_duration": 3600.029485818869,
    "input_throughput": 7064.362417080373,
    "output_throughput": 6285.850737928807,
    "total_throughput": 13350.21315500918,
    "itl": 100.29330281361419,
    "ttft": 1369618.3210252488,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 264,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8877552092075393,
    "arrivals": 158932,
    "finished_requests": 102879,
    "scheduler_time": 266.3665623589016
}
#Debug simulation 
Total elapsed time: 121.58724256185815. Arrivals time: 0.5021390011534095 Scheduler time: 120.83761462243274 Scheduler overhead time: 0.0996856251731515 Adapter cache time: 0.017946324311196804 Engine time: 0.09265327267348766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 121.95974584622309,
    "estimated_duration": 3600.0969389660195,
    "input_throughput": 7182.433817303402,
    "output_throughput": 6386.8849616599255,
    "total_throughput": 13569.318778963327,
    "itl": 101.68142017458842,
    "ttft": 1346023.039460283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8416341295815062,
    "arrivals": 158306,
    "finished_requests": 104397,
    "scheduler_time": 260.16313279453
}
#Debug simulation 
Total elapsed time: 121.9598913541995. Arrivals time: 0.5070539987646043 Scheduler time: 121.2079909290187 Scheduler overhead time: 0.0990807362832129 Adapter cache time: 0.017497715074568987 Engine time: 0.09171177865937352 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 132.73246231582016,
    "estimated_duration": 3600.0635170011205,
    "input_throughput": 7178.358625607537,
    "output_throughput": 6379.924935083542,
    "total_throughput": 13558.28356069108,
    "itl": 99.92376201319492,
    "ttft": 1328447.5846329557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7616483743442252,
    "arrivals": 158306,
    "finished_requests": 104277,
    "scheduler_time": 260.94514532527444
}
#Debug simulation 
Total elapsed time: 132.73266610363498. Arrivals time: 0.5167906004935503 Scheduler time: 131.96709192497656 Scheduler overhead time: 0.10129358805716038 Adapter cache time: 0.017712737433612347 Engine time: 0.09310069773346186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 136.77748887892812,
    "estimated_duration": 3600.0649753874595,
    "input_throughput": 7178.355717654423,
    "output_throughput": 6379.922350575919,
    "total_throughput": 13558.278068230342,
    "itl": 99.92379019607047,
    "ttft": 1328448.3042274811,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 233,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7627622433938122,
    "arrivals": 158306,
    "finished_requests": 104277,
    "scheduler_time": 260.94518972681294
}
#Debug simulation 
Total elapsed time: 136.77763578062877. Arrivals time: 0.6140012331306934 Scheduler time: 135.90295739937574 Scheduler overhead time: 0.10550526808947325 Adapter cache time: 0.01875915192067623 Engine time: 0.09891258226707578 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 128.07367013208568,
    "estimated_duration": 3600.121737166046,
    "input_throughput": 7158.016834254469,
    "output_throughput": 6364.245620770585,
    "total_throughput": 13522.262455025055,
    "itl": 100.9380280291833,
    "ttft": 1350097.7756434185,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 259,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8103794144024165,
    "arrivals": 158306,
    "finished_requests": 104040,
    "scheduler_time": 261.42674791717377
}
#Debug simulation 
Total elapsed time: 128.0738248201087. Arrivals time: 0.6071841102093458 Scheduler time: 127.20926712220535 Scheduler overhead time: 0.10435322253033519 Adapter cache time: 0.018924216274172068 Engine time: 0.09745764406397939 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 135.79811436170712,
    "estimated_duration": 3600.072976551443,
    "input_throughput": 7205.854483774881,
    "output_throughput": 6408.198708821464,
    "total_throughput": 13614.053192596346,
    "itl": 100.95916930062123,
    "ttft": 1321832.8652145832,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8387059096433261,
    "arrivals": 158306,
    "finished_requests": 104736,
    "scheduler_time": 259.23403413458936
}
#Debug simulation 
Total elapsed time: 135.79825675999746. Arrivals time: 0.6247810632921755 Scheduler time: 134.91252865409479 Scheduler overhead time: 0.10597718181088567 Adapter cache time: 0.01902691600844264 Engine time: 0.09859316516667604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 125.71673880470917,
    "estimated_duration": 3600.104428539959,
    "input_throughput": 7182.4188751343045,
    "output_throughput": 6386.871674532256,
    "total_throughput": 13569.29054966656,
    "itl": 101.68210559600422,
    "ttft": 1346034.5014887478,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 275,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8222638310980994,
    "arrivals": 158306,
    "finished_requests": 104397,
    "scheduler_time": 260.16769274631747
}
#Debug simulation 
Total elapsed time: 125.71688494086266. Arrivals time: 0.599023889284581 Scheduler time: 124.86539650056511 Scheduler overhead time: 0.10146791394799948 Adapter cache time: 0.0182685861364007 Engine time: 0.0959605136886239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.0125-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 135, 33, 33, 33, 8640, 33, 135, 8640, 135, 33, 135, 135, 135, 135, 8640, 8640, 8640, 135, 135, 135, 8640, 8640, 33, 8640, 135, 8640, 8640, 8640, 135, 8640, 33, 135, 135, 135, 8640, 33, 33, 8640, 33, 8640, 33, 33, 135, 33, 135, 8640, 33, 33, 135, 33, 33, 33, 33, 33, 8640, 135, 135, 8640, 135, 33, 8640, 8640, 8640, 135, 135, 33, 33, 33, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 33, 33, 8640, 135, 8640, 8640, 33, 8640, 33, 135, 33, 33, 135, 135, 8640, 8640, 8640, 33, 8640, 135, 135, 33, 33, 33, 33, 8640, 135, 8640, 135, 8640, 8640, 8640, 33, 135, 135, 8640, 135, 8640, 33, 33, 33, 135, 135, 135, 8640, 8640, 135, 8640, 33, 135, 135, 33, 8640, 8640, 8640, 8640, 8640, 8640, 135, 135, 135, 135, 135, 8640, 33, 8640, 33, 8640, 135, 33, 135, 33, 33, 135, 135, 135, 8640, 135, 33, 33]
Prompts retrieved: 475464 . Total input tokens: 106102088 . Total output tokens: 95186808
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 135.84514870727435,
    "estimated_duration": 3600.000266555752,
    "input_throughput": 7208.452799595956,
    "output_throughput": 6406.487858976058,
    "total_throughput": 13614.940658572013,
    "itl": 100.945026994069,
    "ttft": 1326572.9752417074,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 253,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8491834877803959,
    "arrivals": 158306,
    "finished_requests": 104774,
    "scheduler_time": 259.204050591501
}
#Debug simulation 
Total elapsed time: 135.8452864419669. Arrivals time: 0.622158985119313 Scheduler time: 134.96083678863943 Scheduler overhead time: 0.10663403570652008 Adapter cache time: 0.019227204378694296 Engine time: 0.09846408199518919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 121.59801482316107,
    "estimated_duration": 3600.0395329390317,
    "input_throughput": 7420.606289339996,
    "output_throughput": 6558.114927345116,
    "total_throughput": 13978.721216685111,
    "itl": 101.5917761574036,
    "ttft": 1247750.635204158,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9518116883630852,
    "arrivals": 157137,
    "finished_requests": 107660,
    "scheduler_time": 249.19176028466106
}
#Debug simulation 
Total elapsed time: 121.5981604969129. Arrivals time: 0.6058224006555974 Scheduler time: 120.74358961777762 Scheduler overhead time: 0.10011125355958939 Adapter cache time: 0.01843967055901885 Engine time: 0.09481763374060392 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 120.35996598377824,
    "estimated_duration": 3600.106218089117,
    "input_throughput": 7420.46883666106,
    "output_throughput": 6557.993450685341,
    "total_throughput": 13978.4622873464,
    "itl": 101.59347861302487,
    "ttft": 1247776.7503558218,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0181622304976976,
    "arrivals": 157137,
    "finished_requests": 107660,
    "scheduler_time": 249.19339539411922
}
#Debug simulation 
Total elapsed time: 120.36010679788888. Arrivals time: 0.596157057210803 Scheduler time: 119.51648128265515 Scheduler overhead time: 0.09992675948888063 Adapter cache time: 0.01843660743907094 Engine time: 0.0933700492605567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 120.63621719088405,
    "estimated_duration": 3600.10747957071,
    "input_throughput": 7420.466236520675,
    "output_throughput": 6557.991152757273,
    "total_throughput": 13978.457389277948,
    "itl": 101.59353668385747,
    "ttft": 1247777.459844381,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.019379820786422,
    "arrivals": 157137,
    "finished_requests": 107660,
    "scheduler_time": 249.19343928541136
}
#Debug simulation 
Total elapsed time: 120.63635822478682. Arrivals time: 0.5903486665338278 Scheduler time: 119.79713359847665 Scheduler overhead time: 0.10031683510169387 Adapter cache time: 0.018428264185786247 Engine time: 0.09481487656012177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 120.38723013410345,
    "estimated_duration": 3600.0603202581237,
    "input_throughput": 7420.563441582717,
    "output_throughput": 6558.077059749711,
    "total_throughput": 13978.640501332427,
    "itl": 101.59229292664965,
    "ttft": 1247758.6890409114,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9736253407876972,
    "arrivals": 157137,
    "finished_requests": 107660,
    "scheduler_time": 249.19211074046032
}
#Debug simulation 
Total elapsed time: 120.3873674320057. Arrivals time: 0.5936645208857954 Scheduler time: 119.54761761054397 Scheduler overhead time: 0.0988719523884356 Adapter cache time: 0.018529163673520088 Engine time: 0.093387046828866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 121.39118727575988,
    "estimated_duration": 3600.087844189663,
    "input_throughput": 7405.344856522751,
    "output_throughput": 6550.896261618425,
    "total_throughput": 13956.241118141177,
    "itl": 101.41417873009958,
    "ttft": 1254434.6929980016,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 313,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0393455010466333,
    "arrivals": 157137,
    "finished_requests": 107536,
    "scheduler_time": 249.65591886302954
}
#Debug simulation 
Total elapsed time: 121.39133436512202. Arrivals time: 0.5937812416814268 Scheduler time: 120.54831379419193 Scheduler overhead time: 0.10016284510493279 Adapter cache time: 0.01818267907947302 Engine time: 0.09424253506585956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 120.7476542009972,
    "estimated_duration": 3600.016202580115,
    "input_throughput": 7420.654379514697,
    "output_throughput": 6558.157428035795,
    "total_throughput": 13978.811807550494,
    "itl": 101.5911617192581,
    "ttft": 1247740.863473087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 311,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9299056417145779,
    "arrivals": 157137,
    "finished_requests": 107660,
    "scheduler_time": 249.19107156132807
}
#Debug simulation 
Total elapsed time: 120.74779557203874. Arrivals time: 0.5938148014247417 Scheduler time: 119.90693487087265 Scheduler overhead time: 0.09948029927909374 Adapter cache time: 0.018461876548826694 Engine time: 0.09317980194464326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.8,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.00625-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.8-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  0.8     ]. Counts: [53 53 54]
Adapter prompts. [33, 8640, 33, 33, 66, 33, 33, 33, 8640, 33, 66, 8640, 66, 33, 66, 66, 66, 66, 8640, 8640, 8640, 66, 66, 66, 8640, 8640, 33, 8640, 66, 8640, 8640, 8640, 66, 8640, 33, 66, 66, 66, 8640, 33, 33, 8640, 33, 8640, 33, 33, 66, 33, 66, 8640, 33, 33, 66, 33, 33, 33, 33, 33, 8640, 66, 66, 8640, 66, 33, 8640, 8640, 8640, 66, 66, 33, 33, 33, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 33, 33, 8640, 66, 8640, 8640, 33, 8640, 33, 66, 33, 33, 66, 66, 8640, 8640, 8640, 33, 8640, 66, 66, 33, 33, 33, 33, 8640, 66, 8640, 66, 8640, 8640, 8640, 33, 66, 66, 8640, 66, 8640, 33, 33, 33, 66, 66, 66, 8640, 8640, 66, 8640, 33, 66, 66, 33, 8640, 8640, 8640, 8640, 8640, 8640, 66, 66, 66, 66, 66, 8640, 33, 8640, 33, 8640, 66, 33, 66, 33, 33, 66, 66, 66, 8640, 66, 33, 33]
Prompts retrieved: 471807 . Total input tokens: 105279297 . Total output tokens: 94459900
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 123.5884686219506,
    "estimated_duration": 3600.039242826184,
    "input_throughput": 7373.833508314812,
    "output_throughput": 6518.874216931164,
    "total_throughput": 13892.707725245977,
    "itl": 101.3384302978583,
    "ttft": 1262729.5304923095,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 304,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0243120041489653,
    "arrivals": 157137,
    "finished_requests": 107030,
    "scheduler_time": 251.49964493171615
}
#Debug simulation 
Total elapsed time: 123.58861896628514. Arrivals time: 0.5931301028467715 Scheduler time: 122.74209372326732 Scheduler overhead time: 0.10261942679062486 Adapter cache time: 0.018379149492830038 Engine time: 0.09563272120431066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 78.16944081522524,
    "estimated_duration": 3600.0188620040353,
    "input_throughput": 6278.161272582428,
    "output_throughput": 5537.032377909151,
    "total_throughput": 11815.19365049158,
    "itl": 69.33903119380606,
    "ttft": 863324.1336064558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 629,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.925046790933733,
    "arrivals": 106283,
    "finished_requests": 90536,
    "scheduler_time": 202.35785713019658
}
#Debug simulation 
Total elapsed time: 78.1695885672234. Arrivals time: 0.46665233513340354 Scheduler time: 77.41536579234526 Scheduler overhead time: 0.11345149716362357 Adapter cache time: 0.02318708226084709 Engine time: 0.10683746403083205 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 83.93947456683964,
    "estimated_duration": 3600.0554513790307,
    "input_throughput": 6218.925875551142,
    "output_throughput": 5474.875947382971,
    "total_throughput": 11693.801822934112,
    "itl": 68.17652642406854,
    "ttft": 893995.6406641892,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 593,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9344949756609335,
    "arrivals": 106283,
    "finished_requests": 89841,
    "scheduler_time": 207.330798179136
}
#Debug simulation 
Total elapsed time: 83.9396273479797. Arrivals time: 0.47209335677325726 Scheduler time: 83.1740652625449 Scheduler overhead time: 0.11671727057546377 Adapter cache time: 0.023320920765399933 Engine time: 0.10894643561914563 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 82.9098484008573,
    "estimated_duration": 3600.0082792504386,
    "input_throughput": 6301.649951961631,
    "output_throughput": 5559.9247133308,
    "total_throughput": 11861.57466529243,
    "itl": 70.32909459573688,
    "ttft": 862909.796976675,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9569550457410636,
    "arrivals": 106283,
    "finished_requests": 91060,
    "scheduler_time": 205.3307729755342
}
#Debug simulation 
Total elapsed time: 82.90999357495457. Arrivals time: 0.4707897696644068 Scheduler time: 82.14938636869192 Scheduler overhead time: 0.11461632419377565 Adapter cache time: 0.02308011380955577 Engine time: 0.107981919310987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 82.30296298395842,
    "estimated_duration": 3600.0006943673825,
    "input_throughput": 6301.978784475413,
    "output_throughput": 5560.156705335706,
    "total_throughput": 11862.135489811119,
    "itl": 70.33216736805625,
    "ttft": 862823.7492321276,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8715334043791398,
    "arrivals": 106283,
    "finished_requests": 91063,
    "scheduler_time": 205.33036234628196
}
#Debug simulation 
Total elapsed time: 82.30310961417854. Arrivals time: 0.47386744525283575 Scheduler time: 81.53985294792801 Scheduler overhead time: 0.11396996024996042 Adapter cache time: 0.02289958205074072 Engine time: 0.10844726068899035 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 80.18576481472701,
    "estimated_duration": 3600.0116112538612,
    "input_throughput": 6278.703082329053,
    "output_throughput": 5541.015461628563,
    "total_throughput": 11819.718543957615,
    "itl": 70.60462786293071,
    "ttft": 873767.7675510515,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 607,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.008180205151442,
    "arrivals": 106283,
    "finished_requests": 90601,
    "scheduler_time": 205.40975751563408
}
#Debug simulation 
Total elapsed time: 80.18590983795002. Arrivals time: 0.4716336019337177 Scheduler time: 79.42641785135493 Scheduler overhead time: 0.11409680033102632 Adapter cache time: 0.022997595835477114 Engine time: 0.10703717032447457 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 79.45782521087676,
    "estimated_duration": 3600.027196617235,
    "input_throughput": 6253.215259360584,
    "output_throughput": 5524.136045051786,
    "total_throughput": 11777.35130441237,
    "itl": 68.82512714674397,
    "ttft": 871804.095906275,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 622,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.859811283429124,
    "arrivals": 106283,
    "finished_requests": 90204,
    "scheduler_time": 203.18508591158027
}
#Debug simulation 
Total elapsed time: 79.45797542762011. Arrivals time: 0.46879899920895696 Scheduler time: 78.69871822930872 Scheduler overhead time: 0.11467505805194378 Adapter cache time: 0.023447724990546703 Engine time: 0.1082687545567751 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.05_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.4 ]. Counts: [53 53 54]
Adapter prompts. [540, 4320, 540, 540, 1080, 540, 540, 540, 4320, 540, 1080, 4320, 1080, 540, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 540, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 540, 1080, 1080, 1080, 4320, 540, 540, 4320, 540, 4320, 540, 540, 1080, 540, 1080, 4320, 540, 540, 1080, 540, 540, 540, 540, 540, 4320, 1080, 1080, 4320, 1080, 540, 4320, 4320, 4320, 1080, 1080, 540, 540, 540, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 540, 540, 4320, 1080, 4320, 4320, 540, 4320, 540, 1080, 540, 540, 1080, 1080, 4320, 4320, 4320, 540, 4320, 1080, 1080, 540, 540, 540, 540, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 540, 1080, 1080, 4320, 1080, 4320, 540, 540, 540, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 540, 1080, 1080, 540, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 540, 4320, 540, 4320, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 4320, 1080, 540, 540]
Prompts retrieved: 319140 . Total input tokens: 71158771 . Total output tokens: 63858637
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 87.58213489875197,
    "estimated_duration": 3600.042675946765,
    "input_throughput": 6186.399441540768,
    "output_throughput": 5441.822712517671,
    "total_throughput": 11628.22215405844,
    "itl": 68.69742114716522,
    "ttft": 917083.4356620705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 572,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9168332682549911,
    "arrivals": 106283,
    "finished_requests": 89436,
    "scheduler_time": 211.4700725113723
}
#Debug simulation 
Total elapsed time: 87.58228218369186. Arrivals time: 0.47906399331986904 Scheduler time: 86.80963865388185 Scheduler overhead time: 0.11675019841641188 Adapter cache time: 0.023138172924518585 Engine time: 0.10914955707266927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 71.99228321528062,
    "estimated_duration": 3600.0273294020035,
    "input_throughput": 6046.439098454219,
    "output_throughput": 5386.946327216181,
    "total_throughput": 11433.3854256704,
    "itl": 65.25254637323248,
    "ttft": 787875.6626249915,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 653,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9984984967881216,
    "arrivals": 101554,
    "finished_requests": 87806,
    "scheduler_time": 190.1138565138691
}
#Debug simulation 
Total elapsed time: 71.99242692999542. Arrivals time: 0.44120812555775046 Scheduler time: 71.25301909493282 Scheduler overhead time: 0.11826999858021736 Adapter cache time: 0.023754168301820755 Engine time: 0.11055383132770658 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 72.36615971010178,
    "estimated_duration": 3600.011112242001,
    "input_throughput": 6046.46633616745,
    "output_throughput": 5386.970594077529,
    "total_throughput": 11433.43693024498,
    "itl": 65.2561858972148,
    "ttft": 787849.4273829533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 653,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.131467592047998,
    "arrivals": 101554,
    "finished_requests": 87806,
    "scheduler_time": 190.10454661642453
}
#Debug simulation 
Total elapsed time: 72.36630669888109. Arrivals time: 0.4483988801948726 Scheduler time: 71.62117119086906 Scheduler overhead time: 0.11724158376455307 Adapter cache time: 0.024222057778388262 Engine time: 0.11014163121581078 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 71.80533998319879,
    "estimated_duration": 3600.019851092397,
    "input_throughput": 6046.451658702625,
    "output_throughput": 5386.957517502384,
    "total_throughput": 11433.409176205008,
    "itl": 65.25350759468353,
    "ttft": 787850.8412584797,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 653,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.135132210850723,
    "arrivals": 101554,
    "finished_requests": 87806,
    "scheduler_time": 190.10501907328594
}
#Debug simulation 
Total elapsed time: 71.80548650212586. Arrivals time: 0.4451034036464989 Scheduler time: 71.06170234782621 Scheduler overhead time: 0.11766327451914549 Adapter cache time: 0.023907267954200506 Engine time: 0.11130984965711832 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 71.64714864594862,
    "estimated_duration": 3600.019978983723,
    "input_throughput": 6046.451443901395,
    "output_throughput": 5386.957326129796,
    "total_throughput": 11433.40877003119,
    "itl": 65.25255444309337,
    "ttft": 787849.1496984905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 653,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.042393812627983,
    "arrivals": 101554,
    "finished_requests": 87806,
    "scheduler_time": 190.11073082222154
}
#Debug simulation 
Total elapsed time: 71.64729120768607. Arrivals time: 0.44686233159154654 Scheduler time: 70.90362761728466 Scheduler overhead time: 0.1172004621475935 Adapter cache time: 0.02414251258596778 Engine time: 0.11013218527659774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 71.06993541214615,
    "estimated_duration": 3600.0654471225553,
    "input_throughput": 6015.903965665521,
    "output_throughput": 5318.389701860385,
    "total_throughput": 11334.293667525906,
    "itl": 63.5595323930673,
    "ttft": 802913.6221413068,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 662,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.191875950973484,
    "arrivals": 101554,
    "finished_requests": 87291,
    "scheduler_time": 189.6870164404471
}
#Debug simulation 
Total elapsed time: 71.07008541515097. Arrivals time: 0.4408828909508884 Scheduler time: 70.32970239361748 Scheduler overhead time: 0.11864135833457112 Adapter cache time: 0.023993239272385836 Engine time: 0.11124528711661696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 71.861264212057,
    "estimated_duration": 3600.0429766330303,
    "input_throughput": 6050.620267975872,
    "output_throughput": 5360.744892564995,
    "total_throughput": 11411.365160540867,
    "itl": 65.09810685482663,
    "ttft": 787725.6809211032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 652,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9495127922761861,
    "arrivals": 101554,
    "finished_requests": 87854,
    "scheduler_time": 189.25996312145378
}
#Debug simulation 
Total elapsed time: 71.86141062201932. Arrivals time: 0.4494330594316125 Scheduler time: 71.11794938426465 Scheduler overhead time: 0.11567484075203538 Adapter cache time: 0.024216019548475742 Engine time: 0.10901627456769347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.025_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 1080, 270, 270, 270, 4320, 270, 1080, 4320, 1080, 270, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 270, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 270, 1080, 1080, 1080, 4320, 270, 270, 4320, 270, 4320, 270, 270, 1080, 270, 1080, 4320, 270, 270, 1080, 270, 270, 270, 270, 270, 4320, 1080, 1080, 4320, 1080, 270, 4320, 4320, 4320, 1080, 1080, 270, 270, 270, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 270, 270, 4320, 1080, 4320, 4320, 270, 4320, 270, 1080, 270, 270, 1080, 1080, 4320, 4320, 4320, 270, 4320, 1080, 1080, 270, 270, 270, 270, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 270, 1080, 1080, 4320, 1080, 4320, 270, 270, 270, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 270, 1080, 1080, 270, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 270, 4320, 270, 4320, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 4320, 1080, 270, 270]
Prompts retrieved: 304830 . Total input tokens: 67996776 . Total output tokens: 60988908
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 71.76000936236233,
    "estimated_duration": 3600.0303171775927,
    "input_throughput": 6046.43408032894,
    "output_throughput": 5386.941856424182,
    "total_throughput": 11433.375936753122,
    "itl": 65.25458211960307,
    "ttft": 787878.8463604577,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 653,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1902123693004265,
    "arrivals": 101554,
    "finished_requests": 87806,
    "scheduler_time": 190.10290403105392
}
#Debug simulation 
Total elapsed time: 71.76015379605815. Arrivals time: 0.44399923691526055 Scheduler time: 71.02026659389958 Scheduler overhead time: 0.11657800758257508 Adapter cache time: 0.023869380354881287 Engine time: 0.11001521069556475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 68.24510071985424,
    "estimated_duration": 3600.0252609868085,
    "input_throughput": 6064.742166285595,
    "output_throughput": 5363.774307157773,
    "total_throughput": 11428.516473443367,
    "itl": 65.40065404174578,
    "ttft": 715118.4536259812,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9831960580684573,
    "arrivals": 99226,
    "finished_requests": 87939,
    "scheduler_time": 182.80301828830207
}
#Debug simulation 
Total elapsed time: 68.24524481594563. Arrivals time: 0.4370288569480181 Scheduler time: 67.51079691853374 Scheduler overhead time: 0.11796298390254378 Adapter cache time: 0.02415459230542183 Engine time: 0.10987164499238133 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 67.61815148638561,
    "estimated_duration": 3600.0286879126234,
    "input_throughput": 5984.221201440293,
    "output_throughput": 5326.620052885929,
    "total_throughput": 11310.841254326222,
    "itl": 63.25313563244937,
    "ttft": 738566.2981033851,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1110757183679434,
    "arrivals": 99226,
    "finished_requests": 86806,
    "scheduler_time": 182.18611518742918
}
#Debug simulation 
Total elapsed time: 67.61829459527507. Arrivals time: 0.42915225541219115 Scheduler time: 66.88807970751077 Scheduler overhead time: 0.11866576550528407 Adapter cache time: 0.024296280927956104 Engine time: 0.11198145151138306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 67.45158425113186,
    "estimated_duration": 3600.019152621251,
    "input_throughput": 6008.373034418593,
    "output_throughput": 5346.699610190951,
    "total_throughput": 11355.072644609543,
    "itl": 63.54118186586076,
    "ttft": 726312.41596503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.118228379134097,
    "arrivals": 99226,
    "finished_requests": 87198,
    "scheduler_time": 181.49633132427581
}
#Debug simulation 
Total elapsed time: 67.45172869088128. Arrivals time: 0.4342760741710663 Scheduler time: 66.71891383640468 Scheduler overhead time: 0.11743168625980616 Adapter cache time: 0.024194540455937386 Engine time: 0.11081805033609271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 71.92684336891398,
    "estimated_duration": 3600.0360119193747,
    "input_throughput": 5930.27928868344,
    "output_throughput": 5267.09536716286,
    "total_throughput": 11197.3746558463,
    "itl": 61.536900048754525,
    "ttft": 773711.0747070933,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 623,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9506493271887184,
    "arrivals": 99226,
    "finished_requests": 85964,
    "scheduler_time": 186.63495752376346
}
#Debug simulation 
Total elapsed time: 71.92699260590598. Arrivals time: 0.4424752430059016 Scheduler time: 71.17880876129493 Scheduler overhead time: 0.12114452011883259 Adapter cache time: 0.024692426435649395 Engine time: 0.11313101137056947 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 68.08003750490025,
    "estimated_duration": 3600.0152091384175,
    "input_throughput": 6064.738544597781,
    "output_throughput": 5363.574284626748,
    "total_throughput": 11428.312829224527,
    "itl": 65.38995921761068,
    "ttft": 715155.1316044902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1444251805730197,
    "arrivals": 99226,
    "finished_requests": 87937,
    "scheduler_time": 182.79268366533327
}
#Debug simulation 
Total elapsed time: 68.08017862122506. Arrivals time: 0.4347843546420336 Scheduler time: 67.34737579477951 Scheduler overhead time: 0.11846563778817654 Adapter cache time: 0.02413078583776951 Engine time: 0.10978946974501014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 75.32875943090767,
    "estimated_duration": 3600.021182474782,
    "input_throughput": 5864.458548959632,
    "output_throughput": 5206.935751170767,
    "total_throughput": 11071.394300130398,
    "itl": 61.089262886188955,
    "ttft": 811628.1046299182,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 599,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7910401266463765,
    "arrivals": 99226,
    "finished_requests": 85138,
    "scheduler_time": 191.42720627862383
}
#Debug simulation 
Total elapsed time: 75.32890984928235. Arrivals time: 0.43081940803676844 Scheduler time: 74.59252959303558 Scheduler overhead time: 0.12072043446823955 Adapter cache time: 0.024103089701384306 Engine time: 0.11358116567134857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 1080, 135, 135, 135, 4320, 135, 1080, 4320, 1080, 135, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 135, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 135, 1080, 1080, 1080, 4320, 135, 135, 4320, 135, 4320, 135, 135, 1080, 135, 1080, 4320, 135, 135, 1080, 135, 135, 135, 135, 135, 4320, 1080, 1080, 4320, 1080, 135, 4320, 4320, 4320, 1080, 1080, 135, 135, 135, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 135, 135, 4320, 1080, 4320, 4320, 135, 4320, 135, 1080, 135, 135, 1080, 1080, 4320, 4320, 4320, 135, 4320, 1080, 1080, 135, 135, 135, 135, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 135, 1080, 1080, 4320, 1080, 4320, 135, 135, 135, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 135, 1080, 1080, 135, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 135, 4320, 135, 4320, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 4320, 1080, 135, 135]
Prompts retrieved: 297675 . Total input tokens: 66367712 . Total output tokens: 59573913
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 67.75444512721151,
    "estimated_duration": 3600.0191882901822,
    "input_throughput": 6064.731841157099,
    "output_throughput": 5363.568356192769,
    "total_throughput": 11428.300197349869,
    "itl": 65.39722481342058,
    "ttft": 715153.5395039102,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 648,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1722167673707014,
    "arrivals": 99226,
    "finished_requests": 87937,
    "scheduler_time": 182.7903708422509
}
#Debug simulation 
Total elapsed time: 67.75458681536838. Arrivals time: 0.4345193197950721 Scheduler time: 67.02405064878985 Scheduler overhead time: 0.11685638315975666 Adapter cache time: 0.024182761553674936 Engine time: 0.1094440184533596 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 62.914194428827614,
    "estimated_duration": 3600.010202379728,
    "input_throughput": 5944.490375569972,
    "output_throughput": 5271.353116570506,
    "total_throughput": 11215.843492140479,
    "itl": 61.985093762906025,
    "ttft": 707116.4129143589,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.075010690386443,
    "arrivals": 98086,
    "finished_requests": 86160,
    "scheduler_time": 174.1685634272845
}
#Debug simulation 
Total elapsed time: 62.91433635400608. Arrivals time: 0.4202827182598412 Scheduler time: 62.197694887407124 Scheduler overhead time: 0.11717071617022157 Adapter cache time: 0.023946905974298716 Engine time: 0.10916849365457892 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 62.394264416303486,
    "estimated_duration": 3600.0346825951437,
    "input_throughput": 5944.192455549891,
    "output_throughput": 5271.237549945041,
    "total_throughput": 11215.430005494933,
    "itl": 61.97244343502684,
    "ttft": 707254.4105698707,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2143907557893607,
    "arrivals": 98086,
    "finished_requests": 86158,
    "scheduler_time": 174.1682099738877
}
#Debug simulation 
Total elapsed time: 62.39440782321617. Arrivals time: 0.41922528157010674 Scheduler time: 61.67478055553511 Scheduler overhead time: 0.11865747906267643 Adapter cache time: 0.024401936680078506 Engine time: 0.11063923547044396 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 62.675557401031256,
    "estimated_duration": 3600.0440023996407,
    "input_throughput": 5944.177067206987,
    "output_throughput": 5271.223903749776,
    "total_throughput": 11215.400970956762,
    "itl": 61.97422555184173,
    "ttft": 707285.6029724635,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 678,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2179651319608125,
    "arrivals": 98086,
    "finished_requests": 86158,
    "scheduler_time": 174.16905351176862
}
#Debug simulation 
Total elapsed time: 62.6757049318403. Arrivals time: 0.4172928729094565 Scheduler time: 61.95874911220744 Scheduler overhead time: 0.11819122591987252 Adapter cache time: 0.02396124880760908 Engine time: 0.11120663257315755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 65.2651373790577,
    "estimated_duration": 3600.03590019779,
    "input_throughput": 5916.219890704377,
    "output_throughput": 5259.591438785293,
    "total_throughput": 11175.811329489668,
    "itl": 60.72963250028849,
    "ttft": 722405.1966052463,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 665,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.07950020212213,
    "arrivals": 98086,
    "finished_requests": 85826,
    "scheduler_time": 176.25803812302402
}
#Debug simulation 
Total elapsed time: 65.26528433617204. Arrivals time: 0.4233407722786069 Scheduler time: 64.54226093459874 Scheduler overhead time: 0.11810757266357541 Adapter cache time: 0.02420584997162223 Engine time: 0.1105505614541471 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 65.3452914999798,
    "estimated_duration": 3600.002444410019,
    "input_throughput": 5916.152649582553,
    "output_throughput": 5259.6155953730395,
    "total_throughput": 11175.768244955592,
    "itl": 60.734975112970886,
    "ttft": 722443.4876392557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 665,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.204247516486796,
    "arrivals": 98086,
    "finished_requests": 85825,
    "scheduler_time": 176.25007782509797
}
#Debug simulation 
Total elapsed time: 65.34543726686388. Arrivals time: 0.4230287801474333 Scheduler time: 64.62026535579935 Scheduler overhead time: 0.11957051139324903 Adapter cache time: 0.02420893171802163 Engine time: 0.11166404886171222 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 64.58839833503589,
    "estimated_duration": 3600.0470118815324,
    "input_throughput": 6003.094384232772,
    "output_throughput": 5328.84013366637,
    "total_throughput": 11331.934517899143,
    "itl": 63.34258882807149,
    "ttft": 684772.8661071154,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 666,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.991373496404815,
    "arrivals": 98086,
    "finished_requests": 87108,
    "scheduler_time": 174.76504591273599
}
#Debug simulation 
Total elapsed time: 64.58853852236643. Arrivals time: 0.42351978411898017 Scheduler time: 63.86690754722804 Scheduler overhead time: 0.1178082269616425 Adapter cache time: 0.024172999896109104 Engine time: 0.10985853057354689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.00625_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 1080, 66, 66, 66, 4320, 66, 1080, 4320, 1080, 66, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 66, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 66, 1080, 1080, 1080, 4320, 66, 66, 4320, 66, 4320, 66, 66, 1080, 66, 1080, 4320, 66, 66, 1080, 66, 66, 66, 66, 66, 4320, 1080, 1080, 4320, 1080, 66, 4320, 4320, 4320, 1080, 1080, 66, 66, 66, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 66, 66, 4320, 1080, 4320, 4320, 66, 4320, 66, 1080, 66, 66, 1080, 1080, 4320, 4320, 4320, 66, 4320, 1080, 1080, 66, 66, 66, 66, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 66, 1080, 1080, 4320, 1080, 4320, 66, 66, 66, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 66, 1080, 1080, 66, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 66, 4320, 66, 4320, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 4320, 1080, 66, 66]
Prompts retrieved: 294018 . Total input tokens: 65534784 . Total output tokens: 58851924
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 65.39965869113803,
    "estimated_duration": 3600.0135732112685,
    "input_throughput": 5916.2565826109685,
    "output_throughput": 5259.624058336518,
    "total_throughput": 11175.880640947486,
    "itl": 60.73596567319259,
    "ttft": 722405.859838293,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 665,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.232290610857311,
    "arrivals": 98086,
    "finished_requests": 85826,
    "scheduler_time": 176.24898714575102
}
#Debug simulation 
Total elapsed time: 65.39980102516711. Arrivals time: 0.4243351547047496 Scheduler time: 64.6725892056711 Scheduler overhead time: 0.11961092567071319 Adapter cache time: 0.024746994953602552 Engine time: 0.11179865850135684 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-8/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [53 53 54]
Adapter prompts. [33, 4320, 33, 33, 1080, 33, 33, 33, 4320, 33, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 33, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 33, 1080, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 33, 33, 1080, 33, 1080, 4320, 33, 33, 1080, 33, 33, 33, 33, 33, 4320, 1080, 1080, 4320, 1080, 33, 4320, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 33, 33, 4320, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 33, 1080, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 33, 1080, 1080, 4320, 1080, 4320, 33, 33, 33, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 33, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 292269 . Total input tokens: 65130866 . Total output tokens: 58502807
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 65.55183561891317,
    "estimated_duration": 3600.1068489654663,
    "input_throughput": 5923.9052880134595,
    "output_throughput": 5201.123684809727,
    "total_throughput": 11125.028972823187,
    "itl": 58.1872860417806,
    "ttft": 703357.202592044,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 634,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9403492296533973,
    "arrivals": 97521,
    "finished_requests": 85716,
    "scheduler_time": 174.34532743980603
}
#Debug simulation 
Total elapsed time: 65.55197801301256. Arrivals time: 0.4248529653996229 Scheduler time: 64.82334853615612 Scheduler overhead time: 0.1200333945453167 Adapter cache time: 0.02420791843906045 Engine time: 0.11256361473351717 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-16/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [53 53 54]
Adapter prompts. [33, 4320, 33, 33, 1080, 33, 33, 33, 4320, 33, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 33, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 33, 1080, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 33, 33, 1080, 33, 1080, 4320, 33, 33, 1080, 33, 33, 33, 33, 33, 4320, 1080, 1080, 4320, 1080, 33, 4320, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 33, 33, 4320, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 33, 1080, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 33, 1080, 1080, 4320, 1080, 4320, 33, 33, 33, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 33, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 292269 . Total input tokens: 65130866 . Total output tokens: 58502807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 61.44704216206446,
    "estimated_duration": 3600.008856791208,
    "input_throughput": 6018.431582224465,
    "output_throughput": 5294.138641922062,
    "total_throughput": 11312.570224146528,
    "itl": 60.87001263717029,
    "ttft": 658050.9382952813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.113527290278586,
    "arrivals": 97521,
    "finished_requests": 86952,
    "scheduler_time": 170.6268483787344
}
#Debug simulation 
Total elapsed time: 61.4471833081916. Arrivals time: 0.4220462152734399 Scheduler time: 60.72495814040303 Scheduler overhead time: 0.11838548490777612 Adapter cache time: 0.023724925238639116 Engine time: 0.11157676065340638 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-8-32/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [53 53 54]
Adapter prompts. [33, 4320, 33, 33, 1080, 33, 33, 33, 4320, 33, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 33, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 33, 1080, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 33, 33, 1080, 33, 1080, 4320, 33, 33, 1080, 33, 33, 33, 33, 33, 4320, 1080, 1080, 4320, 1080, 33, 4320, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 33, 33, 4320, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 33, 1080, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 33, 1080, 1080, 4320, 1080, 4320, 33, 33, 33, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 33, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 292269 . Total input tokens: 65130866 . Total output tokens: 58502807
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 61.180785974022,
    "estimated_duration": 3600.0131966470212,
    "input_throughput": 6018.4243269384815,
    "output_throughput": 5294.132259779246,
    "total_throughput": 11312.556586717727,
    "itl": 60.869441214727665,
    "ttft": 658052.0349161339,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.11687109775842,
    "arrivals": 97521,
    "finished_requests": 86952,
    "scheduler_time": 170.6273263548803
}
#Debug simulation 
Total elapsed time: 61.180921824183315. Arrivals time: 0.42191799683496356 Scheduler time: 60.45856612594798 Scheduler overhead time: 0.1188852391205728 Adapter cache time: 0.02415800280869007 Engine time: 0.1108528757467866 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-16/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [53 53 54]
Adapter prompts. [33, 4320, 33, 33, 1080, 33, 33, 33, 4320, 33, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 33, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 33, 1080, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 33, 33, 1080, 33, 1080, 4320, 33, 33, 1080, 33, 33, 33, 33, 33, 4320, 1080, 1080, 4320, 1080, 33, 4320, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 33, 33, 4320, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 33, 1080, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 33, 1080, 1080, 4320, 1080, 4320, 33, 33, 33, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 33, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 292269 . Total input tokens: 65130866 . Total output tokens: 58502807
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 61.67604876821861,
    "estimated_duration": 3600.0118814117855,
    "input_throughput": 6018.426525721152,
    "output_throughput": 5294.134193947665,
    "total_throughput": 11312.560719668818,
    "itl": 60.87048525068732,
    "ttft": 658049.0242194523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.02363632022169,
    "arrivals": 97521,
    "finished_requests": 86952,
    "scheduler_time": 170.6314343500449
}
#Debug simulation 
Total elapsed time: 61.67618712596595. Arrivals time: 0.4166411799378693 Scheduler time: 60.960170777048916 Scheduler overhead time: 0.11792770959436893 Adapter cache time: 0.024050806183367968 Engine time: 0.11117164650931954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_8-16-32/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [53 53 54]
Adapter prompts. [33, 4320, 33, 33, 1080, 33, 33, 33, 4320, 33, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 33, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 33, 1080, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 33, 33, 1080, 33, 1080, 4320, 33, 33, 1080, 33, 33, 33, 33, 33, 4320, 1080, 1080, 4320, 1080, 33, 4320, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 33, 33, 4320, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 33, 1080, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 33, 1080, 1080, 4320, 1080, 4320, 33, 33, 33, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 33, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 292269 . Total input tokens: 65130866 . Total output tokens: 58502807
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 61.198503331746906,
    "estimated_duration": 3600.043869849883,
    "input_throughput": 6018.63276763444,
    "output_throughput": 5294.088263650724,
    "total_throughput": 11312.721031285164,
    "itl": 60.86888637091576,
    "ttft": 658026.6372289786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 647,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.144536930769685,
    "arrivals": 97521,
    "finished_requests": 86954,
    "scheduler_time": 170.62722384594014
}
#Debug simulation 
Total elapsed time: 61.19864605087787. Arrivals time: 0.4218290983699262 Scheduler time: 60.47620816528797 Scheduler overhead time: 0.11866384325549006 Adapter cache time: 0.024125621188431978 Engine time: 0.1112359082326293 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-16/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [53 53 54]
Adapter prompts. [33, 4320, 33, 33, 1080, 33, 33, 33, 4320, 33, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 33, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 33, 1080, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 33, 33, 1080, 33, 1080, 4320, 33, 33, 1080, 33, 33, 33, 33, 33, 4320, 1080, 1080, 4320, 1080, 33, 4320, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 33, 33, 4320, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 33, 1080, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 33, 1080, 1080, 4320, 1080, 4320, 33, 33, 33, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 33, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 292269 . Total input tokens: 65130866 . Total output tokens: 58502807
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 59.96346915094182,
    "estimated_duration": 3600.0356009409866,
    "input_throughput": 6052.659311009183,
    "output_throughput": 5324.36873540635,
    "total_throughput": 11377.028046415533,
    "itl": 62.75741641754903,
    "ttft": 639569.8293641552,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 660,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.9734331946354027,
    "arrivals": 97521,
    "finished_requests": 87443,
    "scheduler_time": 169.46767547972664
}
#Debug simulation 
Total elapsed time: 59.963608974125236. Arrivals time: 0.4212679974734783 Scheduler time: 59.24363844934851 Scheduler overhead time: 0.11762692220509052 Adapter cache time: 0.023751943837851286 Engine time: 0.11098313750699162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.1-0.003125_size_16-16-32/adapters_160_slots_16_rate_0.4-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      0.4     ]. Counts: [53 53 54]
Adapter prompts. [33, 4320, 33, 33, 1080, 33, 33, 33, 4320, 33, 1080, 4320, 1080, 33, 1080, 1080, 1080, 1080, 4320, 4320, 4320, 1080, 1080, 1080, 4320, 4320, 33, 4320, 1080, 4320, 4320, 4320, 1080, 4320, 33, 1080, 1080, 1080, 4320, 33, 33, 4320, 33, 4320, 33, 33, 1080, 33, 1080, 4320, 33, 33, 1080, 33, 33, 33, 33, 33, 4320, 1080, 1080, 4320, 1080, 33, 4320, 4320, 4320, 1080, 1080, 33, 33, 33, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 33, 33, 4320, 1080, 4320, 4320, 33, 4320, 33, 1080, 33, 33, 1080, 1080, 4320, 4320, 4320, 33, 4320, 1080, 1080, 33, 33, 33, 33, 4320, 1080, 4320, 1080, 4320, 4320, 4320, 33, 1080, 1080, 4320, 1080, 4320, 33, 33, 33, 1080, 1080, 1080, 4320, 4320, 1080, 4320, 33, 1080, 1080, 33, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 1080, 1080, 1080, 1080, 4320, 33, 4320, 33, 4320, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 4320, 1080, 33, 33]
Prompts retrieved: 292269 . Total input tokens: 65130866 . Total output tokens: 58502807
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 64.31307867402211,
    "estimated_duration": 3600.0065132669315,
    "input_throughput": 5915.99901875534,
    "output_throughput": 5214.728620855699,
    "total_throughput": 11130.72763961104,
    "itl": 58.81954241651235,
    "ttft": 708077.9960107309,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 631,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.118624941818415,
    "arrivals": 97521,
    "finished_requests": 85505,
    "scheduler_time": 174.23842727645888
}
#Debug simulation 
Total elapsed time: 64.31322092935443. Arrivals time: 0.42914630519226193 Scheduler time: 63.57820810936391 Scheduler overhead time: 0.12124537024646997 Adapter cache time: 0.02439905796200037 Engine time: 0.11287326226010919 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-8/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 540, 270, 270, 270, 4320, 270, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 270, 4320, 540, 4320, 4320, 4320, 540, 4320, 270, 540, 540, 540, 4320, 270, 270, 4320, 270, 4320, 270, 270, 540, 270, 540, 4320, 270, 270, 540, 270, 270, 270, 270, 270, 4320, 540, 540, 4320, 540, 270, 4320, 4320, 4320, 540, 540, 270, 270, 270, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 270, 270, 4320, 540, 4320, 4320, 270, 4320, 270, 540, 270, 270, 540, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 4320, 270, 540, 540, 4320, 540, 4320, 270, 270, 270, 540, 540, 540, 4320, 4320, 540, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 270, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 276210 . Total input tokens: 61553482 . Total output tokens: 55279979
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 56.837113736663014,
    "estimated_duration": 3600.044771705237,
    "input_throughput": 5710.814532526413,
    "output_throughput": 5023.525580053744,
    "total_throughput": 10734.340112580157,
    "itl": 57.04967798482115,
    "ttft": 638292.2996789351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 724,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2157931266073545,
    "arrivals": 92215,
    "finished_requests": 82500,
    "scheduler_time": 164.83328587548766
}
#Debug simulation 
Total elapsed time: 56.8372572385706. Arrivals time: 0.39480526745319366 Scheduler time: 56.13335943222046 Scheduler overhead time: 0.12144458619877696 Adapter cache time: 0.025131016969680786 Engine time: 0.11465818574652076 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-16/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 540, 270, 270, 270, 4320, 270, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 270, 4320, 540, 4320, 4320, 4320, 540, 4320, 270, 540, 540, 540, 4320, 270, 270, 4320, 270, 4320, 270, 270, 540, 270, 540, 4320, 270, 270, 540, 270, 270, 270, 270, 270, 4320, 540, 540, 4320, 540, 270, 4320, 4320, 4320, 540, 540, 270, 270, 270, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 270, 270, 4320, 540, 4320, 4320, 270, 4320, 270, 540, 270, 270, 540, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 4320, 270, 540, 540, 4320, 540, 4320, 270, 270, 270, 540, 540, 540, 4320, 4320, 540, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 270, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 276210 . Total input tokens: 61553482 . Total output tokens: 55279979
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 54.273939540144056,
    "estimated_duration": 3600.0067092012205,
    "input_throughput": 5777.734510004604,
    "output_throughput": 5094.242172695611,
    "total_throughput": 10871.976682700215,
    "itl": 59.231287157152686,
    "ttft": 602941.1873308168,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 743,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.425905028332496,
    "arrivals": 92215,
    "finished_requests": 83456,
    "scheduler_time": 162.5959930406352
}
#Debug simulation 
Total elapsed time: 54.274077515117824. Arrivals time: 0.3948639612644911 Scheduler time: 53.57349572656676 Scheduler overhead time: 0.12030728906393051 Adapter cache time: 0.024840075056999922 Engine time: 0.11308703199028969 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-8-32/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 540, 270, 270, 270, 4320, 270, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 270, 4320, 540, 4320, 4320, 4320, 540, 4320, 270, 540, 540, 540, 4320, 270, 270, 4320, 270, 4320, 270, 270, 540, 270, 540, 4320, 270, 270, 540, 270, 270, 270, 270, 270, 4320, 540, 540, 4320, 540, 270, 4320, 4320, 4320, 540, 540, 270, 270, 270, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 270, 270, 4320, 540, 4320, 4320, 270, 4320, 270, 540, 270, 270, 540, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 4320, 270, 540, 540, 4320, 540, 4320, 270, 270, 270, 540, 540, 540, 4320, 4320, 540, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 270, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 276210 . Total input tokens: 61553482 . Total output tokens: 55279979
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 54.262629015836865,
    "estimated_duration": 3600.0056137403653,
    "input_throughput": 5777.383490908081,
    "output_throughput": 5094.223445097838,
    "total_throughput": 10871.60693600592,
    "itl": 59.22960852939805,
    "ttft": 603018.0962668015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 743,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4299582519009673,
    "arrivals": 92215,
    "finished_requests": 83454,
    "scheduler_time": 162.5983293704843
}
#Debug simulation 
Total elapsed time: 54.262768337037414. Arrivals time: 0.3953647082671523 Scheduler time: 53.5616935021244 Scheduler overhead time: 0.12098389025777578 Adapter cache time: 0.02492140606045723 Engine time: 0.11239119293168187 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-16/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 540, 270, 270, 270, 4320, 270, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 270, 4320, 540, 4320, 4320, 4320, 540, 4320, 270, 540, 540, 540, 4320, 270, 270, 4320, 270, 4320, 270, 270, 540, 270, 540, 4320, 270, 270, 540, 270, 270, 270, 270, 270, 4320, 540, 540, 4320, 540, 270, 4320, 4320, 4320, 540, 540, 270, 270, 270, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 270, 270, 4320, 540, 4320, 4320, 270, 4320, 270, 540, 270, 270, 540, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 4320, 270, 540, 540, 4320, 540, 4320, 270, 270, 270, 540, 540, 540, 4320, 4320, 540, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 270, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 276210 . Total input tokens: 61553482 . Total output tokens: 55279979
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 54.4264359138906,
    "estimated_duration": 3600.0256667951526,
    "input_throughput": 5788.526507521082,
    "output_throughput": 5105.606932064651,
    "total_throughput": 10894.133439585734,
    "itl": 58.46947842601098,
    "ttft": 596600.5900830586,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3279720349726083,
    "arrivals": 92215,
    "finished_requests": 83595,
    "scheduler_time": 162.0845052401026
}
#Debug simulation 
Total elapsed time: 54.42657879414037. Arrivals time: 0.39858422288671136 Scheduler time: 53.72175696492195 Scheduler overhead time: 0.12119328323751688 Adapter cache time: 0.025145758874714375 Engine time: 0.1123249251395464 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_8-16-32/adapters_160_slots_16_rate_0.4-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 540, 270, 270, 270, 4320, 270, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 270, 4320, 540, 4320, 4320, 4320, 540, 4320, 270, 540, 540, 540, 4320, 270, 270, 4320, 270, 4320, 270, 270, 540, 270, 540, 4320, 270, 270, 540, 270, 270, 270, 270, 270, 4320, 540, 540, 4320, 540, 270, 4320, 4320, 4320, 540, 540, 270, 270, 270, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 270, 270, 4320, 540, 4320, 4320, 270, 4320, 270, 540, 270, 270, 540, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 4320, 270, 540, 540, 4320, 540, 4320, 270, 270, 270, 540, 540, 540, 4320, 4320, 540, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 270, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 276210 . Total input tokens: 61553482 . Total output tokens: 55279979
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 53.764347057789564,
    "estimated_duration": 3600.003256004429,
    "input_throughput": 5744.695915345737,
    "output_throughput": 5054.134317698966,
    "total_throughput": 10798.830233044702,
    "itl": 58.36549411172395,
    "ttft": 616062.0679263583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4934323845431234,
    "arrivals": 92215,
    "finished_requests": 82928,
    "scheduler_time": 161.71858090884703
}
#Debug simulation 
Total elapsed time: 53.76449259882793. Arrivals time: 0.3977461801841855 Scheduler time: 53.058575090020895 Scheduler overhead time: 0.1218506246805191 Adapter cache time: 0.025367739144712687 Engine time: 0.1131457444280386 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_160_slots_16_rate_0.4-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-16/adapters_160_slots_16_rate_0.4-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 540, 270, 270, 270, 4320, 270, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 270, 4320, 540, 4320, 4320, 4320, 540, 4320, 270, 540, 540, 540, 4320, 270, 270, 4320, 270, 4320, 270, 270, 540, 270, 540, 4320, 270, 270, 540, 270, 270, 270, 270, 270, 4320, 540, 540, 4320, 540, 270, 4320, 4320, 4320, 540, 540, 270, 270, 270, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 270, 270, 4320, 540, 4320, 4320, 270, 4320, 270, 540, 270, 270, 540, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 4320, 270, 540, 540, 4320, 540, 4320, 270, 270, 270, 540, 540, 540, 4320, 4320, 540, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 270, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 276210 . Total input tokens: 61553482 . Total output tokens: 55279979
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 54.769140388816595,
    "estimated_duration": 3600.0241990183213,
    "input_throughput": 5793.882442703505,
    "output_throughput": 5097.53795682933,
    "total_throughput": 10891.420399532835,
    "itl": 58.53240839972874,
    "ttft": 597287.2839012549,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 749,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.239547670881687,
    "arrivals": 92215,
    "finished_requests": 83588,
    "scheduler_time": 162.13166697402548
}
#Debug simulation 
Total elapsed time: 54.76928346697241. Arrivals time: 0.3930246555246413 Scheduler time: 54.07276416802779 Scheduler overhead time: 0.11931960796937346 Adapter cache time: 0.025188357569277287 Engine time: 0.11171928560361266 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_160_slots_16_rate_0.4-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.025_size_16-16-32/adapters_160_slots_16_rate_0.4-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  0.4  ]. Counts: [53 53 54]
Adapter prompts. [270, 4320, 270, 270, 540, 270, 270, 270, 4320, 270, 540, 4320, 540, 270, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 270, 4320, 540, 4320, 4320, 4320, 540, 4320, 270, 540, 540, 540, 4320, 270, 270, 4320, 270, 4320, 270, 270, 540, 270, 540, 4320, 270, 270, 540, 270, 270, 270, 270, 270, 4320, 540, 540, 4320, 540, 270, 4320, 4320, 4320, 540, 540, 270, 270, 270, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 270, 270, 4320, 540, 4320, 4320, 270, 4320, 270, 540, 270, 270, 540, 540, 4320, 4320, 4320, 270, 4320, 540, 540, 270, 270, 270, 270, 4320, 540, 4320, 540, 4320, 4320, 4320, 270, 540, 540, 4320, 540, 4320, 270, 270, 270, 540, 540, 540, 4320, 4320, 540, 4320, 270, 540, 540, 270, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 270, 4320, 270, 4320, 540, 270, 540, 270, 270, 540, 540, 540, 4320, 540, 270, 270]
Prompts retrieved: 276210 . Total input tokens: 61553482 . Total output tokens: 55279979
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 54.326979198958725,
    "estimated_duration": 3600.0079591339136,
    "input_throughput": 5788.554702254988,
    "output_throughput": 5105.5653789226235,
    "total_throughput": 10894.120081177613,
    "itl": 58.4778415563443,
    "ttft": 596637.7156585392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 744,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.495415662676107,
    "arrivals": 92215,
    "finished_requests": 83594,
    "scheduler_time": 162.07668336143797
}
#Debug simulation 
Total elapsed time: 54.327127065043896. Arrivals time: 0.3969590663909912 Scheduler time: 53.62277099117637 Scheduler overhead time: 0.12083022017031908 Adapter cache time: 0.025370388757437468 Engine time: 0.1135023208335042 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-8/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 540, 135, 135, 135, 4320, 135, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 135, 4320, 540, 4320, 4320, 4320, 540, 4320, 135, 540, 540, 540, 4320, 135, 135, 4320, 135, 4320, 135, 135, 540, 135, 540, 4320, 135, 135, 540, 135, 135, 135, 135, 135, 4320, 540, 540, 4320, 540, 135, 4320, 4320, 4320, 540, 540, 135, 135, 135, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 135, 135, 4320, 540, 4320, 4320, 135, 4320, 135, 540, 135, 135, 540, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 4320, 135, 540, 540, 4320, 540, 4320, 135, 135, 135, 540, 540, 540, 4320, 4320, 540, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 135, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 269055 . Total input tokens: 59949771 . Total output tokens: 53839434
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 50.4263929259032,
    "estimated_duration": 3600.00318795058,
    "input_throughput": 5582.616445248518,
    "output_throughput": 4970.818375910188,
    "total_throughput": 10553.434821158706,
    "itl": 55.21353297644131,
    "ttft": 586202.8046882907,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 775,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3718780015479304,
    "arrivals": 89808,
    "finished_requests": 81093,
    "scheduler_time": 154.10978369985818
}
#Debug simulation 
Total elapsed time: 50.4265375980176. Arrivals time: 0.3807351873256266 Scheduler time: 49.73541436251253 Scheduler overhead time: 0.1225787689909339 Adapter cache time: 0.02582266740500927 Engine time: 0.11323815630748868 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-16/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 540, 135, 135, 135, 4320, 135, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 135, 4320, 540, 4320, 4320, 4320, 540, 4320, 135, 540, 540, 540, 4320, 135, 135, 4320, 135, 4320, 135, 135, 540, 135, 540, 4320, 135, 135, 540, 135, 135, 135, 135, 135, 4320, 540, 540, 4320, 540, 135, 4320, 4320, 4320, 540, 540, 135, 135, 135, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 135, 135, 4320, 540, 4320, 4320, 135, 4320, 135, 540, 135, 135, 540, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 4320, 135, 540, 540, 4320, 540, 4320, 135, 135, 135, 540, 540, 540, 4320, 4320, 540, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 135, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 269055 . Total input tokens: 59949771 . Total output tokens: 53839434
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 47.88670183578506,
    "estimated_duration": 3600.004657402553,
    "input_throughput": 5603.480806203941,
    "output_throughput": 4973.956065078979,
    "total_throughput": 10577.43687128292,
    "itl": 55.5977353200641,
    "ttft": 567800.0226296512,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 820,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.674934285688219,
    "arrivals": 89808,
    "finished_requests": 81413,
    "scheduler_time": 151.2407821219708
}
#Debug simulation 
Total elapsed time: 47.88684412976727. Arrivals time: 0.3725857650861144 Scheduler time: 47.20295296702534 Scheduler overhead time: 0.12182443030178547 Adapter cache time: 0.025519169867038727 Engine time: 0.11598954536020756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-8-32/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 540, 135, 135, 135, 4320, 135, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 135, 4320, 540, 4320, 4320, 4320, 540, 4320, 135, 540, 540, 540, 4320, 135, 135, 4320, 135, 4320, 135, 135, 540, 135, 540, 4320, 135, 135, 540, 135, 135, 135, 135, 135, 4320, 540, 540, 4320, 540, 135, 4320, 4320, 4320, 540, 540, 135, 135, 135, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 135, 135, 4320, 540, 4320, 4320, 135, 4320, 135, 540, 135, 135, 540, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 4320, 135, 540, 540, 4320, 540, 4320, 135, 135, 135, 540, 540, 540, 4320, 4320, 540, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 135, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 269055 . Total input tokens: 59949771 . Total output tokens: 53839434
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 48.05188710987568,
    "estimated_duration": 3600.0072089057776,
    "input_throughput": 5603.476834739853,
    "output_throughput": 4973.952539790222,
    "total_throughput": 10577.429374530075,
    "itl": 55.597824930554026,
    "ttft": 567799.7968656402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 820,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6798225880041597,
    "arrivals": 89808,
    "finished_requests": 81413,
    "scheduler_time": 151.24074621019167
}
#Debug simulation 
Total elapsed time: 48.05203540902585. Arrivals time: 0.3732057339511812 Scheduler time: 47.371006248984486 Scheduler overhead time: 0.12077539134770632 Adapter cache time: 0.025639040861278772 Engine time: 0.1133506796322763 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-16/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 540, 135, 135, 135, 4320, 135, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 135, 4320, 540, 4320, 4320, 4320, 540, 4320, 135, 540, 540, 540, 4320, 135, 135, 4320, 135, 4320, 135, 135, 540, 135, 540, 4320, 135, 135, 540, 135, 135, 135, 135, 135, 4320, 540, 540, 4320, 540, 135, 4320, 4320, 4320, 540, 540, 135, 135, 135, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 135, 135, 4320, 540, 4320, 4320, 135, 4320, 135, 540, 135, 135, 540, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 4320, 135, 540, 540, 4320, 540, 4320, 135, 135, 135, 540, 540, 540, 4320, 4320, 540, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 135, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 269055 . Total input tokens: 59949771 . Total output tokens: 53839434
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 49.483855289872736,
    "estimated_duration": 3600.014658702661,
    "input_throughput": 5574.813689017706,
    "output_throughput": 4957.866478919293,
    "total_throughput": 10532.680167936998,
    "itl": 54.294260755298716,
    "ttft": 583454.8477577885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 789,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.46987901397513,
    "arrivals": 89808,
    "finished_requests": 81004,
    "scheduler_time": 152.46012500655894
}
#Debug simulation 
Total elapsed time: 49.48399665020406. Arrivals time: 0.37688971357420087 Scheduler time: 48.79636885644868 Scheduler overhead time: 0.12205696245655417 Adapter cache time: 0.02618324337527156 Engine time: 0.11391218332573771 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_8-16-32/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 540, 135, 135, 135, 4320, 135, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 135, 4320, 540, 4320, 4320, 4320, 540, 4320, 135, 540, 540, 540, 4320, 135, 135, 4320, 135, 4320, 135, 135, 540, 135, 540, 4320, 135, 135, 540, 135, 135, 135, 135, 135, 4320, 540, 540, 4320, 540, 135, 4320, 4320, 4320, 540, 540, 135, 135, 135, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 135, 135, 4320, 540, 4320, 4320, 135, 4320, 135, 540, 135, 135, 540, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 4320, 135, 540, 540, 4320, 540, 4320, 135, 135, 135, 540, 540, 540, 4320, 4320, 540, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 135, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 269055 . Total input tokens: 59949771 . Total output tokens: 53839434
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [54 53 53]
---Simulation End---
#Simulation results
{
    "duration": 50.73037000000477,
    "estimated_duration": 3600.0181403414017,
    "input_throughput": 5606.793414125917,
    "output_throughput": 4991.514292285174,
    "total_throughput": 10598.307706411091,
    "itl": 54.57182991911408,
    "ttft": 578137.4507783843,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5966699453070787,
    "arrivals": 89808,
    "finished_requests": 81382,
    "scheduler_time": 154.55394015287203
}
#Debug simulation 
Total elapsed time: 50.73051096824929. Arrivals time: 0.3809060347266495 Scheduler time: 50.03914550645277 Scheduler overhead time: 0.12238512001931667 Adapter cache time: 0.025802954100072384 Engine time: 0.11383280344307423 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-16/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 540, 135, 135, 135, 4320, 135, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 135, 4320, 540, 4320, 4320, 4320, 540, 4320, 135, 540, 540, 540, 4320, 135, 135, 4320, 135, 4320, 135, 135, 540, 135, 540, 4320, 135, 135, 540, 135, 135, 135, 135, 135, 4320, 540, 540, 4320, 540, 135, 4320, 4320, 4320, 540, 540, 135, 135, 135, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 135, 135, 4320, 540, 4320, 4320, 135, 4320, 135, 540, 135, 135, 540, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 4320, 135, 540, 540, 4320, 540, 4320, 135, 135, 135, 540, 540, 540, 4320, 4320, 540, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 135, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 269055 . Total input tokens: 59949771 . Total output tokens: 53839434
Prompts distributed
Adapter sizes. Values: [16]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 50.640577126760036,
    "estimated_duration": 3600.0129776402305,
    "input_throughput": 5617.570304776004,
    "output_throughput": 4992.186448111198,
    "total_throughput": 10609.756752887202,
    "itl": 54.53732416069403,
    "ttft": 569977.1860377827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 775,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.317288978549141,
    "arrivals": 89808,
    "finished_requests": 81632,
    "scheduler_time": 154.3592389356546
}
#Debug simulation 
Total elapsed time: 50.640721236821264. Arrivals time: 0.3806352922692895 Scheduler time: 49.95053708925843 Scheduler overhead time: 0.12148308614268899 Adapter cache time: 0.02568107144907117 Engine time: 0.11357921827584505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.0125_size_16-16-32/adapters_160_slots_16_rate_0.4-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   0.4   ]. Counts: [53 53 54]
Adapter prompts. [135, 4320, 135, 135, 540, 135, 135, 135, 4320, 135, 540, 4320, 540, 135, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 135, 4320, 540, 4320, 4320, 4320, 540, 4320, 135, 540, 540, 540, 4320, 135, 135, 4320, 135, 4320, 135, 135, 540, 135, 540, 4320, 135, 135, 540, 135, 135, 135, 135, 135, 4320, 540, 540, 4320, 540, 135, 4320, 4320, 4320, 540, 540, 135, 135, 135, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 135, 135, 4320, 540, 4320, 4320, 135, 4320, 135, 540, 135, 135, 540, 540, 4320, 4320, 4320, 135, 4320, 540, 540, 135, 135, 135, 135, 4320, 540, 4320, 540, 4320, 4320, 4320, 135, 540, 540, 4320, 540, 4320, 135, 135, 135, 540, 540, 540, 4320, 4320, 540, 4320, 135, 540, 540, 135, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 135, 4320, 135, 4320, 540, 135, 540, 135, 135, 540, 540, 540, 4320, 540, 135, 135]
Prompts retrieved: 269055 . Total input tokens: 59949771 . Total output tokens: 53839434
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 50.62952048005536,
    "estimated_duration": 3600.0055259667747,
    "input_throughput": 5621.224982582643,
    "output_throughput": 4995.150665823163,
    "total_throughput": 10616.375648405805,
    "itl": 55.07619987017384,
    "ttft": 572259.5297346216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 784,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6301204524934447,
    "arrivals": 89808,
    "finished_requests": 81573,
    "scheduler_time": 154.43029358530964
}
#Debug simulation 
Total elapsed time: 50.62966684484854. Arrivals time: 0.3815798447467387 Scheduler time: 49.93744932860136 Scheduler overhead time: 0.12307989783585072 Adapter cache time: 0.025555883534252644 Engine time: 0.11394334631040692 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.4-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 694848,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-8/adapters_160_slots_16_rate_0.4-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 540, 66, 66, 66, 4320, 66, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 66, 4320, 540, 4320, 4320, 4320, 540, 4320, 66, 540, 540, 540, 4320, 66, 66, 4320, 66, 4320, 66, 66, 540, 66, 540, 4320, 66, 66, 540, 66, 66, 66, 66, 66, 4320, 540, 540, 4320, 540, 66, 4320, 4320, 4320, 540, 540, 66, 66, 66, 4320, 66, 540, 540, 66, 4320, 4320, 4320, 66, 66, 4320, 540, 4320, 4320, 66, 4320, 66, 540, 66, 66, 540, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 4320, 66, 540, 540, 4320, 540, 4320, 66, 66, 66, 540, 540, 540, 4320, 4320, 540, 4320, 66, 540, 540, 66, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 66, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 265398 . Total input tokens: 59148136 . Total output tokens: 53102990
Prompts distributed
Adapter sizes. Values: [8]. Counts: [160]
---Simulation End---
#Simulation results
{
    "duration": 47.188344493974,
    "estimated_duration": 3600.0249923094207,
    "input_throughput": 5572.280482178337,
    "output_throughput": 4947.784817619692,
    "total_throughput": 10520.065299798029,
    "itl": 54.13713053135781,
    "ttft": 542780.8989252954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 793,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.426966780938722,
    "arrivals": 88684,
    "finished_requests": 80859,
    "scheduler_time": 147.35368284441287
}
#Debug simulation 
Total elapsed time: 47.18848330480978. Arrivals time: 0.367347857914865 Scheduler time: 46.51148681342602 Scheduler overhead time: 0.12159947259351611 Adapter cache time: 0.02526744920760393 Engine time: 0.11413240432739258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.4-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-16/adapters_160_slots_16_rate_0.4-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 540, 66, 66, 66, 4320, 66, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 66, 4320, 540, 4320, 4320, 4320, 540, 4320, 66, 540, 540, 540, 4320, 66, 66, 4320, 66, 4320, 66, 66, 540, 66, 540, 4320, 66, 66, 540, 66, 66, 66, 66, 66, 4320, 540, 540, 4320, 540, 66, 4320, 4320, 4320, 540, 540, 66, 66, 66, 4320, 66, 540, 540, 66, 4320, 4320, 4320, 66, 66, 4320, 540, 4320, 4320, 66, 4320, 66, 540, 66, 66, 540, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 4320, 66, 540, 540, 4320, 540, 4320, 66, 66, 66, 540, 540, 540, 4320, 4320, 540, 4320, 66, 540, 540, 66, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 66, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 265398 . Total input tokens: 59148136 . Total output tokens: 53102990
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 47.88322654366493,
    "estimated_duration": 3600.064648729564,
    "input_throughput": 5576.361526475479,
    "output_throughput": 4933.028912763302,
    "total_throughput": 10509.390439238781,
    "itl": 55.0790643188526,
    "ttft": 549811.7104265373,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 796,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5970441488339615,
    "arrivals": 88684,
    "finished_requests": 80869,
    "scheduler_time": 149.32548316534888
}
#Debug simulation 
Total elapsed time: 47.8833756390959. Arrivals time: 0.36704971734434366 Scheduler time: 47.20469244895503 Scheduler overhead time: 0.12256833119317889 Adapter cache time: 0.025428904220461845 Engine time: 0.11475050263106823 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.4-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 657616,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-8-32/adapters_160_slots_16_rate_0.4-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 540, 66, 66, 66, 4320, 66, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 66, 4320, 540, 4320, 4320, 4320, 540, 4320, 66, 540, 540, 540, 4320, 66, 66, 4320, 66, 4320, 66, 66, 540, 66, 540, 4320, 66, 66, 540, 66, 66, 66, 66, 66, 4320, 540, 540, 4320, 540, 66, 4320, 4320, 4320, 540, 540, 66, 66, 66, 4320, 66, 540, 540, 66, 4320, 4320, 4320, 66, 66, 4320, 540, 4320, 4320, 66, 4320, 66, 540, 66, 66, 540, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 4320, 66, 540, 540, 4320, 540, 4320, 66, 66, 66, 540, 540, 540, 4320, 4320, 540, 4320, 66, 540, 540, 66, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 66, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 265398 . Total input tokens: 59148136 . Total output tokens: 53102990
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [107  53]
---Simulation End---
#Simulation results
{
    "duration": 48.054830859880894,
    "estimated_duration": 3600.0848717098543,
    "input_throughput": 5576.330202033622,
    "output_throughput": 4933.001202153683,
    "total_throughput": 10509.331404187305,
    "itl": 55.08193186689859,
    "ttft": 549846.6499200644,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 796,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6017194232158256,
    "arrivals": 88684,
    "finished_requests": 80869,
    "scheduler_time": 149.32642523849438
}
#Debug simulation 
Total elapsed time: 48.05497301090509. Arrivals time: 0.37265514535829425 Scheduler time: 47.37176904082298 Scheduler overhead time: 0.12254160409793258 Adapter cache time: 0.025723719038069248 Engine time: 0.11363843595609069 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.4-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 16,
    "served_adapters": 160,
    "served_adapters_rates": [
        0.4,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 683472,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.4-0.05-0.00625_size_8-16-16/adapters_160_slots_16_rate_0.4-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    0.4    ]. Counts: [53 53 54]
Adapter prompts. [66, 4320, 66, 66, 540, 66, 66, 66, 4320, 66, 540, 4320, 540, 66, 540, 540, 540, 540, 4320, 4320, 4320, 540, 540, 540, 4320, 4320, 66, 4320, 540, 4320, 4320, 4320, 540, 4320, 66, 540, 540, 540, 4320, 66, 66, 4320, 66, 4320, 66, 66, 540, 66, 540, 4320, 66, 66, 540, 66, 66, 66, 66, 66, 4320, 540, 540, 4320, 540, 66, 4320, 4320, 4320, 540, 540, 66, 66, 66, 4320, 66, 540, 540, 66, 4320, 4320, 4320, 66, 66, 4320, 540, 4320, 4320, 66, 4320, 66, 540, 66, 66, 540, 540, 4320, 4320, 4320, 66, 4320, 540, 540, 66, 66, 66, 66, 4320, 540, 4320, 540, 4320, 4320, 4320, 66, 540, 540, 4320, 540, 4320, 66, 66, 66, 540, 540, 540, 4320, 4320, 540, 4320, 66, 540, 540, 66, 4320, 4320, 4320, 4320, 4320, 4320, 540, 540, 540, 540, 540, 4320, 66, 4320, 66, 4320, 540, 66, 540, 66, 66, 540, 540, 540, 4320, 540, 66, 66]
Prompts retrieved: 265398 . Total input tokens: 59148136 . Total output tokens: 53102990
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 54 106]
---Simulation End---
#Simulation results
{
    "duration": 45.48427244275808,
    "estimated_duration": 3600.050639604013,
    "input_throughput": 5546.676699580095,
    "output_throughput": 4920.868835875209,
    "total_throughput": 10467.545535455303,
    "itl": 54.08660857373199,
    "ttft": 550319.9789797128,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 812,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5394673613947596,
    "arrivals": 88684,
    "finished_requests": 80479,
    "scheduler_time": 145.46091809586954
}
#Debug simulation 
Total elapsed time: 45.484427778981626. Arrivals time: 0.36803922848775983 Scheduler time: 44.80635666148737 Scheduler overhead time: 0.12257779994979501 Adapter cache time: 0.025744615122675896 Engine time: 0.1130841625854373 
