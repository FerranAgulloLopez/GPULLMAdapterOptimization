INFO 06-01 00:47:10 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:10 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-8-32/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.861428250093013,
    "estimated_duration": 3600.110167690642,
    "input_throughput": 5597.768140780884,
    "output_throughput": 4980.525640831102,
    "total_throughput": 10578.293781611985,
    "itl": 102.50345931744477,
    "ttft": 2069762.6424545974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5656494702026269,
    "arrivals": 740135,
    "finished_requests": 81413,
    "scheduler_time": 69.7850017787421
}
#Debug simulation 
Total elapsed time: 5.861536098178476. Arrivals time: 0.30594043573364615 Scheduler time: 5.416342617478222 Scheduler overhead time: 0.04924762947484851 Adapter cache time: 0.016272663604468107 Engine time: 0.05048834951594472 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-16/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.250017835292965,
    "estimated_duration": 3600.0831088870864,
    "input_throughput": 6245.347210039974,
    "output_throughput": 5538.056038423896,
    "total_throughput": 11783.40324846387,
    "itl": 155.97505436418132,
    "ttft": 2010511.2492850854,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5593261265754705,
    "arrivals": 740135,
    "finished_requests": 90865,
    "scheduler_time": 79.22836306616394
}
#Debug simulation 
Total elapsed time: 6.250152879860252. Arrivals time: 0.2756486744619906 Scheduler time: 5.876213023904711 Scheduler overhead time: 0.033964624628424644 Adapter cache time: 0.013217725791037083 Engine time: 0.03517185291275382 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_8-16-32/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.832593767438084,
    "estimated_duration": 3600.097377010199,
    "input_throughput": 5597.896914870841,
    "output_throughput": 4980.667777073076,
    "total_throughput": 10578.564691943917,
    "itl": 102.50020261929538,
    "ttft": 2069788.0003388296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5729431898146882,
    "arrivals": 740135,
    "finished_requests": 81414,
    "scheduler_time": 69.78413409897341
}
#Debug simulation 
Total elapsed time: 5.8326889523305. Arrivals time: 0.2912323707714677 Scheduler time: 5.402379065286368 Scheduler overhead time: 0.04934344766661525 Adapter cache time: 0.015991756226867437 Engine time: 0.05038632592186332 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-16/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.265869117807597,
    "estimated_duration": 3600.0587042737216,
    "input_throughput": 6245.3895469284835,
    "output_throughput": 5538.093580621819,
    "total_throughput": 11783.483127550302,
    "itl": 155.97438543683435,
    "ttft": 2010495.8688746192,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 179,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5352190027874901,
    "arrivals": 740135,
    "finished_requests": 90865,
    "scheduler_time": 79.22835500782129
}
#Debug simulation 
Total elapsed time: 6.265962508041412. Arrivals time: 0.27839671075344086 Scheduler time: 5.888810939155519 Scheduler overhead time: 0.034095258451998234 Adapter cache time: 0.01324259489774704 Engine time: 0.03542921366170049 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.0125-0.003125_size_16-16-32/adapters_192_slots_160_rate_3.2-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 1.250e-02 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 135, 135, 135, 135, 34560, 135, 33, 34560, 33, 135, 135, 33, 34560, 34560, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 33, 135, 135, 34560, 135, 33, 33, 135, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 34560, 34560, 33, 135, 34560, 33, 135, 33, 135, 33, 33, 135, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 135, 33, 33, 33, 33, 33, 34560, 33, 135, 33, 34560, 34560, 33, 34560, 33, 135, 135, 34560, 33, 33, 33, 34560, 34560, 135, 34560, 135, 34560, 34560, 135, 135, 33, 34560, 135, 135, 135, 34560, 135, 33, 135, 34560, 34560, 135, 34560, 135, 33, 34560, 135, 33, 135, 135, 33, 135, 34560, 33, 135, 34560, 33, 135, 34560, 34560, 135, 33, 34560, 135, 33, 33, 135, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 135, 33, 34560, 34560, 135, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 135, 135, 135, 135, 135, 135, 33, 34560, 135, 34560, 33, 34560, 135, 33, 135, 33, 33, 135, 135, 135, 34560, 135, 33, 33]
Prompts retrieved: 2222592 . Total input tokens: 496013963 . Total output tokens: 444762839
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.841384617146105,
    "estimated_duration": 3600.104674020983,
    "input_throughput": 5597.8855685579265,
    "output_throughput": 4980.657681814807,
    "total_throughput": 10578.543250372733,
    "itl": 102.50037024583527,
    "ttft": 2069792.4476661247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5802369094267495,
    "arrivals": 740135,
    "finished_requests": 81414,
    "scheduler_time": 69.78413739014665
}
#Debug simulation 
Total elapsed time: 5.841478785965592. Arrivals time: 0.2970695784315467 Scheduler time: 5.404468879103661 Scheduler overhead time: 0.04970024526119232 Adapter cache time: 0.01617637649178505 Engine time: 0.05073351599276066 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-8/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.342038756236434,
    "estimated_duration": 3600.136242053146,
    "input_throughput": 6307.446294601866,
    "output_throughput": 5574.907073115072,
    "total_throughput": 11882.353367716938,
    "itl": 154.5371732793052,
    "ttft": 1999735.674381746,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.538645842932164,
    "arrivals": 738702,
    "finished_requests": 91842,
    "scheduler_time": 79.77886407154504
}
#Debug simulation 
Total elapsed time: 6.34213187918067. Arrivals time: 0.28390484349802136 Scheduler time: 5.961288783699274 Scheduler overhead time: 0.034203109331429005 Adapter cache time: 0.010908983647823334 Engine time: 0.03568504052236676 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-16/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.3829859416000545,
    "estimated_duration": 3600.0865108078756,
    "input_throughput": 6307.298152928132,
    "output_throughput": 5574.87853132068,
    "total_throughput": 11882.176684248812,
    "itl": 154.53958096710753,
    "ttft": 1999714.62738441,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5736459088418642,
    "arrivals": 738702,
    "finished_requests": 91839,
    "scheduler_time": 79.77748711318186
}
#Debug simulation 
Total elapsed time: 6.383109796792269. Arrivals time: 0.3536777840927243 Scheduler time: 5.9325597286224365 Scheduler overhead time: 0.03409447055310011 Adapter cache time: 0.010929913260042667 Engine time: 0.03578476468101144 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-8-32/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.8699546051211655,
    "estimated_duration": 3600.0770009964385,
    "input_throughput": 5645.5628572318155,
    "output_throughput": 5000.494988028677,
    "total_throughput": 10646.057845260493,
    "itl": 101.8093052523691,
    "ttft": 2059759.4831081855,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5619314563646933,
    "arrivals": 738702,
    "finished_requests": 82122,
    "scheduler_time": 70.058875246589
}
#Debug simulation 
Total elapsed time: 5.870093322824687. Arrivals time: 0.33167090313509107 Scheduler time: 5.401785544585437 Scheduler overhead time: 0.049205650575459 Adapter cache time: 0.013481752015650272 Engine time: 0.05054302606731653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-16/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.38913240423426,
    "estimated_duration": 3600.0055359338953,
    "input_throughput": 6307.335023058405,
    "output_throughput": 5574.794482862851,
    "total_throughput": 11882.129505921255,
    "itl": 154.53951950645413,
    "ttft": 1999694.1126313442,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5503559756907641,
    "arrivals": 738702,
    "finished_requests": 91836,
    "scheduler_time": 79.77602230823416
}
#Debug simulation 
Total elapsed time: 6.389224828220904. Arrivals time: 0.3613334456458688 Scheduler time: 5.929788971319795 Scheduler overhead time: 0.03510889131575823 Adapter cache time: 0.010888681281358004 Engine time: 0.035842463839799166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_8-16-32/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.9175233617424965,
    "estimated_duration": 3600.0841714632547,
    "input_throughput": 5645.5516126833,
    "output_throughput": 5000.485028293941,
    "total_throughput": 10646.03664097724,
    "itl": 101.80947381981292,
    "ttft": 2059763.5166787396,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5690994221903397,
    "arrivals": 738702,
    "finished_requests": 82122,
    "scheduler_time": 70.05887774758247
}
#Debug simulation 
Total elapsed time: 5.917639997787774. Arrivals time: 0.33435404766350985 Scheduler time: 5.445309141650796 Scheduler overhead time: 0.04971247538924217 Adapter cache time: 0.013708441983908415 Engine time: 0.05102203041315079 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-16/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.362485662102699,
    "estimated_duration": 3600.1068569187487,
    "input_throughput": 6307.497777839569,
    "output_throughput": 5574.95257715151,
    "total_throughput": 11882.450354991079,
    "itl": 154.53668781324563,
    "ttft": 1999714.884085972,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 176,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5262488519027836,
    "arrivals": 738702,
    "finished_requests": 91842,
    "scheduler_time": 79.77847240198275
}
#Debug simulation 
Total elapsed time: 6.362581328954548. Arrivals time: 0.3291131090372801 Scheduler time: 5.93677398795262 Scheduler overhead time: 0.03416178887709975 Adapter cache time: 0.010874601546674967 Engine time: 0.0355523731559515 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        3.2,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_3.2-0.00625-0.003125_size_16-16-32/adapters_192_slots_160_rate_3.2-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [3.125e-03 6.250e-03 3.200e+00]. Counts: [64 64 64]
Adapter prompts. [34560, 33, 34560, 33, 34560, 33, 34560, 33, 33, 33, 66, 66, 66, 66, 34560, 66, 33, 34560, 33, 66, 66, 33, 34560, 34560, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 33, 66, 66, 34560, 66, 33, 33, 66, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 34560, 34560, 33, 66, 34560, 33, 66, 33, 66, 33, 33, 66, 34560, 34560, 33, 33, 33, 34560, 34560, 34560, 66, 33, 33, 33, 33, 33, 34560, 33, 66, 33, 34560, 34560, 33, 34560, 33, 66, 66, 34560, 33, 33, 33, 34560, 34560, 66, 34560, 66, 34560, 34560, 66, 66, 33, 34560, 66, 66, 66, 34560, 66, 33, 66, 34560, 34560, 66, 34560, 66, 33, 34560, 66, 33, 66, 66, 33, 66, 34560, 33, 66, 34560, 33, 66, 34560, 34560, 66, 33, 34560, 66, 33, 33, 66, 34560, 34560, 34560, 33, 34560, 33, 34560, 33, 66, 33, 34560, 34560, 66, 34560, 33, 34560, 34560, 34560, 33, 34560, 34560, 34560, 66, 66, 66, 66, 66, 66, 33, 34560, 66, 34560, 33, 34560, 66, 33, 66, 33, 33, 66, 66, 66, 34560, 66, 33, 33]
Prompts retrieved: 2218176 . Total input tokens: 495031225 . Total output tokens: 443862300
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.852004738058895,
    "estimated_duration": 3600.0913423288885,
    "input_throughput": 5645.540367554165,
    "output_throughput": 5000.475068044927,
    "total_throughput": 10646.01543559909,
    "itl": 101.80964225303816,
    "ttft": 2059767.6254762204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 172,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5762673880159862,
    "arrivals": 738702,
    "finished_requests": 82122,
    "scheduler_time": 70.05888064739311
}
#Debug simulation 
Total elapsed time: 5.8521151430904865. Arrivals time: 0.26702917693182826 Scheduler time: 5.447920163627714 Scheduler overhead time: 0.04955609003081918 Adapter cache time: 0.013621475081890821 Engine time: 0.05060225073248148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.389259637333453,
    "estimated_duration": 3600.1650109542734,
    "input_throughput": 4118.287899273269,
    "output_throughput": 3639.644283006568,
    "total_throughput": 7757.932182279837,
    "itl": 235.48501040200264,
    "ttft": 2201916.4130816087,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 715,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1882487369119588,
    "arrivals": 644931,
    "finished_requests": 60090,
    "scheduler_time": 52.319151136552705
}
#Debug simulation 
Total elapsed time: 5.389329789206386. Arrivals time: 0.6030714162625372 Scheduler time: 4.706880182027817 Scheduler overhead time: 0.02450487855821848 Adapter cache time: 0.01923567522317171 Engine time: 0.024522562976926565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.084359225817025,
    "estimated_duration": 3600.1390825518147,
    "input_throughput": 4117.260655800418,
    "output_throughput": 3638.60387046907,
    "total_throughput": 7755.864526269488,
    "itl": 235.48486992941343,
    "ttft": 2201942.9901604173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 720,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.353456513683785,
    "arrivals": 644931,
    "finished_requests": 60075,
    "scheduler_time": 52.31648620278249
}
#Debug simulation 
Total elapsed time: 5.084449517074972. Arrivals time: 0.290972120128572 Scheduler time: 4.71320033678785 Scheduler overhead time: 0.024702483788132668 Adapter cache time: 0.01943664625287056 Engine time: 0.024978680070489645 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.188080180902034,
    "estimated_duration": 3600.118376491563,
    "input_throughput": 3653.0157135600566,
    "output_throughput": 3237.008559523212,
    "total_throughput": 6890.024273083269,
    "itl": 156.34491116168041,
    "ttft": 2273304.0623500133,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2911,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.537550832777917,
    "arrivals": 644931,
    "finished_requests": 53263,
    "scheduler_time": 45.45257053923074
}
#Debug simulation 
Total elapsed time: 4.18817294202745. Arrivals time: 0.2687052134424448 Scheduler time: 3.763447900302708 Scheduler overhead time: 0.03461273666471243 Adapter cache time: 0.07047747960314155 Engine time: 0.03482619160786271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.10826814500615,
    "estimated_duration": 3600.218224204143,
    "input_throughput": 4118.227028662275,
    "output_throughput": 3639.590487017379,
    "total_throughput": 7757.817515679653,
    "itl": 235.4883178371523,
    "ttft": 2201938.6787654646,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 715,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.24248636237577,
    "arrivals": 644931,
    "finished_requests": 60090,
    "scheduler_time": 52.31915994480289
}
#Debug simulation 
Total elapsed time: 5.108358756173402. Arrivals time: 0.2907995879650116 Scheduler time: 4.737273045349866 Scheduler overhead time: 0.024984341580420732 Adapter cache time: 0.019007498398423195 Engine time: 0.02505638124421239 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.4_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.142208985053003,
    "estimated_duration": 3600.0520547837277,
    "input_throughput": 3652.71107192087,
    "output_throughput": 3236.712920446927,
    "total_throughput": 6889.423992367797,
    "itl": 156.3493989749909,
    "ttft": 2273326.988581402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2902,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 9.631614472046095,
    "arrivals": 644931,
    "finished_requests": 53260,
    "scheduler_time": 45.45000499613778
}
#Debug simulation 
Total elapsed time: 4.142299876082689. Arrivals time: 0.26443642284721136 Scheduler time: 3.723992872517556 Scheduler overhead time: 0.03449252713471651 Adapter cache time: 0.06902992632240057 Engine time: 0.034479858819395304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.4_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.4_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.080854694824666,
    "estimated_duration": 3600.113612769948,
    "input_throughput": 4118.346695340094,
    "output_throughput": 3639.69624556327,
    "total_throughput": 7758.042940903364,
    "itl": 235.48238371164945,
    "ttft": 2201894.3113969914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 715,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.1378859608550167,
    "arrivals": 644931,
    "finished_requests": 60090,
    "scheduler_time": 52.31914891210435
}
#Debug simulation 
Total elapsed time: 5.080976335797459. Arrivals time: 0.28747675474733114 Scheduler time: 4.7136939112097025 Scheduler overhead time: 0.024626439437270164 Adapter cache time: 0.019083881750702858 Engine time: 0.024868127889931202 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.4_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.4
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.4_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.4_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.4 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 4320, 17280, 4320, 17280, 4320, 17280, 4320, 4320, 4320, 8640, 8640, 8640, 8640, 17280, 8640, 4320, 17280, 4320, 8640, 8640, 4320, 17280, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 4320, 4320, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 4320, 8640, 8640, 17280, 8640, 4320, 4320, 8640, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 17280, 17280, 4320, 8640, 17280, 4320, 8640, 4320, 8640, 4320, 4320, 8640, 17280, 17280, 4320, 4320, 4320, 17280, 17280, 17280, 8640, 4320, 4320, 4320, 4320, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 4320, 17280, 4320, 8640, 8640, 17280, 4320, 4320, 4320, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 4320, 17280, 8640, 8640, 8640, 17280, 8640, 4320, 8640, 17280, 17280, 8640, 17280, 8640, 4320, 17280, 8640, 4320, 8640, 8640, 4320, 8640, 17280, 4320, 8640, 17280, 4320, 8640, 17280, 17280, 8640, 4320, 17280, 8640, 4320, 4320, 8640, 17280, 17280, 17280, 4320, 17280, 4320, 17280, 4320, 8640, 4320, 17280, 17280, 8640, 17280, 4320, 17280, 17280, 17280, 4320, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 4320, 17280, 8640, 17280, 4320, 17280, 8640, 4320, 8640, 4320, 4320, 8640, 8640, 8640, 17280, 8640, 4320, 4320]
Prompts retrieved: 1935360 . Total input tokens: 431832050 . Total output tokens: 387188135
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 3.788825123105198,
    "estimated_duration": 3600.130358509305,
    "input_throughput": 3190.81529168753,
    "output_throughput": 2834.136818374775,
    "total_throughput": 6024.952110062305,
    "itl": 117.50233818921754,
    "ttft": 2348583.120524302,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 2056,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.928826610445756,
    "arrivals": 644931,
    "finished_requests": 46539,
    "scheduler_time": 38.85378840883267
}
#Debug simulation 
Total elapsed time: 3.788912975229323. Arrivals time: 0.24966924730688334 Scheduler time: 3.37985277781263 Scheduler overhead time: 0.042266637086868286 Adapter cache time: 0.05451072659343481 Engine time: 0.04263435723260045 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.432960954029113,
    "estimated_duration": 3600.0687340581853,
    "input_throughput": 4163.750224597165,
    "output_throughput": 3693.370038802459,
    "total_throughput": 7857.120263399624,
    "itl": 232.7545256886321,
    "ttft": 2181841.563907824,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.324469182177095,
    "arrivals": 576087,
    "finished_requests": 60822,
    "scheduler_time": 53.05372307480475
}
#Debug simulation 
Total elapsed time: 4.433073132298887. Arrivals time: 0.27976053627207875 Scheduler time: 4.059217090718448 Scheduler overhead time: 0.023635653778910637 Adapter cache time: 0.03493325551971793 Engine time: 0.024503552354872227 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.371213912963867,
    "estimated_duration": 3600.1858423380413,
    "input_throughput": 4163.446459826554,
    "output_throughput": 3693.0396324667286,
    "total_throughput": 7856.486092293283,
    "itl": 232.77472293892797,
    "ttft": 2181941.1409226367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.626460381657773,
    "arrivals": 576087,
    "finished_requests": 60819,
    "scheduler_time": 53.05099260194028
}
#Debug simulation 
Total elapsed time: 4.371328039094806. Arrivals time: 0.2709619360975921 Scheduler time: 4.007130884565413 Scheduler overhead time: 0.02347931358963251 Adapter cache time: 0.034730070270597935 Engine time: 0.024161892477422953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.270661901216954,
    "estimated_duration": 3600.132209088547,
    "input_throughput": 3875.965989463693,
    "output_throughput": 3460.6117987967405,
    "total_throughput": 7336.577788260433,
    "itl": 147.38002488400608,
    "ttft": 2225566.455319216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1403,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.60157123195001,
    "arrivals": 576087,
    "finished_requests": 56704,
    "scheduler_time": 48.581535297346555
}
#Debug simulation 
Total elapsed time: 4.2707517659291625. Arrivals time: 0.25881300680339336 Scheduler time: 3.856848444789648 Scheduler overhead time: 0.035106660798192024 Adapter cache time: 0.0673161637969315 Engine time: 0.03606299078091979 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.404675537254661,
    "estimated_duration": 3600.2206619046347,
    "input_throughput": 4163.574516032557,
    "output_throughput": 3693.2141800902214,
    "total_throughput": 7856.788696122778,
    "itl": 232.7620748634459,
    "ttft": 2181885.8577654823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1412,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.429517438169498,
    "arrivals": 576087,
    "finished_requests": 60822,
    "scheduler_time": 53.05426768525559
}
#Debug simulation 
Total elapsed time: 4.404766594991088. Arrivals time: 0.2721716524101794 Scheduler time: 4.038725655991584 Scheduler overhead time: 0.023547108750790358 Adapter cache time: 0.0351832932792604 Engine time: 0.024211709387600422 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.259813134092838,
    "estimated_duration": 3600.0264124035057,
    "input_throughput": 3875.9954515678187,
    "output_throughput": 3460.677109777715,
    "total_throughput": 7336.672561345534,
    "itl": 147.3833226958714,
    "ttft": 2225667.3611089336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1402,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.658135007861966,
    "arrivals": 576087,
    "finished_requests": 56701,
    "scheduler_time": 48.57937705359634
}
#Debug simulation 
Total elapsed time: 4.259903731290251. Arrivals time: 0.20503229880705476 Scheduler time: 3.9007165748625994 Scheduler overhead time: 0.0348442243412137 Adapter cache time: 0.06698222970589995 Engine time: 0.03582896664738655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.367342928890139,
    "estimated_duration": 3600.210169785439,
    "input_throughput": 4163.949406568512,
    "output_throughput": 3693.3599353707873,
    "total_throughput": 7857.309341939299,
    "itl": 232.7469307498534,
    "ttft": 2181846.694685776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.224941066696661,
    "arrivals": 576087,
    "finished_requests": 60826,
    "scheduler_time": 53.05736689319452
}
#Debug simulation 
Total elapsed time: 4.367435886990279. Arrivals time: 0.27030939189717174 Scheduler time: 4.00370730785653 Scheduler overhead time: 0.02341791382059455 Adapter cache time: 0.03502233698964119 Engine time: 0.02408195286989212 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.1_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.8 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 8640, 8640, 8640, 8640, 17280, 8640, 1080, 17280, 1080, 8640, 8640, 1080, 17280, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 1080, 1080, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 1080, 8640, 8640, 17280, 8640, 1080, 1080, 8640, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 17280, 17280, 1080, 8640, 17280, 1080, 8640, 1080, 8640, 1080, 1080, 8640, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 8640, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 1080, 17280, 1080, 8640, 8640, 17280, 1080, 1080, 1080, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 1080, 17280, 8640, 8640, 8640, 17280, 8640, 1080, 8640, 17280, 17280, 8640, 17280, 8640, 1080, 17280, 8640, 1080, 8640, 8640, 1080, 8640, 17280, 1080, 8640, 17280, 1080, 8640, 17280, 17280, 8640, 1080, 17280, 8640, 1080, 1080, 8640, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 8640, 1080, 17280, 17280, 8640, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 1080, 17280, 8640, 17280, 1080, 17280, 8640, 1080, 8640, 1080, 1080, 8640, 8640, 8640, 17280, 8640, 1080, 1080]
Prompts retrieved: 1728000 . Total input tokens: 385646006 . Total output tokens: 345701708
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.248355666175485,
    "estimated_duration": 3600.111462085676,
    "input_throughput": 3862.8781765393696,
    "output_throughput": 3448.697944703949,
    "total_throughput": 7311.576121243319,
    "itl": 147.50208689607373,
    "ttft": 2226618.9313810407,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1413,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.754696389921,
    "arrivals": 576087,
    "finished_requests": 56506,
    "scheduler_time": 48.427634761486836
}
#Debug simulation 
Total elapsed time: 4.248445259872824. Arrivals time: 0.2042627902701497 Scheduler time: 3.891121043357998 Scheduler overhead time: 0.035019843839108944 Adapter cache time: 0.06549807405099273 Engine time: 0.03598354524001479 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.600961972028017,
    "estimated_duration": 3600.091362754036,
    "input_throughput": 4395.101236512991,
    "output_throughput": 3864.5452568061783,
    "total_throughput": 8259.64649331917,
    "itl": 221.2383520315713,
    "ttft": 2154015.2172917575,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 771,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.359636050572199,
    "arrivals": 564621,
    "finished_requests": 63903,
    "scheduler_time": 55.525236468067796
}
#Debug simulation 
Total elapsed time: 4.601066097151488. Arrivals time: 0.23111600195989013 Scheduler time: 4.281586083583534 Scheduler overhead time: 0.024461361579596996 Adapter cache time: 0.02721009636297822 Engine time: 0.02524435706436634 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.589954666327685,
    "estimated_duration": 3600.2159250216732,
    "input_throughput": 4394.949172362418,
    "output_throughput": 3864.411549125695,
    "total_throughput": 8259.360721488112,
    "itl": 221.25034629683284,
    "ttft": 2154036.176650929,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 772,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.533046252806682,
    "arrivals": 564621,
    "finished_requests": 63903,
    "scheduler_time": 55.52449303511504
}
#Debug simulation 
Total elapsed time: 4.590074265375733. Arrivals time: 0.28316146740689874 Scheduler time: 4.218312442302704 Scheduler overhead time: 0.024502749554812908 Adapter cache time: 0.027329910546541214 Engine time: 0.02531598601490259 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.391512949950993,
    "estimated_duration": 3600.0282994805602,
    "input_throughput": 4060.193082956883,
    "output_throughput": 3578.712978967117,
    "total_throughput": 7638.906061924,
    "itl": 141.99463627710622,
    "ttft": 2204890.1981477784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.395105680394913,
    "arrivals": 564621,
    "finished_requests": 59020,
    "scheduler_time": 50.238145226412335
}
#Debug simulation 
Total elapsed time: 4.391604697331786. Arrivals time: 0.2738961293362081 Scheduler time: 3.9749441808089614 Scheduler overhead time: 0.03636903781443834 Adapter cache time: 0.052165794651955366 Engine time: 0.03718758700415492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.6028033453039825,
    "estimated_duration": 3600.1514671198674,
    "input_throughput": 4395.027860496732,
    "output_throughput": 3864.480738397992,
    "total_throughput": 8259.508598894723,
    "itl": 221.24170150084973,
    "ttft": 2154039.409844413,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 771,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4197354665328574,
    "arrivals": 564621,
    "finished_requests": 63903,
    "scheduler_time": 55.52524141787308
}
#Debug simulation 
Total elapsed time: 4.602923852391541. Arrivals time: 0.23531464347615838 Scheduler time: 4.278706948272884 Scheduler overhead time: 0.024651377461850643 Adapter cache time: 0.027202425058931112 Engine time: 0.02549075661227107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.402399251237512,
    "estimated_duration": 3600.0603711799663,
    "input_throughput": 4060.156912093436,
    "output_throughput": 3578.6810974442847,
    "total_throughput": 7638.838009537721,
    "itl": 141.99587457024228,
    "ttft": 2204903.0999652124,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4271728959307084,
    "arrivals": 564621,
    "finished_requests": 59020,
    "scheduler_time": 50.23814971030974
}
#Debug simulation 
Total elapsed time: 4.40256372699514. Arrivals time: 0.29289398016408086 Scheduler time: 3.967895344365388 Scheduler overhead time: 0.03614869900047779 Adapter cache time: 0.05140137765556574 Engine time: 0.037110425531864166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.5478763319551945,
    "estimated_duration": 3600.0369184673855,
    "input_throughput": 4395.167704762344,
    "output_throughput": 3864.6037013206374,
    "total_throughput": 8259.771406082982,
    "itl": 221.23542820780918,
    "ttft": 2153992.2843289594,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 771,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3053287773695326,
    "arrivals": 564621,
    "finished_requests": 63903,
    "scheduler_time": 55.52523494807216
}
#Debug simulation 
Total elapsed time: 4.547969350125641. Arrivals time: 0.21954984730109572 Scheduler time: 4.239809533581138 Scheduler overhead time: 0.024508686270564795 Adapter cache time: 0.027316146064549685 Engine time: 0.025419358629733324 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.05_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.8  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 8640, 8640, 8640, 8640, 17280, 8640, 540, 17280, 540, 8640, 8640, 540, 17280, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 540, 540, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 540, 8640, 8640, 17280, 8640, 540, 540, 8640, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 17280, 17280, 540, 8640, 17280, 540, 8640, 540, 8640, 540, 540, 8640, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 8640, 540, 540, 540, 540, 540, 17280, 540, 8640, 540, 17280, 17280, 540, 17280, 540, 8640, 8640, 17280, 540, 540, 540, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 540, 17280, 8640, 8640, 8640, 17280, 8640, 540, 8640, 17280, 17280, 8640, 17280, 8640, 540, 17280, 8640, 540, 8640, 8640, 540, 8640, 17280, 540, 8640, 17280, 540, 8640, 17280, 17280, 8640, 540, 17280, 8640, 540, 540, 8640, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 8640, 540, 17280, 17280, 8640, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 540, 17280, 8640, 17280, 540, 17280, 8640, 540, 8640, 540, 540, 8640, 8640, 8640, 17280, 8640, 540, 540]
Prompts retrieved: 1693440 . Total input tokens: 377951543 . Total output tokens: 338793141
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.383831087965518,
    "estimated_duration": 3600.124360660836,
    "input_throughput": 4060.2525178649216,
    "output_throughput": 3578.7633173969093,
    "total_throughput": 7639.015835261831,
    "itl": 141.99656505784628,
    "ttft": 2204981.872084178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 729,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.460874910689901,
    "arrivals": 564621,
    "finished_requests": 59021,
    "scheduler_time": 50.23832015868665
}
#Debug simulation 
Total elapsed time: 4.383923058863729. Arrivals time: 0.2113471133634448 Scheduler time: 4.031139576341957 Scheduler overhead time: 0.03606158075854182 Adapter cache time: 0.051185768097639084 Engine time: 0.036983802914619446 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.6761730229482055,
    "estimated_duration": 3600.1620832759268,
    "input_throughput": 4514.175368796699,
    "output_throughput": 3988.7120823552677,
    "total_throughput": 8502.887451151966,
    "itl": 215.0474207755189,
    "ttft": 2130744.9453207557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 482,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4751550925756025,
    "arrivals": 558945,
    "finished_requests": 65932,
    "scheduler_time": 57.296560936820605
}
#Debug simulation 
Total elapsed time: 4.676267780829221. Arrivals time: 0.2283677477389574 Scheduler time: 4.365251217037439 Scheduler overhead time: 0.024991496931761503 Adapter cache time: 0.020078360568732023 Engine time: 0.025859525427222252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.672212386969477,
    "estimated_duration": 3600.194863013481,
    "input_throughput": 4514.034272688518,
    "output_throughput": 3988.623268013987,
    "total_throughput": 8502.657540702505,
    "itl": 215.05367925560563,
    "ttft": 2130786.5975272674,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 482,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.578083673820372,
    "arrivals": 558945,
    "finished_requests": 65930,
    "scheduler_time": 57.29522133518199
}
#Debug simulation 
Total elapsed time: 4.672311181202531. Arrivals time: 0.2795098265632987 Scheduler time: 4.309477164410055 Scheduler overhead time: 0.025137878954410553 Adapter cache time: 0.020369367208331823 Engine time: 0.026056381408125162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.48726437613368,
    "estimated_duration": 3600.090926383013,
    "input_throughput": 4122.652261705951,
    "output_throughput": 3658.8745310480535,
    "total_throughput": 7781.526792754004,
    "itl": 139.4919329211479,
    "ttft": 2185257.9072615574,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4819034804217606,
    "arrivals": 558945,
    "finished_requests": 60159,
    "scheduler_time": 51.351989450774894
}
#Debug simulation 
Total elapsed time: 4.487400311045349. Arrivals time: 0.2956828745082021 Scheduler time: 4.05686212470755 Scheduler overhead time: 0.036877776961773634 Adapter cache time: 0.0426979442127049 Engine time: 0.03776542795822024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.680904142092913,
    "estimated_duration": 3600.054934204591,
    "input_throughput": 4514.194448977382,
    "output_throughput": 3988.6935789696895,
    "total_throughput": 8502.888027947072,
    "itl": 215.05106232090301,
    "ttft": 2130723.9044971783,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 482,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5139342088252268,
    "arrivals": 558945,
    "finished_requests": 65928,
    "scheduler_time": 57.29400140124312
}
#Debug simulation 
Total elapsed time: 4.681020710151643. Arrivals time: 0.281092272605747 Scheduler time: 4.3171728709712625 Scheduler overhead time: 0.02502491930499673 Adapter cache time: 0.020076468586921692 Engine time: 0.025920933578163385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.448058856185526,
    "estimated_duration": 3600.121385102283,
    "input_throughput": 4122.479886765913,
    "output_throughput": 3658.8060765139357,
    "total_throughput": 7781.285963279848,
    "itl": 139.49284020093535,
    "ttft": 2185273.744842898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 452,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5008923021704004,
    "arrivals": 558945,
    "finished_requests": 60158,
    "scheduler_time": 51.35292084580178
}
#Debug simulation 
Total elapsed time: 4.44815306738019. Arrivals time: 0.2654926856048405 Scheduler time: 4.048687782138586 Scheduler overhead time: 0.036726209335029125 Adapter cache time: 0.04225026862695813 Engine time: 0.03767920425161719 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.71473895618692,
    "estimated_duration": 3600.1281270508334,
    "input_throughput": 4514.217946268813,
    "output_throughput": 3988.7497036844316,
    "total_throughput": 8502.967649953245,
    "itl": 215.04574581626161,
    "ttft": 2130729.6683575916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 482,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4412042421428344,
    "arrivals": 558945,
    "finished_requests": 65932,
    "scheduler_time": 57.29655556210732
}
#Debug simulation 
Total elapsed time: 4.714829670265317. Arrivals time: 0.2878761417232454 Scheduler time: 4.343841145280749 Scheduler overhead time: 0.02496634377166629 Adapter cache time: 0.020572999957948923 Engine time: 0.02586327586323023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.025_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.8   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 8640, 8640, 8640, 8640, 17280, 8640, 270, 17280, 270, 8640, 8640, 270, 17280, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 270, 270, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 270, 8640, 8640, 17280, 8640, 270, 270, 8640, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 17280, 17280, 270, 8640, 17280, 270, 8640, 270, 8640, 270, 270, 8640, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 8640, 270, 270, 270, 270, 270, 17280, 270, 8640, 270, 17280, 17280, 270, 17280, 270, 8640, 8640, 17280, 270, 270, 270, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 270, 17280, 8640, 8640, 8640, 17280, 8640, 270, 8640, 17280, 17280, 8640, 17280, 8640, 270, 17280, 8640, 270, 8640, 8640, 270, 8640, 17280, 270, 8640, 17280, 270, 8640, 17280, 17280, 8640, 270, 17280, 8640, 270, 270, 8640, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 8640, 270, 17280, 17280, 8640, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 270, 17280, 8640, 17280, 270, 17280, 8640, 270, 8640, 270, 270, 8640, 8640, 8640, 17280, 8640, 270, 270]
Prompts retrieved: 1676160 . Total input tokens: 374117875 . Total output tokens: 335339094
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.449182681273669,
    "estimated_duration": 3600.144745069644,
    "input_throughput": 4122.398973075984,
    "output_throughput": 3658.715394162629,
    "total_throughput": 7781.114367238612,
    "itl": 139.4962902300052,
    "ttft": 2185255.7603980093,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 453,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5250224432721733,
    "arrivals": 558945,
    "finished_requests": 60155,
    "scheduler_time": 51.35272516392728
}
#Debug simulation 
Total elapsed time: 4.449297689832747. Arrivals time: 0.26334316143766046 Scheduler time: 4.052155764773488 Scheduler overhead time: 0.036722158547490835 Adapter cache time: 0.042059685569256544 Engine time: 0.03773073572665453 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.709092243108898,
    "estimated_duration": 3600.0689810157874,
    "input_throughput": 4633.934540691192,
    "output_throughput": 4062.478268367343,
    "total_throughput": 8696.412809058535,
    "itl": 210.27771412313643,
    "ttft": 2116770.4722036542,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9793560780584799,
    "arrivals": 556042,
    "finished_requests": 67223,
    "scheduler_time": 58.28914482106393
}
#Debug simulation 
Total elapsed time: 4.709249582141638. Arrivals time: 0.23164166882634163 Scheduler time: 4.3980081412009895 Scheduler overhead time: 0.02557471999898553 Adapter cache time: 0.015781066846102476 Engine time: 0.026285273022949696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.998690103646368,
    "estimated_duration": 3600.177061521306,
    "input_throughput": 4633.922364071279,
    "output_throughput": 4062.624629306289,
    "total_throughput": 8696.546993377568,
    "itl": 210.2805414020344,
    "ttft": 2116797.651240065,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0475242550624597,
    "arrivals": 556042,
    "finished_requests": 67225,
    "scheduler_time": 58.290238168773946
}
#Debug simulation 
Total elapsed time: 4.998756849672645. Arrivals time: 0.5238188174553216 Scheduler time: 4.394924399442971 Scheduler overhead time: 0.025618513580411673 Adapter cache time: 0.015927878208458424 Engine time: 0.026488285046070814 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.448278162162751,
    "estimated_duration": 3600.1453838780585,
    "input_throughput": 4203.844396888563,
    "output_throughput": 3697.361795334336,
    "total_throughput": 7901.206192222899,
    "itl": 137.11756431771337,
    "ttft": 2173988.998019767,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 305,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9990952227264696,
    "arrivals": 556042,
    "finished_requests": 60950,
    "scheduler_time": 51.893840320331016
}
#Debug simulation 
Total elapsed time: 4.448399411980063. Arrivals time: 0.21270805830135942 Scheduler time: 4.101363209541887 Scheduler overhead time: 0.0374670154415071 Adapter cache time: 0.040427299216389656 Engine time: 0.03872106643393636 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.715155560988933,
    "estimated_duration": 3600.1039027779307,
    "input_throughput": 4633.889868325015,
    "output_throughput": 4062.5280255702005,
    "total_throughput": 8696.417893895215,
    "itl": 210.27823813061912,
    "ttft": 2116786.952289721,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0025787700340192,
    "arrivals": 556042,
    "finished_requests": 67224,
    "scheduler_time": 58.289515758950614
}
#Debug simulation 
Total elapsed time: 4.715246986132115. Arrivals time: 0.2318997154943645 Scheduler time: 4.402943333145231 Scheduler overhead time: 0.02588792284950614 Adapter cache time: 0.015807312447577715 Engine time: 0.026726469863206148 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.747316824272275,
    "estimated_duration": 3600.025315971487,
    "input_throughput": 4203.624883653418,
    "output_throughput": 3697.04039050858,
    "total_throughput": 7900.665274161997,
    "itl": 137.11802031234217,
    "ttft": 2173957.9837005995,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 305,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0121736165136155,
    "arrivals": 556042,
    "finished_requests": 60945,
    "scheduler_time": 51.89241081283128
}
#Debug simulation 
Total elapsed time: 4.747386698145419. Arrivals time: 0.5066311983391643 Scheduler time: 4.108495508786291 Scheduler overhead time: 0.037238459568470716 Adapter cache time: 0.03913902351632714 Engine time: 0.03807774465531111 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.748233739752322,
    "estimated_duration": 3600.1511868036573,
    "input_throughput": 4633.9559463922715,
    "output_throughput": 4062.8063214717718,
    "total_throughput": 8696.762267864044,
    "itl": 210.275648546557,
    "ttft": 2116754.112983801,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 320,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9568160943686975,
    "arrivals": 556042,
    "finished_requests": 67226,
    "scheduler_time": 58.29109105216703
}
#Debug simulation 
Total elapsed time: 4.748355587944388. Arrivals time: 0.23256019363179803 Scheduler time: 4.434365631546825 Scheduler overhead time: 0.025879543740302324 Adapter cache time: 0.016632414888590574 Engine time: 0.026784312445670366 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.8    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 8640, 8640, 8640, 8640, 17280, 8640, 135, 17280, 135, 8640, 8640, 135, 17280, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 135, 135, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 135, 8640, 8640, 17280, 8640, 135, 135, 8640, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 17280, 17280, 135, 8640, 17280, 135, 8640, 135, 8640, 135, 135, 8640, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 8640, 135, 135, 135, 135, 135, 17280, 135, 8640, 135, 17280, 17280, 135, 17280, 135, 8640, 8640, 17280, 135, 135, 135, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 135, 17280, 8640, 8640, 8640, 17280, 8640, 135, 8640, 17280, 17280, 8640, 17280, 8640, 135, 17280, 8640, 135, 8640, 8640, 135, 8640, 17280, 135, 8640, 17280, 135, 8640, 17280, 17280, 8640, 135, 17280, 8640, 135, 135, 8640, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 8640, 135, 17280, 17280, 8640, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 135, 17280, 8640, 17280, 135, 17280, 8640, 135, 8640, 135, 135, 8640, 8640, 8640, 17280, 8640, 135, 135]
Prompts retrieved: 1667520 . Total input tokens: 372193491 . Total output tokens: 333578746
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.470287324860692,
    "estimated_duration": 3600.032138337475,
    "input_throughput": 4203.512473915982,
    "output_throughput": 3697.032828753137,
    "total_throughput": 7900.545302669118,
    "itl": 137.12538920709784,
    "ttft": 2173836.3451075032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 305,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0255035178735907,
    "arrivals": 556042,
    "finished_requests": 60944,
    "scheduler_time": 51.89214516806258
}
#Debug simulation 
Total elapsed time: 4.470376367680728. Arrivals time: 0.2151153776794672 Scheduler time: 4.1213334975764155 Scheduler overhead time: 0.03756000241264701 Adapter cache time: 0.04016906814649701 Engine time: 0.038449923042207956 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.781336717307568,
    "estimated_duration": 3600.1275429648963,
    "input_throughput": 4644.603225981608,
    "output_throughput": 4095.851000838772,
    "total_throughput": 8740.45422682038,
    "itl": 209.31037014438851,
    "ttft": 2120967.369186443,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6947307178727342,
    "arrivals": 554638,
    "finished_requests": 67566,
    "scheduler_time": 58.73359369197153
}
#Debug simulation 
Total elapsed time: 4.7814311240799725. Arrivals time: 0.2375424113124609 Scheduler time: 4.465688545256853 Scheduler overhead time: 0.025961038656532764 Adapter cache time: 0.013352248817682266 Engine time: 0.026817680336534977 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.727810791693628,
    "estimated_duration": 3600.0737803544666,
    "input_throughput": 4644.4748136116505,
    "output_throughput": 4095.5624522084822,
    "total_throughput": 8740.037265820132,
    "itl": 209.31395224558207,
    "ttft": 2120968.44866945,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7432994772563719,
    "arrivals": 554638,
    "finished_requests": 67561,
    "scheduler_time": 58.73203936587144
}
#Debug simulation 
Total elapsed time: 4.7279342277906835. Arrivals time: 0.23154068551957607 Scheduler time: 4.420096772257239 Scheduler overhead time: 0.025560310576111078 Adapter cache time: 0.012019751593470573 Engine time: 0.02665415871888399 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.475524458102882,
    "estimated_duration": 3600.1422713203942,
    "input_throughput": 4189.087503608226,
    "output_throughput": 3714.090164302295,
    "total_throughput": 7903.177667910521,
    "itl": 136.83099630351148,
    "ttft": 2182987.752985155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7242165322415564,
    "arrivals": 554638,
    "finished_requests": 60988,
    "scheduler_time": 52.08848213188297
}
#Debug simulation 
Total elapsed time: 4.475617823190987. Arrivals time: 0.21711157448589802 Scheduler time: 4.130074200220406 Scheduler overhead time: 0.03806929476559162 Adapter cache time: 0.034247891046106815 Engine time: 0.03840603120625019 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.761130101047456,
    "estimated_duration": 3600.1668104021683,
    "input_throughput": 4644.796999873462,
    "output_throughput": 4095.8335478774065,
    "total_throughput": 8740.630547750869,
    "itl": 209.30988580536243,
    "ttft": 2120998.5813728394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7114290424180235,
    "arrivals": 554638,
    "finished_requests": 67568,
    "scheduler_time": 58.73404877691997
}
#Debug simulation 
Total elapsed time: 4.761262457817793. Arrivals time: 0.2615379043854773 Scheduler time: 4.423571765888482 Scheduler overhead time: 0.025750176515430212 Adapter cache time: 0.011817221064120531 Engine time: 0.026437193155288696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.46006297506392,
    "estimated_duration": 3600.101999502305,
    "input_throughput": 4189.882398355737,
    "output_throughput": 3714.839746720748,
    "total_throughput": 7904.722145076485,
    "itl": 136.9699424152455,
    "ttft": 2182714.38034069,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7337738200090854,
    "arrivals": 554638,
    "finished_requests": 60999,
    "scheduler_time": 52.10883625836043
}
#Debug simulation 
Total elapsed time: 4.460152342915535. Arrivals time: 0.21366229141131043 Scheduler time: 4.118881770409644 Scheduler overhead time: 0.03738090442493558 Adapter cache time: 0.03419433860108256 Engine time: 0.03835845086723566 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.725559341255575,
    "estimated_duration": 3600.0557273416644,
    "input_throughput": 4644.513937106934,
    "output_throughput": 4095.5835455601227,
    "total_throughput": 8740.097482667057,
    "itl": 209.30864886609618,
    "ttft": 2120987.3374274555,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 227,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6787414169427948,
    "arrivals": 554638,
    "finished_requests": 67562,
    "scheduler_time": 58.732491130556674
}
#Debug simulation 
Total elapsed time: 4.725655515212566. Arrivals time: 0.23104354133829474 Scheduler time: 4.4181894632056355 Scheduler overhead time: 0.025731760077178478 Adapter cache time: 0.01202940521761775 Engine time: 0.0266842283308506 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.8     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 8640, 8640, 8640, 8640, 17280, 8640, 66, 17280, 66, 8640, 8640, 66, 17280, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 66, 66, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 66, 8640, 8640, 17280, 8640, 66, 66, 8640, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 17280, 17280, 66, 8640, 17280, 66, 8640, 66, 8640, 66, 66, 8640, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 8640, 66, 66, 66, 66, 66, 17280, 66, 8640, 66, 17280, 17280, 66, 17280, 66, 8640, 8640, 17280, 66, 66, 66, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 66, 17280, 8640, 8640, 8640, 17280, 8640, 66, 8640, 17280, 17280, 8640, 17280, 8640, 66, 17280, 8640, 66, 8640, 8640, 66, 8640, 17280, 66, 8640, 17280, 66, 8640, 17280, 17280, 8640, 66, 17280, 8640, 66, 66, 8640, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 8640, 66, 17280, 17280, 8640, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 66, 17280, 8640, 17280, 66, 17280, 8640, 66, 8640, 66, 66, 8640, 8640, 8640, 17280, 8640, 66, 66]
Prompts retrieved: 1663104 . Total input tokens: 371209554 . Total output tokens: 332703188
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.488370114937425,
    "estimated_duration": 3600.1459597628455,
    "input_throughput": 4189.988452855305,
    "output_throughput": 3714.9982665927882,
    "total_throughput": 7904.986719448092,
    "itl": 136.97038929569646,
    "ttft": 2182735.0389787164,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 221,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7434568615630296,
    "arrivals": 554638,
    "finished_requests": 61002,
    "scheduler_time": 52.10970807999282
}
#Debug simulation 
Total elapsed time: 4.4884721748530865. Arrivals time: 0.2217293200083077 Scheduler time: 4.137632048688829 Scheduler overhead time: 0.03750687511637807 Adapter cache time: 0.035435940604656935 Engine time: 0.03839151235297322 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.7853391179814935,
    "estimated_duration": 3600.1580785148954,
    "input_throughput": 4683.704057505106,
    "output_throughput": 4121.108483691994,
    "total_throughput": 8804.8125411971,
    "itl": 207.69694470829018,
    "ttft": 2111433.812526143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5294643797003658,
    "arrivals": 553841,
    "finished_requests": 68142,
    "scheduler_time": 59.123393537157185
}
#Debug simulation 
Total elapsed time: 4.785460254177451. Arrivals time: 0.2380518764257431 Scheduler time: 4.472148092463613 Scheduler overhead time: 0.02596044633537531 Adapter cache time: 0.010239068418741226 Engine time: 0.02683170000091195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.765657295938581,
    "estimated_duration": 3600.135687462378,
    "input_throughput": 4683.476530820674,
    "output_throughput": 4120.970787758688,
    "total_throughput": 8804.447318579361,
    "itl": 207.70218147067595,
    "ttft": 2111478.1672885455,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5642671626387173,
    "arrivals": 553841,
    "finished_requests": 68138,
    "scheduler_time": 59.121236559205656
}
#Debug simulation 
Total elapsed time: 4.765799309127033. Arrivals time: 0.24142646370455623 Scheduler time: 4.450002643745393 Scheduler overhead time: 0.02577586332336068 Adapter cache time: 0.010052791330963373 Engine time: 0.026496381498873234 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.523288450203836,
    "estimated_duration": 3600.0263459749617,
    "input_throughput": 4225.701297160953,
    "output_throughput": 3729.0121543164305,
    "total_throughput": 7954.713451477383,
    "itl": 136.33320696051234,
    "ttft": 2176549.5914538438,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5487456384859999,
    "arrivals": 553841,
    "finished_requests": 61506,
    "scheduler_time": 52.315338114805186
}
#Debug simulation 
Total elapsed time: 4.52339190710336. Arrivals time: 0.2481852755881846 Scheduler time: 4.148437128402293 Scheduler overhead time: 0.0377839975990355 Adapter cache time: 0.03276402223855257 Engine time: 0.03835336957126856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.790875359904021,
    "estimated_duration": 3600.1886769846146,
    "input_throughput": 4683.664250097873,
    "output_throughput": 4121.073457857387,
    "total_throughput": 8804.73770795526,
    "itl": 207.69721707285328,
    "ttft": 2111452.2667291863,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5409772294876172,
    "arrivals": 553841,
    "finished_requests": 68142,
    "scheduler_time": 59.12377068611942
}
#Debug simulation 
Total elapsed time: 4.790995100047439. Arrivals time: 0.2409236617386341 Scheduler time: 4.47459867503494 Scheduler overhead time: 0.025980949867516756 Adapter cache time: 0.010340817738324404 Engine time: 0.026965816505253315 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.469698351342231,
    "estimated_duration": 3600.118702650254,
    "input_throughput": 4227.021733699314,
    "output_throughput": 3730.125340065654,
    "total_throughput": 7957.147073764968,
    "itl": 136.34928531886277,
    "ttft": 2176751.0842694775,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5556620967388164,
    "arrivals": 553841,
    "finished_requests": 61527,
    "scheduler_time": 52.330230861074995
}
#Debug simulation 
Total elapsed time: 4.469791951123625. Arrivals time: 0.21506417822092772 Scheduler time: 4.127559207845479 Scheduler overhead time: 0.03762801131233573 Adapter cache time: 0.0334816612303257 Engine time: 0.03831856371834874 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.79207353387028,
    "estimated_duration": 3600.1086081528088,
    "input_throughput": 4683.768417934429,
    "output_throughput": 4121.165113297118,
    "total_throughput": 8804.933531231547,
    "itl": 207.69603030924702,
    "ttft": 2111424.736772308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 173,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.517278701018077,
    "arrivals": 553841,
    "finished_requests": 68142,
    "scheduler_time": 59.12272268411373
}
#Debug simulation 
Total elapsed time: 4.792182109784335. Arrivals time: 0.24135217582806945 Scheduler time: 4.47561383806169 Scheduler overhead time: 0.025868322234600782 Adapter cache time: 0.01021105656400323 Engine time: 0.026996500324457884 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.8,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.8-0.003125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.8-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.8      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 8640, 8640, 8640, 8640, 17280, 8640, 33, 17280, 33, 8640, 8640, 33, 17280, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 33, 33, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 33, 8640, 8640, 17280, 8640, 33, 33, 8640, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 17280, 17280, 33, 8640, 17280, 33, 8640, 33, 8640, 33, 33, 8640, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 8640, 33, 33, 33, 33, 33, 17280, 33, 8640, 33, 17280, 17280, 33, 17280, 33, 8640, 8640, 17280, 33, 33, 33, 17280, 17280, 8640, 17280, 8640, 17280, 17280, 8640, 8640, 33, 17280, 8640, 8640, 8640, 17280, 8640, 33, 8640, 17280, 17280, 8640, 17280, 8640, 33, 17280, 8640, 33, 8640, 8640, 33, 8640, 17280, 33, 8640, 17280, 33, 8640, 17280, 17280, 8640, 33, 17280, 8640, 33, 33, 8640, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 8640, 33, 17280, 17280, 8640, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 8640, 8640, 8640, 8640, 8640, 8640, 33, 17280, 8640, 17280, 33, 17280, 8640, 33, 8640, 33, 33, 8640, 8640, 8640, 17280, 8640, 33, 33]
Prompts retrieved: 1660992 . Total input tokens: 370743297 . Total output tokens: 332258577
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.516544532030821,
    "estimated_duration": 3600.003339864901,
    "input_throughput": 4227.317744813844,
    "output_throughput": 3730.388761390419,
    "total_throughput": 7957.706506204263,
    "itl": 136.410616683523,
    "ttft": 2176659.022195748,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 168,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5627043087780482,
    "arrivals": 553841,
    "finished_requests": 61531,
    "scheduler_time": 52.336112267125436
}
#Debug simulation 
Total elapsed time: 4.51670406293124. Arrivals time: 0.24799573235213757 Scheduler time: 4.142620697617531 Scheduler overhead time: 0.037564733531326056 Adapter cache time: 0.03226470435038209 Engine time: 0.03840893553569913 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.381580476183444,
    "estimated_duration": 3600.1407200280614,
    "input_throughput": 4173.024936614949,
    "output_throughput": 3675.6241016870185,
    "total_throughput": 7848.649038301968,
    "itl": 232.9261604206726,
    "ttft": 2152322.3164010216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1962,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.004676953596236,
    "arrivals": 484157,
    "finished_requests": 60962,
    "scheduler_time": 52.808685537571556
}
#Debug simulation 
Total elapsed time: 4.381671421229839. Arrivals time: 0.26051129633560777 Scheduler time: 4.0142539092339575 Scheduler overhead time: 0.024048282764852047 Adapter cache time: 0.047308698296546936 Engine time: 0.024453494232147932 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.380517982877791,
    "estimated_duration": 3600.1865903258636,
    "input_throughput": 4172.712614498201,
    "output_throughput": 3675.25062049697,
    "total_throughput": 7847.963234995172,
    "itl": 232.95091492749165,
    "ttft": 2152368.0040852525,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1964,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.417116338668642,
    "arrivals": 484157,
    "finished_requests": 60959,
    "scheduler_time": 52.80381198800683
}
#Debug simulation 
Total elapsed time: 4.380612072069198. Arrivals time: 0.2604083032347262 Scheduler time: 4.013729209546 Scheduler overhead time: 0.023725660983473063 Adapter cache time: 0.04740260122343898 Engine time: 0.02434877958148718 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.645723926834762,
    "estimated_duration": 3600.112600345451,
    "input_throughput": 3977.8689140517,
    "output_throughput": 3520.2896150481274,
    "total_throughput": 7498.158529099827,
    "itl": 144.60527837455794,
    "ttft": 2185739.3059915705,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1958,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.407749073617118,
    "arrivals": 484157,
    "finished_requests": 58148,
    "scheduler_time": 49.41340428327162
}
#Debug simulation 
Total elapsed time: 4.645786882843822. Arrivals time: 0.48800567630678415 Scheduler time: 3.9821447608992457 Scheduler overhead time: 0.0359908202663064 Adapter cache time: 0.08553591091185808 Engine time: 0.037078849505633116 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.381720668170601,
    "estimated_duration": 3600.0257396154143,
    "input_throughput": 4173.020441127424,
    "output_throughput": 3675.6251085620274,
    "total_throughput": 7848.645549689452,
    "itl": 232.93233163787684,
    "ttft": 2152305.4811235787,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1960,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.132214464770913,
    "arrivals": 484157,
    "finished_requests": 60960,
    "scheduler_time": 52.80531423686123
}
#Debug simulation 
Total elapsed time: 4.381813614163548. Arrivals time: 0.2586823860183358 Scheduler time: 4.015640640165657 Scheduler overhead time: 0.02369639277458191 Adapter cache time: 0.04809568915516138 Engine time: 0.02459881082177162 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.385284279938787,
    "estimated_duration": 3600.112973796901,
    "input_throughput": 3978.685697991673,
    "output_throughput": 3521.2783855030125,
    "total_throughput": 7499.964083494686,
    "itl": 144.81305533834535,
    "ttft": 2185729.648344589,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1959,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.494670368004473,
    "arrivals": 484157,
    "finished_requests": 58163,
    "scheduler_time": 49.425876359771486
}
#Debug simulation 
Total elapsed time: 4.385373393073678. Arrivals time: 0.20935624046251178 Scheduler time: 3.998888777103275 Scheduler overhead time: 0.036516175139695406 Adapter cache time: 0.08619642909616232 Engine time: 0.03723375452682376 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.42235357593745,
    "estimated_duration": 3600.2430384062604,
    "input_throughput": 4173.069662166693,
    "output_throughput": 3675.6515765274644,
    "total_throughput": 7848.721238694157,
    "itl": 232.9180022107587,
    "ttft": 2152301.1036445173,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1962,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 5.866478678597897,
    "arrivals": 484157,
    "finished_requests": 60965,
    "scheduler_time": 52.812305113203905
}
#Debug simulation 
Total elapsed time: 4.422468021977693. Arrivals time: 0.26287139393389225 Scheduler time: 4.052616399712861 Scheduler overhead time: 0.024342623073607683 Adapter cache time: 0.0469169095158577 Engine time: 0.024545961990952492 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.1_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 1.6]. Counts: [64 64 64]
Adapter prompts. [17280, 1080, 17280, 1080, 17280, 1080, 17280, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 17280, 4320, 1080, 17280, 1080, 4320, 4320, 1080, 17280, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 1080, 4320, 4320, 17280, 4320, 1080, 1080, 4320, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 17280, 17280, 1080, 4320, 17280, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 17280, 17280, 1080, 1080, 1080, 17280, 17280, 17280, 4320, 1080, 1080, 1080, 1080, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 1080, 17280, 1080, 4320, 4320, 17280, 1080, 1080, 1080, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 1080, 17280, 4320, 4320, 4320, 17280, 4320, 1080, 4320, 17280, 17280, 4320, 17280, 4320, 1080, 17280, 4320, 1080, 4320, 4320, 1080, 4320, 17280, 1080, 4320, 17280, 1080, 4320, 17280, 17280, 4320, 1080, 17280, 4320, 1080, 1080, 4320, 17280, 17280, 17280, 1080, 17280, 1080, 17280, 1080, 4320, 1080, 17280, 17280, 4320, 17280, 1080, 17280, 17280, 17280, 1080, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 17280, 4320, 17280, 1080, 17280, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 17280, 4320, 1080, 1080]
Prompts retrieved: 1451520 . Total input tokens: 323875371 . Total output tokens: 290377037
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.3683022619225085,
    "estimated_duration": 3600.005675105168,
    "input_throughput": 3978.4884504610095,
    "output_throughput": 3520.9916716672133,
    "total_throughput": 7499.480122128223,
    "itl": 144.80468132337833,
    "ttft": 2185796.0637301495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1962,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 6.5881016875056515,
    "arrivals": 484157,
    "finished_requests": 58160,
    "scheduler_time": 49.42217329622237
}
#Debug simulation 
Total elapsed time: 4.368394714780152. Arrivals time: 0.2533108480274677 Scheduler time: 3.9392641191370785 Scheduler overhead time: 0.03594577684998512 Adapter cache time: 0.08616591757163405 Engine time: 0.036778174340724945 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.577688643708825,
    "estimated_duration": 3600.12588165119,
    "input_throughput": 4396.315995689808,
    "output_throughput": 3857.890100673338,
    "total_throughput": 8254.206096363147,
    "itl": 221.63573139811734,
    "ttft": 2125191.582942924,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9870360380784353,
    "arrivals": 472692,
    "finished_requests": 63794,
    "scheduler_time": 55.37857237721293
}
#Debug simulation 
Total elapsed time: 4.577778882812709. Arrivals time: 0.2647174964658916 Scheduler time: 4.2134483442641795 Scheduler overhead time: 0.024587328545749187 Adapter cache time: 0.03849593549966812 Engine time: 0.025164599530398846 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.540296528022736,
    "estimated_duration": 3600.0276249631593,
    "input_throughput": 4396.187098748154,
    "output_throughput": 3857.76789702888,
    "total_throughput": 8253.954995777034,
    "itl": 221.64813775726654,
    "ttft": 2125178.2178865364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 977,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.2060700750723607,
    "arrivals": 472692,
    "finished_requests": 63789,
    "scheduler_time": 55.373824668423744
}
#Debug simulation 
Total elapsed time: 4.540392279159278. Arrivals time: 0.26283870404586196 Scheduler time: 4.176854300778359 Scheduler overhead time: 0.024446567054837942 Adapter cache time: 0.039425247348845005 Engine time: 0.025377018842846155 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.510681925807148,
    "estimated_duration": 3600.0471054511877,
    "input_throughput": 4154.365640758915,
    "output_throughput": 3659.693502357323,
    "total_throughput": 7814.0591431162375,
    "itl": 139.3527667122996,
    "ttft": 2165709.0669556307,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 916,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.006736889425635,
    "arrivals": 472692,
    "finished_requests": 60272,
    "scheduler_time": 51.35553113962497
}
#Debug simulation 
Total elapsed time: 4.510828753001988. Arrivals time: 0.25713124265894294 Scheduler time: 4.088630559388548 Scheduler overhead time: 0.03705318318679929 Adapter cache time: 0.07195919612422585 Engine time: 0.03849362954497337 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.543030662927777,
    "estimated_duration": 3600.0601910243213,
    "input_throughput": 4396.2048299800435,
    "output_throughput": 3857.8232760181572,
    "total_throughput": 8254.0281059982,
    "itl": 221.6415126312079,
    "ttft": 2125198.3773136702,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.060071663323296,
    "arrivals": 472692,
    "finished_requests": 63790,
    "scheduler_time": 55.376160954368245
}
#Debug simulation 
Total elapsed time: 4.543145291972905. Arrivals time: 0.21978227933868766 Scheduler time: 4.223608863539994 Scheduler overhead time: 0.02446656906977296 Adapter cache time: 0.038530176505446434 Engine time: 0.025307844392955303 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.488895941060036,
    "estimated_duration": 3600.007168981242,
    "input_throughput": 4153.829228132272,
    "output_throughput": 3659.212435326303,
    "total_throughput": 7813.041663458575,
    "itl": 139.32582633052147,
    "ttft": 2165911.3737512976,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 919,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0564459051191792,
    "arrivals": 472692,
    "finished_requests": 60264,
    "scheduler_time": 51.351708548849636
}
#Debug simulation 
Total elapsed time: 4.488989291712642. Arrivals time: 0.25230069691315293 Scheduler time: 4.072633997537196 Scheduler overhead time: 0.036977315321564674 Adapter cache time: 0.07148778717964888 Engine time: 0.03807094367220998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.536856774240732,
    "estimated_duration": 3600.0578047897,
    "input_throughput": 4396.399129742463,
    "output_throughput": 3857.9630531269563,
    "total_throughput": 8254.36218286942,
    "itl": 221.63210899007981,
    "ttft": 2125165.123736959,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 976,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.9182890878244567,
    "arrivals": 472692,
    "finished_requests": 63794,
    "scheduler_time": 55.37857886856084
}
#Debug simulation 
Total elapsed time: 4.53694884525612. Arrivals time: 0.2651894730515778 Scheduler time: 4.171865715645254 Scheduler overhead time: 0.024809070862829685 Adapter cache time: 0.0382543639279902 Engine time: 0.025471108965575695 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.05_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 4320, 4320, 4320, 4320, 17280, 4320, 540, 17280, 540, 4320, 4320, 540, 17280, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 540, 4320, 4320, 17280, 4320, 540, 540, 4320, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 17280, 17280, 540, 4320, 17280, 540, 4320, 540, 4320, 540, 540, 4320, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 4320, 540, 540, 540, 540, 540, 17280, 540, 4320, 540, 17280, 17280, 540, 17280, 540, 4320, 4320, 17280, 540, 540, 540, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 540, 17280, 4320, 4320, 4320, 17280, 4320, 540, 4320, 17280, 17280, 4320, 17280, 4320, 540, 17280, 4320, 540, 4320, 4320, 540, 4320, 17280, 540, 4320, 17280, 540, 4320, 17280, 17280, 4320, 540, 17280, 4320, 540, 540, 4320, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 4320, 540, 17280, 17280, 4320, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 540, 17280, 4320, 17280, 540, 17280, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 17280, 4320, 540, 540]
Prompts retrieved: 1416960 . Total input tokens: 316104279 . Total output tokens: 283434944
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.488185374997556,
    "estimated_duration": 3600.108113617213,
    "input_throughput": 4153.783588730862,
    "output_throughput": 3659.213441444081,
    "total_throughput": 7812.997030174943,
    "itl": 139.33231052079103,
    "ttft": 2165975.163197693,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 918,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 3.0949011357874006,
    "arrivals": 472692,
    "finished_requests": 60266,
    "scheduler_time": 51.35224564740988
}
#Debug simulation 
Total elapsed time: 4.48830612981692. Arrivals time: 0.2505099130794406 Scheduler time: 4.0731125408783555 Scheduler overhead time: 0.03707367507740855 Adapter cache time: 0.07205710653215647 Engine time: 0.0379954669624567 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.905408366117626,
    "estimated_duration": 3600.0827956992316,
    "input_throughput": 4562.393125964158,
    "output_throughput": 3998.2938773507476,
    "total_throughput": 8560.687003314904,
    "itl": 213.92885330087515,
    "ttft": 2098756.4834603383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7597804527613585,
    "arrivals": 466835,
    "finished_requests": 66281,
    "scheduler_time": 57.34951282906819
}
#Debug simulation 
Total elapsed time: 4.905513198114932. Arrivals time: 0.49661860382184386 Scheduler time: 4.315221572760493 Scheduler overhead time: 0.02520337700843811 Adapter cache time: 0.03044944116845727 Engine time: 0.026114525739103556 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.699150767177343,
    "estimated_duration": 3600.1072709081373,
    "input_throughput": 4562.362108687042,
    "output_throughput": 3998.266695083512,
    "total_throughput": 8560.628803770554,
    "itl": 213.93675576738264,
    "ttft": 2098780.454091988,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8772754247393526,
    "arrivals": 466835,
    "finished_requests": 66281,
    "scheduler_time": 57.348112761229785
}
#Debug simulation 
Total elapsed time: 4.699245190247893. Arrivals time: 0.23350732121616602 Scheduler time: 4.370524917729199 Scheduler overhead time: 0.025339757557958364 Adapter cache time: 0.03175958897918463 Engine time: 0.02627354860305786 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.567699783015996,
    "estimated_duration": 3600.091236955712,
    "input_throughput": 4258.720124261287,
    "output_throughput": 3750.22884459361,
    "total_throughput": 8008.948968854897,
    "itl": 135.94156973344542,
    "ttft": 2144441.338257011,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 547,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7905981257371715,
    "arrivals": 466835,
    "finished_requests": 61913,
    "scheduler_time": 52.59001168849868
}
#Debug simulation 
Total elapsed time: 4.567820943892002. Arrivals time: 0.23261856054887176 Scheduler time: 4.180456686299294 Scheduler overhead time: 0.03817793354392052 Adapter cache time: 0.05941921006888151 Engine time: 0.03910788707435131 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.6613520151004195,
    "estimated_duration": 3600.124387540124,
    "input_throughput": 4562.340417138417,
    "output_throughput": 3998.247685501554,
    "total_throughput": 8560.588102639971,
    "itl": 213.9305260321192,
    "ttft": 2098774.525318188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 575,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7985464113461684,
    "arrivals": 466835,
    "finished_requests": 66281,
    "scheduler_time": 57.34950325454614
}
#Debug simulation 
Total elapsed time: 4.661443701013923. Arrivals time: 0.22310345759615302 Scheduler time: 4.344564003869891 Scheduler overhead time: 0.025286979041993618 Adapter cache time: 0.030623688828200102 Engine time: 0.02604734245687723 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.50864193495363,
    "estimated_duration": 3600.0504364432763,
    "input_throughput": 4261.731403729388,
    "output_throughput": 3752.1977090258574,
    "total_throughput": 8013.929112755245,
    "itl": 135.8140905328413,
    "ttft": 2144329.2249252587,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8179978652857278,
    "arrivals": 466835,
    "finished_requests": 61946,
    "scheduler_time": 52.610213962565794
}
#Debug simulation 
Total elapsed time: 4.508735032752156. Arrivals time: 0.21226523909717798 Scheduler time: 4.141295793931931 Scheduler overhead time: 0.037816859781742096 Adapter cache time: 0.06044816505163908 Engine time: 0.039062704890966415 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.66068768594414,
    "estimated_duration": 3600.2249203956726,
    "input_throughput": 4562.785760117074,
    "output_throughput": 3998.324359806379,
    "total_throughput": 8561.110119923453,
    "itl": 213.92434002003233,
    "ttft": 2098830.7341819606,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 574,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7162888692738247,
    "arrivals": 466835,
    "finished_requests": 66288,
    "scheduler_time": 57.35200168336851
}
#Debug simulation 
Total elapsed time: 4.6608343459665775. Arrivals time: 0.2253464600071311 Scheduler time: 4.3412766279652715 Scheduler overhead time: 0.025286967866122723 Adapter cache time: 0.030516935046762228 Engine time: 0.026510972995311022 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.526839392725378,
    "estimated_duration": 3600.0662033987455,
    "input_throughput": 4257.92836407519,
    "output_throughput": 3749.158275827696,
    "total_throughput": 8007.0866399028855,
    "itl": 135.77531119611373,
    "ttft": 2144319.3585558827,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 548,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.8410108081996401,
    "arrivals": 466835,
    "finished_requests": 61896,
    "scheduler_time": 52.57282141516043
}
#Debug simulation 
Total elapsed time: 4.5269581326283514. Arrivals time: 0.21731122769415379 Scheduler time: 4.154750537592918 Scheduler overhead time: 0.03781969891861081 Adapter cache time: 0.05974387191236019 Engine time: 0.03946034563705325 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.775013350881636,
    "estimated_duration": 3600.0005027179723,
    "input_throughput": 4660.586849177564,
    "output_throughput": 4083.223596469476,
    "total_throughput": 8743.81044564704,
    "itl": 209.26010848036415,
    "ttft": 2089724.525046121,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0956546123279278,
    "arrivals": 463927,
    "finished_requests": 67576,
    "scheduler_time": 58.58799683740917
}
#Debug simulation 
Total elapsed time: 4.775105888955295. Arrivals time: 0.26626897417008877 Scheduler time: 4.419924448244274 Scheduler overhead time: 0.025763032492250204 Adapter cache time: 0.024551548063755035 Engine time: 0.02660353807732463 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.770993577782065,
    "estimated_duration": 3600.1671160480737,
    "input_throughput": 4660.371160330314,
    "output_throughput": 4083.034627608024,
    "total_throughput": 8743.405787938338,
    "itl": 209.2625808296377,
    "ttft": 2089773.548696318,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1725868351850726,
    "arrivals": 463927,
    "finished_requests": 67576,
    "scheduler_time": 58.589712302727676
}
#Debug simulation 
Total elapsed time: 4.7710851998999715. Arrivals time: 0.2222419590689242 Scheduler time: 4.4599064704962075 Scheduler overhead time: 0.025706562213599682 Adapter cache time: 0.024540909565985203 Engine time: 0.02668072283267975 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.917490337044001,
    "estimated_duration": 3600.139114529257,
    "input_throughput": 4332.713404615087,
    "output_throughput": 3805.781544581103,
    "total_throughput": 8138.49494919619,
    "itl": 134.36453312144232,
    "ttft": 2140613.372147908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1312906490080121,
    "arrivals": 463927,
    "finished_requests": 62808,
    "scheduler_time": 53.396334837777
}
#Debug simulation 
Total elapsed time: 4.917552574072033. Arrivals time: 0.4913035393692553 Scheduler time: 4.275924677494913 Scheduler overhead time: 0.03829234978184104 Adapter cache time: 0.05469523835927248 Engine time: 0.03931447630748153 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.8570660701952875,
    "estimated_duration": 3600.0256609527387,
    "input_throughput": 4660.554279371361,
    "output_throughput": 4083.195061479029,
    "total_throughput": 8743.74934085039,
    "itl": 209.26098009143305,
    "ttft": 2089736.7468662364,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1206952297431438,
    "arrivals": 463927,
    "finished_requests": 67576,
    "scheduler_time": 58.587966025031314
}
#Debug simulation 
Total elapsed time: 4.857210712041706. Arrivals time: 0.22810894856229424 Scheduler time: 4.538853469770402 Scheduler overhead time: 0.025784657336771488 Adapter cache time: 0.025644049048423767 Engine time: 0.02667877497151494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.654413056094199,
    "estimated_duration": 3600.0123910673083,
    "input_throughput": 4332.803975537307,
    "output_throughput": 3805.7060675666557,
    "total_throughput": 8138.510043103963,
    "itl": 134.36376986645595,
    "ttft": 2140626.5214745137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1467583647370387,
    "arrivals": 463927,
    "finished_requests": 62807,
    "scheduler_time": 53.39405044688816
}
#Debug simulation 
Total elapsed time: 4.654520659241825. Arrivals time: 0.2657994842156768 Scheduler time: 4.238889543805271 Scheduler overhead time: 0.038150633219629526 Adapter cache time: 0.05427367985248566 Engine time: 0.03924847999587655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.77751141320914,
    "estimated_duration": 3600.2048392070333,
    "input_throughput": 4660.458709814853,
    "output_throughput": 4083.084340067092,
    "total_throughput": 8743.543049881944,
    "itl": 209.2566139398419,
    "ttft": 2089786.8436646017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 358,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0704380055749778,
    "arrivals": 463927,
    "finished_requests": 67579,
    "scheduler_time": 58.59171005117032
}
#Debug simulation 
Total elapsed time: 4.77762833610177. Arrivals time: 0.2657437031157315 Scheduler time: 4.4223737539723516 Scheduler overhead time: 0.025902328081429005 Adapter cache time: 0.02503573475405574 Engine time: 0.026590941473841667 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.616654501762241,
    "estimated_duration": 3600.055160079055,
    "input_throughput": 4332.739446041564,
    "output_throughput": 3805.607522885607,
    "total_throughput": 8138.3469689271715,
    "itl": 134.36526469093064,
    "ttft": 2140655.8345587137,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.1615973115339913,
    "arrivals": 463927,
    "finished_requests": 62806,
    "scheduler_time": 53.39301548148165
}
#Debug simulation 
Total elapsed time: 4.616744061000645. Arrivals time: 0.2538839103654027 Scheduler time: 4.2122082794085145 Scheduler overhead time: 0.03822117717936635 Adapter cache time: 0.05480316327884793 Engine time: 0.03956390917301178 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.076825401280075,
    "estimated_duration": 3600.1319176310067,
    "input_throughput": 4718.681256318664,
    "output_throughput": 4146.255009961538,
    "total_throughput": 8864.936266280201,
    "itl": 206.276598102429,
    "ttft": 2079404.6903721974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7253355953120617,
    "arrivals": 462544,
    "finished_requests": 68493,
    "scheduler_time": 59.45832438679588
}
#Debug simulation 
Total elapsed time: 5.076889459043741. Arrivals time: 0.2321291365660727 Scheduler time: 4.757698175497353 Scheduler overhead time: 0.026104923337697983 Adapter cache time: 0.02174440585076809 Engine time: 0.02694871462881565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.805262436158955,
    "estimated_duration": 3600.0838945043834,
    "input_throughput": 4718.744200914987,
    "output_throughput": 4146.3103187085535,
    "total_throughput": 8865.05451962354,
    "itl": 206.27866185777,
    "ttft": 2079411.6667338964,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 238,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7770072211371768,
    "arrivals": 462544,
    "finished_requests": 68493,
    "scheduler_time": 59.457051464275246
}
#Debug simulation 
Total elapsed time: 4.805430000182241. Arrivals time: 0.2297327616252005 Scheduler time: 4.487646766006947 Scheduler overhead time: 0.026052003260701895 Adapter cache time: 0.022317911963909864 Engine time: 0.027419426012784243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.651559757068753,
    "estimated_duration": 3600.1456640087995,
    "input_throughput": 4360.551062403243,
    "output_throughput": 3843.098944111552,
    "total_throughput": 8203.650006514796,
    "itl": 132.33610154298728,
    "ttft": 2133285.6759027163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.755326215717945,
    "arrivals": 462544,
    "finished_requests": 63322,
    "scheduler_time": 53.89963823287657
}
#Debug simulation 
Total elapsed time: 4.651648642029613. Arrivals time: 0.2385217398405075 Scheduler time: 4.262980735395104 Scheduler overhead time: 0.039312068838626146 Adapter cache time: 0.05216081580147147 Engine time: 0.04031269019469619 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.800122210290283,
    "estimated_duration": 3600.152475950932,
    "input_throughput": 4718.654310749124,
    "output_throughput": 4146.2313331763025,
    "total_throughput": 8864.885643925427,
    "itl": 206.27702787927464,
    "ttft": 2079413.8392832165,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7421467360039262,
    "arrivals": 462544,
    "finished_requests": 68493,
    "scheduler_time": 59.45839333344039
}
#Debug simulation 
Total elapsed time: 4.800216443836689. Arrivals time: 0.2293415181338787 Scheduler time: 4.48381176777184 Scheduler overhead time: 0.026070396415889263 Adapter cache time: 0.02159218629822135 Engine time: 0.027183275669813156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.611485808156431,
    "estimated_duration": 3600.1413130801416,
    "input_throughput": 4360.293287090356,
    "output_throughput": 3842.9235957305386,
    "total_throughput": 8203.216882820894,
    "itl": 132.3356177441515,
    "ttft": 2133334.8415162466,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7647577496990595,
    "arrivals": 462544,
    "finished_requests": 63319,
    "scheduler_time": 53.900590867942014
}
#Debug simulation 
Total elapsed time: 4.61157942423597. Arrivals time: 0.21421064296737313 Scheduler time: 4.24862896092236 Scheduler overhead time: 0.038874870631843805 Adapter cache time: 0.051702908240258694 Engine time: 0.03990985546261072 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.811669717077166,
    "estimated_duration": 3600.0310196921123,
    "input_throughput": 4718.813506627192,
    "output_throughput": 4146.3712169004075,
    "total_throughput": 8865.1847235276,
    "itl": 206.27417441512438,
    "ttft": 2079396.8547292007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 237,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7086419198918166,
    "arrivals": 462544,
    "finished_requests": 68493,
    "scheduler_time": 59.456968834743826
}
#Debug simulation 
Total elapsed time: 4.8117642547003925. Arrivals time: 0.2275867979042232 Scheduler time: 4.496626315638423 Scheduler overhead time: 0.02627810323610902 Adapter cache time: 0.021832370199263096 Engine time: 0.02724181953817606 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.672377782408148,
    "estimated_duration": 3600.0535475437414,
    "input_throughput": 4360.59041697053,
    "output_throughput": 3843.0783923865783,
    "total_throughput": 8203.668809357108,
    "itl": 132.33248271404256,
    "ttft": 2133228.9681499414,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 231,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7748180526122483,
    "arrivals": 462544,
    "finished_requests": 63321,
    "scheduler_time": 53.89981057780751
}
#Debug simulation 
Total elapsed time: 4.672515924088657. Arrivals time: 0.21966884285211563 Scheduler time: 4.303623557090759 Scheduler overhead time: 0.038942590821534395 Adapter cache time: 0.05165947088971734 Engine time: 0.040122637059539557 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.835825497284532,
    "estimated_duration": 3600.0160048319353,
    "input_throughput": 4706.511853630217,
    "output_throughput": 4156.24624443816,
    "total_throughput": 8862.758098068376,
    "itl": 206.41938318269206,
    "ttft": 2073516.5952109064,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6151580365304827,
    "arrivals": 461815,
    "finished_requests": 68597,
    "scheduler_time": 59.60897185945941
}
#Debug simulation 
Total elapsed time: 4.835947383195162. Arrivals time: 0.2661139154806733 Scheduler time: 4.482304524630308 Scheduler overhead time: 0.026981775648891926 Adapter cache time: 0.021075707394629717 Engine time: 0.0271514724008739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.878024279605597,
    "estimated_duration": 3600.134350751698,
    "input_throughput": 4706.371304299305,
    "output_throughput": 4156.116284070303,
    "total_throughput": 8862.487588369608,
    "itl": 206.42148875146714,
    "ttft": 2073541.1985244532,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6561604772647862,
    "arrivals": 461815,
    "finished_requests": 68598,
    "scheduler_time": 59.61064342897949
}
#Debug simulation 
Total elapsed time: 4.878117881715298. Arrivals time: 0.26806121971458197 Scheduler time: 4.5240493635647 Scheduler overhead time: 0.026253622956573963 Adapter cache time: 0.02037364337593317 Engine time: 0.027126159518957138 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.641406709328294,
    "estimated_duration": 3600.0007063132703,
    "input_throughput": 4335.972204845914,
    "output_throughput": 3840.1864687848156,
    "total_throughput": 8176.15867363073,
    "itl": 132.47579953427112,
    "ttft": 2128526.740455513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6373283497989204,
    "arrivals": 461815,
    "finished_requests": 63212,
    "scheduler_time": 53.861233639480325
}
#Debug simulation 
Total elapsed time: 4.641497602220625. Arrivals time: 0.2559513635933399 Scheduler time: 4.238323834724724 Scheduler overhead time: 0.0385620198212564 Adapter cache time: 0.05058641731739044 Engine time: 0.039849105291068554 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.91189412586391,
    "estimated_duration": 3600.0640531877184,
    "input_throughput": 4706.449038037856,
    "output_throughput": 4156.190772981174,
    "total_throughput": 8862.63981101903,
    "itl": 206.42106438894743,
    "ttft": 2073529.1231821212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.629601781566163,
    "arrivals": 461815,
    "finished_requests": 68597,
    "scheduler_time": 59.60930601089315
}
#Debug simulation 
Total elapsed time: 4.911987909115851. Arrivals time: 0.29910720931366086 Scheduler time: 4.526697515044361 Scheduler overhead time: 0.02633688412606716 Adapter cache time: 0.020394891034811735 Engine time: 0.02711633825674653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.651154080871493,
    "estimated_duration": 3600.0561375889856,
    "input_throughput": 4335.905720196333,
    "output_throughput": 3840.212061042694,
    "total_throughput": 8176.117781239028,
    "itl": 132.47264300870933,
    "ttft": 2128605.8348338846,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6451250845566411,
    "arrivals": 461815,
    "finished_requests": 63213,
    "scheduler_time": 53.86125520785814
}
#Debug simulation 
Total elapsed time: 4.65132368914783. Arrivals time: 0.21673528011888266 Scheduler time: 4.2875918708741665 Scheduler overhead time: 0.03885527281090617 Adapter cache time: 0.05005613528192043 Engine time: 0.039700130466371775 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.861187811009586,
    "estimated_duration": 3600.176821616346,
    "input_throughput": 4706.429111554799,
    "output_throughput": 4156.107252888288,
    "total_throughput": 8862.536364443087,
    "itl": 206.41816164491274,
    "ttft": 2073505.4584465784,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 201,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6010001092753381,
    "arrivals": 461815,
    "finished_requests": 68599,
    "scheduler_time": 59.61207816130068
}
#Debug simulation 
Total elapsed time: 4.8612825609743595. Arrivals time: 0.27174345776438713 Scheduler time: 4.503460060339421 Scheduler overhead time: 0.026191992219537497 Adapter cache time: 0.020466890186071396 Engine time: 0.027127770707011223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.663388940040022,
    "estimated_duration": 3600.118701509062,
    "input_throughput": 4335.830091784742,
    "output_throughput": 3840.0606052809067,
    "total_throughput": 8175.890697065648,
    "itl": 132.4725090802592,
    "ttft": 2128611.6959007233,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 195,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6536763420328514,
    "arrivals": 461815,
    "finished_requests": 63212,
    "scheduler_time": 53.86270753185135
}
#Debug simulation 
Total elapsed time: 4.663480369839817. Arrivals time: 0.21535318903625011 Scheduler time: 4.300706003326923 Scheduler overhead time: 0.038868787698447704 Adapter cache time: 0.050262260250747204 Engine time: 0.03986093308776617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.928625358268619,
    "estimated_duration": 3600.1867703441117,
    "input_throughput": 4777.974615565804,
    "output_throughput": 4219.162495991322,
    "total_throughput": 8997.137111557126,
    "itl": 203.72847682379185,
    "ttft": 2041743.3309887634,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.226533574371244,
    "arrivals": 403594,
    "finished_requests": 69285,
    "scheduler_time": 60.49298155386824
}
#Debug simulation 
Total elapsed time: 4.928714664187282. Arrivals time: 0.22736772987991571 Scheduler time: 4.578515042550862 Scheduler overhead time: 0.02685650810599327 Adapter cache time: 0.055749470833688974 Engine time: 0.027739138808101416 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.943521074950695,
    "estimated_duration": 3600.0853720315995,
    "input_throughput": 4777.483093489549,
    "output_throughput": 4218.866896322781,
    "total_throughput": 8996.34998981233,
    "itl": 203.74497343224365,
    "ttft": 2041752.3392346322,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1383,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.520136347820478,
    "arrivals": 403594,
    "finished_requests": 69277,
    "scheduler_time": 60.48672532686359
}
#Debug simulation 
Total elapsed time: 4.943621674086899. Arrivals time: 0.23066052282229066 Scheduler time: 4.590378231368959 Scheduler overhead time: 0.026843206956982613 Adapter cache time: 0.055441427044570446 Engine time: 0.027748589403927326 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.961415387224406,
    "estimated_duration": 3600.0749733801076,
    "input_throughput": 4629.536363336253,
    "output_throughput": 4100.7379316156475,
    "total_throughput": 8730.2742949519,
    "itl": 123.82607070141759,
    "ttft": 2068220.5262599178,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1349,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.416649116948183,
    "arrivals": 403594,
    "finished_requests": 67133,
    "scheduler_time": 57.47839698228128
}
#Debug simulation 
Total elapsed time: 4.961560267023742. Arrivals time: 0.23756469786167145 Scheduler time: 4.5441300687380135 Scheduler overhead time: 0.04185632662847638 Adapter cache time: 0.07549372501671314 Engine time: 0.042640152387320995 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.941287251655012,
    "estimated_duration": 3600.0683399124378,
    "input_throughput": 4777.967909469843,
    "output_throughput": 4219.232127234964,
    "total_throughput": 8997.200036704808,
    "itl": 203.73347191373892,
    "ttft": 2041695.5844168402,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.313127350557991,
    "arrivals": 403594,
    "finished_requests": 69282,
    "scheduler_time": 60.48961940649903
}
#Debug simulation 
Total elapsed time: 4.941393014974892. Arrivals time: 0.23378689587116241 Scheduler time: 4.585267026908696 Scheduler overhead time: 0.026951016392558813 Adapter cache time: 0.055211208295077085 Engine time: 0.027642675675451756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.95265344902873,
    "estimated_duration": 3600.0169493196695,
    "input_throughput": 4628.970706137711,
    "output_throughput": 4100.4462500627,
    "total_throughput": 8729.416956200412,
    "itl": 123.83260534425897,
    "ttft": 2068318.4567994785,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1344,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.460993091408139,
    "arrivals": 403594,
    "finished_requests": 67128,
    "scheduler_time": 57.4765692049946
}
#Debug simulation 
Total elapsed time: 4.9527449598535895. Arrivals time: 0.2214108295738697 Scheduler time: 4.5509695825167 Scheduler overhead time: 0.04160712007433176 Adapter cache time: 0.07638592319563031 Engine time: 0.042679859325289726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.936658292077482,
    "estimated_duration": 3600.1890992305784,
    "input_throughput": 4777.971524794704,
    "output_throughput": 4219.159766704008,
    "total_throughput": 8997.131291498712,
    "itl": 203.72319188271308,
    "ttft": 2041748.7114264558,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1381,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.129259457259795,
    "arrivals": 403594,
    "finished_requests": 69285,
    "scheduler_time": 60.49436673365223
}
#Debug simulation 
Total elapsed time: 4.936753242276609. Arrivals time: 0.2298568938858807 Scheduler time: 4.584595662076026 Scheduler overhead time: 0.026807877700775862 Adapter cache time: 0.055287332739681005 Engine time: 0.027650973293930292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.961521470919251,
    "estimated_duration": 3600.0593677518154,
    "input_throughput": 4628.859498616138,
    "output_throughput": 4100.275159967202,
    "total_throughput": 8729.13465858334,
    "itl": 123.83328007529866,
    "ttft": 2068364.6670160987,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 1345,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 4.519031028784823,
    "arrivals": 403594,
    "finished_requests": 67127,
    "scheduler_time": 57.47602007200692
}
#Debug simulation 
Total elapsed time: 4.9616132183000445. Arrivals time: 0.228844849858433 Scheduler time: 4.5522267781198025 Scheduler overhead time: 0.04148261295631528 Adapter cache time: 0.07681350037455559 Engine time: 0.04258203972131014 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.12394605204463,
    "estimated_duration": 3600.00773701063,
    "input_throughput": 5022.099206656959,
    "output_throughput": 4406.296363455054,
    "total_throughput": 9428.395570112014,
    "itl": 194.29537624640483,
    "ttft": 2012066.551563856,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3045472711814075,
    "arrivals": 397773,
    "finished_requests": 72898,
    "scheduler_time": 63.144747830306976
}
#Debug simulation 
Total elapsed time: 5.124115825165063. Arrivals time: 0.23632336361333728 Scheduler time: 4.769427578896284 Scheduler overhead time: 0.028168704360723495 Adapter cache time: 0.047820217441767454 Engine time: 0.02912659151479602 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.085315291769803,
    "estimated_duration": 3600.031599101272,
    "input_throughput": 5021.644255709614,
    "output_throughput": 4405.985770780396,
    "total_throughput": 9427.630026490011,
    "itl": 194.30598581877553,
    "ttft": 2012105.704652091,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.46642900956097,
    "arrivals": 397773,
    "finished_requests": 72892,
    "scheduler_time": 63.142608082675494
}
#Debug simulation 
Total elapsed time: 5.085406268015504. Arrivals time: 0.2315463381819427 Scheduler time: 4.735763520002365 Scheduler overhead time: 0.02824639482423663 Adapter cache time: 0.04765969980508089 Engine time: 0.029106807429343462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.03286812081933,
    "estimated_duration": 3600.0534811305297,
    "input_throughput": 4814.806249644024,
    "output_throughput": 4235.13069456181,
    "total_throughput": 9049.936944205834,
    "itl": 119.77513735816885,
    "ttft": 2045956.948061896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.3528585381992144,
    "arrivals": 397773,
    "finished_requests": 69874,
    "scheduler_time": 59.312228952781545
}
#Debug simulation 
Total elapsed time: 5.032958726864308. Arrivals time: 0.2241730554960668 Scheduler time: 4.637888473924249 Scheduler overhead time: 0.04285100195556879 Adapter cache time: 0.06415020674467087 Engine time: 0.043670954648405313 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.149652626365423,
    "estimated_duration": 3600.099597720545,
    "input_throughput": 5021.97106198044,
    "output_throughput": 4406.183931701139,
    "total_throughput": 9428.154993681579,
    "itl": 194.2986808784477,
    "ttft": 2012106.3154188753,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.363054393995536,
    "arrivals": 397773,
    "finished_requests": 72898,
    "scheduler_time": 63.145219783303915
}
#Debug simulation 
Total elapsed time: 5.1497604344040155. Arrivals time: 0.2593976007774472 Scheduler time: 4.771689200773835 Scheduler overhead time: 0.028536529280245304 Adapter cache time: 0.04777404060587287 Engine time: 0.02915043570101261 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.05367645714432,
    "estimated_duration": 3600.08272078618,
    "input_throughput": 4814.602425639502,
    "output_throughput": 4234.866302369473,
    "total_throughput": 9049.468728008975,
    "itl": 119.77857193385654,
    "ttft": 2045947.0615132204,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 718,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.383165200725199,
    "arrivals": 397773,
    "finished_requests": 69872,
    "scheduler_time": 59.3114894871075
}
#Debug simulation 
Total elapsed time: 5.053793857805431. Arrivals time: 0.22411778569221497 Scheduler time: 4.6587121887132525 Scheduler overhead time: 0.04267926141619682 Adapter cache time: 0.06386790983378887 Engine time: 0.04410332068800926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.08423920115456,
    "estimated_duration": 3600.1027038852494,
    "input_throughput": 5022.0989474794405,
    "output_throughput": 4406.364569233032,
    "total_throughput": 9428.463516712472,
    "itl": 194.29283620840596,
    "ttft": 2012069.575782613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 753,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.2515078720612953,
    "arrivals": 397773,
    "finished_requests": 72900,
    "scheduler_time": 63.14746901564874
}
#Debug simulation 
Total elapsed time: 5.084396139718592. Arrivals time: 0.2302840710617602 Scheduler time: 4.7374127940274775 Scheduler overhead time: 0.027813749387860298 Adapter cache time: 0.04712919006124139 Engine time: 0.028786363545805216 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.280825140886009,
    "estimated_duration": 3600.109269753947,
    "input_throughput": 4816.09381850374,
    "output_throughput": 4236.033091585103,
    "total_throughput": 9052.126910088844,
    "itl": 119.93197217341732,
    "ttft": 2045883.9122463786,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 717,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.4118516499176708,
    "arrivals": 397773,
    "finished_requests": 69891,
    "scheduler_time": 59.32953177460506
}
#Debug simulation 
Total elapsed time: 5.28091468475759. Arrivals time: 0.22165296878665686 Scheduler time: 4.8882982255890965 Scheduler overhead time: 0.042827685829252005 Adapter cache time: 0.06387081183493137 Engine time: 0.044131623581051826 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.1430553970858455,
    "estimated_duration": 3600.1159269057835,
    "input_throughput": 5150.449423426375,
    "output_throughput": 4539.820197969899,
    "total_throughput": 9690.269621396274,
    "itl": 189.1054801030318,
    "ttft": 1993781.9582129258,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3435541195864895,
    "arrivals": 394957,
    "finished_requests": 74902,
    "scheduler_time": 65.03788836428114
}
#Debug simulation 
Total elapsed time: 5.1431525642983615. Arrivals time: 0.24033960653468966 Scheduler time: 4.791569101624191 Scheduler overhead time: 0.028372293803840876 Adapter cache time: 0.04100048774853349 Engine time: 0.028728500474244356 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.121017866767943,
    "estimated_duration": 3600.02839827792,
    "input_throughput": 5150.432704605735,
    "output_throughput": 4539.665022591951,
    "total_throughput": 9690.097727197686,
    "itl": 189.1083182408483,
    "ttft": 1993797.2281204367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4384794375416894,
    "arrivals": 394957,
    "finished_requests": 74899,
    "scheduler_time": 65.03518847570402
}
#Debug simulation 
Total elapsed time: 5.121107792016119. Arrivals time: 0.21504684537649155 Scheduler time: 4.795117664616555 Scheduler overhead time: 0.02851375937461853 Adapter cache time: 0.04040448134765029 Engine time: 0.028919332660734653 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.06946915294975,
    "estimated_duration": 3600.056478910017,
    "input_throughput": 4892.792127898243,
    "output_throughput": 4328.220429674394,
    "total_throughput": 9221.012557572638,
    "itl": 117.86550631565706,
    "ttft": 2033264.150381526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3970387829467736,
    "arrivals": 394957,
    "finished_requests": 71154,
    "scheduler_time": 60.61155215098977
}
#Debug simulation 
Total elapsed time: 5.069560647942126. Arrivals time: 0.23231833521276712 Scheduler time: 4.673193889670074 Scheduler overhead time: 0.0431907013989985 Adapter cache time: 0.05694155674427748 Engine time: 0.04364272113889456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.138814738951623,
    "estimated_duration": 3600.1539655254596,
    "input_throughput": 5150.395004646329,
    "output_throughput": 4539.772230995274,
    "total_throughput": 9690.167235641604,
    "itl": 189.1071126586317,
    "ttft": 1993799.9101508155,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3763729491387475,
    "arrivals": 394957,
    "finished_requests": 74902,
    "scheduler_time": 65.0379224365588
}
#Debug simulation 
Total elapsed time: 5.138937815092504. Arrivals time: 0.22305943444371223 Scheduler time: 4.80359876062721 Scheduler overhead time: 0.02859299397096038 Adapter cache time: 0.04156729765236378 Engine time: 0.028896452859044075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.082571371924132,
    "estimated_duration": 3600.1149313557275,
    "input_throughput": 4892.861293560595,
    "output_throughput": 4328.228764111178,
    "total_throughput": 9221.090057671774,
    "itl": 117.8682003264554,
    "ttft": 2033240.0453918711,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 427,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.419156863465909,
    "arrivals": 394957,
    "finished_requests": 71157,
    "scheduler_time": 60.612831386054225
}
#Debug simulation 
Total elapsed time: 5.082660840824246. Arrivals time: 0.23320872522890568 Scheduler time: 4.683202073443681 Scheduler overhead time: 0.04345082212239504 Adapter cache time: 0.0572701096534729 Engine time: 0.045160971116274595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.15814723726362,
    "estimated_duration": 3600.077818817624,
    "input_throughput": 5150.503942742502,
    "output_throughput": 4539.868253561205,
    "total_throughput": 9690.372196303708,
    "itl": 189.10443289073743,
    "ttft": 1993765.2641692813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 439,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.3126320794620454,
    "arrivals": 394957,
    "finished_requests": 74902,
    "scheduler_time": 65.03785545936434
}
#Debug simulation 
Total elapsed time: 5.158235311042517. Arrivals time: 0.22135833092033863 Scheduler time: 4.825104961171746 Scheduler overhead time: 0.028490407392382622 Adapter cache time: 0.041120086796581745 Engine time: 0.02886917907744646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.059146006591618,
    "estimated_duration": 3600.0858452243083,
    "input_throughput": 4892.8044378070945,
    "output_throughput": 4328.022349986265,
    "total_throughput": 9220.82678779336,
    "itl": 117.87060267675625,
    "ttft": 2033253.7009879325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 426,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.4345134112983946,
    "arrivals": 394957,
    "finished_requests": 71153,
    "scheduler_time": 60.61203303734573
}
#Debug simulation 
Total elapsed time: 5.059234988875687. Arrivals time: 0.23120564641430974 Scheduler time: 4.6641268017701805 Scheduler overhead time: 0.04288972122594714 Adapter cache time: 0.05709670530632138 Engine time: 0.04368054820224643 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.183789620641619,
    "estimated_duration": 3600.0759166162056,
    "input_throughput": 5193.2013193690445,
    "output_throughput": 4571.591650064237,
    "total_throughput": 9764.792969433282,
    "itl": 187.37952915715368,
    "ttft": 1982263.0365722328,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8783599825086992,
    "arrivals": 393528,
    "finished_requests": 75413,
    "scheduler_time": 65.52326870214817
}
#Debug simulation 
Total elapsed time: 5.183899703901261. Arrivals time: 0.22148717939853668 Scheduler time: 4.853105808608234 Scheduler overhead time: 0.02858131006360054 Adapter cache time: 0.03787612449377775 Engine time: 0.02955066505819559 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.1571295340545475,
    "estimated_duration": 3600.001364829062,
    "input_throughput": 5193.268308910135,
    "output_throughput": 4571.557155723871,
    "total_throughput": 9764.825464634006,
    "itl": 187.37946147117003,
    "ttft": 1982274.6068005632,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.939454903006559,
    "arrivals": 393528,
    "finished_requests": 75411,
    "scheduler_time": 65.52187908028327
}
#Debug simulation 
Total elapsed time: 5.157244248315692. Arrivals time: 0.22029188415035605 Scheduler time: 4.828651641961187 Scheduler overhead time: 0.028630874119699 Adapter cache time: 0.03753115143626928 Engine time: 0.0288195563480258 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.078088466078043,
    "estimated_duration": 3600.004205457132,
    "input_throughput": 4913.61009333983,
    "output_throughput": 4336.941322549774,
    "total_throughput": 9250.551415889604,
    "itl": 116.63461888485432,
    "ttft": 2025119.7394480254,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9314716044627178,
    "arrivals": 393528,
    "finished_requests": 71324,
    "scheduler_time": 60.74518508660113
}
#Debug simulation 
Total elapsed time: 5.078194328118116. Arrivals time: 0.21534992195665836 Scheduler time: 4.701060995925218 Scheduler overhead time: 0.04351979400962591 Adapter cache time: 0.05372823355719447 Engine time: 0.044022484216839075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.178820383735001,
    "estimated_duration": 3600.1051265907695,
    "input_throughput": 5193.159183577696,
    "output_throughput": 4571.554557793006,
    "total_throughput": 9764.713741370702,
    "itl": 187.37958169167086,
    "ttft": 1982275.8149156421,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8990039664809621,
    "arrivals": 393528,
    "finished_requests": 75413,
    "scheduler_time": 65.52359483778424
}
#Debug simulation 
Total elapsed time: 5.178935410920531. Arrivals time: 0.21940829837694764 Scheduler time: 4.850283560808748 Scheduler overhead time: 0.029206341598182917 Adapter cache time: 0.03781112004071474 Engine time: 0.028898332733660936 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.093006290029734,
    "estimated_duration": 3600.0319209232357,
    "input_throughput": 4913.658930964016,
    "output_throughput": 4337.042932662633,
    "total_throughput": 9250.70186362665,
    "itl": 116.63033390242437,
    "ttft": 2025076.8433932604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9439212293177893,
    "arrivals": 393528,
    "finished_requests": 71325,
    "scheduler_time": 60.74577258592459
}
#Debug simulation 
Total elapsed time: 5.093087825924158. Arrivals time: 0.21249903133139014 Scheduler time: 4.718330060131848 Scheduler overhead time: 0.04335019178688526 Adapter cache time: 0.054354450199753046 Engine time: 0.04415300767868757 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.193283120170236,
    "estimated_duration": 3600.1662337006833,
    "input_throughput": 5193.170477792555,
    "output_throughput": 4571.724451479424,
    "total_throughput": 9764.894929271979,
    "itl": 187.3770470361822,
    "ttft": 1982257.8013210392,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 287,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8581444346369256,
    "arrivals": 393528,
    "finished_requests": 75415,
    "scheduler_time": 65.52617928383278
}
#Debug simulation 
Total elapsed time: 5.19338705111295. Arrivals time: 0.24162236787378788 Scheduler time: 4.843649696558714 Scheduler overhead time: 0.02855276595801115 Adapter cache time: 0.03725891001522541 Engine time: 0.028999555855989456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.069537410978228,
    "estimated_duration": 3600.0627841738633,
    "input_throughput": 4913.859024283523,
    "output_throughput": 4337.166304054636,
    "total_throughput": 9251.025328338159,
    "itl": 116.63116317842992,
    "ttft": 2025049.9936319503,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 284,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9564966079592758,
    "arrivals": 393528,
    "finished_requests": 71327,
    "scheduler_time": 60.74698728059289
}
#Debug simulation 
Total elapsed time: 5.069642851129174. Arrivals time: 0.21986751770600677 Scheduler time: 4.688798203598708 Scheduler overhead time: 0.04336867015808821 Adapter cache time: 0.05329813854768872 Engine time: 0.04386750468984246 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.213494064286351,
    "estimated_duration": 3600.149936346147,
    "input_throughput": 5242.008064573423,
    "output_throughput": 4627.307555114636,
    "total_throughput": 9869.315619688059,
    "itl": 185.41935043071794,
    "ttft": 1974218.6714322465,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6580048649455412,
    "arrivals": 392778,
    "finished_requests": 76522,
    "scheduler_time": 66.30001956246707
}
#Debug simulation 
Total elapsed time: 5.21357500506565. Arrivals time: 0.21610707975924015 Scheduler time: 4.890267905313522 Scheduler overhead time: 0.02899130154401064 Adapter cache time: 0.03539861598983407 Engine time: 0.02949951495975256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.253349557053298,
    "estimated_duration": 3600.1152204577447,
    "input_throughput": 5242.058613224183,
    "output_throughput": 4627.352176212253,
    "total_throughput": 9869.410789436437,
    "itl": 185.42111472816967,
    "ttft": 1974243.1683090325,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7029243252147015,
    "arrivals": 392778,
    "finished_requests": 76522,
    "scheduler_time": 66.29895932786663
}
#Debug simulation 
Total elapsed time: 5.253431905992329. Arrivals time: 0.22353177098557353 Scheduler time: 4.923416878562421 Scheduler overhead time: 0.029087054543197155 Adapter cache time: 0.03449566336348653 Engine time: 0.029379198793321848 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.092262900900096,
    "estimated_duration": 3600.07036487397,
    "input_throughput": 4951.219613348863,
    "output_throughput": 4382.561006013096,
    "total_throughput": 9333.78061936196,
    "itl": 116.30073239094943,
    "ttft": 2018152.39117453,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6870281024649769,
    "arrivals": 392778,
    "finished_requests": 72227,
    "scheduler_time": 61.3769697019072
}
#Debug simulation 
Total elapsed time: 5.092341033741832. Arrivals time: 0.2147941691800952 Scheduler time: 4.718867256771773 Scheduler overhead time: 0.04356414498761296 Adapter cache time: 0.05067160213366151 Engine time: 0.043977155815809965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.253042240161449,
    "estimated_duration": 3600.1807862750297,
    "input_throughput": 5242.354237306959,
    "output_throughput": 4627.459005256552,
    "total_throughput": 9869.81324256351,
    "itl": 185.41697061768525,
    "ttft": 1974196.619497656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6735054622869955,
    "arrivals": 392778,
    "finished_requests": 76526,
    "scheduler_time": 66.30163137242234
}
#Debug simulation 
Total elapsed time: 5.253121456131339. Arrivals time: 0.22148612421005964 Scheduler time: 4.923586747143418 Scheduler overhead time: 0.02917173085734248 Adapter cache time: 0.035384992603212595 Engine time: 0.02996997768059373 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.101747978013009,
    "estimated_duration": 3600.051337648114,
    "input_throughput": 4950.911064408315,
    "output_throughput": 4382.300006395651,
    "total_throughput": 9333.211070803965,
    "itl": 116.30222598600653,
    "ttft": 2018179.6287569844,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6957051137276019,
    "arrivals": 392778,
    "finished_requests": 72222,
    "scheduler_time": 61.37468632710583
}
#Debug simulation 
Total elapsed time: 5.101843303069472. Arrivals time: 0.21739521948620677 Scheduler time: 4.725886470172554 Scheduler overhead time: 0.0434991829097271 Adapter cache time: 0.05050210049375892 Engine time: 0.04399124812334776 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.221872407011688,
    "estimated_duration": 3600.122071724935,
    "input_throughput": 5242.048637244628,
    "output_throughput": 4627.343370059153,
    "total_throughput": 9869.39200730378,
    "itl": 185.4184489496107,
    "ttft": 1974211.0166576058,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 215,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6428608134039686,
    "arrivals": 392778,
    "finished_requests": 76522,
    "scheduler_time": 66.29990872156398
}
#Debug simulation 
Total elapsed time: 5.221950535196811. Arrivals time: 0.22340493788942695 Scheduler time: 4.89232775894925 Scheduler overhead time: 0.028920058626681566 Adapter cache time: 0.03464158484712243 Engine time: 0.02935776999220252 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.099746011197567,
    "estimated_duration": 3600.08941912689,
    "input_throughput": 4951.337571032628,
    "output_throughput": 4382.385036376764,
    "total_throughput": 9333.722607409392,
    "itl": 116.30126971407165,
    "ttft": 2018255.9066964975,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 210,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7048851401358865,
    "arrivals": 392778,
    "finished_requests": 72226,
    "scheduler_time": 61.37644682894365
}
#Debug simulation 
Total elapsed time: 5.0998280751518905. Arrivals time: 0.21849714498966932 Scheduler time: 4.723450473509729 Scheduler overhead time: 0.043230544310063124 Adapter cache time: 0.05082262633368373 Engine time: 0.0434447075240314 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.3307354571297765,
    "estimated_duration": 3600.1627850504096,
    "input_throughput": 5320.0119393300765,
    "output_throughput": 4686.842792238835,
    "total_throughput": 10006.85473156891,
    "itl": 183.33295250287188,
    "ttft": 1965266.2258570222,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 842,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.576930680391432,
    "arrivals": 386275,
    "finished_requests": 77054,
    "scheduler_time": 67.10474624670948
}
#Debug simulation 
Total elapsed time: 5.330830079037696. Arrivals time: 0.22990749776363373 Scheduler time: 4.986563679762185 Scheduler overhead time: 0.030261983163654804 Adapter cache time: 0.040821915958076715 Engine time: 0.029537233524024487 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.554434243123978,
    "estimated_duration": 3600.183488788512,
    "input_throughput": 5319.8294085963125,
    "output_throughput": 4686.659458481609,
    "total_throughput": 10006.488867077922,
    "itl": 183.34161344828573,
    "ttft": 1965283.9375179235,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 841,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.746714438886854,
    "arrivals": 386275,
    "finished_requests": 77052,
    "scheduler_time": 67.1014659795879
}
#Debug simulation 
Total elapsed time: 5.55449667526409. Arrivals time: 0.44657871639356017 Scheduler time: 4.9945874265395105 Scheduler overhead time: 0.029357696417719126 Adapter cache time: 0.041174260433763266 Engine time: 0.02933134278282523 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.218207082245499,
    "estimated_duration": 3600.1054515798382,
    "input_throughput": 5042.853950856664,
    "output_throughput": 4456.508903915422,
    "total_throughput": 9499.362854772085,
    "itl": 114.27832468726372,
    "ttft": 2003959.50038934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 814,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6605497324280316,
    "arrivals": 386275,
    "finished_requests": 73066,
    "scheduler_time": 62.408477324499856
}
#Debug simulation 
Total elapsed time: 5.218314485158771. Arrivals time: 0.2576263723894954 Scheduler time: 4.7980966116301715 Scheduler overhead time: 0.044354795943945646 Adapter cache time: 0.05261491239070892 Engine time: 0.04476524842903018 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.374358725734055,
    "estimated_duration": 3600.027467513677,
    "input_throughput": 5320.0599642167135,
    "output_throughput": 4686.862573205047,
    "total_throughput": 10006.92253742176,
    "itl": 183.33581453367484,
    "ttft": 1965228.585768737,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 842,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6283516796049406,
    "arrivals": 386275,
    "finished_requests": 77052,
    "scheduler_time": 67.10089643383604
}
#Debug simulation 
Total elapsed time: 5.374441119842231. Arrivals time: 0.2356202113442123 Scheduler time: 5.023364271037281 Scheduler overhead time: 0.02962713548913598 Adapter cache time: 0.042196440510451794 Engine time: 0.029865963850170374 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_192_slots_160_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.233553402125835,
    "estimated_duration": 3600.0888948118895,
    "input_throughput": 5042.8385327270125,
    "output_throughput": 4456.455790055791,
    "total_throughput": 9499.294322782804,
    "itl": 114.27581001327341,
    "ttft": 2003987.4849941367,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 814,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.6965153153426984,
    "arrivals": 386275,
    "finished_requests": 73065,
    "scheduler_time": 62.40747962490869
}
#Debug simulation 
Total elapsed time: 5.23366116033867. Arrivals time: 0.24735378799960017 Scheduler time: 4.823966356460005 Scheduler overhead time: 0.04437287664040923 Adapter cache time: 0.05237860605120659 Engine time: 0.044753036461770535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_192_slots_160_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_192_slots_160_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.326031090226024,
    "estimated_duration": 3600.054555305558,
    "input_throughput": 5320.171876777123,
    "output_throughput": 4686.983694492334,
    "total_throughput": 10007.155571269455,
    "itl": 183.32854585620765,
    "ttft": 1965200.8015412267,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 840,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.5116422477177753,
    "arrivals": 386275,
    "finished_requests": 77054,
    "scheduler_time": 67.1040749359406
}
#Debug simulation 
Total elapsed time: 5.326110631227493. Arrivals time: 0.2675374010577798 Scheduler time: 4.944764821790159 Scheduler overhead time: 0.029245933517813683 Adapter cache time: 0.041432630736380816 Engine time: 0.029571975115686655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_192_slots_160_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_192_slots_160_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.240238383412361,
    "estimated_duration": 3600.107664486077,
    "input_throughput": 5042.85085112632,
    "output_throughput": 4456.506164598359,
    "total_throughput": 9499.357015724678,
    "itl": 114.28107082622768,
    "ttft": 2003966.4356835107,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 814,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 2.7292970397323564,
    "arrivals": 386275,
    "finished_requests": 73066,
    "scheduler_time": 62.407828957233676
}
#Debug simulation 
Total elapsed time: 5.240318536292762. Arrivals time: 0.2674568025395274 Scheduler time: 4.81112976372242 Scheduler overhead time: 0.04425902804359794 Adapter cache time: 0.05222629336640239 Engine time: 0.044457055162638426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.424745765980333,
    "estimated_duration": 3600.1122555630945,
    "input_throughput": 5496.451942414495,
    "output_throughput": 4816.018159769334,
    "total_throughput": 10312.470102183828,
    "itl": 177.76592099535776,
    "ttft": 1938658.9360743526,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6036955778207826,
    "arrivals": 383398,
    "finished_requests": 79437,
    "scheduler_time": 69.00926657479474
}
#Debug simulation 
Total elapsed time: 5.424825035966933. Arrivals time: 0.22791210422292352 Scheduler time: 5.086676803883165 Scheduler overhead time: 0.030122040770947933 Adapter cache time: 0.03589368984103203 Engine time: 0.03035453986376524 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.442091185133904,
    "estimated_duration": 3600.0733381845575,
    "input_throughput": 5496.2208103177345,
    "output_throughput": 4815.608008903079,
    "total_throughput": 10311.828819220815,
    "itl": 177.7713702812426,
    "ttft": 1938715.5642385604,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7130634785303922,
    "arrivals": 383398,
    "finished_requests": 79432,
    "scheduler_time": 69.00577137375768
}
#Debug simulation 
Total elapsed time: 5.4421704900451005. Arrivals time: 0.22942037135362625 Scheduler time: 5.100228465162218 Scheduler overhead time: 0.03112497692927718 Adapter cache time: 0.03640294587239623 Engine time: 0.03091569524258375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.284129292238504,
    "estimated_duration": 3600.0375406280264,
    "input_throughput": 5168.964431619164,
    "output_throughput": 4547.022028315946,
    "total_throughput": 9715.986459935111,
    "itl": 112.22222000542519,
    "ttft": 1985277.3425720236,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 505,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6529901567474108,
    "arrivals": 383398,
    "finished_requests": 74855,
    "scheduler_time": 63.65710715771515
}
#Debug simulation 
Total elapsed time: 5.284207034856081. Arrivals time: 0.2255964046344161 Scheduler time: 4.900975376367569 Scheduler overhead time: 0.045046193059533834 Adapter cache time: 0.04595365095883608 Engine time: 0.0454101343639195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.4765712935477495,
    "estimated_duration": 3600.194586862838,
    "input_throughput": 5496.326246421826,
    "output_throughput": 4815.908024323841,
    "total_throughput": 10312.234270745668,
    "itl": 177.76918045819093,
    "ttft": 1938700.8645904262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6386991305742353,
    "arrivals": 383398,
    "finished_requests": 79437,
    "scheduler_time": 69.00965258491696
}
#Debug simulation 
Total elapsed time: 5.47667291155085. Arrivals time: 0.23293872270733118 Scheduler time: 5.131205531768501 Scheduler overhead time: 0.031012244056910276 Adapter cache time: 0.03667237004265189 Engine time: 0.030707938596606255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.268479038961232,
    "estimated_duration": 3600.067315161015,
    "input_throughput": 5168.986958002955,
    "output_throughput": 4546.888868178811,
    "total_throughput": 9715.875826181767,
    "itl": 112.22262617923613,
    "ttft": 1985191.8213778262,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 505,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6743683004379315,
    "arrivals": 383398,
    "finished_requests": 74856,
    "scheduler_time": 63.65780261669364
}
#Debug simulation 
Total elapsed time: 5.268557399976999. Arrivals time: 0.2227626321837306 Scheduler time: 4.887263183947653 Scheduler overhead time: 0.04533463390544057 Adapter cache time: 0.04629440512508154 Engine time: 0.04563281778246164 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.419970192946494,
    "estimated_duration": 3600.0073908680292,
    "input_throughput": 5496.50760445491,
    "output_throughput": 4815.9128906176065,
    "total_throughput": 10312.420495072516,
    "itl": 177.7628852752575,
    "ttft": 1938656.7231728912,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 524,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5667863545287213,
    "arrivals": 383398,
    "finished_requests": 79435,
    "scheduler_time": 69.00818688450458
}
#Debug simulation 
Total elapsed time: 5.4200508068315685. Arrivals time: 0.23088788287714124 Scheduler time: 5.079102207440883 Scheduler overhead time: 0.030057542957365513 Adapter cache time: 0.03590407641604543 Engine time: 0.03017057105898857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.274346891324967,
    "estimated_duration": 3600.00780303718,
    "input_throughput": 5168.53157493221,
    "output_throughput": 4546.746811545995,
    "total_throughput": 9715.278386478205,
    "itl": 112.223765512006,
    "ttft": 1985294.7826677004,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 505,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6962494592741104,
    "arrivals": 383398,
    "finished_requests": 74850,
    "scheduler_time": 63.65506994829331
}
#Debug simulation 
Total elapsed time: 5.274432228412479. Arrivals time: 0.22241480089724064 Scheduler time: 4.893866591155529 Scheduler overhead time: 0.045116971246898174 Adapter cache time: 0.04598920093849301 Engine time: 0.04543492430821061 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.508154931943864,
    "estimated_duration": 3600.1359801163176,
    "input_throughput": 5540.6504393635505,
    "output_throughput": 4890.082512780864,
    "total_throughput": 10430.732952144414,
    "itl": 175.7395755056498,
    "ttft": 1937047.6085951296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8997833967162284,
    "arrivals": 381994,
    "finished_requests": 80415,
    "scheduler_time": 69.99971339339068
}
#Debug simulation 
Total elapsed time: 5.50825145188719. Arrivals time: 0.23269651271402836 Scheduler time: 5.168287883978337 Scheduler overhead time: 0.030476038809865713 Adapter cache time: 0.03212753403931856 Engine time: 0.030526052229106426 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.506423821672797,
    "estimated_duration": 3600.109527029077,
    "input_throughput": 5540.669207489613,
    "output_throughput": 4890.019002985131,
    "total_throughput": 10430.688210474744,
    "itl": 175.74285608483518,
    "ttft": 1937077.4619083938,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9612024457077551,
    "arrivals": 381994,
    "finished_requests": 80414,
    "scheduler_time": 69.99863386429332
}
#Debug simulation 
Total elapsed time: 5.506522730924189. Arrivals time: 0.24851257354021072 Scheduler time: 5.1504736905917525 Scheduler overhead time: 0.030377041548490524 Adapter cache time: 0.03256704192608595 Engine time: 0.030538517981767654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.297815051861107,
    "estimated_duration": 3600.018924629028,
    "input_throughput": 5176.42389947167,
    "output_throughput": 4586.695055138227,
    "total_throughput": 9763.118954609898,
    "itl": 110.99408348991315,
    "ttft": 1985116.3570835823,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9098380164243327,
    "arrivals": 381994,
    "finished_requests": 75131,
    "scheduler_time": 64.20674441141537
}
#Debug simulation 
Total elapsed time: 5.297893657814711. Arrivals time: 0.22083377465605736 Scheduler time: 4.922840100713074 Scheduler overhead time: 0.0455200495198369 Adapter cache time: 0.041884643491357565 Engine time: 0.045582421123981476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.489599345251918,
    "estimated_duration": 3600.1761830283185,
    "input_throughput": 5540.750503832522,
    "output_throughput": 4890.028461104766,
    "total_throughput": 10430.778964937288,
    "itl": 175.73936126089689,
    "ttft": 1937059.0388924624,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9207515091821583,
    "arrivals": 381994,
    "finished_requests": 80416,
    "scheduler_time": 70.00021088023304
}
#Debug simulation 
Total elapsed time: 5.489682029001415. Arrivals time: 0.2318180906586349 Scheduler time: 5.150648812763393 Scheduler overhead time: 0.030227203853428364 Adapter cache time: 0.03252590773627162 Engine time: 0.03049179445952177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.304405757691711,
    "estimated_duration": 3600.0067501469225,
    "input_throughput": 5176.2172388258705,
    "output_throughput": 4586.564455560016,
    "total_throughput": 9762.781694385887,
    "itl": 110.99351792838058,
    "ttft": 1985034.2376801562,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.9214073647745,
    "arrivals": 381994,
    "finished_requests": 75128,
    "scheduler_time": 64.20637650833739
}
#Debug simulation 
Total elapsed time: 5.3044850546866655. Arrivals time: 0.22162105794996023 Scheduler time: 4.9281922578811646 Scheduler overhead time: 0.045466253999620676 Adapter cache time: 0.04203419014811516 Engine time: 0.045751786790788174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.490696984343231,
    "estimated_duration": 3600.13509365061,
    "input_throughput": 5540.629860023759,
    "output_throughput": 4889.984276159085,
    "total_throughput": 10430.614136182843,
    "itl": 175.74121315940994,
    "ttft": 1937061.812949485,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 294,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.8790747867012408,
    "arrivals": 381994,
    "finished_requests": 80414,
    "scheduler_time": 69.99923037569454
}
#Debug simulation 
Total elapsed time: 5.490777662023902. Arrivals time: 0.23023518174886703 Scheduler time: 5.152479687239975 Scheduler overhead time: 0.030359224881976843 Adapter cache time: 0.032582753803581 Engine time: 0.030975383706390858 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.527849166188389,
    "estimated_duration": 3600.093967903069,
    "input_throughput": 5176.486271233285,
    "output_throughput": 4586.9627702019525,
    "total_throughput": 9763.449041435239,
    "itl": 110.99310724264022,
    "ttft": 1985210.355066139,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 278,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.933605482056742,
    "arrivals": 381994,
    "finished_requests": 75136,
    "scheduler_time": 64.20759499253275
}
#Debug simulation 
Total elapsed time: 5.527912170160562. Arrivals time: 0.22811521450057626 Scheduler time: 5.145707822404802 Scheduler overhead time: 0.04537961259484291 Adapter cache time: 0.04157040826976299 Engine time: 0.04571041604503989 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.883000241126865,
    "estimated_duration": 3600.15392622357,
    "input_throughput": 5598.850608352645,
    "output_throughput": 4954.754814806289,
    "total_throughput": 10553.605423158935,
    "itl": 173.7675101834613,
    "ttft": 1926593.993188703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6396419384819447,
    "arrivals": 381208,
    "finished_requests": 81133,
    "scheduler_time": 70.9274426683869
}
#Debug simulation 
Total elapsed time: 5.883063350338489. Arrivals time: 0.23805089900270104 Scheduler time: 5.537436687853187 Scheduler overhead time: 0.03096025437116623 Adapter cache time: 0.03128044679760933 Engine time: 0.031073187943547964 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.798697150312364,
    "estimated_duration": 3600.0488848119444,
    "input_throughput": 5598.847861493107,
    "output_throughput": 4954.801884844899,
    "total_throughput": 10553.649746338006,
    "itl": 173.76992938803147,
    "ttft": 1926557.17104821,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6837582374899672,
    "arrivals": 381208,
    "finished_requests": 81131,
    "scheduler_time": 70.92477845626533
}
#Debug simulation 
Total elapsed time: 5.7987616853788495. Arrivals time: 0.45500236516818404 Scheduler time: 5.235346638597548 Scheduler overhead time: 0.03079194761812687 Adapter cache time: 0.032199548091739416 Engine time: 0.031136742793023586 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.609767385292798,
    "estimated_duration": 3600.0968874364353,
    "input_throughput": 5211.241693375468,
    "output_throughput": 4624.236380442119,
    "total_throughput": 9835.478073817587,
    "itl": 109.93286886648129,
    "ttft": 1982394.0784111288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6616682091914146,
    "arrivals": 381208,
    "finished_requests": 75558,
    "scheduler_time": 64.73962911922274
}
#Debug simulation 
Total elapsed time: 5.609852513298392. Arrivals time: 0.4472642131149769 Scheduler time: 5.006711676251143 Scheduler overhead time: 0.0462955217808485 Adapter cache time: 0.041063448414206505 Engine time: 0.046701621264219284 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.823872980196029,
    "estimated_duration": 3600.1684930119336,
    "input_throughput": 5598.827954615174,
    "output_throughput": 4954.734767171041,
    "total_throughput": 10553.562721786215,
    "itl": 173.76779348785288,
    "ttft": 1926600.7166955122,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6551565651991419,
    "arrivals": 381208,
    "finished_requests": 81133,
    "scheduler_time": 70.9275902815609
}
#Debug simulation 
Total elapsed time: 5.823967627249658. Arrivals time: 0.4593088929541409 Scheduler time: 5.2564034862443805 Scheduler overhead time: 0.030952666886150837 Adapter cache time: 0.03188325744122267 Engine time: 0.03100411081686616 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.587531272787601,
    "estimated_duration": 3600.0022310095173,
    "input_throughput": 5211.378714823469,
    "output_throughput": 4624.357967503712,
    "total_throughput": 9835.73668232718,
    "itl": 109.93401463216887,
    "ttft": 1982491.174481123,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6702194666676247,
    "arrivals": 381208,
    "finished_requests": 75558,
    "scheduler_time": 64.73904567887257
}
#Debug simulation 
Total elapsed time: 5.5876130340620875. Arrivals time: 0.44732976937666535 Scheduler time: 4.986321159172803 Scheduler overhead time: 0.04614245565608144 Adapter cache time: 0.04019531514495611 Engine time: 0.04616495314985514 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.784361706115305,
    "estimated_duration": 3600.083996157439,
    "input_throughput": 5598.932142003975,
    "output_throughput": 4954.850503221402,
    "total_throughput": 10553.782645225378,
    "itl": 173.7674310045688,
    "ttft": 1926556.9866004852,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 209,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6249205116345555,
    "arrivals": 381208,
    "finished_requests": 81132,
    "scheduler_time": 70.92647279918445
}
#Debug simulation 
Total elapsed time: 5.784425158984959. Arrivals time: 0.4527776800096035 Scheduler time: 5.224476696457714 Scheduler overhead time: 0.030601296573877335 Adapter cache time: 0.031398925464600325 Engine time: 0.030905026011168957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.570724239107221,
    "estimated_duration": 3600.0419703733874,
    "input_throughput": 5211.321188584409,
    "output_throughput": 4624.3069211421835,
    "total_throughput": 9835.628109726593,
    "itl": 109.9312067342926,
    "ttft": 1982437.00819687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 202,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6791479855030795,
    "arrivals": 381208,
    "finished_requests": 75558,
    "scheduler_time": 64.73900294923159
}
#Debug simulation 
Total elapsed time: 5.570809944998473. Arrivals time: 0.23018245352432132 Scheduler time: 5.186958227306604 Scheduler overhead time: 0.045775456354022026 Adapter cache time: 0.04013938456773758 Engine time: 0.04615812283009291 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.746353405993432,
    "estimated_duration": 3600.045918319274,
    "input_throughput": 5782.7442961392035,
    "output_throughput": 5070.414770854361,
    "total_throughput": 10853.159066993565,
    "itl": 168.91994608492442,
    "ttft": 1896972.9223344263,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.618998016540447,
    "arrivals": 377644,
    "finished_requests": 83739,
    "scheduler_time": 72.59858887909127
}
#Debug simulation 
Total elapsed time: 5.746432509273291. Arrivals time: 0.24236544221639633 Scheduler time: 5.3955035037361085 Scheduler overhead time: 0.03199434466660023 Adapter cache time: 0.029729557689279318 Engine time: 0.032073954585939646 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.715949567034841,
    "estimated_duration": 3600.074730096494,
    "input_throughput": 5782.6980162275695,
    "output_throughput": 5070.374191790941,
    "total_throughput": 10853.072208018511,
    "itl": 168.92640468420817,
    "ttft": 1896993.9309132523,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.7288309206417831,
    "arrivals": 377644,
    "finished_requests": 83739,
    "scheduler_time": 72.59705688758376
}
#Debug simulation 
Total elapsed time: 5.7160280561074615. Arrivals time: 0.24020944023504853 Scheduler time: 5.368623528629541 Scheduler overhead time: 0.03153790533542633 Adapter cache time: 0.029269430320709944 Engine time: 0.031767317559570074 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.6681246482767165,
    "estimated_duration": 3600.111278149375,
    "input_throughput": 5364.0128062726935,
    "output_throughput": 4708.836669269371,
    "total_throughput": 10072.849475542063,
    "itl": 108.03058031882821,
    "ttft": 1953910.7243754272,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6289875448495248,
    "arrivals": 377644,
    "finished_requests": 77537,
    "scheduler_time": 65.91966755402538
}
#Debug simulation 
Total elapsed time: 5.66818580403924. Arrivals time: 0.4481425303965807 Scheduler time: 5.068845327477902 Scheduler overhead time: 0.04668522113934159 Adapter cache time: 0.03590942267328501 Engine time: 0.046789707615971565 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.726275204215199,
    "estimated_duration": 3600.17430837202,
    "input_throughput": 5782.585013061196,
    "output_throughput": 5070.544211581451,
    "total_throughput": 10853.129224642647,
    "itl": 168.92448221316502,
    "ttft": 1896997.8575736217,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6544665726856254,
    "arrivals": 377644,
    "finished_requests": 83743,
    "scheduler_time": 72.60058381426722
}
#Debug simulation 
Total elapsed time: 5.7263739868067205. Arrivals time: 0.2520284322090447 Scheduler time: 5.366038524080068 Scheduler overhead time: 0.0317424638196826 Adapter cache time: 0.029886426869779825 Engine time: 0.03195768874138594 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.414179597981274,
    "estimated_duration": 3600.086929166171,
    "input_throughput": 5363.466599533533,
    "output_throughput": 4708.66379993947,
    "total_throughput": 10072.130399473002,
    "itl": 108.03297974146619,
    "ttft": 1953987.6045239212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6498626733943862,
    "arrivals": 377644,
    "finished_requests": 77532,
    "scheduler_time": 65.91826542183254
}
#Debug simulation 
Total elapsed time: 5.414265785831958. Arrivals time: 0.22017776453867555 Scheduler time: 5.044051993172616 Scheduler overhead time: 0.046349389012902975 Adapter cache time: 0.035108542535454035 Engine time: 0.04681567195802927 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.916683378163725,
    "estimated_duration": 3600.0049540849127,
    "input_throughput": 5782.810097629928,
    "output_throughput": 5070.472466791348,
    "total_throughput": 10853.282564421275,
    "itl": 168.91837300769302,
    "ttft": 1896953.2230775813,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 529,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.5817366060032316,
    "arrivals": 377644,
    "finished_requests": 83739,
    "scheduler_time": 72.59852924833235
}
#Debug simulation 
Total elapsed time: 5.916748283896595. Arrivals time: 0.46012882282957435 Scheduler time: 5.3495233207941055 Scheduler overhead time: 0.03150576027110219 Adapter cache time: 0.02944931387901306 Engine time: 0.031636213418096304 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_192_slots_160_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.439489678945392,
    "estimated_duration": 3600.026501334201,
    "input_throughput": 5363.776347991784,
    "output_throughput": 4708.903668825043,
    "total_throughput": 10072.680016816827,
    "itl": 108.03560068340573,
    "ttft": 1953964.849763431,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 498,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.6712408170849067,
    "arrivals": 377644,
    "finished_requests": 77534,
    "scheduler_time": 65.9171248066219
}
#Debug simulation 
Total elapsed time: 5.439567422028631. Arrivals time: 0.23130127787590027 Scheduler time: 5.056786120403558 Scheduler overhead time: 0.04670994961634278 Adapter cache time: 0.035914977081120014 Engine time: 0.04691685922443867 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.754207561258227,
    "estimated_duration": 3600.0679810170523,
    "input_throughput": 5877.406513313221,
    "output_throughput": 5145.187006931762,
    "total_throughput": 11022.593520244984,
    "itl": 166.29843472272492,
    "ttft": 1886020.456239119,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.025263394217472,
    "arrivals": 376253,
    "finished_requests": 85266,
    "scheduler_time": 73.63658798365347
}
#Debug simulation 
Total elapsed time: 5.754286712035537. Arrivals time: 0.24531803978607059 Scheduler time: 5.402165393345058 Scheduler overhead time: 0.03189729619771242 Adapter cache time: 0.027865243144333363 Engine time: 0.03221597196534276 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.7618357529863715,
    "estimated_duration": 3600.0635477933206,
    "input_throughput": 5877.297919635144,
    "output_throughput": 5145.089455810735,
    "total_throughput": 11022.38737544588,
    "itl": 166.30374863225316,
    "ttft": 1886025.7745979053,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.093600795441313,
    "arrivals": 376253,
    "finished_requests": 85263,
    "scheduler_time": 73.635247296224
}
#Debug simulation 
Total elapsed time: 5.761948972940445. Arrivals time: 0.24198562558740377 Scheduler time: 5.414324853569269 Scheduler overhead time: 0.032114367466419935 Adapter cache time: 0.026702711824327707 Engine time: 0.032094425056129694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.482465726323426,
    "estimated_duration": 3600.1042516816065,
    "input_throughput": 5425.971481485752,
    "output_throughput": 4759.569946341492,
    "total_throughput": 10185.541427827244,
    "itl": 107.3193609626376,
    "ttft": 1945494.2418661017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.063338074646896,
    "arrivals": 376253,
    "finished_requests": 78718,
    "scheduler_time": 66.64286771779521
}
#Debug simulation 
Total elapsed time: 5.482548290397972. Arrivals time: 0.23418754152953625 Scheduler time: 5.1012459341436625 Scheduler overhead time: 0.04671603022143245 Adapter cache time: 0.03145046113058925 Engine time: 0.04692615149542689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.760125982109457,
    "estimated_duration": 3600.096171189324,
    "input_throughput": 5877.360490903196,
    "output_throughput": 5145.146718089131,
    "total_throughput": 11022.507208992327,
    "itl": 166.29907177765418,
    "ttft": 1886029.7083475296,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0466123338206696,
    "arrivals": 376253,
    "finished_requests": 85266,
    "scheduler_time": 73.63668726113163
}
#Debug simulation 
Total elapsed time: 5.760224748402834. Arrivals time: 0.24161004275083542 Scheduler time: 5.412401950452477 Scheduler overhead time: 0.031989339739084244 Adapter cache time: 0.02712197322398424 Engine time: 0.03224195959046483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.473582039121538,
    "estimated_duration": 3600.080135069376,
    "input_throughput": 5425.711725059583,
    "output_throughput": 4759.5868306053135,
    "total_throughput": 10185.298555664896,
    "itl": 107.31820145252193,
    "ttft": 1945448.2089184688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.07742249872536,
    "arrivals": 376253,
    "finished_requests": 78716,
    "scheduler_time": 66.64241065303086
}
#Debug simulation 
Total elapsed time: 5.473661343101412. Arrivals time: 0.23010901035740972 Scheduler time: 5.095561174675822 Scheduler overhead time: 0.046969865914434195 Adapter cache time: 0.031807100400328636 Engine time: 0.047189447563141584 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.769059754908085,
    "estimated_duration": 3600.0226142219885,
    "input_throughput": 5877.480579263735,
    "output_throughput": 5145.251845592383,
    "total_throughput": 11022.732424856118,
    "itl": 166.2970447507557,
    "ttft": 1885999.9737625245,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 335,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0016668487922302,
    "arrivals": 376253,
    "finished_requests": 85266,
    "scheduler_time": 73.63629985866136
}
#Debug simulation 
Total elapsed time: 5.769143282901496. Arrivals time: 0.2501286561600864 Scheduler time: 5.412799436133355 Scheduler overhead time: 0.03215537592768669 Adapter cache time: 0.027075208723545074 Engine time: 0.032145336735993624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_192_slots_160_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.484168875031173,
    "estimated_duration": 3600.0063849708176,
    "input_throughput": 5425.7309324628695,
    "output_throughput": 4759.54155846279,
    "total_throughput": 10185.27249092566,
    "itl": 107.31544027622927,
    "ttft": 1945550.611618733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 325,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 1.0910039076581644,
    "arrivals": 376253,
    "finished_requests": 78715,
    "scheduler_time": 66.64066716239108
}
#Debug simulation 
Total elapsed time: 5.484253169037402. Arrivals time: 0.23328140703961253 Scheduler time: 5.103967182803899 Scheduler overhead time: 0.04674659948796034 Adapter cache time: 0.031100389547646046 Engine time: 0.047119901049882174 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 587328,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_192_slots_160_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.8399927681311965,
    "estimated_duration": 3600.1371956047255,
    "input_throughput": 5897.456915231159,
    "output_throughput": 5207.5579294279805,
    "total_throughput": 11105.01484465914,
    "itl": 164.9665938238943,
    "ttft": 1886247.1151649712,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.710033156592398,
    "arrivals": 375490,
    "finished_requests": 85669,
    "scheduler_time": 74.50901818255885
}
#Debug simulation 
Total elapsed time: 5.84007283905521. Arrivals time: 0.24390719085931778 Scheduler time: 5.492135351523757 Scheduler overhead time: 0.03222544165328145 Adapter cache time: 0.024690207093954086 Engine time: 0.03225553594529629 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_192_slots_160_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.882074517197907,
    "estimated_duration": 3600.0732152222117,
    "input_throughput": 5897.337840305435,
    "output_throughput": 5207.350761849121,
    "total_throughput": 11104.688602154556,
    "itl": 164.9682545543868,
    "ttft": 1886235.8431316698,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7582497287308828,
    "arrivals": 375490,
    "finished_requests": 85666,
    "scheduler_time": 74.50701289851122
}
#Debug simulation 
Total elapsed time: 5.882177429273725. Arrivals time: 0.24700830597430468 Scheduler time: 5.530227970331907 Scheduler overhead time: 0.03220452833920717 Adapter cache time: 0.02473532035946846 Engine time: 0.032952962443232536 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 250432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_192_slots_160_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.56240513920784,
    "estimated_duration": 3600.0821330169947,
    "input_throughput": 5406.042773721387,
    "output_throughput": 4787.845211063309,
    "total_throughput": 10193.887984784695,
    "itl": 105.98947591416633,
    "ttft": 1951855.2919976015,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 228,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7461956591717933,
    "arrivals": 375490,
    "finished_requests": 78535,
    "scheduler_time": 66.99343422158294
}
#Debug simulation 
Total elapsed time: 5.56250775186345. Arrivals time: 0.2318528234027326 Scheduler time: 5.182741926982999 Scheduler overhead time: 0.04756898107007146 Adapter cache time: 0.029779081232845783 Engine time: 0.047980186995118856 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 160,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 477168,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_192_slots_160_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.865515418350697,
    "estimated_duration": 3600.0076413400247,
    "input_throughput": 5897.40303776059,
    "output_throughput": 5207.340891371562,
    "total_throughput": 11104.743929132153,
    "itl": 164.96757031910806,
    "ttft": 1886215.6233079357,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 232,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.7251535079372133,
    "arrivals": 375490,
    "finished_requests": 85665,
    "scheduler_time": 74.50592933632194
}
#Debug simulation 
Total elapsed time: 5.865607053041458. Arrivals time: 0.27782473200932145 Scheduler time: 5.482547899708152 Scheduler overhead time: 0.03262569522485137 Adapter cache time: 0.024808953516185284 Engine time: 0.0327200242318213 
