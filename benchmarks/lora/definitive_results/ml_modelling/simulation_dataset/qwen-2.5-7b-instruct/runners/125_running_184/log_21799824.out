INFO 06-01 00:47:13 [__init__.py:243] No platform detected, vLLM is running on UnspecifiedPlatform
WARNING 06-01 00:47:13 [_custom_ops.py:21] Failed to import from vllm._C with ModuleNotFoundError("No module named 'vllm._C'")
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_192_slots_192_rate_1.6-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-16/adapters_192_slots_192_rate_1.6-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.843061123043299,
    "estimated_duration": 3600.1304441652633,
    "input_throughput": 4582.707003502856,
    "output_throughput": 4015.3881155691824,
    "total_throughput": 8598.095119072039,
    "itl": 210.90798237116337,
    "ttft": 2096801.6691821127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 466835,
    "finished_requests": 66573,
    "scheduler_time": 40.6205156313851
}
#Debug simulation 
Total elapsed time: 4.843175154994242. Arrivals time: 0.22642291139345616 Scheduler time: 4.516741176252253 Scheduler overhead time: 0.0270045034121722 Adapter cache time: 0.032761201611720026 Engine time: 0.027694778982549906 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_192_slots_192_rate_1.6-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.025_size_16-16-32/adapters_192_slots_192_rate_1.6-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 4320, 4320, 4320, 4320, 17280, 4320, 270, 17280, 270, 4320, 4320, 270, 17280, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 270, 4320, 4320, 17280, 4320, 270, 270, 4320, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 17280, 17280, 270, 4320, 17280, 270, 4320, 270, 4320, 270, 270, 4320, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 4320, 270, 270, 270, 270, 270, 17280, 270, 4320, 270, 17280, 17280, 270, 17280, 270, 4320, 4320, 17280, 270, 270, 270, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 270, 17280, 4320, 4320, 4320, 17280, 4320, 270, 4320, 17280, 17280, 4320, 17280, 4320, 270, 17280, 4320, 270, 4320, 4320, 270, 4320, 17280, 270, 4320, 17280, 270, 4320, 17280, 17280, 4320, 270, 17280, 4320, 270, 270, 4320, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 4320, 270, 17280, 17280, 4320, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 270, 17280, 4320, 17280, 270, 17280, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 17280, 4320, 270, 270]
Prompts retrieved: 1399680 . Total input tokens: 312262604 . Total output tokens: 279992099
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.518790666013956,
    "estimated_duration": 3600.0122336244626,
    "input_throughput": 3984.9984025072386,
    "output_throughput": 3506.8639162066133,
    "total_throughput": 7491.862318713852,
    "itl": 95.76477707471778,
    "ttft": 2189436.9915349954,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246534,
    "arrivals": 466835,
    "finished_requests": 57807,
    "scheduler_time": 10.270369114692679
}
#Debug simulation 
Total elapsed time: 4.5188923119567335. Arrivals time: 0.20276215265039355 Scheduler time: 4.077097380533814 Scheduler overhead time: 0.05358975869603455 Adapter cache time: 0.1052280665608123 Engine time: 0.054980556946247816 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.959360803943127,
    "estimated_duration": 3600.0792410647405,
    "input_throughput": 4686.387679347362,
    "output_throughput": 4104.321880323379,
    "total_throughput": 8790.70955967074,
    "itl": 208.2288270768078,
    "ttft": 2086842.850476878,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 463927,
    "finished_requests": 67940,
    "scheduler_time": 41.73053752062278
}
#Debug simulation 
Total elapsed time: 4.959481610916555. Arrivals time: 0.2670787895331159 Scheduler time: 4.597579655353911 Scheduler overhead time: 0.02719643001910299 Adapter cache time: 0.02678935683798045 Engine time: 0.028144851443357766 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.949602841981687,
    "estimated_duration": 3600.2142438799874,
    "input_throughput": 4681.683049460726,
    "output_throughput": 4101.144820783668,
    "total_throughput": 8782.827870244393,
    "itl": 206.78393517237794,
    "ttft": 2087434.578597703,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815837,
    "arrivals": 463927,
    "finished_requests": 67875,
    "scheduler_time": 41.545142422174365
}
#Debug simulation 
Total elapsed time: 4.949696371913888. Arrivals time: 0.2288532864768058 Scheduler time: 4.625167694874108 Scheduler overhead time: 0.027437950484454632 Adapter cache time: 0.027121290448121727 Engine time: 0.02835379191674292 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.598229367984459,
    "estimated_duration": 3600.029966658227,
    "input_throughput": 4025.1926051191404,
    "output_throughput": 3546.992974576055,
    "total_throughput": 7572.185579695195,
    "itl": 95.55057499937897,
    "ttft": 2185513.1438183,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 463927,
    "finished_requests": 58375,
    "scheduler_time": 10.790020686705398
}
#Debug simulation 
Total elapsed time: 4.598323395010084. Arrivals time: 0.24906721175648272 Scheduler time: 4.1151591192465276 Scheduler overhead time: 0.053727221325971186 Adapter cache time: 0.10029251570813358 Engine time: 0.05488687788601965 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.9683034549234435,
    "estimated_duration": 3600.008813191933,
    "input_throughput": 4681.8843715706735,
    "output_throughput": 4101.296070691821,
    "total_throughput": 8783.180442262495,
    "itl": 206.77709531848845,
    "ttft": 2087346.756720351,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 463927,
    "finished_requests": 67874,
    "scheduler_time": 41.54189979484361
}
#Debug simulation 
Total elapsed time: 4.968427993939258. Arrivals time: 0.22751392261125147 Scheduler time: 4.645273497560993 Scheduler overhead time: 0.027577171218581498 Adapter cache time: 0.027018330874852836 Engine time: 0.028203208465129137 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.566730192047544,
    "estimated_duration": 3600.0584534558884,
    "input_throughput": 4019.9733385182735,
    "output_throughput": 3542.1272084509637,
    "total_throughput": 7562.100546969237,
    "itl": 95.4785210757245,
    "ttft": 2185980.7541836356,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941029,
    "arrivals": 463927,
    "finished_requests": 58296,
    "scheduler_time": 10.678579282624348
}
#Debug simulation 
Total elapsed time: 4.566823650035076. Arrivals time: 0.24652772664558142 Scheduler time: 4.085590707836673 Scheduler overhead time: 0.05375242966692895 Adapter cache time: 0.09897631662897766 Engine time: 0.05495276383589953 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.001296265050769,
    "estimated_duration": 3600.201320884478,
    "input_throughput": 4681.69985445679,
    "output_throughput": 4101.159541926009,
    "total_throughput": 8782.859396382799,
    "itl": 206.77702418450977,
    "ttft": 2087417.5505931566,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 463927,
    "finished_requests": 67875,
    "scheduler_time": 41.54451823972034
}
#Debug simulation 
Total elapsed time: 5.001387877040543. Arrivals time: 0.23059483873657882 Scheduler time: 4.67438913078513 Scheduler overhead time: 0.027632499812170863 Adapter cache time: 0.027460923767648637 Engine time: 0.028449794161133468 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.0125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 4320, 4320, 4320, 4320, 17280, 4320, 135, 17280, 135, 4320, 4320, 135, 17280, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 135, 4320, 4320, 17280, 4320, 135, 135, 4320, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 17280, 17280, 135, 4320, 17280, 135, 4320, 135, 4320, 135, 135, 4320, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 4320, 135, 135, 135, 135, 135, 17280, 135, 4320, 135, 17280, 17280, 135, 17280, 135, 4320, 4320, 17280, 135, 135, 135, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 135, 17280, 4320, 4320, 4320, 17280, 4320, 135, 4320, 17280, 17280, 4320, 17280, 4320, 135, 17280, 4320, 135, 4320, 4320, 135, 4320, 17280, 135, 4320, 17280, 135, 4320, 17280, 17280, 4320, 135, 17280, 4320, 135, 135, 4320, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 4320, 135, 17280, 17280, 4320, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 135, 17280, 4320, 17280, 135, 17280, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 17280, 4320, 135, 135]
Prompts retrieved: 1391040 . Total input tokens: 310318632 . Total output tokens: 278256538
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.595619810977951,
    "estimated_duration": 3600.003556719974,
    "input_throughput": 4021.833526517879,
    "output_throughput": 3543.7976099178495,
    "total_throughput": 7565.631136435728,
    "itl": 95.61472365673895,
    "ttft": 2186133.774070898,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246535,
    "arrivals": 463927,
    "finished_requests": 58321,
    "scheduler_time": 10.76151417310102
}
#Debug simulation 
Total elapsed time: 4.595713738934137. Arrivals time: 0.24880217981990427 Scheduler time: 4.113233177922666 Scheduler overhead time: 0.053560795611701906 Adapter cache time: 0.09988909820094705 Engine time: 0.05497034976724535 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.000776831060648,
    "estimated_duration": 3600.0436687207184,
    "input_throughput": 4741.343875438465,
    "output_throughput": 4165.208919626376,
    "total_throughput": 8906.552795064841,
    "itl": 205.27454583962694,
    "ttft": 2076442.9881734282,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5784321836032897,
    "arrivals": 462544,
    "finished_requests": 68821,
    "scheduler_time": 42.375992265684246
}
#Debug simulation 
Total elapsed time: 5.000896406010725. Arrivals time: 0.23104016459546983 Scheduler time: 4.677585025317967 Scheduler overhead time: 0.027555898646824062 Adapter cache time: 0.02331409347243607 Engine time: 0.028443212970159948 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.029673598939553,
    "estimated_duration": 3600.1191133224643,
    "input_throughput": 4733.749763149443,
    "output_throughput": 4158.557405669653,
    "total_throughput": 8892.307168819098,
    "itl": 203.15814433664704,
    "ttft": 2077688.005253353,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6161939205415561,
    "arrivals": 462544,
    "finished_requests": 68713,
    "scheduler_time": 42.08226171530003
}
#Debug simulation 
Total elapsed time: 5.029765641898848. Arrivals time: 0.23023618408478796 Scheduler time: 4.705489442567341 Scheduler overhead time: 0.027904153801500797 Adapter cache time: 0.024246239569038153 Engine time: 0.02875060075893998 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.570469612022862,
    "estimated_duration": 3600.000407863598,
    "input_throughput": 4040.9967643957,
    "output_throughput": 3563.7729295740974,
    "total_throughput": 7604.769693969797,
    "itl": 94.43666600012298,
    "ttft": 2181698.9695114247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6106194665469253,
    "arrivals": 462544,
    "finished_requests": 58731,
    "scheduler_time": 10.640941760695885
}
#Debug simulation 
Total elapsed time: 4.5705646420829. Arrivals time: 0.22956884559243917 Scheduler time: 4.109804174047895 Scheduler overhead time: 0.05412465054541826 Adapter cache time: 0.09353817359078676 Engine time: 0.05806763027794659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.019129503052682,
    "estimated_duration": 3600.0221842096685,
    "input_throughput": 4733.877217409796,
    "output_throughput": 4158.669373112968,
    "total_throughput": 8892.546590522763,
    "itl": 203.17163243388197,
    "ttft": 2077656.586217904,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5912696061166944,
    "arrivals": 462544,
    "finished_requests": 68713,
    "scheduler_time": 42.08316863989826
}
#Debug simulation 
Total elapsed time: 5.019263212103397. Arrivals time: 0.2300467558670789 Scheduler time: 4.695511324331164 Scheduler overhead time: 0.028156780055724084 Adapter cache time: 0.023801792762242258 Engine time: 0.028694327105768025 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.573430800926872,
    "estimated_duration": 3600.0946655891266,
    "input_throughput": 4043.690056027389,
    "output_throughput": 3566.48899339398,
    "total_throughput": 7610.179049421368,
    "itl": 94.6309102797294,
    "ttft": 2181578.800880528,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6181646937318166,
    "arrivals": 462544,
    "finished_requests": 58776,
    "scheduler_time": 10.768641422356913
}
#Debug simulation 
Total elapsed time: 4.573521782993339. Arrivals time: 0.20674444176256657 Scheduler time: 4.13726844987832 Scheduler overhead time: 0.05418083583936095 Adapter cache time: 0.09476240875665098 Engine time: 0.05525095318444073 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.998433954897337,
    "estimated_duration": 3600.059105662425,
    "input_throughput": 4733.828667755774,
    "output_throughput": 4158.626722670217,
    "total_throughput": 8892.45539042599,
    "itl": 203.16643983004425,
    "ttft": 2077648.741502092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 189,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5651195057365119,
    "arrivals": 462544,
    "finished_requests": 68713,
    "scheduler_time": 42.08315123982951
}
#Debug simulation 
Total elapsed time: 4.998530734912492. Arrivals time: 0.23211139417253435 Scheduler time: 4.672576146782376 Scheduler overhead time: 0.027884414768777788 Adapter cache time: 0.024018329102545977 Engine time: 0.028938655159436166 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 4320, 4320, 4320, 4320, 17280, 4320, 66, 17280, 66, 4320, 4320, 66, 17280, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 66, 4320, 4320, 17280, 4320, 66, 66, 4320, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 17280, 17280, 66, 4320, 17280, 66, 4320, 66, 4320, 66, 66, 4320, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 4320, 66, 66, 66, 66, 66, 17280, 66, 4320, 66, 17280, 17280, 66, 17280, 66, 4320, 4320, 17280, 66, 66, 66, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 66, 17280, 4320, 4320, 4320, 17280, 4320, 66, 4320, 17280, 17280, 4320, 17280, 4320, 66, 17280, 4320, 66, 4320, 4320, 66, 4320, 17280, 66, 4320, 17280, 66, 4320, 17280, 17280, 4320, 66, 17280, 4320, 66, 66, 4320, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 4320, 66, 17280, 17280, 4320, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 66, 17280, 4320, 17280, 66, 17280, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 17280, 4320, 66, 66]
Prompts retrieved: 1386624 . Total input tokens: 309332421 . Total output tokens: 277379693
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.535298442933708,
    "estimated_duration": 3600.0158639669703,
    "input_throughput": 4040.803304674956,
    "output_throughput": 3563.594296460496,
    "total_throughput": 7604.397601135452,
    "itl": 94.43682729143607,
    "ttft": 2181725.414189887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6260871822759524,
    "arrivals": 462544,
    "finished_requests": 58729,
    "scheduler_time": 10.641656708297095
}
#Debug simulation 
Total elapsed time: 4.535389314987697. Arrivals time: 0.20791682158596814 Scheduler time: 4.100348080159165 Scheduler overhead time: 0.05410465714521706 Adapter cache time: 0.0922866634791717 Engine time: 0.05535576422698796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.064180238055997,
    "estimated_duration": 3600.1857643728704,
    "input_throughput": 4728.427951818574,
    "output_throughput": 4176.369494261117,
    "total_throughput": 8904.797446079692,
    "itl": 205.41954938728264,
    "ttft": 2071332.3445263286,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5570087693957605,
    "arrivals": 461815,
    "finished_requests": 68920,
    "scheduler_time": 42.55072465722001
}
#Debug simulation 
Total elapsed time: 5.064273320022039. Arrivals time: 0.27644233824685216 Scheduler time: 4.696552447974682 Scheduler overhead time: 0.027896039769984782 Adapter cache time: 0.021828418830409646 Engine time: 0.028642508783377707 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.022106699063443,
    "estimated_duration": 3600.150840498931,
    "input_throughput": 4720.102227062518,
    "output_throughput": 4169.094758796248,
    "total_throughput": 8889.196985858765,
    "itl": 202.7559316476276,
    "ttft": 2072443.5840261444,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5936291872034792,
    "arrivals": 461815,
    "finished_requests": 68803,
    "scheduler_time": 42.179123257347634
}
#Debug simulation 
Total elapsed time: 5.022239862009883. Arrivals time: 0.27280293114017695 Scheduler time: 4.656678838771768 Scheduler overhead time: 0.027858438901603222 Adapter cache time: 0.02312659623567015 Engine time: 0.0286990535678342 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.604015546035953,
    "estimated_duration": 3600.030359817544,
    "input_throughput": 4009.3478547029613,
    "output_throughput": 3558.2066593041777,
    "total_throughput": 7567.554514007139,
    "itl": 94.07954856661215,
    "ttft": 2178668.522321922,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5818788069300376,
    "arrivals": 461815,
    "finished_requests": 58494,
    "scheduler_time": 10.359388582659033
}
#Debug simulation 
Total elapsed time: 4.604113838053308. Arrivals time: 0.2533853193745017 Scheduler time: 4.122134128236212 Scheduler overhead time: 0.05459953797981143 Adapter cache time: 0.09251274657435715 Engine time: 0.055935572599992156 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.047480921028182,
    "estimated_duration": 3600.168290606426,
    "input_throughput": 4720.079348606679,
    "output_throughput": 4169.074551087656,
    "total_throughput": 8889.153899694335,
    "itl": 202.7483945612879,
    "ttft": 2072453.7717875547,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5695220634154984,
    "arrivals": 461815,
    "finished_requests": 68803,
    "scheduler_time": 42.17835308697773
}
#Debug simulation 
Total elapsed time: 5.047574528958648. Arrivals time: 0.29414063156582415 Scheduler time: 4.66085989784915 Scheduler overhead time: 0.028086059144698083 Adapter cache time: 0.02266863000113517 Engine time: 0.02873440901748836 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.594119829009287,
    "estimated_duration": 3600.068050164318,
    "input_throughput": 4009.9461451405327,
    "output_throughput": 3558.7238411827348,
    "total_throughput": 7568.669986323268,
    "itl": 94.1152577161884,
    "ttft": 2178496.906308885,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5891725265420987,
    "arrivals": 461815,
    "finished_requests": 58504,
    "scheduler_time": 10.384568327568264
}
#Debug simulation 
Total elapsed time: 4.594212722964585. Arrivals time: 0.2101264769444242 Scheduler time: 4.156402828288265 Scheduler overhead time: 0.05428612185642123 Adapter cache time: 0.09228033933322877 Engine time: 0.05564781208522618 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.072854524012655,
    "estimated_duration": 3600.088911014104,
    "input_throughput": 4720.183423251412,
    "output_throughput": 4169.166476439059,
    "total_throughput": 8889.349899690471,
    "itl": 202.75979355603812,
    "ttft": 2072417.4668876159,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5441891536721967,
    "arrivals": 461815,
    "finished_requests": 68803,
    "scheduler_time": 42.17973039476633
}
#Debug simulation 
Total elapsed time: 5.07294639700558. Arrivals time: 0.27383034536615014 Scheduler time: 4.706073627341539 Scheduler overhead time: 0.02810149360448122 Adapter cache time: 0.02286825212650001 Engine time: 0.028830583789385855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.4-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 4320, 4320, 4320, 4320, 17280, 4320, 33, 17280, 33, 4320, 4320, 33, 17280, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 33, 4320, 4320, 17280, 4320, 33, 33, 4320, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 17280, 17280, 33, 4320, 17280, 33, 4320, 33, 4320, 33, 33, 4320, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 4320, 33, 33, 33, 33, 33, 17280, 33, 4320, 33, 17280, 17280, 33, 17280, 33, 4320, 4320, 17280, 33, 33, 33, 17280, 17280, 4320, 17280, 4320, 17280, 17280, 4320, 4320, 33, 17280, 4320, 4320, 4320, 17280, 4320, 33, 4320, 17280, 17280, 4320, 17280, 4320, 33, 17280, 4320, 33, 4320, 4320, 33, 4320, 17280, 33, 4320, 17280, 33, 4320, 17280, 17280, 4320, 33, 17280, 4320, 33, 33, 4320, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 4320, 33, 17280, 17280, 4320, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 4320, 4320, 4320, 4320, 4320, 4320, 33, 17280, 4320, 17280, 33, 17280, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 17280, 4320, 33, 33]
Prompts retrieved: 1384512 . Total input tokens: 308866781 . Total output tokens: 276959555
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.612311013974249,
    "estimated_duration": 3600.0025580800725,
    "input_throughput": 4012.949092931915,
    "output_throughput": 3561.2146917021287,
    "total_throughput": 7574.163784634044,
    "itl": 94.3149677307085,
    "ttft": 2178244.691249247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 178,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5968435075134049,
    "arrivals": 461815,
    "finished_requests": 58542,
    "scheduler_time": 10.510636052410563
}
#Debug simulation 
Total elapsed time: 4.612450035987422. Arrivals time: 0.20812377997208387 Scheduler time: 4.175134779186919 Scheduler overhead time: 0.054333823383785784 Adapter cache time: 0.09363863046746701 Engine time: 0.05556938727386296 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.130378545029089,
    "estimated_duration": 3600.1375027433187,
    "input_throughput": 4806.849456948042,
    "output_throughput": 4245.298127739231,
    "total_throughput": 9052.147584687273,
    "itl": 202.50426224957357,
    "ttft": 2038411.5455566049,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 403594,
    "finished_requests": 69721,
    "scheduler_time": 43.34864145564176
}
#Debug simulation 
Total elapsed time: 5.130472974968143. Arrivals time: 0.23429060087073594 Scheduler time: 4.766675465856679 Scheduler overhead time: 0.028232866199687123 Adapter cache time: 0.05909600562881678 Engine time: 0.0290365528780967 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.125577881932259,
    "estimated_duration": 3600.085121724282,
    "input_throughput": 4807.453828122429,
    "output_throughput": 4246.105434495536,
    "total_throughput": 9053.559262617966,
    "itl": 199.29123748818384,
    "ttft": 2038930.1879574677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815837,
    "arrivals": 403594,
    "finished_requests": 69722,
    "scheduler_time": 43.035636164080486
}
#Debug simulation 
Total elapsed time: 5.125710812979378. Arrivals time: 0.23761134350206703 Scheduler time: 4.756492592045106 Scheduler overhead time: 0.028694502776488662 Adapter cache time: 0.05996989936102182 Engine time: 0.029531439184211195 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.9168539989041165,
    "estimated_duration": 3600.0273263013196,
    "input_throughput": 4356.914428234309,
    "output_throughput": 3866.216208503033,
    "total_throughput": 8223.130636737342,
    "itl": 87.21627036610549,
    "ttft": 2111529.172929131,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 403594,
    "finished_requests": 63204,
    "scheduler_time": 11.812980645095994
}
#Debug simulation 
Total elapsed time: 4.916945281904191. Arrivals time: 0.2363149948650971 Scheduler time: 4.438810054794885 Scheduler overhead time: 0.05846449499949813 Adapter cache time: 0.09585536119993776 Engine time: 0.059973145951516926 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.123250622069463,
    "estimated_duration": 3600.1707156339735,
    "input_throughput": 4807.34008663594,
    "output_throughput": 4246.073924666126,
    "total_throughput": 9053.414011302066,
    "itl": 199.29946927554957,
    "ttft": 2038935.8309734531,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 403594,
    "finished_requests": 69724,
    "scheduler_time": 43.03848950993085
}
#Debug simulation 
Total elapsed time: 5.123350132023916. Arrivals time: 0.23165577556937933 Scheduler time: 4.758594183600508 Scheduler overhead time: 0.028731764061376452 Adapter cache time: 0.06097203912213445 Engine time: 0.029911493649706244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.9262382090091705,
    "estimated_duration": 3600.0251802633334,
    "input_throughput": 4356.747026656276,
    "output_throughput": 3866.199624465377,
    "total_throughput": 8222.946651121652,
    "itl": 87.21888983823906,
    "ttft": 2111496.812673916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941028,
    "arrivals": 403594,
    "finished_requests": 63202,
    "scheduler_time": 11.811856109928254
}
#Debug simulation 
Total elapsed time: 4.926370771951042. Arrivals time: 0.21804597380105406 Scheduler time: 4.466526470845565 Scheduler overhead time: 0.058369756559841335 Adapter cache time: 0.09589547733776271 Engine time: 0.06000114080961794 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.100720028975047,
    "estimated_duration": 3600.1777124157147,
    "input_throughput": 4807.330743788995,
    "output_throughput": 4246.065672614454,
    "total_throughput": 9053.39641640345,
    "itl": 199.29401737516827,
    "ttft": 2038939.7956254184,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 403594,
    "finished_requests": 69724,
    "scheduler_time": 43.03758349881401
}
#Debug simulation 
Total elapsed time: 5.100840148981661. Arrivals time: 0.2294198841555044 Scheduler time: 4.739980080281384 Scheduler overhead time: 0.028552183881402016 Adapter cache time: 0.06009096477646381 Engine time: 0.029527624137699604 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.05_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  1.6 ]. Counts: [64 64 64]
Adapter prompts. [17280, 540, 17280, 540, 17280, 540, 17280, 540, 540, 540, 1080, 1080, 1080, 1080, 17280, 1080, 540, 17280, 540, 1080, 1080, 540, 17280, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 540, 1080, 1080, 17280, 1080, 540, 540, 1080, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 17280, 17280, 540, 1080, 17280, 540, 1080, 540, 1080, 540, 540, 1080, 17280, 17280, 540, 540, 540, 17280, 17280, 17280, 1080, 540, 540, 540, 540, 540, 17280, 540, 1080, 540, 17280, 17280, 540, 17280, 540, 1080, 1080, 17280, 540, 540, 540, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 540, 17280, 1080, 1080, 1080, 17280, 1080, 540, 1080, 17280, 17280, 1080, 17280, 1080, 540, 17280, 1080, 540, 1080, 1080, 540, 1080, 17280, 540, 1080, 17280, 540, 1080, 17280, 17280, 1080, 540, 17280, 1080, 540, 540, 1080, 17280, 17280, 17280, 540, 17280, 540, 17280, 540, 1080, 540, 17280, 17280, 1080, 17280, 540, 17280, 17280, 17280, 540, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 540, 17280, 1080, 17280, 540, 17280, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 17280, 1080, 540, 540]
Prompts retrieved: 1209600 . Total input tokens: 269680978 . Total output tokens: 241889124
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.91261827305425,
    "estimated_duration": 3600.0436895910034,
    "input_throughput": 4356.36324229769,
    "output_throughput": 3866.0728035723396,
    "total_throughput": 8222.436045870028,
    "itl": 87.22239856919931,
    "ttft": 2111595.6427894724,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246534,
    "arrivals": 403594,
    "finished_requests": 63199,
    "scheduler_time": 11.812700576502406
}
#Debug simulation 
Total elapsed time: 4.912711484008469. Arrivals time: 0.21957669558469206 Scheduler time: 4.451156154507771 Scheduler overhead time: 0.05863560037687421 Adapter cache time: 0.09582860488444567 Engine time: 0.05990523798391223 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.345732346992008,
    "estimated_duration": 3600.199078144369,
    "input_throughput": 5052.320331512124,
    "output_throughput": 4430.507217456815,
    "total_throughput": 9482.827548968939,
    "itl": 193.21158572486556,
    "ttft": 2008708.3593633224,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 397773,
    "finished_requests": 73311,
    "scheduler_time": 45.16059439141888
}
#Debug simulation 
Total elapsed time: 5.345827621058561. Arrivals time: 0.2381859077140689 Scheduler time: 4.982413128367625 Scheduler overhead time: 0.029589548124931753 Adapter cache time: 0.05110118316952139 Engine time: 0.03066180634777993 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.3381479639792815,
    "estimated_duration": 3600.1720967530196,
    "input_throughput": 5050.9724289014985,
    "output_throughput": 4430.06961094564,
    "total_throughput": 9481.04203984714,
    "itl": 190.88576562302367,
    "ttft": 2009233.3480945681,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815838,
    "arrivals": 397773,
    "finished_requests": 73298,
    "scheduler_time": 44.90111201097886
}
#Debug simulation 
Total elapsed time: 5.338259099982679. Arrivals time: 0.23552531749010086 Scheduler time: 4.976075498969294 Scheduler overhead time: 0.029718153877183795 Adapter cache time: 0.051999627030454576 Engine time: 0.030849477858282626 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.9885619500419125,
    "estimated_duration": 3600.071262383329,
    "input_throughput": 4476.838324953107,
    "output_throughput": 3952.145100200249,
    "total_throughput": 8428.983425153356,
    "itl": 84.89816800162616,
    "ttft": 2093520.874537479,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 397773,
    "finished_requests": 65036,
    "scheduler_time": 11.85632946979921
}
#Debug simulation 
Total elapsed time: 4.9887096310267225. Arrivals time: 0.2228144274558872 Scheduler time: 4.531656988663599 Scheduler overhead time: 0.060047813574783504 Adapter cache time: 0.08433913963381201 Engine time: 0.06157355452887714 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.34557887900155,
    "estimated_duration": 3600.1464983708993,
    "input_throughput": 5051.263610586129,
    "output_throughput": 4430.148330690753,
    "total_throughput": 9481.411941276883,
    "itl": 191.03358363908896,
    "ttft": 2009183.30657256,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6002397570014009,
    "arrivals": 397773,
    "finished_requests": 73300,
    "scheduler_time": 44.918260623325445
}
#Debug simulation 
Total elapsed time: 5.345675115007907. Arrivals time: 0.25735859328415245 Scheduler time: 4.96180180774536 Scheduler overhead time: 0.029846660559996963 Adapter cache time: 0.051871662377379835 Engine time: 0.03099453344475478 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.002185876015574,
    "estimated_duration": 3600.0647045197925,
    "input_throughput": 4476.807036208477,
    "output_throughput": 3951.9703582377047,
    "total_throughput": 8428.777394446182,
    "itl": 84.90099498714645,
    "ttft": 2093537.5550450212,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.635571540594103,
    "arrivals": 397773,
    "finished_requests": 65034,
    "scheduler_time": 11.85718918510746
}
#Debug simulation 
Total elapsed time: 5.0022793060634285. Arrivals time: 0.22334608365781605 Scheduler time: 4.546149775502272 Scheduler overhead time: 0.05977065290790051 Adapter cache time: 0.08349581167567521 Engine time: 0.061326703638769686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.308446352020837,
    "estimated_duration": 3600.1049758774175,
    "input_throughput": 5051.32187029293,
    "output_throughput": 4430.199426646682,
    "total_throughput": 9481.52129693961,
    "itl": 190.93611773687118,
    "ttft": 2009174.063162871,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 397773,
    "finished_requests": 73300,
    "scheduler_time": 44.90771221868449
}
#Debug simulation 
Total elapsed time: 5.308566621970385. Arrivals time: 0.23734601156320423 Scheduler time: 4.945421418175101 Scheduler overhead time: 0.029712061630561948 Adapter cache time: 0.051528787007555366 Engine time: 0.030673566041514277 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.025_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 1080, 1080, 1080, 1080, 17280, 1080, 270, 17280, 270, 1080, 1080, 270, 17280, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 270, 1080, 1080, 17280, 1080, 270, 270, 1080, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 17280, 17280, 270, 1080, 17280, 270, 1080, 270, 1080, 270, 270, 1080, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 1080, 270, 270, 270, 270, 270, 17280, 270, 1080, 270, 17280, 17280, 270, 17280, 270, 1080, 1080, 17280, 270, 270, 270, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 270, 17280, 1080, 1080, 1080, 17280, 1080, 270, 1080, 17280, 17280, 1080, 17280, 1080, 270, 17280, 1080, 270, 1080, 1080, 270, 1080, 17280, 270, 1080, 17280, 270, 1080, 17280, 17280, 1080, 270, 17280, 1080, 270, 270, 1080, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 1080, 270, 17280, 17280, 1080, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 270, 17280, 1080, 17280, 270, 17280, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 17280, 1080, 270, 270]
Prompts retrieved: 1192320 . Total input tokens: 265877621 . Total output tokens: 238439135
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.012785459985025,
    "estimated_duration": 3600.0107013442453,
    "input_throughput": 4479.7233502606405,
    "output_throughput": 3954.60435567151,
    "total_throughput": 8434.32770593215,
    "itl": 85.04525572820991,
    "ttft": 2093434.5269184974,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246535,
    "arrivals": 397773,
    "finished_requests": 65070,
    "scheduler_time": 11.971265825670303
}
#Debug simulation 
Total elapsed time: 5.0128792689647526. Arrivals time: 0.22317623370327055 Scheduler time: 4.555994044872932 Scheduler overhead time: 0.06007011968176812 Adapter cache time: 0.08407457964494824 Engine time: 0.0613705973373726 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.483004919020459,
    "estimated_duration": 3600.0262785397927,
    "input_throughput": 5178.672475569244,
    "output_throughput": 4563.36780037713,
    "total_throughput": 9742.040275946374,
    "itl": 188.09845718581596,
    "ttft": 1990461.9401944727,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 394957,
    "finished_requests": 75319,
    "scheduler_time": 46.52464429679505
}
#Debug simulation 
Total elapsed time: 5.48314544709865. Arrivals time: 0.258460181648843 Scheduler time: 5.102424948010594 Scheduler overhead time: 0.030223520006984472 Adapter cache time: 0.04664598614908755 Engine time: 0.03128710191231221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.447369118104689,
    "estimated_duration": 3600.117464645657,
    "input_throughput": 5176.6160918402975,
    "output_throughput": 4561.966702832709,
    "total_throughput": 9738.582794673006,
    "itl": 186.14145417760142,
    "ttft": 1990939.7136171916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815837,
    "arrivals": 394957,
    "finished_requests": 75286,
    "scheduler_time": 46.28579959149923
}
#Debug simulation 
Total elapsed time: 5.447492550010793. Arrivals time: 0.24111686437390745 Scheduler time: 5.082262960029766 Scheduler overhead time: 0.03040870127733797 Adapter cache time: 0.048043769667856395 Engine time: 0.03143823845311999 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.103377748979256,
    "estimated_duration": 3600.0621029258023,
    "input_throughput": 4528.034109953789,
    "output_throughput": 4015.1421244240446,
    "total_throughput": 8543.176234377834,
    "itl": 84.27648789691567,
    "ttft": 2082046.0578736216,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 394957,
    "finished_requests": 65812,
    "scheduler_time": 12.433414624961442
}
#Debug simulation 
Total elapsed time: 5.103470082976855. Arrivals time: 0.24050643621012568 Scheduler time: 4.63452670595143 Scheduler overhead time: 0.06066242977976799 Adapter cache time: 0.07762114540673792 Engine time: 0.06183021329343319 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.489186331978999,
    "estimated_duration": 3600.1112382601063,
    "input_throughput": 5176.625044787998,
    "output_throughput": 4561.97459274546,
    "total_throughput": 9738.599637533458,
    "itl": 186.14375150940305,
    "ttft": 1990930.7609648057,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 394957,
    "finished_requests": 75286,
    "scheduler_time": 46.28612202303243
}
#Debug simulation 
Total elapsed time: 5.489282323978841. Arrivals time: 0.2433630108134821 Scheduler time: 5.121334614348598 Scheduler overhead time: 0.030693542095832527 Adapter cache time: 0.047962014097720385 Engine time: 0.031613607658073306 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.121588081936352,
    "estimated_duration": 3600.0751072381013,
    "input_throughput": 4527.690816013287,
    "output_throughput": 4014.9006810829424,
    "total_throughput": 8542.59149709623,
    "itl": 84.2793520620857,
    "ttft": 2081961.6404474336,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941027,
    "arrivals": 394957,
    "finished_requests": 65807,
    "scheduler_time": 12.433987516120078
}
#Debug simulation 
Total elapsed time: 5.121679771924391. Arrivals time: 0.2470972896553576 Scheduler time: 4.644955949042924 Scheduler overhead time: 0.060549624962732196 Adapter cache time: 0.07879827730357647 Engine time: 0.06179688370320946 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.4867574939271435,
    "estimated_duration": 3600.108051152318,
    "input_throughput": 5176.629627556561,
    "output_throughput": 4561.978631375564,
    "total_throughput": 9738.608258932125,
    "itl": 186.14561064364355,
    "ttft": 1990904.9054795145,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 394957,
    "finished_requests": 75286,
    "scheduler_time": 46.28697507953679
}
#Debug simulation 
Total elapsed time: 5.486880811979063. Arrivals time: 0.24125794239807874 Scheduler time: 5.120659875450656 Scheduler overhead time: 0.030708682374097407 Adapter cache time: 0.048277285997755826 Engine time: 0.03172436961904168 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.0125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.1    1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 1080, 1080, 1080, 1080, 17280, 1080, 135, 17280, 135, 1080, 1080, 135, 17280, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 135, 135, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 135, 1080, 1080, 17280, 1080, 135, 135, 1080, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 17280, 17280, 135, 1080, 17280, 135, 1080, 135, 1080, 135, 135, 1080, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 1080, 135, 135, 135, 135, 135, 17280, 135, 1080, 135, 17280, 17280, 135, 17280, 135, 1080, 1080, 17280, 135, 135, 135, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 135, 17280, 1080, 1080, 1080, 17280, 1080, 135, 1080, 17280, 17280, 1080, 17280, 1080, 135, 17280, 1080, 135, 1080, 1080, 135, 1080, 17280, 135, 1080, 17280, 135, 1080, 17280, 17280, 1080, 135, 17280, 1080, 135, 135, 1080, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 1080, 135, 17280, 17280, 1080, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 135, 17280, 1080, 17280, 135, 17280, 1080, 135, 1080, 135, 135, 1080, 1080, 1080, 17280, 1080, 135, 135]
Prompts retrieved: 1183680 . Total input tokens: 263936442 . Total output tokens: 236723287
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.085679505951703,
    "estimated_duration": 3600.0256975500283,
    "input_throughput": 4527.717402432094,
    "output_throughput": 4014.8849520258577,
    "total_throughput": 8542.602354457951,
    "itl": 84.28072148369728,
    "ttft": 2081994.0339073315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246535,
    "arrivals": 394957,
    "finished_requests": 65806,
    "scheduler_time": 12.434682459778319
}
#Debug simulation 
Total elapsed time: 5.085774803999811. Arrivals time: 0.2435988311190158 Scheduler time: 4.6129799376940355 Scheduler overhead time: 0.06039278523530811 Adapter cache time: 0.07826876256149262 Engine time: 0.06200850324239582 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.515359818004072,
    "estimated_duration": 3600.0837753200076,
    "input_throughput": 5221.810705871416,
    "output_throughput": 4598.309659760211,
    "total_throughput": 9820.120365631627,
    "itl": 186.38204577986977,
    "ttft": 1979372.4778592505,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 393528,
    "finished_requests": 75844,
    "scheduler_time": 46.89426386658877
}
#Debug simulation 
Total elapsed time: 5.515453684958629. Arrivals time: 0.24341097625438124 Scheduler time: 5.152414954849519 Scheduler overhead time: 0.0306201355997473 Adapter cache time: 0.043269271845929325 Engine time: 0.03151024261023849 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.556429286021739,
    "estimated_duration": 3600.1245402118197,
    "input_throughput": 5217.1442377087615,
    "output_throughput": 4594.732436402532,
    "total_throughput": 9811.876674111294,
    "itl": 183.7147418848455,
    "ttft": 1979945.4463656424,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815837,
    "arrivals": 393528,
    "finished_requests": 75786,
    "scheduler_time": 46.545524996060564
}
#Debug simulation 
Total elapsed time: 5.556537062046118. Arrivals time: 0.24972605681978166 Scheduler time: 5.185632556327619 Scheduler overhead time: 0.03102039627265185 Adapter cache time: 0.04384861094877124 Engine time: 0.031810432905331254 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.091269436990842,
    "estimated_duration": 3600.0690686569146,
    "input_throughput": 4553.606802359456,
    "output_throughput": 4023.340864791711,
    "total_throughput": 8576.947667151167,
    "itl": 83.29992420347463,
    "ttft": 2077626.9475939583,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 393528,
    "finished_requests": 66116,
    "scheduler_time": 12.056662812477121
}
#Debug simulation 
Total elapsed time: 5.09138581994921. Arrivals time: 0.22455470846034586 Scheduler time: 4.640324153471738 Scheduler overhead time: 0.06100085622165352 Adapter cache time: 0.0742401716997847 Engine time: 0.06251077412161976 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.5223450569901615,
    "estimated_duration": 3600.127503060782,
    "input_throughput": 5217.139944080167,
    "output_throughput": 4594.728655009173,
    "total_throughput": 9811.86859908934,
    "itl": 183.70860692396448,
    "ttft": 1979947.3668477433,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6002397570014008,
    "arrivals": 393528,
    "finished_requests": 75786,
    "scheduler_time": 46.544715797014
}
#Debug simulation 
Total elapsed time: 5.522491359966807. Arrivals time: 0.24173677142243832 Scheduler time: 5.15986621635966 Scheduler overhead time: 0.03095252683851868 Adapter cache time: 0.043648954248055816 Engine time: 0.03177563927602023 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.1164438320556656,
    "estimated_duration": 3600.038445194186,
    "input_throughput": 4546.539224282403,
    "output_throughput": 4016.9490465594085,
    "total_throughput": 8563.48827084181,
    "itl": 82.98365709696667,
    "ttft": 2078456.11511244,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941029,
    "arrivals": 393528,
    "finished_requests": 66018,
    "scheduler_time": 11.790331369531655
}
#Debug simulation 
Total elapsed time: 5.116533634020016. Arrivals time: 0.23132485058158636 Scheduler time: 4.656149870948866 Scheduler overhead time: 0.06159627716988325 Adapter cache time: 0.075688372598961 Engine time: 0.0628764129942283 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.566425703000277,
    "estimated_duration": 3600.1627938745837,
    "input_throughput": 5217.7898821578665,
    "output_throughput": 4594.99665630295,
    "total_throughput": 9812.786538460818,
    "itl": 183.8875638130372,
    "ttft": 1979927.25346936,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 393528,
    "finished_requests": 75793,
    "scheduler_time": 46.57034151850185
}
#Debug simulation 
Total elapsed time: 5.566554595017806. Arrivals time: 0.26433610706590116 Scheduler time: 5.179634753381833 Scheduler overhead time: 0.03089862468186766 Adapter cache time: 0.04371750948484987 Engine time: 0.03351005236618221 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.1     1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 1080, 1080, 1080, 1080, 17280, 1080, 66, 17280, 66, 1080, 1080, 66, 17280, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 66, 66, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 66, 1080, 1080, 17280, 1080, 66, 66, 1080, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 17280, 17280, 66, 1080, 17280, 66, 1080, 66, 1080, 66, 66, 1080, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 1080, 66, 66, 66, 66, 66, 17280, 66, 1080, 66, 17280, 17280, 66, 17280, 66, 1080, 1080, 17280, 66, 66, 66, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 66, 17280, 1080, 1080, 1080, 17280, 1080, 66, 1080, 17280, 17280, 1080, 17280, 1080, 66, 17280, 1080, 66, 1080, 1080, 66, 1080, 17280, 66, 1080, 17280, 66, 1080, 17280, 17280, 1080, 66, 17280, 1080, 66, 66, 1080, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 1080, 66, 17280, 17280, 1080, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 66, 17280, 1080, 17280, 66, 17280, 1080, 66, 1080, 66, 66, 1080, 1080, 1080, 17280, 1080, 66, 66]
Prompts retrieved: 1179264 . Total input tokens: 262924408 . Total output tokens: 235826971
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.072818986023776,
    "estimated_duration": 3600.0705032016235,
    "input_throughput": 4550.871708048446,
    "output_throughput": 4020.962641461158,
    "total_throughput": 8571.834349509603,
    "itl": 83.20361023208723,
    "ttft": 2077557.2652449482,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246536,
    "arrivals": 393528,
    "finished_requests": 66078,
    "scheduler_time": 11.973156444012538
}
#Debug simulation 
Total elapsed time: 5.072911103954539. Arrivals time: 0.22866461868397892 Scheduler time: 4.61720649455674 Scheduler overhead time: 0.06094285740982741 Adapter cache time: 0.07497744681313634 Engine time: 0.062369330786168575 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.545122915063985,
    "estimated_duration": 3600.1736345801974,
    "input_throughput": 5274.926691755085,
    "output_throughput": 4656.135148313383,
    "total_throughput": 9931.061840068469,
    "itl": 184.44467251684372,
    "ttft": 1970957.7804600308,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5692507203714915,
    "arrivals": 392778,
    "finished_requests": 76966,
    "scheduler_time": 47.48138598050571
}
#Debug simulation 
Total elapsed time: 5.5452187050832435. Arrivals time: 0.24336547497659922 Scheduler time: 5.18543804937508 Scheduler overhead time: 0.030781365698203444 Adapter cache time: 0.03951398527715355 Engine time: 0.03175277228001505 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.593149921973236,
    "estimated_duration": 3600.1770174371427,
    "input_throughput": 5270.929431550186,
    "output_throughput": 4653.275635853506,
    "total_throughput": 9924.205067403691,
    "itl": 182.16496327196046,
    "ttft": 1971524.6298479023,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6064065790199688,
    "arrivals": 392778,
    "finished_requests": 76910,
    "scheduler_time": 47.17123658169161
}
#Debug simulation 
Total elapsed time: 5.593300512991846. Arrivals time: 0.2513148030266166 Scheduler time: 5.224389981362037 Scheduler overhead time: 0.031262579606845975 Adapter cache time: 0.03961000649724156 Engine time: 0.032057300792075694 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.1060144091024995,
    "estimated_duration": 3600.077421106381,
    "input_throughput": 4568.085092721689,
    "output_throughput": 4053.385050679108,
    "total_throughput": 8621.470143400797,
    "itl": 83.35990567948376,
    "ttft": 2070098.7384688205,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6041951813548827,
    "arrivals": 392778,
    "finished_requests": 66589,
    "scheduler_time": 12.582346910695405
}
#Debug simulation 
Total elapsed time: 5.106109993997961. Arrivals time: 0.22372651076875627 Scheduler time: 4.6611638544127345 Scheduler overhead time: 0.06087954551912844 Adapter cache time: 0.06944446510169655 Engine time: 0.062126171425916255 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.514928995980881,
    "estimated_duration": 3600.0731751425283,
    "input_throughput": 5270.898694787988,
    "output_throughput": 4653.317914666212,
    "total_throughput": 9924.2166094542,
    "itl": 182.16152687308124,
    "ttft": 1971485.515185483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5814822645951071,
    "arrivals": 392778,
    "finished_requests": 76909,
    "scheduler_time": 47.17061283275299
}
#Debug simulation 
Total elapsed time: 5.515025045024231. Arrivals time: 0.24938816775102168 Scheduler time: 5.1484372350387275 Scheduler overhead time: 0.031101338448934257 Adapter cache time: 0.03981143923010677 Engine time: 0.031912752077914774 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.132018269971013,
    "estimated_duration": 3600.0613871527257,
    "input_throughput": 4567.943218603333,
    "output_throughput": 4053.3472712644466,
    "total_throughput": 8621.290489867779,
    "itl": 83.3590092629986,
    "ttft": 2070212.9394430323,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6117404085397737,
    "arrivals": 392778,
    "finished_requests": 66587,
    "scheduler_time": 12.580558162950107
}
#Debug simulation 
Total elapsed time: 5.132107032928616. Arrivals time: 0.22725471784360707 Scheduler time: 4.682978920987807 Scheduler overhead time: 0.06107553164474666 Adapter cache time: 0.0692850217455998 Engine time: 0.06274457869585603 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.533120055100881,
    "estimated_duration": 3600.0728815753023,
    "input_throughput": 5270.8991246023725,
    "output_throughput": 4653.318294120095,
    "total_throughput": 9924.217418722466,
    "itl": 182.16567292675006,
    "ttft": 1971484.9591544673,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 186,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5561493548518054,
    "arrivals": 392778,
    "finished_requests": 76909,
    "scheduler_time": 47.170958391805144
}
#Debug simulation 
Total elapsed time: 5.533214831026271. Arrivals time: 0.24508870334830135 Scheduler time: 5.170657935435884 Scheduler overhead time: 0.030948826926760375 Adapter cache time: 0.040087983245030046 Engine time: 0.03194231353700161 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.1,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.1-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.1-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.1      1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 1080, 1080, 1080, 1080, 17280, 1080, 33, 17280, 33, 1080, 1080, 33, 17280, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 33, 33, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 33, 1080, 1080, 17280, 1080, 33, 33, 1080, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 17280, 17280, 33, 1080, 17280, 33, 1080, 33, 1080, 33, 33, 1080, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 1080, 33, 33, 33, 33, 33, 17280, 33, 1080, 33, 17280, 17280, 33, 17280, 33, 1080, 1080, 17280, 33, 33, 33, 17280, 17280, 1080, 17280, 1080, 17280, 17280, 1080, 1080, 33, 17280, 1080, 1080, 1080, 17280, 1080, 33, 1080, 17280, 17280, 1080, 17280, 1080, 33, 17280, 1080, 33, 1080, 1080, 33, 1080, 17280, 33, 1080, 17280, 33, 1080, 17280, 17280, 1080, 33, 17280, 1080, 33, 33, 1080, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 1080, 33, 17280, 17280, 1080, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 1080, 1080, 1080, 1080, 1080, 1080, 33, 17280, 1080, 17280, 33, 17280, 1080, 33, 1080, 33, 33, 1080, 1080, 1080, 17280, 1080, 33, 33]
Prompts retrieved: 1177152 . Total input tokens: 262473937 . Total output tokens: 235408446
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.110528626013547,
    "estimated_duration": 3600.009123887157,
    "input_throughput": 4567.971756205878,
    "output_throughput": 4053.316671665577,
    "total_throughput": 8621.288427871455,
    "itl": 83.36175686832595,
    "ttft": 2070170.1346764138,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6195371432974947,
    "arrivals": 392778,
    "finished_requests": 66585,
    "scheduler_time": 12.582520498908183
}
#Debug simulation 
Total elapsed time: 5.11066651402507. Arrivals time: 0.22544183011632413 Scheduler time: 4.663235268671997 Scheduler overhead time: 0.06098000600468367 Adapter cache time: 0.0693424578057602 Engine time: 0.06277348671574146 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-8/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.6277001349953935,
    "estimated_duration": 3600.133822923597,
    "input_throughput": 5349.851685335183,
    "output_throughput": 4713.253960715367,
    "total_throughput": 10063.10564605055,
    "itl": 182.26595429978514,
    "ttft": 1961309.9735208384,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 386275,
    "finished_requests": 77487,
    "scheduler_time": 48.08245443485846
}
#Debug simulation 
Total elapsed time: 5.627793889027089. Arrivals time: 0.24571152683347464 Scheduler time: 5.257551343529485 Scheduler overhead time: 0.031211545458063483 Adapter cache time: 0.046616108855232596 Engine time: 0.032237469102256 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-16/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.961649521952495,
    "estimated_duration": 3600.1488887526857,
    "input_throughput": 5347.149130454317,
    "output_throughput": 4711.636525087409,
    "total_throughput": 10058.785655541726,
    "itl": 179.8382838516709,
    "ttft": 1961706.9210271826,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815838,
    "arrivals": 386275,
    "finished_requests": 77448,
    "scheduler_time": 47.75671020753591
}
#Debug simulation 
Total elapsed time: 5.961756429984234. Arrivals time: 0.5086024617776275 Scheduler time: 5.327222038642503 Scheduler overhead time: 0.03159126581158489 Adapter cache time: 0.047320520505309105 Engine time: 0.03231908741872758 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-8-32/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.229434704990126,
    "estimated_duration": 3600.066605377031,
    "input_throughput": 4655.955802307874,
    "output_throughput": 4117.538263836527,
    "total_throughput": 8773.494066144402,
    "itl": 81.78351339852404,
    "ttft": 2058350.3390550518,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 386275,
    "finished_requests": 67406,
    "scheduler_time": 12.4925066326339
}
#Debug simulation 
Total elapsed time: 5.229526113951579. Arrivals time: 0.26085866219364107 Scheduler time: 4.745069952681661 Scheduler overhead time: 0.06247239874210209 Adapter cache time: 0.06808423914480954 Engine time: 0.06376314105000347 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-16/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.687936633010395,
    "estimated_duration": 3600.1183634547383,
    "input_throughput": 5347.194468774866,
    "output_throughput": 4711.676474915228,
    "total_throughput": 10058.870943690094,
    "itl": 179.83879176104134,
    "ttft": 1961689.235172032,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6002397570014011,
    "arrivals": 386275,
    "finished_requests": 77448,
    "scheduler_time": 47.756862130846834
}
#Debug simulation 
Total elapsed time: 5.688038632040843. Arrivals time: 0.2499734980519861 Scheduler time: 5.311144347651862 Scheduler overhead time: 0.03150181856472045 Adapter cache time: 0.048370984266512096 Engine time: 0.03236975008621812 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_8-16-32/adapters_192_slots_192_rate_1.6-0.05-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.203098024008796,
    "estimated_duration": 3600.056632296744,
    "input_throughput": 4661.662777591739,
    "output_throughput": 4122.505981393871,
    "total_throughput": 8784.16875898561,
    "itl": 81.99507224262486,
    "ttft": 2058308.2807746292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941028,
    "arrivals": 386275,
    "finished_requests": 67483,
    "scheduler_time": 12.679962232652745
}
#Debug simulation 
Total elapsed time: 5.203188919927925. Arrivals time: 0.2631716860923916 Scheduler time: 4.719075127737597 Scheduler overhead time: 0.06220779335126281 Adapter cache time: 0.06608563615009189 Engine time: 0.06330351834185421 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_192_slots_192_rate_1.6-0.05-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-16/adapters_192_slots_192_rate_1.6-0.05-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.6780909459339455,
    "estimated_duration": 3600.087945136414,
    "input_throughput": 5347.239648966565,
    "output_throughput": 4711.716285407927,
    "total_throughput": 10058.955934374491,
    "itl": 179.83918172442216,
    "ttft": 1961667.052930045,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 386275,
    "finished_requests": 77448,
    "scheduler_time": 47.7571154319651
}
#Debug simulation 
Total elapsed time: 5.678183440002613. Arrivals time: 0.28111525310669094 Scheduler time: 5.270293825422414 Scheduler overhead time: 0.031537643168121576 Adapter cache time: 0.04771688464097679 Engine time: 0.032656420953571796 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_192_slots_192_rate_1.6-0.05-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.025_size_16-16-32/adapters_192_slots_192_rate_1.6-0.05-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.05  1.6  ]. Counts: [64 64 64]
Adapter prompts. [17280, 270, 17280, 270, 17280, 270, 17280, 270, 270, 270, 540, 540, 540, 540, 17280, 540, 270, 17280, 270, 540, 540, 270, 17280, 17280, 270, 540, 270, 540, 270, 270, 540, 270, 270, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 270, 540, 540, 17280, 540, 270, 270, 540, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 17280, 17280, 270, 540, 17280, 270, 540, 270, 540, 270, 270, 540, 17280, 17280, 270, 270, 270, 17280, 17280, 17280, 540, 270, 270, 270, 270, 270, 17280, 270, 540, 270, 17280, 17280, 270, 17280, 270, 540, 540, 17280, 270, 270, 270, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 270, 17280, 540, 540, 540, 17280, 540, 270, 540, 17280, 17280, 540, 17280, 540, 270, 17280, 540, 270, 540, 540, 270, 540, 17280, 270, 540, 17280, 270, 540, 17280, 17280, 540, 270, 17280, 540, 270, 270, 540, 17280, 17280, 17280, 270, 17280, 270, 17280, 270, 540, 270, 17280, 17280, 540, 17280, 270, 17280, 17280, 17280, 270, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 270, 17280, 540, 17280, 270, 17280, 540, 270, 540, 270, 270, 540, 540, 540, 17280, 540, 270, 270]
Prompts retrieved: 1157760 . Total input tokens: 258113091 . Total output tokens: 231546715
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.236427322030067,
    "estimated_duration": 3600.0173321547377,
    "input_throughput": 4659.263401368267,
    "output_throughput": 4120.607383609782,
    "total_throughput": 8779.87078497805,
    "itl": 81.94566109228327,
    "ttft": 2058075.8225250666,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246536,
    "arrivals": 386275,
    "finished_requests": 67452,
    "scheduler_time": 12.633828072140316
}
#Debug simulation 
Total elapsed time: 5.2365246140398085. Arrivals time: 0.257770725293085 Scheduler time: 4.757788103772327 Scheduler overhead time: 0.06226802745368332 Adapter cache time: 0.06598920386750251 Engine time: 0.06367636902723461 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.740702896961011,
    "estimated_duration": 3600.157785837015,
    "input_throughput": 5528.358528700619,
    "output_throughput": 4845.268468127554,
    "total_throughput": 10373.626996828172,
    "itl": 176.7585963974343,
    "ttft": 1934514.790638875,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 383398,
    "finished_requests": 79921,
    "scheduler_time": 49.399780670311195
}
#Debug simulation 
Total elapsed time: 5.740873915958218. Arrivals time: 0.2508868690347299 Scheduler time: 5.369655613205396 Scheduler overhead time: 0.03197633777745068 Adapter cache time: 0.04057674191426486 Engine time: 0.03278819436673075 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.781615416985005,
    "estimated_duration": 3600.027499875429,
    "input_throughput": 5523.0958654310325,
    "output_throughput": 4842.717173855828,
    "total_throughput": 10365.81303928686,
    "itl": 174.47513737390292,
    "ttft": 1934683.9747016188,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815838,
    "arrivals": 383398,
    "finished_requests": 79841,
    "scheduler_time": 49.04526693688185
}
#Debug simulation 
Total elapsed time: 5.781707145972177. Arrivals time: 0.2532294421689585 Scheduler time: 5.405847012647428 Scheduler overhead time: 0.03260566806420684 Adapter cache time: 0.04141860955860466 Engine time: 0.03345727105624974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.243163462961093,
    "estimated_duration": 3600.047494186458,
    "input_throughput": 4740.141908560099,
    "output_throughput": 4182.202602691606,
    "total_throughput": 8922.344511251706,
    "itl": 80.84031984019067,
    "ttft": 2045332.207043251,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 383398,
    "finished_requests": 68757,
    "scheduler_time": 12.90187732057581
}
#Debug simulation 
Total elapsed time: 5.243255513953045. Arrivals time: 0.22806672262959182 Scheduler time: 4.795628487714566 Scheduler overhead time: 0.06308968993835151 Adapter cache time: 0.06259020464494824 Engine time: 0.0643868821207434 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.83626671100501,
    "estimated_duration": 3600.186318190162,
    "input_throughput": 5523.214145760135,
    "output_throughput": 4843.035181791677,
    "total_throughput": 10366.249327551812,
    "itl": 174.4744603202054,
    "ttft": 1934656.7290656886,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6002397570014011,
    "arrivals": 383398,
    "finished_requests": 79850,
    "scheduler_time": 49.04796158339711
}
#Debug simulation 
Total elapsed time: 5.8363589809741825. Arrivals time: 0.2637633391423151 Scheduler time: 5.448660437017679 Scheduler overhead time: 0.03323960991110653 Adapter cache time: 0.041264775674790144 Engine time: 0.03427027666475624 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.225596752949059,
    "estimated_duration": 3600.082789126344,
    "input_throughput": 4742.953148625705,
    "output_throughput": 4185.9709575337565,
    "total_throughput": 8928.924106159462,
    "itl": 81.01342650859864,
    "ttft": 2044727.5605297629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941028,
    "arrivals": 383398,
    "finished_requests": 68805,
    "scheduler_time": 13.056232018899452
}
#Debug simulation 
Total elapsed time: 5.225719823967665. Arrivals time: 0.2268386509967968 Scheduler time: 4.777713001705706 Scheduler overhead time: 0.06275443860795349 Adapter cache time: 0.06179180240724236 Engine time: 0.06713856291025877 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.757825829903595,
    "estimated_duration": 3600.1733969829734,
    "input_throughput": 5523.233968859317,
    "output_throughput": 4843.052563693632,
    "total_throughput": 10366.28653255295,
    "itl": 174.47647674505717,
    "ttft": 1934648.6329332448,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 383398,
    "finished_requests": 79850,
    "scheduler_time": 49.04848760770881
}
#Debug simulation 
Total elapsed time: 5.757921376964077. Arrivals time: 0.2496241257758811 Scheduler time: 5.385449202847667 Scheduler overhead time: 0.03245571837760508 Adapter cache time: 0.04187790316063911 Engine time: 0.03338132181670517 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.0125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.05-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.05   1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 540, 540, 540, 540, 17280, 540, 135, 17280, 135, 540, 540, 135, 17280, 17280, 135, 540, 135, 540, 135, 135, 540, 135, 135, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 135, 540, 540, 17280, 540, 135, 135, 540, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 17280, 17280, 135, 540, 17280, 135, 540, 135, 540, 135, 135, 540, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 540, 135, 135, 135, 135, 135, 17280, 135, 540, 135, 17280, 17280, 135, 17280, 135, 540, 540, 17280, 135, 135, 135, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 135, 17280, 540, 540, 540, 17280, 540, 135, 540, 17280, 17280, 540, 17280, 540, 135, 17280, 540, 135, 540, 540, 135, 540, 17280, 135, 540, 17280, 135, 540, 17280, 17280, 540, 135, 17280, 540, 135, 135, 540, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 540, 135, 17280, 17280, 540, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 135, 17280, 540, 17280, 135, 17280, 540, 135, 540, 135, 135, 540, 540, 540, 17280, 540, 135, 135]
Prompts retrieved: 1149120 . Total input tokens: 256193890 . Total output tokens: 229819428
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.227444797055796,
    "estimated_duration": 3600.0556530944077,
    "input_throughput": 4742.679182007705,
    "output_throughput": 4185.460573935427,
    "total_throughput": 8928.139755943133,
    "itl": 81.01692271945109,
    "ttft": 2044772.340951046,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246535,
    "arrivals": 383398,
    "finished_requests": 68800,
    "scheduler_time": 13.0564731863886
}
#Debug simulation 
Total elapsed time: 5.227534917998128. Arrivals time: 0.23063786618877202 Scheduler time: 4.776299507007934 Scheduler overhead time: 0.0629744838224724 Adapter cache time: 0.06315414106938988 Engine time: 0.06506981945130974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.854763265931979,
    "estimated_duration": 3600.02963218826,
    "input_throughput": 5570.137484621506,
    "output_throughput": 4916.516198018455,
    "total_throughput": 10486.65368263996,
    "itl": 174.78035784785646,
    "ttft": 1933240.8290119043,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5845531590911552,
    "arrivals": 381994,
    "finished_requests": 80824,
    "scheduler_time": 50.17612375784773
}
#Debug simulation 
Total elapsed time: 5.854873661999591. Arrivals time: 0.2625778435030952 Scheduler time: 5.473898606142029 Scheduler overhead time: 0.03238220477942377 Adapter cache time: 0.037505225045606494 Engine time: 0.03346707089804113 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.8411089919973165,
    "estimated_duration": 3600.0699332700938,
    "input_throughput": 5565.537439935277,
    "output_throughput": 4913.042337467565,
    "total_throughput": 10478.579777402842,
    "itl": 172.50736640667267,
    "ttft": 1934074.634268132,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6233998070866816,
    "arrivals": 381994,
    "finished_requests": 80767,
    "scheduler_time": 49.81221211241206
}
#Debug simulation 
Total elapsed time: 5.841271066921763. Arrivals time: 0.276070294319652 Scheduler time: 5.445914440788329 Scheduler overhead time: 0.033013900625519454 Adapter cache time: 0.037166057620197535 Engine time: 0.033823256264440715 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.251219216035679,
    "estimated_duration": 3600.075251368216,
    "input_throughput": 4734.732418029836,
    "output_throughput": 4199.9363747337175,
    "total_throughput": 8934.668792763552,
    "itl": 80.04300297082665,
    "ttft": 2045735.0413000383,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6210990130715097,
    "arrivals": 381994,
    "finished_requests": 68720,
    "scheduler_time": 12.7165838661634
}
#Debug simulation 
Total elapsed time: 5.251309665036388. Arrivals time: 0.2349316383479163 Scheduler time: 4.803952610469423 Scheduler overhead time: 0.06291582388803363 Adapter cache time: 0.055229225545190275 Engine time: 0.06463605677708983 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 5.888224220019765,
    "estimated_duration": 3600.1165086778597,
    "input_throughput": 5565.319053343119,
    "output_throughput": 4913.014608656566,
    "total_throughput": 10478.333661999686,
    "itl": 172.50446143013946,
    "ttft": 1934104.7641638152,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5972497067064987,
    "arrivals": 381994,
    "finished_requests": 80766,
    "scheduler_time": 49.8123606022295
}
#Debug simulation 
Total elapsed time: 5.888321105972864. Arrivals time: 0.2572446863632649 Scheduler time: 5.511623257771134 Scheduler overhead time: 0.03282085817772895 Adapter cache time: 0.03747104236390442 Engine time: 0.03381228691432625 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.240522591979243,
    "estimated_duration": 3600.0191802884915,
    "input_throughput": 4739.761413891306,
    "output_throughput": 4205.330094598773,
    "total_throughput": 8945.091508490079,
    "itl": 80.32459398060386,
    "ttft": 2044879.7589113098,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6291472554020603,
    "arrivals": 381994,
    "finished_requests": 68806,
    "scheduler_time": 12.978072257460433
}
#Debug simulation 
Total elapsed time: 5.240645789075643. Arrivals time: 0.22876639035530388 Scheduler time: 4.801401607459411 Scheduler overhead time: 0.06282665149774402 Adapter cache time: 0.05382382276002318 Engine time: 0.06430955696851015 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 5.77723107195925,
    "estimated_duration": 3600.000642252946,
    "input_throughput": 5565.644562624551,
    "output_throughput": 4913.136901256486,
    "total_throughput": 10478.781463881038,
    "itl": 172.5206803697924,
    "ttft": 1934028.1498980653,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5710996063263163,
    "arrivals": 381994,
    "finished_requests": 80767,
    "scheduler_time": 49.814165266882945
}
#Debug simulation 
Total elapsed time: 5.777325848932378. Arrivals time: 0.2526146803284064 Scheduler time: 5.4061520010000095 Scheduler overhead time: 0.03245908429380506 Adapter cache time: 0.037157719605602324 Engine time: 0.03381453990004957 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.05-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.05    1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 540, 540, 540, 540, 17280, 540, 66, 17280, 66, 540, 540, 66, 17280, 17280, 66, 540, 66, 540, 66, 66, 540, 66, 66, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 66, 540, 540, 17280, 540, 66, 66, 540, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 17280, 17280, 66, 540, 17280, 66, 540, 66, 540, 66, 66, 540, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 540, 66, 66, 66, 66, 66, 17280, 66, 540, 66, 17280, 17280, 66, 17280, 66, 540, 540, 17280, 66, 66, 66, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 66, 17280, 540, 540, 540, 17280, 540, 66, 540, 17280, 17280, 540, 17280, 540, 66, 17280, 540, 66, 540, 540, 66, 540, 17280, 66, 540, 17280, 66, 540, 17280, 17280, 540, 66, 17280, 540, 66, 66, 540, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 540, 66, 17280, 17280, 540, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 66, 17280, 540, 17280, 66, 17280, 540, 66, 540, 66, 66, 540, 540, 540, 17280, 540, 66, 66]
Prompts retrieved: 1144704 . Total input tokens: 255207159 . Total output tokens: 228944725
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.519562751054764,
    "estimated_duration": 3600.0006057079827,
    "input_throughput": 4735.738647645916,
    "output_throughput": 4201.141515370847,
    "total_throughput": 8936.880163016762,
    "itl": 80.10814873007469,
    "ttft": 2045751.0178599062,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6370697439461962,
    "arrivals": 381994,
    "finished_requests": 68736,
    "scheduler_time": 12.774730442307533
}
#Debug simulation 
Total elapsed time: 5.519626568071544. Arrivals time: 0.23649260692764074 Scheduler time: 5.0687772274250165 Scheduler overhead time: 0.06331277324352413 Adapter cache time: 0.0566820886451751 Engine time: 0.06468016502913088 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.1582544289994985,
    "estimated_duration": 3600.0706734275655,
    "input_throughput": 5630.191415298565,
    "output_throughput": 4981.541371499088,
    "total_throughput": 10611.732786797653,
    "itl": 172.79672151367768,
    "ttft": 1923124.617395394,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.563129744883626,
    "arrivals": 381208,
    "finished_requests": 81589,
    "scheduler_time": 50.90346784129087
}
#Debug simulation 
Total elapsed time: 6.158318549045362. Arrivals time: 0.2610829353798181 Scheduler time: 5.780524641158991 Scheduler overhead time: 0.03285955113824457 Adapter cache time: 0.03518809925299138 Engine time: 0.03354825510177761 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.1805732840439305,
    "estimated_duration": 3600.1809237319812,
    "input_throughput": 5623.9712472592755,
    "output_throughput": 4976.1440826226935,
    "total_throughput": 10600.115329881968,
    "itl": 170.5484220695957,
    "ttft": 1923733.9880969247,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6004264784301644,
    "arrivals": 381208,
    "finished_requests": 81490,
    "scheduler_time": 50.535360622703855
}
#Debug simulation 
Total elapsed time: 6.180639945087023. Arrivals time: 0.5201893755001947 Scheduler time: 5.541805072221905 Scheduler overhead time: 0.03318709332961589 Adapter cache time: 0.03550586197525263 Engine time: 0.03448174090590328 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.548174072988331,
    "estimated_duration": 3600.0123990029947,
    "input_throughput": 4754.240014489951,
    "output_throughput": 4231.482092733572,
    "total_throughput": 8985.722107223522,
    "itl": 79.72726888013896,
    "ttft": 2046205.668623292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.595064624808731,
    "arrivals": 381208,
    "finished_requests": 68989,
    "scheduler_time": 13.064060255538108
}
#Debug simulation 
Total elapsed time: 5.548277691937983. Arrivals time: 0.4960642714286223 Scheduler time: 4.844796955003403 Scheduler overhead time: 0.0630130689824 Adapter cache time: 0.05028293619398028 Engine time: 0.06448270403780043 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.212361527956091,
    "estimated_duration": 3600.011186949149,
    "input_throughput": 5624.158634117599,
    "output_throughput": 4976.291758465874,
    "total_throughput": 10600.450392583472,
    "itl": 170.54179518655636,
    "ttft": 1923681.3171831944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.575910759323743,
    "arrivals": 381208,
    "finished_requests": 81487,
    "scheduler_time": 50.53203161113637
}
#Debug simulation 
Total elapsed time: 6.212468449957669. Arrivals time: 0.5224470405373722 Scheduler time: 5.571669264463708 Scheduler overhead time: 0.033140998682938516 Adapter cache time: 0.03561045869719237 Engine time: 0.034135812195017934 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.519127896986902,
    "estimated_duration": 3600.0294920213464,
    "input_throughput": 4753.204671773988,
    "output_throughput": 4230.842839970181,
    "total_throughput": 8984.04751174417,
    "itl": 79.69213275633504,
    "ttft": 2046176.530055749,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6024840982072072,
    "arrivals": 381208,
    "finished_requests": 68980,
    "scheduler_time": 13.029237716562017
}
#Debug simulation 
Total elapsed time: 5.519195228931494. Arrivals time: 0.49294838157948107 Scheduler time: 4.815226605045609 Scheduler overhead time: 0.06396476877853274 Adapter cache time: 0.05088887573219836 Engine time: 0.06643233797512949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.189856115030125,
    "estimated_duration": 3600.1270489715944,
    "input_throughput": 5624.055408206722,
    "output_throughput": 4976.218549041921,
    "total_throughput": 10600.273957248643,
    "itl": 170.5503500624861,
    "ttft": 1923697.8284748946,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.550169254262001,
    "arrivals": 381208,
    "finished_requests": 81490,
    "scheduler_time": 50.535810193655024
}
#Debug simulation 
Total elapsed time: 6.189921892015263. Arrivals time: 0.5238747471012175 Scheduler time: 5.547089229221456 Scheduler overhead time: 0.0334300606045872 Adapter cache time: 0.035754025215283036 Engine time: 0.03431091213133186 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.05,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.05-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.05-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.05     1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 540, 540, 540, 540, 17280, 540, 33, 17280, 33, 540, 540, 33, 17280, 17280, 33, 540, 33, 540, 33, 33, 540, 33, 33, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 33, 540, 540, 17280, 540, 33, 33, 540, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 17280, 17280, 33, 540, 17280, 33, 540, 33, 540, 33, 33, 540, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 540, 33, 33, 33, 33, 33, 17280, 33, 540, 33, 17280, 17280, 33, 17280, 33, 540, 540, 17280, 33, 33, 33, 17280, 17280, 540, 17280, 540, 17280, 17280, 540, 540, 33, 17280, 540, 540, 540, 17280, 540, 33, 540, 17280, 17280, 540, 17280, 540, 33, 17280, 540, 33, 540, 540, 33, 540, 17280, 33, 540, 17280, 33, 540, 17280, 17280, 540, 33, 17280, 540, 33, 33, 540, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 540, 33, 17280, 17280, 540, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 540, 540, 540, 540, 540, 540, 33, 17280, 540, 17280, 33, 17280, 540, 33, 540, 33, 33, 540, 540, 540, 17280, 540, 33, 33]
Prompts retrieved: 1142592 . Total input tokens: 254746012 . Total output tokens: 228524015
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.529425318934955,
    "estimated_duration": 3600.0846989646107,
    "input_throughput": 4753.528161412912,
    "output_throughput": 4230.959622805733,
    "total_throughput": 8984.487784218645,
    "itl": 79.71116434124396,
    "ttft": 2046210.9415795733,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 182,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.610406586751343,
    "arrivals": 381208,
    "finished_requests": 68983,
    "scheduler_time": 13.047068860101632
}
#Debug simulation 
Total elapsed time: 5.529516938957386. Arrivals time: 0.23263677139766514 Scheduler time: 5.0883970520226285 Scheduler overhead time: 0.06300885870587081 Adapter cache time: 0.05102705233730376 Engine time: 0.06483436143025756 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.050676685990766,
    "estimated_duration": 3600.1055053897176,
    "input_throughput": 5819.196123179217,
    "output_throughput": 5104.642620191941,
    "total_throughput": 10923.838743371158,
    "itl": 167.89223747317223,
    "ttft": 1893021.0324552227,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 377644,
    "finished_requests": 84287,
    "scheduler_time": 52.021137177745274
}
#Debug simulation 
Total elapsed time: 6.050772511982359. Arrivals time: 0.26202750066295266 Scheduler time: 5.669858760084026 Scheduler overhead time: 0.033853720407933 Adapter cache time: 0.0344076274195686 Engine time: 0.03489812382031232 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.034306324901991,
    "estimated_duration": 3600.0146906023388,
    "input_throughput": 5813.624331765776,
    "output_throughput": 5101.2025167395495,
    "total_throughput": 10914.826848505325,
    "itl": 166.23896708959,
    "ttft": 1893903.2749556806,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815839,
    "arrivals": 377644,
    "finished_requests": 84210,
    "scheduler_time": 51.72016370910613
}
#Debug simulation 
Total elapsed time: 6.03441225993447. Arrivals time: 0.2585792583413422 Scheduler time: 5.657971540465951 Scheduler overhead time: 0.034108821535483 Adapter cache time: 0.03327417466789484 Engine time: 0.0347177468938753 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.568105156999081,
    "estimated_duration": 3600.0246169360134,
    "input_throughput": 4874.033615618432,
    "output_throughput": 4294.144525366385,
    "total_throughput": 9168.178140984817,
    "itl": 78.58925205112973,
    "ttft": 2020749.429022105,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 377644,
    "finished_requests": 70595,
    "scheduler_time": 13.26146301267443
}
#Debug simulation 
Total elapsed time: 5.568170138983987. Arrivals time: 0.49191758630331606 Scheduler time: 4.872648014687002 Scheduler overhead time: 0.06396447506267577 Adapter cache time: 0.04431304044555873 Engine time: 0.0653029402019456 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.027451694011688,
    "estimated_duration": 3600.1763175730866,
    "input_throughput": 5813.646375550077,
    "output_throughput": 5101.147105034023,
    "total_throughput": 10914.7934805841,
    "itl": 166.23869957997513,
    "ttft": 1893911.9441875524,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 377644,
    "finished_requests": 84213,
    "scheduler_time": 51.7228460338439
}
#Debug simulation 
Total elapsed time: 6.0275857269298285. Arrivals time: 0.2590226110769436 Scheduler time: 5.649426469462924 Scheduler overhead time: 0.03403244842775166 Adapter cache time: 0.034299534279853106 Engine time: 0.03481621795799583 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.369385145022534,
    "estimated_duration": 3600.039987256258,
    "input_throughput": 4875.347791171813,
    "output_throughput": 4295.39978854109,
    "total_throughput": 9170.747579712903,
    "itl": 78.63318787447234,
    "ttft": 2020678.6096295763,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.635571540594103,
    "arrivals": 377644,
    "finished_requests": 70617,
    "scheduler_time": 13.305562652191025
}
#Debug simulation 
Total elapsed time: 5.369476163992658. Arrivals time: 0.23546197498217225 Scheduler time: 4.928602751111612 Scheduler overhead time: 0.06425140192732215 Adapter cache time: 0.04489344730973244 Engine time: 0.06595103652216494 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.29047136195004,
    "estimated_duration": 3600.1436668715633,
    "input_throughput": 5813.699101121648,
    "output_throughput": 5101.193368752076,
    "total_throughput": 10914.892469873723,
    "itl": 166.21523341903307,
    "ttft": 1893897.6894071626,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 377644,
    "finished_requests": 84213,
    "scheduler_time": 51.71987571217085
}
#Debug simulation 
Total elapsed time: 6.290581068024039. Arrivals time: 0.5180820667883381 Scheduler time: 5.654503233497962 Scheduler overhead time: 0.03410215210169554 Adapter cache time: 0.03325673402287066 Engine time: 0.034785053809173405 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.0125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.025-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.025  1.6   ]. Counts: [64 64 64]
Adapter prompts. [17280, 135, 17280, 135, 17280, 135, 17280, 135, 135, 135, 270, 270, 270, 270, 17280, 270, 135, 17280, 135, 270, 270, 135, 17280, 17280, 135, 270, 135, 270, 135, 135, 270, 135, 135, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 135, 270, 270, 17280, 270, 135, 135, 270, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 17280, 17280, 135, 270, 17280, 135, 270, 135, 270, 135, 135, 270, 17280, 17280, 135, 135, 135, 17280, 17280, 17280, 270, 135, 135, 135, 135, 135, 17280, 135, 270, 135, 17280, 17280, 135, 17280, 135, 270, 270, 17280, 135, 135, 135, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 135, 17280, 270, 270, 270, 17280, 270, 135, 270, 17280, 17280, 270, 17280, 270, 135, 17280, 270, 135, 270, 270, 135, 270, 17280, 135, 270, 17280, 135, 270, 17280, 17280, 270, 135, 17280, 270, 135, 135, 270, 17280, 17280, 17280, 135, 17280, 135, 17280, 135, 270, 135, 17280, 17280, 270, 17280, 135, 17280, 17280, 17280, 135, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 135, 17280, 270, 17280, 135, 17280, 270, 135, 270, 135, 135, 270, 270, 270, 17280, 270, 135, 135]
Prompts retrieved: 1131840 . Total input tokens: 252328530 . Total output tokens: 226372045
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.345503053977154,
    "estimated_duration": 3600.0411497133364,
    "input_throughput": 4875.284273180479,
    "output_throughput": 4295.258680926826,
    "total_throughput": 9170.542954107306,
    "itl": 78.63493628862321,
    "ttft": 2020766.027435288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246536,
    "arrivals": 377644,
    "finished_requests": 70614,
    "scheduler_time": 13.307461350551717
}
#Debug simulation 
Total elapsed time: 5.345596400904469. Arrivals time: 0.23405270429793745 Scheduler time: 4.906746520544402 Scheduler overhead time: 0.06416386726778 Adapter cache time: 0.04473104851786047 Engine time: 0.06570162542629987 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.1075356509536505,
    "estimated_duration": 3600.0607564985644,
    "input_throughput": 5912.433550344302,
    "output_throughput": 5178.886208057648,
    "total_throughput": 11091.31975840195,
    "itl": 165.3198857007701,
    "ttft": 1882194.6632134677,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 376253,
    "finished_requests": 85774,
    "scheduler_time": 52.72754993746332
}
#Debug simulation 
Total elapsed time: 6.10762906295713. Arrivals time: 0.27021590841468424 Scheduler time: 5.721921920776367 Scheduler overhead time: 0.034119365620426834 Adapter cache time: 0.030624428181909025 Engine time: 0.034922800259664655 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.106143387034535,
    "estimated_duration": 3600.0351276937336,
    "input_throughput": 5905.3782104674565,
    "output_throughput": 5173.211188061602,
    "total_throughput": 11078.589398529059,
    "itl": 163.76443938529684,
    "ttft": 1883051.4273124742,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815838,
    "arrivals": 376253,
    "finished_requests": 85675,
    "scheduler_time": 52.42804098333101
}
#Debug simulation 
Total elapsed time: 6.106278077000752. Arrivals time: 0.2657280599232763 Scheduler time: 5.72437575715594 Scheduler overhead time: 0.03424444957636297 Adapter cache time: 0.030502064735628664 Engine time: 0.035315182991325855 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.340085383038968,
    "estimated_duration": 3600.0595966959286,
    "input_throughput": 4942.6281765809645,
    "output_throughput": 4327.72474497342,
    "total_throughput": 9270.352921554384,
    "itl": 78.26699504263003,
    "ttft": 2014958.7717306833,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 376253,
    "finished_requests": 71639,
    "scheduler_time": 13.590835899232648
}
#Debug simulation 
Total elapsed time: 5.340180680039339. Arrivals time: 0.23352420469745994 Scheduler time: 4.904167345725 Scheduler overhead time: 0.06438062060624361 Adapter cache time: 0.042091037379577756 Engine time: 0.06585263507440686 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.145183322951198,
    "estimated_duration": 3600.034251788792,
    "input_throughput": 5905.379647273218,
    "output_throughput": 5173.21244672775,
    "total_throughput": 11078.592094000967,
    "itl": 163.75971310971158,
    "ttft": 1883052.0029807289,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 376253,
    "finished_requests": 85675,
    "scheduler_time": 52.42728011403671
}
#Debug simulation 
Total elapsed time: 6.1452924609184265. Arrivals time: 0.2725316466530785 Scheduler time: 5.755679603316821 Scheduler overhead time: 0.034480809583328664 Adapter cache time: 0.031192749040201306 Engine time: 0.035293507971800864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.336663583992049,
    "estimated_duration": 3600.0406823339104,
    "input_throughput": 4942.980530004324,
    "output_throughput": 4328.092478637941,
    "total_throughput": 9271.073008642265,
    "itl": 78.26516175833034,
    "ttft": 2014996.9968914965,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.635571540594103,
    "arrivals": 376253,
    "finished_requests": 71642,
    "scheduler_time": 13.588442674695381
}
#Debug simulation 
Total elapsed time: 5.336755822063424. Arrivals time: 0.23232674854807556 Scheduler time: 4.90311446855776 Scheduler overhead time: 0.06411830452270806 Adapter cache time: 0.04161671199835837 Engine time: 0.06549234664998949 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.09255043999292,
    "estimated_duration": 3600.0326032695375,
    "input_throughput": 5905.382351452076,
    "output_throughput": 5173.214815634164,
    "total_throughput": 11078.59716708624,
    "itl": 163.76109986148953,
    "ttft": 1883048.3987260691,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 376253,
    "finished_requests": 85675,
    "scheduler_time": 52.4276140391618
}
#Debug simulation 
Total elapsed time: 6.092644284944981. Arrivals time: 0.2664710459066555 Scheduler time: 5.709344190661795 Scheduler overhead time: 0.03434391156770289 Adapter cache time: 0.031067135627381504 Engine time: 0.035323342541232705 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.025-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.025   1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 270, 270, 270, 270, 17280, 270, 66, 17280, 66, 270, 270, 66, 17280, 17280, 66, 270, 66, 270, 66, 66, 270, 66, 66, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 66, 270, 270, 17280, 270, 66, 66, 270, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 17280, 17280, 66, 270, 17280, 66, 270, 66, 270, 66, 66, 270, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 270, 66, 66, 66, 66, 66, 17280, 66, 270, 66, 17280, 17280, 66, 17280, 66, 270, 270, 17280, 66, 66, 66, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 66, 17280, 270, 270, 270, 17280, 270, 66, 270, 17280, 17280, 270, 17280, 270, 66, 17280, 270, 66, 270, 270, 66, 270, 17280, 66, 270, 17280, 66, 270, 17280, 17280, 270, 66, 17280, 270, 66, 66, 270, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 270, 66, 17280, 17280, 270, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 66, 17280, 270, 17280, 66, 17280, 270, 66, 270, 66, 66, 270, 270, 270, 17280, 270, 66, 66]
Prompts retrieved: 1127424 . Total input tokens: 251370679 . Total output tokens: 225477813
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.354756885091774,
    "estimated_duration": 3600.045756102078,
    "input_throughput": 4942.901620024698,
    "output_throughput": 4327.963602571003,
    "total_throughput": 9270.865222595701,
    "itl": 78.26533749637997,
    "ttft": 2015033.5729470605,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246536,
    "arrivals": 376253,
    "finished_requests": 71640,
    "scheduler_time": 13.588028163579688
}
#Debug simulation 
Total elapsed time: 5.354846108006313. Arrivals time: 0.24013219005428255 Scheduler time: 4.913209756370634 Scheduler overhead time: 0.06451058620586991 Adapter cache time: 0.041476353886537254 Engine time: 0.0653300384292379 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.2288073940435424,
    "estimated_duration": 3600.0495889050303,
    "input_throughput": 5933.683265317799,
    "output_throughput": 5239.274497254036,
    "total_throughput": 11172.957762571834,
    "itl": 163.95817742245694,
    "ttft": 1882989.3216496683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5661902326275587,
    "arrivals": 375490,
    "finished_requests": 86189,
    "scheduler_time": 53.467396596286775
}
#Debug simulation 
Total elapsed time: 6.228899121051654. Arrivals time: 0.26681954599916935 Scheduler time: 5.848125849268399 Scheduler overhead time: 0.0342865283600986 Adapter cache time: 0.027762179961428046 Engine time: 0.035793295595794916 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.201336654950865,
    "estimated_duration": 3600.1651010950095,
    "input_throughput": 5926.203493698317,
    "output_throughput": 5233.386378382867,
    "total_throughput": 11159.589872081184,
    "itl": 162.00407226948593,
    "ttft": 1884090.450211557,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6038251240435069,
    "arrivals": 375490,
    "finished_requests": 86072,
    "scheduler_time": 53.08160634930485
}
#Debug simulation 
Total elapsed time: 6.201428645988926. Arrivals time: 0.2745968601666391 Scheduler time: 5.811259216163307 Scheduler overhead time: 0.03488760453183204 Adapter cache time: 0.02824688924010843 Engine time: 0.03613592789042741 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.409081451012753,
    "estimated_duration": 3600.0136397315955,
    "input_throughput": 4887.87148076248,
    "output_throughput": 4337.403566383204,
    "total_throughput": 9225.275047145684,
    "itl": 77.47066543668635,
    "ttft": 2022711.0867143536,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5981081436574482,
    "arrivals": 375490,
    "finished_requests": 71009,
    "scheduler_time": 13.245447582986815
}
#Debug simulation 
Total elapsed time: 5.409226402058266. Arrivals time: 0.23492247704416513 Scheduler time: 4.974055475671776 Scheduler overhead time: 0.06518270471133292 Adapter cache time: 0.03803154418710619 Engine time: 0.06662341661285609 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.23781192896422,
    "estimated_duration": 3600.1470940115564,
    "input_throughput": 5926.503679666459,
    "output_throughput": 5233.555326486757,
    "total_throughput": 11160.059006153217,
    "itl": 162.08743469161897,
    "ttft": 1884043.7966014699,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5776750236633241,
    "arrivals": 375490,
    "finished_requests": 86076,
    "scheduler_time": 53.09877302393524
}
#Debug simulation 
Total elapsed time: 6.237905321992002. Arrivals time: 0.2959855943918228 Scheduler time: 5.827391179278493 Scheduler overhead time: 0.03463795001152903 Adapter cache time: 0.027750741108320653 Engine time: 0.0358967580832541 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.634152102051303,
    "estimated_duration": 3600.017302837385,
    "input_throughput": 4887.823451884901,
    "output_throughput": 4337.267486934994,
    "total_throughput": 9225.090938819894,
    "itl": 77.47089606182892,
    "ttft": 2022749.0855794228,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.605904878415169,
    "arrivals": 375490,
    "finished_requests": 71008,
    "scheduler_time": 13.24627256576395
}
#Debug simulation 
Total elapsed time: 5.634218646096997. Arrivals time: 0.4770954395644367 Scheduler time: 4.95792364818044 Scheduler overhead time: 0.0662601706571877 Adapter cache time: 0.03732107987161726 Engine time: 0.06545370712410659 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.2179700509877875,
    "estimated_duration": 3600.005786915706,
    "input_throughput": 5926.4657511228515,
    "output_throughput": 5233.617975970538,
    "total_throughput": 11160.08372709339,
    "itl": 162.0740327347557,
    "ttft": 1884032.2443013457,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5531593045569032,
    "arrivals": 375490,
    "finished_requests": 86072,
    "scheduler_time": 53.0940228939809
}
#Debug simulation 
Total elapsed time: 6.218064005952328. Arrivals time: 0.3034209448378533 Scheduler time: 5.80071003222838 Scheduler overhead time: 0.03445864177774638 Adapter cache time: 0.028070462751202285 Engine time: 0.03545299370307475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.025,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.025-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.025-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.025    1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 270, 270, 270, 270, 17280, 270, 33, 17280, 33, 270, 270, 33, 17280, 17280, 33, 270, 33, 270, 33, 33, 270, 33, 33, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 33, 270, 270, 17280, 270, 33, 33, 270, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 17280, 17280, 33, 270, 17280, 33, 270, 33, 270, 33, 33, 270, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 270, 33, 33, 33, 33, 33, 17280, 33, 270, 33, 17280, 17280, 33, 17280, 33, 270, 270, 17280, 33, 33, 33, 17280, 17280, 270, 17280, 270, 17280, 17280, 270, 270, 33, 17280, 270, 270, 270, 17280, 270, 33, 270, 17280, 17280, 270, 17280, 270, 33, 17280, 270, 33, 270, 270, 33, 270, 17280, 33, 270, 17280, 33, 270, 17280, 17280, 270, 33, 17280, 270, 33, 33, 270, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 270, 33, 17280, 17280, 270, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 270, 270, 270, 270, 270, 270, 33, 17280, 270, 17280, 33, 17280, 270, 33, 270, 33, 33, 270, 270, 270, 17280, 270, 33, 33]
Prompts retrieved: 1125312 . Total input tokens: 250892733 . Total output tokens: 225066789
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.38079733797349,
    "estimated_duration": 3600.0309911664644,
    "input_throughput": 4888.191808120546,
    "output_throughput": 4337.461271393251,
    "total_throughput": 9225.653079513797,
    "itl": 77.46841854279374,
    "ttft": 2022819.252361461,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 183,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6134501056000601,
    "arrivals": 375490,
    "finished_requests": 71012,
    "scheduler_time": 13.244966387642041
}
#Debug simulation 
Total elapsed time: 5.380904696066864. Arrivals time: 0.26815577666275203 Scheduler time: 4.91587955551222 Scheduler overhead time: 0.06410073093138635 Adapter cache time: 0.0374484647763893 Engine time: 0.06536706211045384 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-8/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.277043480076827,
    "estimated_duration": 3600.109337067623,
    "input_throughput": 6076.464615882606,
    "output_throughput": 5350.378612581063,
    "total_throughput": 11426.84322846367,
    "itl": 160.61003272689246,
    "ttft": 1864929.6264106955,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 373265,
    "finished_requests": 88355,
    "scheduler_time": 54.57843723913763
}
#Debug simulation 
Total elapsed time: 6.277136558084749. Arrivals time: 0.29789678496308625 Scheduler time: 5.869121658150107 Scheduler overhead time: 0.03453999711200595 Adapter cache time: 0.023860802757553756 Engine time: 0.03565211780369282 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-16/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.2440687590278685,
    "estimated_duration": 3600.1458433898647,
    "input_throughput": 6067.581689810577,
    "output_throughput": 5342.236908349952,
    "total_throughput": 11409.81859816053,
    "itl": 158.97543177601014,
    "ttft": 1866479.3391205517,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815838,
    "arrivals": 373265,
    "finished_requests": 88221,
    "scheduler_time": 54.2193875794346
}
#Debug simulation 
Total elapsed time: 6.244164637057111. Arrivals time: 0.2977081217104569 Scheduler time: 5.835538810468279 Scheduler overhead time: 0.03470035386271775 Adapter cache time: 0.024023649631999433 Engine time: 0.03608979273121804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-8-32/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.402009933954105,
    "estimated_duration": 3600.0494207577135,
    "input_throughput": 4948.9662273164295,
    "output_throughput": 4384.409255325688,
    "total_throughput": 9333.375482642117,
    "itl": 77.08373575827572,
    "ttft": 2009242.7314451728,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 373265,
    "finished_requests": 71962,
    "scheduler_time": 13.670485096260677
}
#Debug simulation 
Total elapsed time: 5.40209883695934. Arrivals time: 0.25839919631835073 Scheduler time: 4.952551158261485 Scheduler overhead time: 0.06462166307028383 Adapter cache time: 0.03156277735251933 Engine time: 0.06499072152655572 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-16/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.298387927934527,
    "estimated_duration": 3600.073173621707,
    "input_throughput": 6067.704167808498,
    "output_throughput": 5342.344744801838,
    "total_throughput": 11410.048912610335,
    "itl": 158.97872845580565,
    "ttft": 1866448.8943351288,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 373265,
    "finished_requests": 88221,
    "scheduler_time": 54.2204132050958
}
#Debug simulation 
Total elapsed time: 6.2985099919606. Arrivals time: 0.2826908746501431 Scheduler time: 5.907035933225416 Scheduler overhead time: 0.03480019129347056 Adapter cache time: 0.022647399571724236 Engine time: 0.03525222395546734 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_8-16-32/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.43186655791942,
    "estimated_duration": 3600.0168004710513,
    "input_throughput": 4948.639683478404,
    "output_throughput": 4384.25009514811,
    "total_throughput": 9332.889778626513,
    "itl": 77.08551014685976,
    "ttft": 2009296.0895480376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941028,
    "arrivals": 373265,
    "finished_requests": 71957,
    "scheduler_time": 13.671121363848297
}
#Debug simulation 
Total elapsed time: 5.431956765940413. Arrivals time: 0.25235766416881233 Scheduler time: 4.987748371670023 Scheduler overhead time: 0.06446841498836875 Adapter cache time: 0.031810119398869574 Engine time: 0.06528704462107271 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-16/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.325420615961775,
    "estimated_duration": 3600.050668524426,
    "input_throughput": 6067.742099017013,
    "output_throughput": 5342.378141550734,
    "total_throughput": 11410.120240567747,
    "itl": 158.97866660305831,
    "ttft": 1866427.9129580029,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 373265,
    "finished_requests": 88221,
    "scheduler_time": 54.220603677153086
}
#Debug simulation 
Total elapsed time: 6.325510255992413. Arrivals time: 0.28786117187701166 Scheduler time: 5.9286217199405655 Scheduler overhead time: 0.03475750132929534 Adapter cache time: 0.022847209591418505 Engine time: 0.03532946528866887 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.00625_size_16-16-32/adapters_192_slots_192_rate_1.6-0.0125-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.0125  1.6    ]. Counts: [64 64 64]
Adapter prompts. [17280, 66, 17280, 66, 17280, 66, 17280, 66, 66, 66, 135, 135, 135, 135, 17280, 135, 66, 17280, 66, 135, 135, 66, 17280, 17280, 66, 135, 66, 135, 66, 66, 135, 66, 66, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 66, 135, 135, 17280, 135, 66, 66, 135, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 17280, 17280, 66, 135, 17280, 66, 135, 66, 135, 66, 66, 135, 17280, 17280, 66, 66, 66, 17280, 17280, 17280, 135, 66, 66, 66, 66, 66, 17280, 66, 135, 66, 17280, 17280, 66, 17280, 66, 135, 135, 17280, 66, 66, 66, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 66, 17280, 135, 135, 135, 17280, 135, 66, 135, 17280, 17280, 135, 17280, 135, 66, 17280, 135, 66, 135, 135, 66, 135, 17280, 66, 135, 17280, 66, 135, 17280, 17280, 135, 66, 17280, 135, 66, 66, 135, 17280, 17280, 17280, 66, 17280, 66, 17280, 66, 135, 66, 17280, 17280, 135, 17280, 66, 17280, 17280, 17280, 66, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 66, 17280, 135, 17280, 66, 17280, 135, 66, 135, 66, 66, 135, 135, 135, 17280, 135, 66, 66]
Prompts retrieved: 1118784 . Total input tokens: 249387702 . Total output tokens: 223757994
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.415818694978952,
    "estimated_duration": 3600.038453684675,
    "input_throughput": 4949.088524808455,
    "output_throughput": 4384.516499773629,
    "total_throughput": 9333.605024582084,
    "itl": 77.08300426713073,
    "ttft": 2009341.6201767283,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246536,
    "arrivals": 373265,
    "finished_requests": 71963,
    "scheduler_time": 13.669833185496348
}
#Debug simulation 
Total elapsed time: 5.415908141992986. Arrivals time: 0.25406152475625277 Scheduler time: 4.970731553272344 Scheduler overhead time: 0.06435860984493047 Adapter cache time: 0.03142344718798995 Engine time: 0.06515003729145974 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.718850853969343,
    "estimated_duration": 3600.0136669479725,
    "input_throughput": 6147.378606693449,
    "output_throughput": 5442.612948914442,
    "total_throughput": 11589.99155560789,
    "itl": 158.38918238405523,
    "ttft": 1851652.2844016326,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5661902326275587,
    "arrivals": 372565,
    "finished_requests": 89398,
    "scheduler_time": 55.56460581245102
}
#Debug simulation 
Total elapsed time: 6.718947936082259. Arrivals time: 0.5011656649876386 Scheduler time: 6.1114254526328295 Scheduler overhead time: 0.03511225082911551 Adapter cache time: 0.019221134949475527 Engine time: 0.03573632426559925 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.599018794950098,
    "estimated_duration": 3600.0684337752755,
    "input_throughput": 6133.993674348708,
    "output_throughput": 5430.77315324735,
    "total_throughput": 11564.766827596057,
    "itl": 156.28972169173284,
    "ttft": 1853762.1625282546,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.603825124043507,
    "arrivals": 372565,
    "finished_requests": 89203,
    "scheduler_time": 55.07985009103039
}
#Debug simulation 
Total elapsed time: 6.59911844599992. Arrivals time: 0.5015410055639222 Scheduler time: 5.990964602213353 Scheduler overhead time: 0.034959126031026244 Adapter cache time: 0.01931948389392346 Engine time: 0.03611486917361617 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.675168156041764,
    "estimated_duration": 3600.055305283953,
    "input_throughput": 4976.0185555224525,
    "output_throughput": 4410.424188954966,
    "total_throughput": 9386.44274447742,
    "itl": 76.3163187109777,
    "ttft": 2006782.902862629,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6014889100007736,
    "arrivals": 372565,
    "finished_requests": 72243,
    "scheduler_time": 13.595773849067333
}
#Debug simulation 
Total elapsed time: 5.675273768021725. Arrivals time: 0.2298497330630198 Scheduler time: 5.258291229023598 Scheduler overhead time: 0.06433613784611225 Adapter cache time: 0.026987023651599884 Engine time: 0.06542696012184024 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.603908583056182,
    "estimated_duration": 3600.087739559813,
    "input_throughput": 6133.960780272563,
    "output_throughput": 5430.744030252591,
    "total_throughput": 11564.704810525154,
    "itl": 156.27667042464097,
    "ttft": 1853768.602389862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5789008096186451,
    "arrivals": 372565,
    "finished_requests": 89203,
    "scheduler_time": 55.07745770596751
}
#Debug simulation 
Total elapsed time: 6.603995781042613. Arrivals time: 0.48370409081690013 Scheduler time: 6.01354669756256 Scheduler overhead time: 0.03522539557889104 Adapter cache time: 0.019041048479266465 Engine time: 0.036191097693517804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.6782899459358305,
    "estimated_duration": 3600.0341402369236,
    "input_throughput": 4975.867256309158,
    "output_throughput": 4410.2084540106935,
    "total_throughput": 9386.075710319852,
    "itl": 76.31686803889302,
    "ttft": 2006741.8531794776,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6091598909720797,
    "arrivals": 372565,
    "finished_requests": 72241,
    "scheduler_time": 13.596524641274662
}
#Debug simulation 
Total elapsed time: 5.6783783519640565. Arrivals time: 0.4425236415117979 Scheduler time: 5.048149441019632 Scheduler overhead time: 0.06484983291011304 Adapter cache time: 0.02724872692488134 Engine time: 0.06528486358001828 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.593855218030512,
    "estimated_duration": 3600.0193389191923,
    "input_throughput": 6134.077325992854,
    "output_throughput": 5430.847214801269,
    "total_throughput": 11564.924540794122,
    "itl": 156.2901193712647,
    "ttft": 1853729.9040427513,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5531593045569032,
    "arrivals": 372565,
    "finished_requests": 89203,
    "scheduler_time": 55.080352645584085
}
#Debug simulation 
Total elapsed time: 6.593921806081198. Arrivals time: 0.4751087767072022 Scheduler time: 6.012971691670828 Scheduler overhead time: 0.03511257155332714 Adapter cache time: 0.018919048132374883 Engine time: 0.03548181487713009 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.0125,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.0125-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.0125-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.0125   1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 135, 135, 135, 135, 17280, 135, 33, 17280, 33, 135, 135, 33, 17280, 17280, 33, 135, 33, 135, 33, 33, 135, 33, 33, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 33, 135, 135, 17280, 135, 33, 33, 135, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 17280, 17280, 33, 135, 17280, 33, 135, 33, 135, 33, 33, 135, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 135, 33, 33, 33, 33, 33, 17280, 33, 135, 33, 17280, 17280, 33, 17280, 33, 135, 135, 17280, 33, 33, 33, 17280, 17280, 135, 17280, 135, 17280, 17280, 135, 135, 33, 17280, 135, 135, 135, 17280, 135, 33, 135, 17280, 17280, 135, 17280, 135, 33, 17280, 135, 33, 135, 135, 33, 135, 17280, 33, 135, 17280, 33, 135, 17280, 17280, 135, 33, 17280, 135, 33, 33, 135, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 135, 33, 17280, 17280, 135, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 135, 135, 135, 135, 135, 135, 33, 17280, 135, 17280, 33, 17280, 135, 33, 135, 33, 33, 135, 135, 135, 17280, 135, 33, 33]
Prompts retrieved: 1116672 . Total input tokens: 248929512 . Total output tokens: 223340384
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.648328416980803,
    "estimated_duration": 3600.023985583312,
    "input_throughput": 4975.8812918290905,
    "output_throughput": 4410.220893966479,
    "total_throughput": 9386.10218579557,
    "itl": 76.3181456529105,
    "ttft": 2006705.7066474017,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6169566257298005,
    "arrivals": 372565,
    "finished_requests": 72241,
    "scheduler_time": 13.59797039739232
}
#Debug simulation 
Total elapsed time: 5.648392383940518. Arrivals time: 0.4471598935779184 Scheduler time: 5.01373673020862 Scheduler overhead time: 0.06441577710211277 Adapter cache time: 0.02729619399178773 Engine time: 0.0655975624686107 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-8/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.481840229011141,
    "estimated_duration": 3600.1225498412514,
    "input_throughput": 6276.796605436788,
    "output_throughput": 5519.6109923747545,
    "total_throughput": 11796.407597811542,
    "itl": 155.43536419974555,
    "ttft": 1835594.5474233143,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5814926713472225,
    "arrivals": 371093,
    "finished_requests": 91144,
    "scheduler_time": 56.31944594815356
}
#Debug simulation 
Total elapsed time: 6.481921868980862. Arrivals time: 0.2630002845544368 Scheduler time: 6.115329569554888 Scheduler overhead time: 0.035546537139452994 Adapter cache time: 0.01564325240906328 Engine time: 0.035942625952884555 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-16/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 6.411174118984491,
    "estimated_duration": 3600.032987255226,
    "input_throughput": 6267.257016775894,
    "output_throughput": 5511.633940645691,
    "total_throughput": 11778.890957421585,
    "itl": 154.09521788104948,
    "ttft": 1836918.9835848573,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6195925661548984,
    "arrivals": 371093,
    "finished_requests": 90996,
    "scheduler_time": 55.97903077471361
}
#Debug simulation 
Total elapsed time: 6.4112599259242415. Arrivals time: 0.2616247828118503 Scheduler time: 6.045893783215433 Scheduler overhead time: 0.0355793540365994 Adapter cache time: 0.01541302923578769 Engine time: 0.036193675361573696 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-8-32/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.473791061900556,
    "estimated_duration": 3600.067846865738,
    "input_throughput": 5037.233677634162,
    "output_throughput": 4448.024504299628,
    "total_throughput": 9485.258181933788,
    "itl": 75.94680034415516,
    "ttft": 1995710.2804964706,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6109567140415335,
    "arrivals": 371093,
    "finished_requests": 73202,
    "scheduler_time": 13.909834176139766
}
#Debug simulation 
Total elapsed time: 5.47389593697153. Arrivals time: 0.22960813029203564 Scheduler time: 5.059937329497188 Scheduler overhead time: 0.06507850321941078 Adapter cache time: 0.023123550810851157 Engine time: 0.06571375322528183 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-16/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 6.488939968985505,
    "estimated_duration": 3600.161805947527,
    "input_throughput": 6267.076930466401,
    "output_throughput": 5511.645328612548,
    "total_throughput": 11778.722259078948,
    "itl": 154.1033977987408,
    "ttft": 1836920.51456935,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5938510610931561,
    "arrivals": 371093,
    "finished_requests": 90999,
    "scheduler_time": 55.98367984280681
}
#Debug simulation 
Total elapsed time: 6.489047101000324. Arrivals time: 0.2622558333678171 Scheduler time: 6.122016645618714 Scheduler overhead time: 0.03597156039904803 Adapter cache time: 0.01582285191398114 Engine time: 0.036412986693903804 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_8-16-32/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 5.490598010015674,
    "estimated_duration": 3600.01885352547,
    "input_throughput": 5037.5641733765415,
    "output_throughput": 4448.368370159343,
    "total_throughput": 9485.932543535884,
    "itl": 75.9692258835692,
    "ttft": 1995630.6717805502,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6186276950128393,
    "arrivals": 371093,
    "finished_requests": 73206,
    "scheduler_time": 13.934193501542232
}
#Debug simulation 
Total elapsed time: 5.490700882044621. Arrivals time: 0.24739326420240104 Scheduler time: 5.057874434511177 Scheduler overhead time: 0.06537635310087353 Adapter cache time: 0.02361762069631368 Engine time: 0.06580022675916553 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-16/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 6.507735660998151,
    "estimated_duration": 3600.0015920229944,
    "input_throughput": 6267.311672859918,
    "output_throughput": 5511.682007020974,
    "total_throughput": 11778.993679880892,
    "itl": 154.09086889716428,
    "ttft": 1836900.9810134862,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 190,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5681095560314141,
    "arrivals": 371093,
    "finished_requests": 90996,
    "scheduler_time": 55.97866213518181
}
#Debug simulation 
Total elapsed time: 6.5078647579066455. Arrivals time: 0.2601229519350454 Scheduler time: 6.143510912661441 Scheduler overhead time: 0.035586214857175946 Adapter cache time: 0.015565744717605412 Engine time: 0.03640884382184595 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        1.6,
        0.00625,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_1.6-0.00625-0.003125_size_16-16-32/adapters_192_slots_192_rate_1.6-0.00625-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.00625  1.6     ]. Counts: [64 64 64]
Adapter prompts. [17280, 33, 17280, 33, 17280, 33, 17280, 33, 33, 33, 66, 66, 66, 66, 17280, 66, 33, 17280, 33, 66, 66, 33, 17280, 17280, 33, 66, 33, 66, 33, 33, 66, 33, 33, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 33, 66, 66, 17280, 66, 33, 33, 66, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 17280, 17280, 33, 66, 17280, 33, 66, 33, 66, 33, 33, 66, 17280, 17280, 33, 33, 33, 17280, 17280, 17280, 66, 33, 33, 33, 33, 33, 17280, 33, 66, 33, 17280, 17280, 33, 17280, 33, 66, 66, 17280, 33, 33, 33, 17280, 17280, 66, 17280, 66, 17280, 17280, 66, 66, 33, 17280, 66, 66, 66, 17280, 66, 33, 66, 17280, 17280, 66, 17280, 66, 33, 17280, 66, 33, 66, 66, 33, 66, 17280, 33, 66, 17280, 33, 66, 17280, 17280, 66, 33, 17280, 66, 33, 33, 66, 17280, 17280, 17280, 33, 17280, 33, 17280, 33, 66, 33, 17280, 17280, 66, 17280, 33, 17280, 17280, 17280, 33, 17280, 17280, 17280, 66, 66, 66, 66, 66, 66, 33, 17280, 66, 17280, 33, 17280, 66, 33, 66, 33, 33, 66, 66, 66, 17280, 66, 33, 33]
Prompts retrieved: 1112256 . Total input tokens: 247965347 . Total output tokens: 222444720
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 5.445387651911005,
    "estimated_duration": 3600.076748946751,
    "input_throughput": 5037.024003809152,
    "output_throughput": 4447.811009774903,
    "total_throughput": 9484.835013584056,
    "itl": 75.94765446316501,
    "ttft": 1995682.2175459887,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.626550183556975,
    "arrivals": 371093,
    "finished_requests": 73199,
    "scheduler_time": 13.908620388250796
}
#Debug simulation 
Total elapsed time: 5.4454883019207045. Arrivals time: 0.22446550044696778 Scheduler time: 5.036422066972591 Scheduler overhead time: 0.06526641093660146 Adapter cache time: 0.02325747872237116 Engine time: 0.06558303791098297 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.247085495037027,
    "estimated_duration": 3600.0567722721576,
    "input_throughput": 3984.6210511136783,
    "output_throughput": 3506.2366508273917,
    "total_throughput": 7490.8577019410695,
    "itl": 243.21592569610155,
    "ttft": 2086379.5393697664,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 299534,
    "finished_requests": 58088,
    "scheduler_time": 36.09248849233347
}
#Debug simulation 
Total elapsed time: 4.247192048002034. Arrivals time: 0.19124390452634543 Scheduler time: 3.9586888107005507 Scheduler overhead time: 0.023112497525289655 Adapter cache time: 0.03999971225857735 Engine time: 0.02339163259603083 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.238907341961749,
    "estimated_duration": 3600.0127890671492,
    "input_throughput": 3980.93557987432,
    "output_throughput": 3502.8136673002214,
    "total_throughput": 7483.749247174542,
    "itl": 240.28776605843228,
    "ttft": 2087415.8536292333,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815837,
    "arrivals": 299534,
    "finished_requests": 58023,
    "scheduler_time": 35.83196345911969
}
#Debug simulation 
Total elapsed time: 4.23899145796895. Arrivals time: 0.18883299932349473 Scheduler time: 3.952605993952602 Scheduler overhead time: 0.023367279092781246 Adapter cache time: 0.03970463422592729 Engine time: 0.02368010056670755 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.108694405062124,
    "estimated_duration": 3600.042660276675,
    "input_throughput": 3531.5664839990823,
    "output_throughput": 3121.831617166522,
    "total_throughput": 6653.398101165604,
    "itl": 107.8817712537906,
    "ttft": 2179622.4329469902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635525,
    "arrivals": 299534,
    "finished_requests": 51391,
    "scheduler_time": 10.00217636407709
}
#Debug simulation 
Total elapsed time: 4.1087761770468205. Arrivals time: 0.17945017549209297 Scheduler time: 3.672640094300732 Scheduler overhead time: 0.04745227319654077 Adapter cache time: 0.13941898429766297 Engine time: 0.047783871996216476 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.2952860509976745,
    "estimated_duration": 3600.24976211213,
    "input_throughput": 3980.77298714745,
    "output_throughput": 3502.69060016599,
    "total_throughput": 7483.46358731344,
    "itl": 240.2919113934584,
    "ttft": 2087464.5766158456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 299534,
    "finished_requests": 58025,
    "scheduler_time": 35.83495247525779
}
#Debug simulation 
Total elapsed time: 4.295371744083241. Arrivals time: 0.19254408916458488 Scheduler time: 4.004830172401853 Scheduler overhead time: 0.023509793682023883 Adapter cache time: 0.03992779192049056 Engine time: 0.023674426367506385 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.1_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.428074533934705,
    "estimated_duration": 3600.082741977113,
    "input_throughput": 3530.8332921868187,
    "output_throughput": 3120.873825758149,
    "total_throughput": 6651.707117944968,
    "itl": 107.7853745944867,
    "ttft": 2179835.8682231344,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941027,
    "arrivals": 299534,
    "finished_requests": 51379,
    "scheduler_time": 9.951501111323797
}
#Debug simulation 
Total elapsed time: 4.428163131931797. Arrivals time: 0.1805214952910319 Scheduler time: 3.9922922354890034 Scheduler overhead time: 0.04762765404302627 Adapter cache time: 0.13719509926158935 Engine time: 0.04822057893034071 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.1_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.1_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.266691857948899,
    "estimated_duration": 3600.2620827271176,
    "input_throughput": 3980.7593643693854,
    "output_throughput": 3502.6786134546583,
    "total_throughput": 7483.437977824044,
    "itl": 240.26723270610682,
    "ttft": 2087469.4361752686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 299534,
    "finished_requests": 58025,
    "scheduler_time": 35.833257225679624
}
#Debug simulation 
Total elapsed time: 4.26677835197188. Arrivals time: 0.18773236533161253 Scheduler time: 3.981746964622289 Scheduler overhead time: 0.023499710019677877 Adapter cache time: 0.039603415643796325 Engine time: 0.0234994770726189 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.1_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.1
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.1_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.1_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.1 0.4 0.8]. Counts: [64 64 64]
Adapter prompts. [8640, 1080, 8640, 1080, 8640, 1080, 8640, 1080, 1080, 1080, 4320, 4320, 4320, 4320, 8640, 4320, 1080, 8640, 1080, 4320, 4320, 1080, 8640, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 1080, 1080, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 1080, 4320, 4320, 8640, 4320, 1080, 1080, 4320, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 8640, 8640, 1080, 4320, 8640, 1080, 4320, 1080, 4320, 1080, 1080, 4320, 8640, 8640, 1080, 1080, 1080, 8640, 8640, 8640, 4320, 1080, 1080, 1080, 1080, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 1080, 8640, 1080, 4320, 4320, 8640, 1080, 1080, 1080, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 1080, 8640, 4320, 4320, 4320, 8640, 4320, 1080, 4320, 8640, 8640, 4320, 8640, 4320, 1080, 8640, 4320, 1080, 4320, 4320, 1080, 4320, 8640, 1080, 4320, 8640, 1080, 4320, 8640, 8640, 4320, 1080, 8640, 4320, 1080, 1080, 4320, 8640, 8640, 8640, 1080, 8640, 1080, 8640, 1080, 4320, 1080, 8640, 8640, 4320, 8640, 1080, 8640, 8640, 8640, 1080, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 1080, 8640, 4320, 8640, 1080, 8640, 4320, 1080, 4320, 1080, 1080, 4320, 4320, 4320, 8640, 4320, 1080, 1080]
Prompts retrieved: 898560 . Total input tokens: 200312975 . Total output tokens: 179687260
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.108399314922281,
    "estimated_duration": 3600.1067375508437,
    "input_throughput": 3522.375286191333,
    "output_throughput": 3113.0493668708123,
    "total_throughput": 6635.424653062146,
    "itl": 108.0098481109213,
    "ttft": 2181472.199784683,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246534,
    "arrivals": 299534,
    "finished_requests": 51245,
    "scheduler_time": 9.90017469679333
}
#Debug simulation 
Total elapsed time: 4.1085040889447555. Arrivals time: 0.17832413082942367 Scheduler time: 3.6749129098607227 Scheduler overhead time: 0.04684196144808084 Adapter cache time: 0.1386166582815349 Engine time: 0.04781980416737497 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.4937212149379775,
    "estimated_duration": 3600.1894177789654,
    "input_throughput": 4194.6823479406075,
    "output_throughput": 3710.965576984053,
    "total_throughput": 7905.64792492466,
    "itl": 230.9191767495184,
    "ttft": 2040501.1919054405,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 287913,
    "finished_requests": 61203,
    "scheduler_time": 38.2792026101484
}
#Debug simulation 
Total elapsed time: 4.49380547599867. Arrivals time: 0.19575935893226415 Scheduler time: 4.201089195208624 Scheduler overhead time: 0.02446553437039256 Adapter cache time: 0.03652971377596259 Engine time: 0.024722598493099213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.501311709056608,
    "estimated_duration": 3600.033748478844,
    "input_throughput": 4190.21599627336,
    "output_throughput": 3707.8082964194864,
    "total_throughput": 7898.024292692848,
    "itl": 228.27622177682917,
    "ttft": 2041216.2055303978,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815837,
    "arrivals": 287913,
    "finished_requests": 61129,
    "scheduler_time": 38.01058752971989
}
#Debug simulation 
Total elapsed time: 4.501395913073793. Arrivals time: 0.19810658355709165 Scheduler time: 4.2054216171381995 Scheduler overhead time: 0.024780991720035672 Adapter cache time: 0.036933555849827826 Engine time: 0.024797203834168613 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.217279953067191,
    "estimated_duration": 3600.0014264975907,
    "input_throughput": 3655.162440533218,
    "output_throughput": 3249.9706566457467,
    "total_throughput": 6905.133097178965,
    "itl": 104.12375105593577,
    "ttft": 2150260.1614713944,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 287913,
    "finished_requests": 53298,
    "scheduler_time": 10.657952861629354
}
#Debug simulation 
Total elapsed time: 4.217362339026295. Arrivals time: 0.18131903989706188 Scheduler time: 3.7946028602309525 Scheduler overhead time: 0.04893573047593236 Adapter cache time: 0.12007747346069664 Engine time: 0.049588839523494244 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.4657684720586985,
    "estimated_duration": 3600.2291909625187,
    "input_throughput": 4190.74458867057,
    "output_throughput": 3708.191143361847,
    "total_throughput": 7898.935732032417,
    "itl": 228.2822658532493,
    "ttft": 2041103.6574358076,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6002397570014009,
    "arrivals": 287913,
    "finished_requests": 61138,
    "scheduler_time": 38.01316632851732
}
#Debug simulation 
Total elapsed time: 4.465851890970953. Arrivals time: 0.19618739711586386 Scheduler time: 4.171726263244636 Scheduler overhead time: 0.024608578183688223 Adapter cache time: 0.03706646291539073 Engine time: 0.024820684571750462 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.245919119915925,
    "estimated_duration": 3600.0553186561447,
    "input_throughput": 3653.9463523883783,
    "output_throughput": 3248.42563929418,
    "total_throughput": 6902.3719916825585,
    "itl": 103.94240274469664,
    "ttft": 2150614.2872819914,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.635571540594103,
    "arrivals": 287913,
    "finished_requests": 53278,
    "scheduler_time": 10.569257149467543
}
#Debug simulation 
Total elapsed time: 4.246004001935944. Arrivals time: 0.19808269198983908 Scheduler time: 3.8077806556830183 Scheduler overhead time: 0.04901493713259697 Adapter cache time: 0.11878417455591261 Engine time: 0.0496628163382411 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.879069066024385,
    "estimated_duration": 3600.0049370498605,
    "input_throughput": 4190.249531258093,
    "output_throughput": 3707.837970616407,
    "total_throughput": 7898.0875018745,
    "itl": 228.271836747129,
    "ttft": 2041196.6367325007,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 287913,
    "finished_requests": 61129,
    "scheduler_time": 38.010540631068146
}
#Debug simulation 
Total elapsed time: 4.879157540970482. Arrivals time: 0.19819868670310825 Scheduler time: 4.582713304436766 Scheduler overhead time: 0.024827234912663698 Adapter cache time: 0.03676304570399225 Engine time: 0.025174180162139237 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.05_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.4  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 4320, 4320, 4320, 4320, 8640, 4320, 540, 8640, 540, 4320, 4320, 540, 8640, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 540, 540, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 540, 4320, 4320, 8640, 4320, 540, 540, 4320, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 8640, 8640, 540, 4320, 8640, 540, 4320, 540, 4320, 540, 540, 4320, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 4320, 540, 540, 540, 540, 540, 8640, 540, 4320, 540, 8640, 8640, 540, 8640, 540, 4320, 4320, 8640, 540, 540, 540, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 540, 8640, 4320, 4320, 4320, 8640, 4320, 540, 4320, 8640, 8640, 4320, 8640, 4320, 540, 8640, 4320, 540, 4320, 4320, 540, 4320, 8640, 540, 4320, 8640, 540, 4320, 8640, 8640, 4320, 540, 8640, 4320, 540, 540, 4320, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 4320, 540, 8640, 8640, 4320, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 540, 8640, 4320, 8640, 540, 8640, 4320, 540, 4320, 540, 540, 4320, 4320, 4320, 8640, 4320, 540, 540]
Prompts retrieved: 864000 . Total input tokens: 192620655 . Total output tokens: 172735942
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.42055744305253,
    "estimated_duration": 3600.02607255401,
    "input_throughput": 3649.153293681572,
    "output_throughput": 3243.2154003031415,
    "total_throughput": 6892.368693984713,
    "itl": 103.65550744542843,
    "ttft": 2151517.018104613,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246535,
    "arrivals": 287913,
    "finished_requests": 53195,
    "scheduler_time": 10.396816670463036
}
#Debug simulation 
Total elapsed time: 4.420622011995874. Arrivals time: 0.38311171252280474 Scheduler time: 3.796210385626182 Scheduler overhead time: 0.04883757664356381 Adapter cache time: 0.11991286487318575 Engine time: 0.04950405820272863 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.680687050102279,
    "estimated_duration": 3600.1076157485923,
    "input_throughput": 4403.896964258741,
    "output_throughput": 3886.442988202351,
    "total_throughput": 8290.339952461092,
    "itl": 220.10638554339732,
    "ttft": 2001749.679857409,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 282093,
    "finished_requests": 64248,
    "scheduler_time": 40.0435180971415
}
#Debug simulation 
Total elapsed time: 4.680790672078729. Arrivals time: 0.20406378176994622 Scheduler time: 4.3845551169943064 Scheduler overhead time: 0.025928348186425865 Adapter cache time: 0.028593987692147493 Engine time: 0.025899780564941466 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.709820756921545,
    "estimated_duration": 3600.10176972479,
    "input_throughput": 4397.749845057937,
    "output_throughput": 3882.3958026798987,
    "total_throughput": 8280.145647737836,
    "itl": 217.8504453558628,
    "ttft": 2002786.0561472431,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815839,
    "arrivals": 282093,
    "finished_requests": 64167,
    "scheduler_time": 39.77558498779306
}
#Debug simulation 
Total elapsed time: 4.709904458955862. Arrivals time: 0.20328803395386785 Scheduler time: 4.4145555689465255 Scheduler overhead time: 0.025935700978152454 Adapter cache time: 0.028200477943755686 Engine time: 0.026080662850290537 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.3247284550452605,
    "estimated_duration": 3600.08641225612,
    "input_throughput": 3752.1154920097533,
    "output_throughput": 3327.5359611417434,
    "total_throughput": 7079.651453151497,
    "itl": 101.6495530394861,
    "ttft": 2129325.5901755495,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 282093,
    "finished_requests": 54738,
    "scheduler_time": 10.926226749380554
}
#Debug simulation 
Total elapsed time: 4.324812510982156. Arrivals time: 0.20933117100503296 Scheduler time: 3.8856811565347016 Scheduler overhead time: 0.05018241156358272 Adapter cache time: 0.10576910839881748 Engine time: 0.05058430088683963 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.703742233919911,
    "estimated_duration": 3600.0372863327484,
    "input_throughput": 4397.494731541161,
    "output_throughput": 3882.4011776382854,
    "total_throughput": 8279.895909179446,
    "itl": 217.82419946368685,
    "ttft": 2002736.9135448092,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6002397570014012,
    "arrivals": 282093,
    "finished_requests": 64165,
    "scheduler_time": 39.772497437663176
}
#Debug simulation 
Total elapsed time: 4.7038233139319345. Arrivals time: 0.2246990631101653 Scheduler time: 4.386864701402374 Scheduler overhead time: 0.025954720680601895 Adapter cache time: 0.0281839813105762 Engine time: 0.026137578301131725 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.324763169046491,
    "estimated_duration": 3600.0602790706043,
    "input_throughput": 3750.3674809260624,
    "output_throughput": 3325.3973744824657,
    "total_throughput": 7075.764855408528,
    "itl": 101.50855483141363,
    "ttft": 2129921.842134484,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941029,
    "arrivals": 282093,
    "finished_requests": 54704,
    "scheduler_time": 10.842167923617327
}
#Debug simulation 
Total elapsed time: 4.3248459689784795. Arrivals time: 0.1886207478819415 Scheduler time: 3.9058612788794562 Scheduler overhead time: 0.05009278608486056 Adapter cache time: 0.10634971794206649 Engine time: 0.050515174865722656 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.685889261076227,
    "estimated_duration": 3600.070510067566,
    "input_throughput": 4397.7880310191085,
    "output_throughput": 3882.4295137868507,
    "total_throughput": 8280.21754480596,
    "itl": 217.84756424606414,
    "ttft": 2002764.810553278,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 282093,
    "finished_requests": 64167,
    "scheduler_time": 39.77558833915713
}
#Debug simulation 
Total elapsed time: 4.685974530992098. Arrivals time: 0.22867955511901528 Scheduler time: 4.365070050349459 Scheduler overhead time: 0.025782979326322675 Adapter cache time: 0.028328793472610414 Engine time: 0.02623695961665362 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.025_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.4   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 4320, 4320, 4320, 4320, 8640, 4320, 270, 8640, 270, 4320, 4320, 270, 8640, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 270, 270, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 270, 4320, 4320, 8640, 4320, 270, 270, 4320, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 8640, 8640, 270, 4320, 8640, 270, 4320, 270, 4320, 270, 270, 4320, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 4320, 270, 270, 270, 270, 270, 8640, 270, 4320, 270, 8640, 8640, 270, 8640, 270, 4320, 4320, 8640, 270, 270, 270, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 270, 8640, 4320, 4320, 4320, 8640, 4320, 270, 4320, 8640, 8640, 4320, 8640, 4320, 270, 8640, 4320, 270, 4320, 4320, 270, 4320, 8640, 270, 4320, 8640, 270, 4320, 8640, 8640, 4320, 270, 8640, 4320, 270, 270, 4320, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 4320, 270, 8640, 8640, 4320, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 270, 8640, 4320, 8640, 270, 8640, 4320, 270, 4320, 270, 270, 4320, 4320, 4320, 8640, 4320, 270, 270]
Prompts retrieved: 846720 . Total input tokens: 188747722 . Total output tokens: 169236971
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.307136255083606,
    "estimated_duration": 3600.0310484150023,
    "input_throughput": 3752.1390283417504,
    "output_throughput": 3327.2621371623177,
    "total_throughput": 7079.401165504069,
    "itl": 101.65277551678096,
    "ttft": 2129216.437652292,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246537,
    "arrivals": 282093,
    "finished_requests": 54735,
    "scheduler_time": 10.925375979072422
}
#Debug simulation 
Total elapsed time: 4.30721950402949. Arrivals time: 0.21212354593444616 Scheduler time: 3.865175573970191 Scheduler overhead time: 0.049687971943058074 Adapter cache time: 0.10659010999370366 Engine time: 0.050386616261675954 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.804961955989711,
    "estimated_duration": 3600.0589092542805,
    "input_throughput": 4563.92059523365,
    "output_throughput": 3992.0293979236394,
    "total_throughput": 8555.94999315729,
    "itl": 213.49459410572123,
    "ttft": 1979179.2374302908,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 279137,
    "finished_requests": 66223,
    "scheduler_time": 41.05964703133609
}
#Debug simulation 
Total elapsed time: 4.80505875707604. Arrivals time: 0.21244044229388237 Scheduler time: 4.506616745144129 Scheduler overhead time: 0.026306922547519207 Adapter cache time: 0.021109894034452736 Engine time: 0.026458152919076383 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.789568557986058,
    "estimated_duration": 3600.114073668004,
    "input_throughput": 4558.630272312156,
    "output_throughput": 3987.8103044031313,
    "total_throughput": 8546.440576715288,
    "itl": 211.5110491077124,
    "ttft": 1980162.3478160242,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815837,
    "arrivals": 279137,
    "finished_requests": 66150,
    "scheduler_time": 40.80621824898793
}
#Debug simulation 
Total elapsed time: 4.789689955068752. Arrivals time: 0.20903822500258684 Scheduler time: 4.49327276844997 Scheduler overhead time: 0.02657091012224555 Adapter cache time: 0.021709647495299578 Engine time: 0.026785029331222177 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.3035424769623205,
    "estimated_duration": 3600.0351544346577,
    "input_throughput": 3818.899374653186,
    "output_throughput": 3365.6699116047257,
    "total_throughput": 7184.569286257912,
    "itl": 100.33334563162744,
    "ttft": 2115610.3688378506,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 279137,
    "finished_requests": 55410,
    "scheduler_time": 10.991698646812885
}
#Debug simulation 
Total elapsed time: 4.3036241319496185. Arrivals time: 0.1875930381938815 Scheduler time: 3.8957340866327286 Scheduler overhead time: 0.05021332192700356 Adapter cache time: 0.09576570405624807 Engine time: 0.050802524434402585 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.795030768029392,
    "estimated_duration": 3600.1201747931964,
    "input_throughput": 4558.622546799494,
    "output_throughput": 3987.803546259311,
    "total_throughput": 8546.426093058804,
    "itl": 211.50219466093662,
    "ttft": 1980167.7348872686,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 279137,
    "finished_requests": 66150,
    "scheduler_time": 40.8054947432491
}
#Debug simulation 
Total elapsed time: 4.795114255044609. Arrivals time: 0.21008104248903692 Scheduler time: 4.497749635018408 Scheduler overhead time: 0.02660906338132918 Adapter cache time: 0.02198270650114864 Engine time: 0.026507488219067454 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.326325401081704,
    "estimated_duration": 3600.0389251294537,
    "input_throughput": 3816.0265168538426,
    "output_throughput": 3363.630297293123,
    "total_throughput": 7179.656814146966,
    "itl": 100.16867224330947,
    "ttft": 2115478.8560211533,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941029,
    "arrivals": 279137,
    "finished_requests": 55374,
    "scheduler_time": 10.901532025698188
}
#Debug simulation 
Total elapsed time: 4.326408217079006. Arrivals time: 0.1916039064526558 Scheduler time: 3.913396218442358 Scheduler overhead time: 0.050442795269191265 Adapter cache time: 0.09618201002012938 Engine time: 0.05109418020583689 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.773899512016214,
    "estimated_duration": 3600.0892969359743,
    "input_throughput": 4558.661645967464,
    "output_throughput": 3987.8377495299455,
    "total_throughput": 8546.49939549741,
    "itl": 211.50693343496454,
    "ttft": 1980146.2126312456,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 279137,
    "finished_requests": 66150,
    "scheduler_time": 40.80607793922011
}
#Debug simulation 
Total elapsed time: 4.773984141997062. Arrivals time: 0.20485792472027242 Scheduler time: 4.481934977113269 Scheduler overhead time: 0.026435410836711526 Adapter cache time: 0.021800921647809446 Engine time: 0.026769356918521225 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.0125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.0125_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.0125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.0125 0.4    0.8   ]. Counts: [64 64 64]
Adapter prompts. [8640, 135, 8640, 135, 8640, 135, 8640, 135, 135, 135, 4320, 4320, 4320, 4320, 8640, 4320, 135, 8640, 135, 4320, 4320, 135, 8640, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 135, 135, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 135, 4320, 4320, 8640, 4320, 135, 135, 4320, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 8640, 8640, 135, 4320, 8640, 135, 4320, 135, 4320, 135, 135, 4320, 8640, 8640, 135, 135, 135, 8640, 8640, 8640, 4320, 135, 135, 135, 135, 135, 8640, 135, 4320, 135, 8640, 8640, 135, 8640, 135, 4320, 4320, 8640, 135, 135, 135, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 135, 8640, 4320, 4320, 4320, 8640, 4320, 135, 4320, 8640, 8640, 4320, 8640, 4320, 135, 8640, 4320, 135, 4320, 4320, 135, 4320, 8640, 135, 4320, 8640, 135, 4320, 8640, 8640, 4320, 135, 8640, 4320, 135, 135, 4320, 8640, 8640, 8640, 135, 8640, 135, 8640, 135, 4320, 135, 8640, 8640, 4320, 8640, 135, 8640, 8640, 8640, 135, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 135, 8640, 4320, 8640, 135, 8640, 4320, 135, 4320, 135, 135, 4320, 4320, 4320, 8640, 4320, 135, 135]
Prompts retrieved: 838080 . Total input tokens: 186842283 . Total output tokens: 167496337
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.3464190339436755,
    "estimated_duration": 3600.09020505295,
    "input_throughput": 3823.8166867814716,
    "output_throughput": 3369.965003369009,
    "total_throughput": 7193.78169015048,
    "itl": 100.51871690866456,
    "ttft": 2114307.042297376,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246536,
    "arrivals": 279137,
    "finished_requests": 55485,
    "scheduler_time": 11.14223494318239
}
#Debug simulation 
Total elapsed time: 4.346504473942332. Arrivals time: 0.19511264166794717 Scheduler time: 3.931929739075713 Scheduler overhead time: 0.05049219273496419 Adapter cache time: 0.09446708706673235 Engine time: 0.05088921531569213 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.893108839052729,
    "estimated_duration": 3600.2006981218506,
    "input_throughput": 4605.334643885133,
    "output_throughput": 4078.745112087916,
    "total_throughput": 8684.079755973049,
    "itl": 210.26015530634,
    "ttft": 1962605.2607847075,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 277607,
    "finished_requests": 67254,
    "scheduler_time": 42.096836333269756
}
#Debug simulation 
Total elapsed time: 4.893192230956629. Arrivals time: 0.20992623327765614 Scheduler time: 4.6011220519430935 Scheduler overhead time: 0.026590716326609254 Adapter cache time: 0.016251710709184408 Engine time: 0.026928267441689968 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.844638834008947,
    "estimated_duration": 3600.2237180416364,
    "input_throughput": 4596.9971024474535,
    "output_throughput": 4070.0881799564145,
    "total_throughput": 8667.085282403868,
    "itl": 207.94249453370463,
    "ttft": 1964406.7806325902,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.626389857381584,
    "arrivals": 277607,
    "finished_requests": 67127,
    "scheduler_time": 41.77167393946583
}
#Debug simulation 
Total elapsed time: 4.84473033901304. Arrivals time: 0.2095351304160431 Scheduler time: 4.552277121227235 Scheduler overhead time: 0.02676425699610263 Adapter cache time: 0.016564352554269135 Engine time: 0.02718357159756124 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.363494356046431,
    "estimated_duration": 3600.09948600585,
    "input_throughput": 3813.674886866082,
    "output_throughput": 3395.749214020508,
    "total_throughput": 7209.42410088659,
    "itl": 99.42240236889765,
    "ttft": 2109192.3287509163,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.624142531920227,
    "arrivals": 277607,
    "finished_requests": 55670,
    "scheduler_time": 11.173782340711405
}
#Debug simulation 
Total elapsed time: 4.363594338065013. Arrivals time: 0.18829534156247973 Scheduler time: 3.961908483528532 Scheduler overhead time: 0.05091614346019924 Adapter cache time: 0.08743851655162871 Engine time: 0.0513356871670112 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.871986995916814,
    "estimated_duration": 3600.009039493784,
    "input_throughput": 4596.955401625062,
    "output_throughput": 4070.167279930048,
    "total_throughput": 8667.12268155511,
    "itl": 207.93829946512267,
    "ttft": 1964349.692202916,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.600239757001401,
    "arrivals": 277607,
    "finished_requests": 67123,
    "scheduler_time": 41.768536260501115
}
#Debug simulation 
Total elapsed time: 4.872067334945314. Arrivals time: 0.21133297530468553 Scheduler time: 4.577378729823977 Scheduler overhead time: 0.026873765978962183 Adapter cache time: 0.01673113857395947 Engine time: 0.02728699368890375 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.348597467993386,
    "estimated_duration": 3600.040104990716,
    "input_throughput": 3812.875856846989,
    "output_throughput": 3394.8546803846,
    "total_throughput": 7207.73053723159,
    "itl": 99.5651487684744,
    "ttft": 2109966.3570731897,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6320650204643627,
    "arrivals": 277607,
    "finished_requests": 55653,
    "scheduler_time": 11.211600413797147
}
#Debug simulation 
Total elapsed time: 4.348683740012348. Arrivals time: 0.1907390357227996 Scheduler time: 3.9437339091673493 Scheduler overhead time: 0.050605509080924094 Adapter cache time: 0.08867593284230679 Engine time: 0.0512363767484203 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.864631513948552,
    "estimated_duration": 3600.1434116438713,
    "input_throughput": 4596.870765335244,
    "output_throughput": 4070.1503591794963,
    "total_throughput": 8667.02112451474,
    "itl": 207.93728534167178,
    "ttft": 1964370.3961665896,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 277607,
    "finished_requests": 67125,
    "scheduler_time": 41.77033451960106
}
#Debug simulation 
Total elapsed time: 4.864718597033061. Arrivals time: 0.21167155692819506 Scheduler time: 4.5701046623289585 Scheduler overhead time: 0.026851195143535733 Adapter cache time: 0.016658115899190307 Engine time: 0.027091221185401082 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.00625
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.00625_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.00625_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.00625 0.4     0.8    ]. Counts: [64 64 64]
Adapter prompts. [8640, 66, 8640, 66, 8640, 66, 8640, 66, 66, 66, 4320, 4320, 4320, 4320, 8640, 4320, 66, 8640, 66, 4320, 4320, 66, 8640, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 66, 66, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 66, 4320, 4320, 8640, 4320, 66, 66, 4320, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 8640, 8640, 66, 4320, 8640, 66, 4320, 66, 4320, 66, 66, 4320, 8640, 8640, 66, 66, 66, 8640, 8640, 8640, 4320, 66, 66, 66, 66, 66, 8640, 66, 4320, 66, 8640, 8640, 66, 8640, 66, 4320, 4320, 8640, 66, 66, 66, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 66, 8640, 4320, 4320, 4320, 8640, 4320, 66, 4320, 8640, 8640, 4320, 8640, 4320, 66, 8640, 4320, 66, 4320, 4320, 66, 4320, 8640, 66, 4320, 8640, 66, 4320, 8640, 8640, 4320, 66, 8640, 4320, 66, 66, 4320, 8640, 8640, 8640, 66, 8640, 66, 8640, 66, 4320, 66, 8640, 8640, 4320, 8640, 66, 8640, 8640, 8640, 66, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 66, 8640, 4320, 8640, 66, 8640, 4320, 66, 4320, 66, 66, 4320, 4320, 4320, 8640, 4320, 66, 66]
Prompts retrieved: 833664 . Total input tokens: 185847865 . Total output tokens: 166623229
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.343754130997695,
    "estimated_duration": 3600.0807385630424,
    "input_throughput": 3812.730046015227,
    "output_throughput": 3394.797474702798,
    "total_throughput": 7207.527520718025,
    "itl": 99.50081517318574,
    "ttft": 2109760.7895061313,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 191,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6401132627949134,
    "arrivals": 277607,
    "finished_requests": 55653,
    "scheduler_time": 11.189152992748642
}
#Debug simulation 
Total elapsed time: 4.343846232979558. Arrivals time: 0.18832035665400326 Scheduler time: 3.943148761987686 Scheduler overhead time: 0.05074447055812925 Adapter cache time: 0.08663792663719505 Engine time: 0.05140087043400854 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-8/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.901315708993934,
    "estimated_duration": 3600.0125198728624,
    "input_throughput": 4683.736766725322,
    "output_throughput": 4113.940137220144,
    "total_throughput": 8797.676903945467,
    "itl": 207.74472508707032,
    "ttft": 1955942.999507688,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5723112081154242,
    "arrivals": 276925,
    "finished_requests": 67930,
    "scheduler_time": 42.35431745547612
}
#Debug simulation 
Total elapsed time: 4.901399172027595. Arrivals time: 0.21371843898668885 Scheduler time: 4.60887223307509 Scheduler overhead time: 0.02691253472585231 Adapter cache time: 0.01237823988776654 Engine time: 0.027108830749057233 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-16/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.939055641996674,
    "estimated_duration": 3600.1637763608505,
    "input_throughput": 4675.727840641648,
    "output_throughput": 4106.896774275533,
    "total_throughput": 8782.624614917182,
    "itl": 205.43217140857257,
    "ttft": 1957588.7698482077,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6102138199517516,
    "arrivals": 276925,
    "finished_requests": 67816,
    "scheduler_time": 42.02313615692412
}
#Debug simulation 
Total elapsed time: 4.939137883950025. Arrivals time: 0.21573874400928617 Scheduler time: 4.643228554050438 Scheduler overhead time: 0.02735633892007172 Adapter cache time: 0.012887477758340538 Engine time: 0.02741403190884739 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-8-32/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.374933279934339,
    "estimated_duration": 3600.0947713217524,
    "input_throughput": 3858.7948046988854,
    "output_throughput": 3401.97238627233,
    "total_throughput": 7260.767190971215,
    "itl": 98.8514591098802,
    "ttft": 2107512.458391905,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6018261574953818,
    "arrivals": 276925,
    "finished_requests": 55925,
    "scheduler_time": 11.078476813861103
}
#Debug simulation 
Total elapsed time: 4.3750174780143425. Arrivals time: 0.19195032748393714 Scheduler time: 3.9706378674600273 Scheduler overhead time: 0.05091628059744835 Adapter cache time: 0.08612894383259118 Engine time: 0.051500648725777864 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-16/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.89991639892105,
    "estimated_duration": 3600.148489672292,
    "input_throughput": 4675.747694376984,
    "output_throughput": 4106.914212681786,
    "total_throughput": 8782.66190705877,
    "itl": 205.43568164078357,
    "ttft": 1957578.4171422005,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5848809102084498,
    "arrivals": 276925,
    "finished_requests": 67816,
    "scheduler_time": 42.02350983641884
}
#Debug simulation 
Total elapsed time: 4.900024856906384. Arrivals time: 0.21284256887156516 Scheduler time: 4.606235512183048 Scheduler overhead time: 0.027190280379727483 Adapter cache time: 0.013029615627601743 Engine time: 0.028095167130231857 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_8-16-32/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.359547183965333,
    "estimated_duration": 3600.0912190195795,
    "input_throughput": 3857.4636460967054,
    "output_throughput": 3400.7582739345626,
    "total_throughput": 7258.2219200312675,
    "itl": 98.92098916299916,
    "ttft": 2107528.2746140193,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 184,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6094971384666877,
    "arrivals": 276925,
    "finished_requests": 55904,
    "scheduler_time": 11.084494087870121
}
#Debug simulation 
Total elapsed time: 4.359629194019362. Arrivals time: 0.18873063835781068 Scheduler time: 3.960225827759132 Scheduler overhead time: 0.051044155028648674 Adapter cache time: 0.08409966202452779 Engine time: 0.051659360877238214 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-16/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.917182545992546,
    "estimated_duration": 3600.133752747178,
    "input_throughput": 4675.766834261321,
    "output_throughput": 4106.931024081406,
    "total_throughput": 8782.697858342726,
    "itl": 205.428959226927,
    "ttft": 1957570.958313451,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 187,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5591394051467076,
    "arrivals": 276925,
    "finished_requests": 67816,
    "scheduler_time": 42.02300060306161
}
#Debug simulation 
Total elapsed time: 4.917269138037227. Arrivals time: 0.22115548374131322 Scheduler time: 4.615925736841746 Scheduler overhead time: 0.027192703797481954 Adapter cache time: 0.01301161793526262 Engine time: 0.02745708031579852 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.4,
        0.003125
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.4-0.003125_size_16-16-32/adapters_192_slots_192_rate_0.8-0.4-0.003125_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.003125 0.4      0.8     ]. Counts: [64 64 64]
Adapter prompts. [8640, 33, 8640, 33, 8640, 33, 8640, 33, 33, 33, 4320, 4320, 4320, 4320, 8640, 4320, 33, 8640, 33, 4320, 4320, 33, 8640, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 33, 33, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 33, 4320, 4320, 8640, 4320, 33, 33, 4320, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 8640, 8640, 33, 4320, 8640, 33, 4320, 33, 4320, 33, 33, 4320, 8640, 8640, 33, 33, 33, 8640, 8640, 8640, 4320, 33, 33, 33, 33, 33, 8640, 33, 4320, 33, 8640, 8640, 33, 8640, 33, 4320, 4320, 8640, 33, 33, 33, 8640, 8640, 4320, 8640, 4320, 8640, 8640, 4320, 4320, 33, 8640, 4320, 4320, 4320, 8640, 4320, 33, 4320, 8640, 8640, 4320, 8640, 4320, 33, 8640, 4320, 33, 4320, 4320, 33, 4320, 8640, 33, 4320, 8640, 33, 4320, 8640, 8640, 4320, 33, 8640, 4320, 33, 33, 4320, 8640, 8640, 8640, 33, 8640, 33, 8640, 33, 4320, 33, 8640, 8640, 4320, 8640, 33, 8640, 8640, 8640, 33, 8640, 8640, 8640, 4320, 4320, 4320, 4320, 4320, 4320, 33, 8640, 4320, 8640, 33, 8640, 4320, 33, 4320, 33, 33, 4320, 4320, 4320, 8640, 4320, 33, 33]
Prompts retrieved: 831552 . Total input tokens: 185391684 . Total output tokens: 166189692
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.38080418901518,
    "estimated_duration": 3600.0318496408277,
    "input_throughput": 3861.766945586062,
    "output_throughput": 3404.137105404407,
    "total_throughput": 7265.904050990469,
    "itl": 99.04316467296363,
    "ttft": 2106873.8706107656,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 185,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6209261471405638,
    "arrivals": 276925,
    "finished_requests": 55971,
    "scheduler_time": 11.19863143634218
}
#Debug simulation 
Total elapsed time: 4.380887555074878. Arrivals time: 0.1944871109444648 Scheduler time: 3.974394158925861 Scheduler overhead time: 0.051187055301852524 Adapter cache time: 0.08486974355764687 Engine time: 0.051971668377518654 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-8/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.65490733506158,
    "estimated_duration": 3600.1932106940017,
    "input_throughput": 4314.676488433688,
    "output_throughput": 3815.783819377805,
    "total_throughput": 8130.460307811493,
    "itl": 225.02506600908535,
    "ttft": 1933521.9506502934,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 218234,
    "finished_requests": 63013,
    "scheduler_time": 39.68996758429831
}
#Debug simulation 
Total elapsed time: 4.654995911056176. Arrivals time: 0.19424594810698181 Scheduler time: 4.33761603361927 Scheduler overhead time: 0.025307785137556493 Adapter cache time: 0.06056613696273416 Engine time: 0.02560398750938475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-16/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.661599796963856,
    "estimated_duration": 3600.2075284918324,
    "input_throughput": 4314.151025206723,
    "output_throughput": 3815.4094982836386,
    "total_throughput": 8129.560523490361,
    "itl": 221.72433968420552,
    "ttft": 1934112.8858134977,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.626389857381584,
    "arrivals": 218234,
    "finished_requests": 63004,
    "scheduler_time": 39.43069062868917
}
#Debug simulation 
Total elapsed time: 4.6616825069068. Arrivals time: 0.1981697225710377 Scheduler time: 4.339433161425404 Scheduler overhead time: 0.02565986872650683 Adapter cache time: 0.06060219102073461 Engine time: 0.025933768250979483 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-8-32/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.578672806033865,
    "estimated_duration": 3600.0891342406026,
    "input_throughput": 3978.6148247747064,
    "output_throughput": 3544.4055755835093,
    "total_throughput": 7523.020400358216,
    "itl": 95.36087337645843,
    "ttft": 2011318.6737430315,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 218234,
    "finished_requests": 58160,
    "scheduler_time": 12.371175239081783
}
#Debug simulation 
Total elapsed time: 4.578761520097032. Arrivals time: 0.19303877069614828 Scheduler time: 4.128040414303541 Scheduler overhead time: 0.05359582824166864 Adapter cache time: 0.1252124522579834 Engine time: 0.053950568311847746 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-16/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.676866178982891,
    "estimated_duration": 3600.182457802554,
    "input_throughput": 4314.181067778487,
    "output_throughput": 3815.436067755359,
    "total_throughput": 8129.617135533846,
    "itl": 221.7278275157469,
    "ttft": 1934094.8559607982,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6002397570014011,
    "arrivals": 218234,
    "finished_requests": 63004,
    "scheduler_time": 39.43145632968742
}
#Debug simulation 
Total elapsed time: 4.676952890935354. Arrivals time: 0.19724229851271957 Scheduler time: 4.354193661594763 Scheduler overhead time: 0.026004150393418968 Adapter cache time: 0.061378789599984884 Engine time: 0.026030881330370903 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_8-16-32/adapters_192_slots_192_rate_0.8-0.1-0.05_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.575812571099959,
    "estimated_duration": 3600.003167516463,
    "input_throughput": 3978.645665992876,
    "output_throughput": 3544.387159193145,
    "total_throughput": 7523.032825186021,
    "itl": 95.36209925684945,
    "ttft": 2011319.8946135999,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.635571540594103,
    "arrivals": 218234,
    "finished_requests": 58158,
    "scheduler_time": 12.37039121395338
}
#Debug simulation 
Total elapsed time: 4.575918066082522. Arrivals time: 0.19263517681974918 Scheduler time: 4.126938165863976 Scheduler overhead time: 0.05354483099654317 Adapter cache time: 0.12386687914840877 Engine time: 0.053983667748980224 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_192_slots_192_rate_0.8-0.1-0.05_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-16/adapters_192_slots_192_rate_0.8-0.1-0.05_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.667104117106646,
    "estimated_duration": 3600.1600944882152,
    "input_throughput": 4314.207866416547,
    "output_throughput": 3815.459768311413,
    "total_throughput": 8129.667634727961,
    "itl": 221.7348732803771,
    "ttft": 1934078.2822349807,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 218234,
    "finished_requests": 63004,
    "scheduler_time": 39.43163674813552
}
#Debug simulation 
Total elapsed time: 4.6671861780341715. Arrivals time: 0.19795907498337328 Scheduler time: 4.343245377065614 Scheduler overhead time: 0.025783231714740396 Adapter cache time: 0.06237565935589373 Engine time: 0.025981660932302475 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_192_slots_192_rate_0.8-0.1-0.05_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.05
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.05_size_16-16-32/adapters_192_slots_192_rate_0.8-0.1-0.05_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.05 0.1  0.8 ]. Counts: [64 64 64]
Adapter prompts. [8640, 540, 8640, 540, 8640, 540, 8640, 540, 540, 540, 1080, 1080, 1080, 1080, 8640, 1080, 540, 8640, 540, 1080, 1080, 540, 8640, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 540, 540, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 540, 1080, 1080, 8640, 1080, 540, 540, 1080, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 8640, 8640, 540, 1080, 8640, 540, 1080, 540, 1080, 540, 540, 1080, 8640, 8640, 540, 540, 540, 8640, 8640, 8640, 1080, 540, 540, 540, 540, 540, 8640, 540, 1080, 540, 8640, 8640, 540, 8640, 540, 1080, 1080, 8640, 540, 540, 540, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 540, 8640, 1080, 1080, 1080, 8640, 1080, 540, 1080, 8640, 8640, 1080, 8640, 1080, 540, 8640, 1080, 540, 1080, 1080, 540, 1080, 8640, 540, 1080, 8640, 540, 1080, 8640, 8640, 1080, 540, 8640, 1080, 540, 540, 1080, 8640, 8640, 8640, 540, 8640, 540, 8640, 540, 1080, 540, 8640, 8640, 1080, 8640, 540, 8640, 8640, 8640, 540, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 540, 8640, 1080, 8640, 540, 8640, 1080, 540, 1080, 540, 540, 1080, 1080, 1080, 8640, 1080, 540, 540]
Prompts retrieved: 656640 . Total input tokens: 146416665 . Total output tokens: 131238857
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.616807706071995,
    "estimated_duration": 3600.085155323168,
    "input_throughput": 3978.6192220539956,
    "output_throughput": 3544.4094929622743,
    "total_throughput": 7523.02871501627,
    "itl": 95.35718910997868,
    "ttft": 2011278.7631330483,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246537,
    "arrivals": 218234,
    "finished_requests": 58160,
    "scheduler_time": 12.368657248518115
}
#Debug simulation 
Total elapsed time: 4.616892945021391. Arrivals time: 0.19399746775161475 Scheduler time: 4.165262209484354 Scheduler overhead time: 0.05360071384347975 Adapter cache time: 0.1250902039464563 Engine time: 0.05404992960393429 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-8-8
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        8
    ],
    "available_gpu_memory": 565744,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-8/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-8-8",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [8]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.841145336977206,
    "estimated_duration": 3600.179134705407,
    "input_throughput": 4544.796352567847,
    "output_throughput": 4000.9423034386455,
    "total_throughput": 8545.738656006493,
    "itl": 214.00388306309952,
    "ttft": 1885991.6656500127,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.587613646835088,
    "arrivals": 212464,
    "finished_requests": 65908,
    "scheduler_time": 41.6148701615404
}
#Debug simulation 
Total elapsed time: 4.8412286340026185. Arrivals time: 0.20695969415828586 Scheduler time: 4.515198228764348 Scheduler overhead time: 0.026507337228395045 Adapter cache time: 0.05342902755364776 Engine time: 0.026930153253488243 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-8-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-16/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-8-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.834831856074743,
    "estimated_duration": 3600.006090226341,
    "input_throughput": 4542.439537643077,
    "output_throughput": 3999.2751787524894,
    "total_throughput": 8541.714716395567,
    "itl": 210.95915906020767,
    "ttft": 1886664.3012801306,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6263898573815836,
    "arrivals": 212464,
    "finished_requests": 65876,
    "scheduler_time": 41.33805081051653
}
#Debug simulation 
Total elapsed time: 4.834916559047997. Arrivals time: 0.20637674839235842 Scheduler time: 4.507577271317132 Scheduler overhead time: 0.026846394408494234 Adapter cache time: 0.05439931631553918 Engine time: 0.02734451089054346 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-8-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        8,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-8-32/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-8-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.67700772290118,
    "estimated_duration": 3600.0827984627967,
    "input_throughput": 4126.054269180305,
    "output_throughput": 3647.502220117535,
    "total_throughput": 7773.55648929784,
    "itl": 92.04617796948615,
    "ttft": 1979826.100939769,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6275232982635524,
    "arrivals": 212464,
    "finished_requests": 59814,
    "scheduler_time": 12.587883830001502
}
#Debug simulation 
Total elapsed time: 4.677090136916377. Arrivals time: 0.19370409729890525 Scheduler time: 4.234307019971311 Scheduler overhead time: 0.05521072051487863 Adapter cache time: 0.11279592174105346 Engine time: 0.055575194652192295 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-16/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16]. Counts: [ 64 128]
---Simulation End---
#Simulation results
{
    "duration": 4.850612306036055,
    "estimated_duration": 3600.2204149285803,
    "input_throughput": 4542.319668037439,
    "output_throughput": 3999.325413604053,
    "total_throughput": 8541.645081641493,
    "itl": 210.95041481377424,
    "ttft": 1886717.2679927652,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6002397570014011,
    "arrivals": 212464,
    "finished_requests": 65878,
    "scheduler_time": 41.34067871748167
}
#Debug simulation 
Total elapsed time: 4.850695727043785. Arrivals time: 0.20596349646802992 Scheduler time: 4.5243356300052255 Scheduler overhead time: 0.026883115991950035 Adapter cache time: 0.0537717523984611 Engine time: 0.02729995409026742 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        8,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_8-16-32/adapters_192_slots_192_rate_0.8-0.1-0.025_size_8-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [ 8 16 32]. Counts: [64 64 64]
---Simulation End---
#Simulation results
{
    "duration": 4.684263447066769,
    "estimated_duration": 3600.0708875994383,
    "input_throughput": 4126.773739698936,
    "output_throughput": 3648.4781578171637,
    "total_throughput": 7775.251897516099,
    "itl": 92.0901047212221,
    "ttft": 1979522.4625951687,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6355715405941027,
    "arrivals": 212464,
    "finished_requests": 59828,
    "scheduler_time": 12.616540231044208
}
#Debug simulation 
Total elapsed time: 4.684346706024371. Arrivals time: 0.19603549933526665 Scheduler time: 4.241420833393931 Scheduler overhead time: 0.05504136928357184 Adapter cache time: 0.11069776047952473 Engine time: 0.05557730479631573 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_192_slots_192_rate_0.8-0.1-0.025_size_16-16-16
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        16
    ],
    "available_gpu_memory": 430432,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-16/adapters_192_slots_192_rate_0.8-0.1-0.025_size_16-16-16",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [16]. Counts: [192]
---Simulation End---
#Simulation results
{
    "duration": 4.884372400003485,
    "estimated_duration": 3600.161276381737,
    "input_throughput": 4542.394283079334,
    "output_throughput": 3999.3911090757715,
    "total_throughput": 8541.785392155105,
    "itl": 210.95123966029186,
    "ttft": 1886680.4052047853,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.5740896566212185,
    "arrivals": 212464,
    "finished_requests": 65878,
    "scheduler_time": 41.34101063780673
}
#Debug simulation 
Total elapsed time: 4.884477659012191. Arrivals time: 0.20322807622142136 Scheduler time: 4.56068166566547 Scheduler overhead time: 0.02673888357821852 Adapter cache time: 0.054260131320916116 Engine time: 0.027142552542500198 
------------>Running benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_192_slots_192_rate_0.8-0.1-0.025_size_16-16-32
With arguments {
    "total_time": 3600.0,
    "model": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/models/qwen/qwen-2.5-7b-instruct",
    "adapter_slots": 192,
    "served_adapters": 192,
    "served_adapters_rates": [
        0.8,
        0.1,
        0.025
    ],
    "served_adapters_sizes": [
        16,
        16,
        32
    ],
    "available_gpu_memory": 165664,
    "max_num_batched_tokens": 2048,
    "dataset_path": "/gpfs/scratch/bsc98/bsc098069/llm_benchmarking/data/ShareGPT_V3_unfiltered_cleaned_split.json",
    "output_path": "benchmarks/lora/definitive_results/finding_maximum/simulation_dataset/with_offloading/qwen-2.5-7b-instruct/results/rate_0.8-0.1-0.025_size_16-16-32/adapters_192_slots_192_rate_0.8-0.1-0.025_size_16-16-32",
    "print_outcome": true,
    "mean_version": true,
    "include_computation_overhead": true,
    "include_preemption": true,
    "without_offloading": false,
    "include_network_collapse": 25500
}
Adapter rates. Values: [0.025 0.1   0.8  ]. Counts: [64 64 64]
Adapter prompts. [8640, 270, 8640, 270, 8640, 270, 8640, 270, 270, 270, 1080, 1080, 1080, 1080, 8640, 1080, 270, 8640, 270, 1080, 1080, 270, 8640, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 270, 270, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 270, 1080, 1080, 8640, 1080, 270, 270, 1080, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 8640, 8640, 270, 1080, 8640, 270, 1080, 270, 1080, 270, 270, 1080, 8640, 8640, 270, 270, 270, 8640, 8640, 8640, 1080, 270, 270, 270, 270, 270, 8640, 270, 1080, 270, 8640, 8640, 270, 8640, 270, 1080, 1080, 8640, 270, 270, 270, 8640, 8640, 1080, 8640, 1080, 8640, 8640, 1080, 1080, 270, 8640, 1080, 1080, 1080, 8640, 1080, 270, 1080, 8640, 8640, 1080, 8640, 1080, 270, 8640, 1080, 270, 1080, 1080, 270, 1080, 8640, 270, 1080, 8640, 270, 1080, 8640, 8640, 1080, 270, 8640, 1080, 270, 270, 1080, 8640, 8640, 8640, 270, 8640, 270, 8640, 270, 1080, 270, 8640, 8640, 1080, 8640, 270, 8640, 8640, 8640, 270, 8640, 8640, 8640, 1080, 1080, 1080, 1080, 1080, 1080, 270, 8640, 1080, 8640, 270, 8640, 1080, 270, 1080, 270, 270, 1080, 1080, 1080, 8640, 1080, 270, 270]
Prompts retrieved: 639360 . Total input tokens: 142573519 . Total output tokens: 127794992
Prompts distributed
Adapter sizes. Values: [16 32]. Counts: [128  64]
---Simulation End---
#Simulation results
{
    "duration": 4.674922637990676,
    "estimated_duration": 3600.0410029136947,
    "input_throughput": 4125.52912257929,
    "output_throughput": 3646.793186348255,
    "total_throughput": 7772.322308927544,
    "itl": 91.9741597755292,
    "ttft": 1979633.290611741,
    "total_loads_from_disk": 0,
    "total_loads_from_memory": 192,
    "loading_time_from_disk": 0,
    "loading_time_from_memory": 0.6436197829246534,
    "arrivals": 212464,
    "finished_requests": 59802,
    "scheduler_time": 12.541956077732454
}
#Debug simulation 
Total elapsed time: 4.675006747012958. Arrivals time: 0.1946768406778574 Scheduler time: 4.229745827266015 Scheduler overhead time: 0.05504083458799869 Adapter cache time: 0.11382364307064563 Engine time: 0.05599691905081272 
